{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "AREAS = [\n",
    "'Semantic segmentation',\n",
    "'Image classification',\n",
    "'Object detection',\n",
    "'Object Recognition',\n",
    "'Domain adaptation',\n",
    "'Image generation',\n",
    "'Image Captioning',\n",
    "'Image augmentation',\n",
    "'Pose estimation',\n",
    "'Autonomous vehicles',\n",
    "'Denoising',\n",
    "'Super-Resolution',\n",
    "'Object Tracking',\n",
    "'Action Recognition',\n",
    "'Face Recognition',\n",
    "'Depth Estimation',\n",
    "'Optical Character Recognition',\n",
    "'3D Reconstruction',\n",
    "'Image Retrieval',\n",
    "'Optical Flow Estimation',\n",
    "'Style Transfer',\n",
    "'Image Compression']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Scholar Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from serpapi import GoogleSearch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [12:59<00:00, 35.44s/it]\n"
     ]
    }
   ],
   "source": [
    "gs_raw = {}\n",
    "total_paper = 400\n",
    "for area in tqdm(AREAS):\n",
    "    area_raw = []\n",
    "    for ii in range(total_paper//20):\n",
    "        params = {\n",
    "        'api_key': '9cdc23221a6d7cce098cebc84adb8ab98cb53e8870970758225b506e8cb33757',\n",
    "        'engine': 'google_scholar',\n",
    "        'q': f'{area}',\n",
    "        'hl': 'en',\n",
    "        'as_ylo': '2000',\n",
    "        'num': '20',\n",
    "        'start': f'{20*ii}'\n",
    "        }\n",
    "        search = GoogleSearch(params)\n",
    "        area_raw += search.get_dict().get('organic_results')\n",
    "    gs_raw[area] = area_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = []\n",
    "keys_to_extract = ['title', 'link', 'snippet']\n",
    "for area in AREAS:\n",
    "    for rank, paper in enumerate(gs_raw[area]):\n",
    "        paper_result = {'area': area, 'rank': rank + 1}\n",
    "        for key in keys_to_extract:\n",
    "            paper_result[key] = paper.get(key)\n",
    "        if 'resources' in paper:\n",
    "            for resource in paper['resources']:\n",
    "                if resource.get('file_format') == 'PDF' and resource['link']:\n",
    "                    paper_result['pdf_link'] = resource['link']\n",
    "                    break\n",
    "        if 'pdf_link' not in paper_result:\n",
    "            paper_result['pdf_link'] = None\n",
    "        \n",
    "        gs_results.append(paper_result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.DataFrame(gs_results)\n",
    "gs_df.to_csv('./data/gs_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:46<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "ss_results = []\n",
    "for area in tqdm(AREAS):\n",
    "    raw = requests.get('https://api.semanticscholar.org/graph/v1/paper/search?query={}&limit=100&fields=url,title,abstract,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy'.format('+'.join(area.lower().split(' ')))).json().get('data')\n",
    "    raw += requests.get('https://api.semanticscholar.org/graph/v1/paper/search?query={}&limit=100&offset=100&fields=url,title,abstract,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy'.format('+'.join(area.lower().split(' ')))).json().get('data')\n",
    "    for rank, paper in enumerate(raw):\n",
    "        paper['area'] = area\n",
    "        paper['rank'] = rank + 1\n",
    "        ss_results.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df = pd.DataFrame(ss_results)\n",
    "ss_df.dropna()\n",
    "ss_df = ss_df[['area','rank','title','referenceCount', 'url', 'citationCount', 'influentialCitationCount', 'isOpenAccess', 'fieldsOfStudy', 'abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df.to_csv('./data/ss_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paperwithcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [08:40<00:00, 23.67s/it]\n"
     ]
    }
   ],
   "source": [
    "total_paper = 200\n",
    "pc_results = []\n",
    "for area in tqdm(AREAS):\n",
    "    area_formated = '-'.join(area.lower().split(' '))\n",
    "    for ii in range(total_paper//10):\n",
    "        html_raw = requests.get(f'https://paperswithcode.com/task/{area_formated}?page={ii+1}')\n",
    "        soup = BeautifulSoup(html_raw.text, 'html.parser')\n",
    "        paper_cards = soup.find_all('div', class_='paper-card infinite-item')\n",
    "        for jj, paper_card in enumerate(paper_cards):\n",
    "            rank = ii * 10 + jj + 1\n",
    "            result = {'area': area, 'rank': rank}\n",
    "            result['title'] = paper_card.find('h1').text\n",
    "            pc_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = pd.DataFrame(pc_results)\n",
    "pc_df.to_csv('./data/pc_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = pc_df['area'].unique().tolist()\n",
    "for area in AREAS:\n",
    "    if area not in areas:\n",
    "        print(area)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7ad60629c7992fdadfcbec1ee9d18d5ed6e222b8ad46df459875a57fae42b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
