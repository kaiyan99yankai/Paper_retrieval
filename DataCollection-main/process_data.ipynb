{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df = pd.read_csv('./data/gs_results.csv')\n",
    "gs_df['title'] = gs_df['title'].str.casefold()\n",
    "pc_df = pd.read_csv('./data/pc_results.csv')\n",
    "pc_df['title'] = pc_df['title'].str.casefold()\n",
    "ss_df = pd.read_csv('./data/ss_results.csv')\n",
    "ss_df['title'] = ss_df['title'].str.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIER1 = 40\n",
    "TIER2 = 100\n",
    "TIER3 = 200\n",
    "gs_rate_df = gs_df[['area','rank','title']][gs_df['rank']<=200].copy()\n",
    "pc_rate_df = pc_df.copy()\n",
    "ss_rate_df = ss_df[['area','rank','title']].copy()\n",
    "gs_rate_df['rate'] = gs_rate_df['rank'].apply(lambda x: 5 if x<=TIER1 else 4 if x<=TIER2 else 3 if x<=TIER3 else 2)\n",
    "pc_rate_df['rate'] = pc_rate_df['rank'].apply(lambda x: 5 if x<=TIER1 else 4 if x<=TIER2 else 3)\n",
    "ss_rate_df['rate'] = ss_rate_df['rank'].apply(lambda x: 5 if x<=TIER1 else 4 if x<=TIER2 else 3)\n",
    "\n",
    "rate_df = gs_rate_df.merge(pc_rate_df, on=['area', 'title'], how='outer', suffixes=('_gs', '_pc'))\n",
    "rate_df = rate_df.merge(ss_rate_df, on=['area', 'title'], how='outer')\n",
    "rate_df.rename(columns={'rank': 'rank_ss', 'rate': 'rate_ss'}, inplace=True)\n",
    "rate_df['rate_ss'].fillna(1, inplace=True)\n",
    "rate_df['rank_ss'].fillna(201, inplace=True)\n",
    "rate_df['rate_pc'].fillna(1, inplace=True)\n",
    "rate_df['rank_pc'].fillna(201, inplace=True)\n",
    "rate_df['rate_gs'].fillna(1, inplace=True)\n",
    "rate_df['rank_gs'].fillna(401, inplace=True)\n",
    "\n",
    "rate_df['agg_rate'] = (rate_df['rate_gs']*0.3 + rate_df['rate_pc']*0.4 + rate_df['rate_ss']*0.3)\n",
    "rate_df['agg_rank'] = (rate_df['rank_gs']*0.3 + rate_df['rank_pc']*0.4 + rate_df['rank_ss']*0.3)\n",
    "rate_df = rate_df.sort_values(by=['area', 'agg_rate', 'agg_rank'], ascending=[True, False, True]).reset_index(drop=True)\n",
    "rate_df['rank'] = rate_df.groupby('area').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the total dataframe that contains download information and the pdf link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill info from semantic scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = rate_df.merge(gs_df[['area', 'title', 'pdf_link']], on=['area', 'title'], how='left')\n",
    "total_df = total_df.merge(ss_df[['area', 'title', 'abstract', 'citationCount', 'referenceCount', 'influentialCitationCount', 'fieldsOfStudy']], on=['area', 'title'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['pdf_link'].fillna('', inplace=True)\n",
    "total_df['abstract'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv('./data/total_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = []\n",
    "lack_info_df = total_df[total_df['abstract'] == ''].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lack_info_df.drop(drop_idx, inplace=True)\n",
    "drop_idx = []\n",
    "for idx, paper in tqdm(lack_info_df.iterrows(), total=lack_info_df.shape[0]):\n",
    "    if paper['abstract'] == '':\n",
    "        title_formatted = paper['title'].replace(' ', '+')\n",
    "        data = requests.get(f'https://api.semanticscholar.org/graph/v1/paper/search?query={title_formatted}&limit=1&fields=abstract,referenceCount,citationCount,influentialCitationCount').json()\n",
    "        if data.get('message') is not None:\n",
    "            lack_info_df = total_df[total_df['abstract'] == '']\n",
    "            print(data.get('message'))\n",
    "            time.sleep(120)\n",
    "        else:\n",
    "            data = data.get('data')\n",
    "            if data:\n",
    "                data = data[0]\n",
    "                paper['abstract'] = data.get('abstract')\n",
    "                paper['citationCount'] = data.get('citationCount')\n",
    "                paper['referenceCount'] = data.get('referenceCount')\n",
    "                paper['influentialCitationCount'] = data.get('influentialCitationCount')\n",
    "            else:\n",
    "                drop_idx.append(idx)\n",
    "    total_df.loc[idx] = paper\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('./data/total_results_with_info.csv')\n",
    "total_df['pdf_link'].fillna('', inplace=True)\n",
    "total_df['abstract'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill info from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lack_link_df = total_df[total_df['pdf_link'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lack_link_df = lack_link_df.drop(drop_idxs)\n",
    "drop_idxs = []\n",
    "for idx, paper in tqdm(lack_link_df.iterrows(), total = lack_link_df.shape[0]):\n",
    "    if paper['pdf_link'] == '':\n",
    "        title_formatted = paper['title'].replace(' ', '+')\n",
    "        response = requests.get(f'http://export.arxiv.org/api/query?search_query=ti:{title_formatted}&start=0&max_results=1')\n",
    "        data = xmltodict.parse(response.text)['feed']\n",
    "        if 'entry' in data:\n",
    "            paper['pdf_link'] = data['entry']['link'][1]['@href']\n",
    "            if paper['abstract'] == '':\n",
    "                paper['abstract'] = data['entry']['summary'].replace('\\n',' ')\n",
    "        else:\n",
    "            drop_idxs.append(idx)\n",
    "    total_df.loc[idx] = paper\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(total_df['pdf_link'] == '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lack_abstract_df = total_df[total_df['abstract'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lack_abstract_df = lack_abstract_df.drop(drop_idxs)\n",
    "drop_idxs = []\n",
    "for idx, paper in tqdm(lack_abstract_df.iterrows(), total = lack_abstract_df.shape[0]):\n",
    "    if paper['abstract'] == '':\n",
    "        title_formatted = paper['title'].replace(' ', '+')\n",
    "        response = requests.get(f'http://export.arxiv.org/api/query?search_query=ti:{title_formatted}&start=0&max_results=1')\n",
    "        data = xmltodict.parse(response.text)['feed']\n",
    "        if 'entry' in data:\n",
    "            paper['abstract'] = data['entry']['summary'].replace('\\n',' ')\n",
    "        else:\n",
    "            drop_idxs.append(idx)\n",
    "    total_df.loc[idx] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(total_df['abstract'] == '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4987/3439163017.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtotal_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pdf_link'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'total_df' is not defined"
     ]
    }
   ],
   "source": [
    "total_df = total_df[total_df['pdf_link']!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.sort_values(by=['area', 'agg_rate', 'agg_rank'], ascending=[True, False, True]).reset_index(drop=True)\n",
    "total_df['rank'] = total_df.groupby('area').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv('./data/total_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate aria2 download file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AREAS = [\n",
    "'Semantic segmentation',\n",
    "'Image classification',\n",
    "'Object detection',\n",
    "'Object Recognition',\n",
    "'Domain adaptation',\n",
    "'Image generation',\n",
    "'Image Captioning',\n",
    "'Image augmentation',\n",
    "'Pose estimation',\n",
    "'Autonomous vehicles',\n",
    "'Denoising',\n",
    "'Super-Resolution',\n",
    "'Object Tracking',\n",
    "'Action Recognition',\n",
    "'Face Recognition',\n",
    "'Depth Estimation',\n",
    "'Optical Character Recognition',\n",
    "'3D Reconstruction',\n",
    "'Image Retrieval',\n",
    "'Optical Flow Estimation',\n",
    "'Style Transfer',\n",
    "'Image Compression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_csv('./data/total_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for area in AREAS:\n",
    "    area_formatted = area.lower().replace(' ', '_')\n",
    "    if not os.path.exists(f'./data/pdf/{area_formatted}'):\n",
    "        os.mkdir(f'./data/pdf/{area_formatted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11359/11359 [00:00<00:00, 30166.65it/s]\n"
     ]
    }
   ],
   "source": [
    "file_names = []\n",
    "with open('data_download_aria2.txt', 'w') as f:\n",
    "    for idx, paper in tqdm(total_df.iterrows(), total = total_df.shape[0]):\n",
    "        area = paper['area']\n",
    "        area_formatted = area.lower().replace(' ', '_')\n",
    "        title = paper['title']\n",
    "        pdf_link = paper['pdf_link']\n",
    "        letters = [word[0] for word in title.split(' ')]\n",
    "        file_name = ''.join(letters)\n",
    "        file_names.append(file_name)\n",
    "        f.write(pdf_link + '\\n')\n",
    "        f.write(f'\\tout=./data/pdf/{area_formatted}/{file_name}.pdf\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['file_name'] = file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv('./data/total_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11359/11359 [00:27<00:00, 414.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('data_download_aria2.txt', 'w') as f:\n",
    "    for idx, paper in tqdm(total_df.iterrows(), total = total_df.shape[0]):\n",
    "        area = paper['area']\n",
    "        area_formatted = area.lower().replace(' ', '_')\n",
    "        title = paper['title']\n",
    "        pdf_link = paper['pdf_link']\n",
    "        letters = [word[0] for word in title.split(' ')]\n",
    "        file_name = ''.join(letters)\n",
    "        file_path = f'./data/pdf/{area_formatted}/{file_name}.pdf'\n",
    "        if os.path.exists(file_path):\n",
    "            bValid = True\n",
    "            try:\n",
    "                reader = PdfFileReader(file_path)\n",
    "                if reader.getNumPages() < 1:  # 进一步通过页数判断。\n",
    "                    bValid = False\n",
    "            except:\n",
    "                bValid = False\n",
    "            if bValid:\n",
    "                continue\n",
    "            else:\n",
    "                os.remove(file_path)\n",
    "        if pdf_link.find('arxiv') != -1:\n",
    "            continue\n",
    "        f.write(pdf_link + '\\n')\n",
    "        f.write(f'\\tout=./data/pdf/{area_formatted}/{file_name}.pdf\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a7ad60629c7992fdadfcbec1ee9d18d5ed6e222b8ad46df459875a57fae42b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
