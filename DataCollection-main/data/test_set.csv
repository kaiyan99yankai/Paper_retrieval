Unnamed: 0,area,rank_gs,title,rate_gs,rank_pc,rate_pc,rank_ss,rate_ss,agg_rate,agg_rank,rank,pdf_link,abstract,file_name,citationCount,referenceCount,influentialCitationCount
0,3D Reconstruction,9.0,occupancy networks: learning 3d reconstruction in function space,5.0,18.0,5.0,1.0,5.0,5.0,10.2,1,http://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf,"With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.",3ocnele3dreinfusp,623.0,86.0,136.0
1,3D Reconstruction,68.0,weakly supervised 3d reconstruction with adversarial constraint,4.0,4.0,5.0,29.0,5.0,4.7,30.7,2,https://arxiv.org/pdf/1705.10904,"Supervised 3D reconstruction has witnessed a significant progress through the use of deep neural networks. However, this increase in performance requires large scale annotations of 2D/3D data. In this paper, we explore inexpensive 2D supervision as an alternative for expensive 3D CAD annotation. Specifically, we use foreground masks as weak supervision through a raytrace pooling layer that enables perspective projection and backpropagation. Additionally, since the 3D reconstruction from masks is an ill posed problem, we propose to constrain the 3D reconstruction to the manifold of unlabeled realistic 3D shapes that match mask observations. We demonstrate that learning a log-barrier solution to this constrained optimization problem resembles the GAN objective, enabling the use of existing tools for training GANs. We evaluate and analyze the manifold constrained reconstruction on various datasets for single and multi-view reconstruction of both synthetic and real images.",3wesu3drewiadco,85.0,60.0,3.0
2,3D Reconstruction,95.0,infinitam v3: a framework for large-scale 3d reconstruction with loop closure,4.0,17.0,5.0,32.0,5.0,4.7,44.9,3,https://arxiv.org/pdf/1708.00783,"Volumetric models have become a popular representation for 3D scenes in recent years. One breakthrough leading to their popularity was KinectFusion, which focuses on 3D reconstruction using RGB-D sensors. However, monocular SLAM has since also been tackled with very similar approaches. Representing the reconstruction volumetrically as a TSDF leads to most of the simplicity and efficiency that can be achieved with GPU implementations of these systems. However, this representation is memory-intensive and limits applicability to small-scale reconstructions. Several avenues have been explored to overcome this. With the aim of summarizing them and providing for a fast, flexible 3D reconstruction pipeline, we propose a new, unifying framework called InfiniTAM. The idea is that steps like camera tracking, scene representation and integration of new data can easily be replaced and adapted to the user's needs. 
This report describes the technical implementation details of InfiniTAM v3, the third version of our InfiniTAM system. We have added various new features, as well as making numerous enhancements to the low-level code that significantly improve our camera tracking performance. The new features that we expect to be of most interest are (i) a robust camera tracking module; (ii) an implementation of Glocker et al.'s keyframe-based random ferns camera relocaliser; (iii) a novel approach to globally-consistent TSDF-based reconstruction, based on dividing the scene into rigid submaps and optimising the relative poses between them; and (iv) an implementation of Keller et al.'s surfel-based reconstruction approach.",3inv3afrfola3drewilocl,59.0,13.0,6.0
3,3D Reconstruction,30.0,pix2vox: context-aware 3d reconstruction from single and multi-view images,5.0,45.0,4.0,5.0,5.0,4.6,28.5,4,http://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.pdf,"Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.",3pico3drefrsianmuim,70.0,49.0,8.0
4,3D Reconstruction,33.0,image2mesh: a learning framework for single image 3d reconstruction,5.0,94.0,4.0,15.0,5.0,4.6,52.0,5,https://arxiv.org/pdf/1711.10669,"One challenge that remains open in 3D deep learning is how to efficiently represent 3D data to feed deep networks. Recent works have relied on volumetric or point cloud representations, but such approaches suffer from a number of issues such as computational complexity, unordered data, and lack of finer geometry. This paper demonstrates that a mesh representation (i.e. vertices and faces to form polygonal surfaces) is able to capture fine-grained geometry for 3D reconstruction tasks. A mesh however is also unstructured data similar to point clouds. We address this problem by proposing a learning framework to infer the parameters of a compact mesh representation rather than learning from the mesh itself. This compact representation encodes a mesh using free-form deformation and a sparse linear combination of models allowing us to reconstruct 3D meshes from single images. In contrast to prior work, we do not rely on silhouettes and landmarks to perform 3D reconstruction. We evaluate our method on synthetic and real-world datasets with very promising results. Our framework efficiently reconstructs 3D objects in a low-dimensional way while preserving its important geometrical aspects.",3imalefrfosiim3dre,70.0,53.0,3.0
5,3D Reconstruction,60.0,semi-dense 3d reconstruction with a stereo event camera,4.0,53.0,4.0,18.0,5.0,4.3,44.6,6,http://openaccess.thecvf.com/content_ECCV_2018/papers/Yi_Zhou_Semi-Dense_3D_Reconstruction_ECCV_2018_paper.pdf,"Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.",3se3drewiastevca,52.0,45.0,3.0
6,3D Reconstruction,401.0,a point set generation network for 3d object reconstruction from a single image,1.0,30.0,5.0,9.0,5.0,3.8,135.0,7,http://arxiv.org/pdf/1612.00603v2,"Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images, however, these representations obscure the natural invariance of 3D shapes under geometric transformations, and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output &#x2013; point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthordox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3D reconstruction benchmarks, but it also shows strong performance for 3D shape completion and promising ability in making multiple plausible predictions.",3aposegenefo3dobrefrasiim,966.0,27.0,162.0
7,3D Reconstruction,96.0,intrinsic3d: high-quality 3d reconstruction by joint appearance and geometry optimization with spatially-varying lighting,4.0,35.0,5.0,201.0,1.0,3.5,103.1,8,http://openaccess.thecvf.com/content_ICCV_2017/papers/Maier_Intrinsic3D_High-Quality_3D_ICCV_2017_paper.pdf,"Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.",3inhi3drebyjoapangeopwispli,64.0,34.0,6.0
8,3D Reconstruction,401.0,video based reconstruction of 3d people models,1.0,28.0,5.0,43.0,4.0,3.5,144.4,9,http://arxiv.org/pdf/1803.04758v3,"This paper describes a method to obtain accurate 3D body models and texture of arbitrary people from a single, monocular video in which a person is moving. Based on a parametric body model, we present a robust processing pipeline to infer 3D model shapes including clothed people with 4.5mm reconstruction accuracy. At the core of our approach is the transformation of dynamic body pose into a canonical frame of reference. Our main contribution is a method to transform the silhouette cones corresponding to dynamic human silhouettes to obtain a visual hull in a common reference frame. This enables efficient estimation of a consensus 3D shape, texture and implanted animation skeleton based on a large number of frames. Results on 4 different datasets demonstrate the effectiveness of our approach to produce accurate 3D models. Requiring only an RGB camera, our method enables everyone to create their own fully animatable digital double, e.g., for social VR applications or virtual try-on for online fashion shopping.",3vibareof3dpemo,204.0,101.0,20.0
9,3D Reconstruction,401.0,differentiable volumetric rendering: learning implicit 3d representations without 3d supervision,1.0,25.0,5.0,98.0,4.0,3.5,159.70000000000002,10,http://arxiv.org/pdf/1912.07372v2,"Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.",3divoreleim3drewi3dsu,178.0,107.0,21.0
10,3D Reconstruction,401.0,implicit functions in feature space for 3d shape reconstruction and completion,1.0,62.0,4.0,2.0,5.0,3.4000000000000004,145.7,11,http://arxiv.org/pdf/2003.01456v2,"While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks (IF-Nets), which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions. Code and project website is available at https://virtualhumans.mpi-inf.mpg.de/ifnets/.",3imfuinfespfo3dshreanco,105.0,108.0,16.0
11,3D Reconstruction,7.0,kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera,5.0,201.0,1.0,10.0,5.0,3.4,85.5,12,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/kinectfusion-uist-comp.pdf,"KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.",3kire3dreaninusamodeca,1983.0,37.0,185.0
12,3D Reconstruction,4.0,a review of 3d reconstruction techniques in civil engineering and their applications,5.0,201.0,1.0,16.0,5.0,3.4,86.4,13,https://www.phdyar.ir/wp-content/uploads/2018/09/A-review-of-3D-reconstruction-techniques-in-civil-engineering-and-their-applications.pdf,"Abstract Three-dimensional (3D) reconstruction techniques have been used to obtain the 3D representations of objects in civil engineering in the form of point cloud models, mesh models and geometric models more often than ever, among which, point cloud models are the basis. In order to clarify the status quo of the research and application of the techniques in civil engineering, literature retrieval is implemented by using major literature databases in the world and the result is summarized by analyzing the abstracts or the full papers when required. First, the research methodology is introduced, and the framework of 3D reconstruction techniques is established. Second, 3D reconstruction techniques for generating point clouds and processing point clouds along with the corresponding algorithms and methods are reviewed respectively. Third, their applications in reconstructing and managing construction sites and reconstructing pipelines of Mechanical, Electrical and Plumbing (MEP) systems, are presented as typical examples, and the achievements are highlighted. Finally, the challenges are discussed and the key research directions to be addressed in the future are proposed. This paper contributes to the knowledge body of 3D reconstruction in two aspects, i.e. summarizing systematically the up-to-date achievements and challenges for the applications of 3D reconstruction techniques in civil engineering, and proposing key future research directions to be addressed in the field.",3areof3dreteincienanthap,71.0,82.0,1.0
13,3D Reconstruction,16.0,what do single-view 3d reconstruction networks learn?,5.0,201.0,1.0,6.0,5.0,3.4,87.0,14,https://openaccess.thecvf.com/content_CVPR_2019/papers/Tatarchenko_What_Do_Single-View_3D_Reconstruction_Networks_Learn_CVPR_2019_paper.pdf,"Convolutional networks for single-view object reconstruction have shown impressive performance and have become a popular subject of research. All existing techniques are united by the idea of having an encoder-decoder network that performs non-trivial reasoning about the 3D structure of the output space. In this work, we set up two alternative approaches that perform image classification and retrieval respectively. These simple baselines yield better results than state-of-the-art methods, both qualitatively and quantitatively. We show that encoder-decoder methods are statistically indistinguishable from these baselines, thus indicating that the current state of the art in single-view object reconstruction does not actually perform reconstruction but image classification. We identify aspects of popular experimental procedures that elicit this behavior and discuss ways to improve the current state of research.",3whdosi3drenele,135.0,70.0,17.0
14,3D Reconstruction,24.0,real-time 3d reconstruction at scale using voxel hashing,5.0,201.0,1.0,38.0,5.0,3.4,99.0,15,http://graphics.stanford.edu/~niessner/papers/2013/4hashing/niessner2013hashing.pdf,"Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras. The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model. This is challenging particularly when real-time performance is desired without trading quality or scale. We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure. Our system uses a simple spatial hashing scheme that compresses space, and allows for real-time access and updates of implicit surface data, without the need for a regular or hierarchical grid data structure. Surface data is only stored densely where measurements are observed. Additionally, data can be streamed efficiently in or out of the hash table, allowing for further scalability during sensor motion. We show interactive reconstructions of a variety of scenes, reconstructing both fine-grained details and large scale environments. We illustrate how all parts of our pipeline from depth map pre-processing, camera pose estimation, depth map fusion, and surface rendering are performed at real-time rates on commodity graphics hardware. We conclude with a comparison to current state-of-the-art online systems, illustrating improved performance and reconstruction quality.",3re3dreatscusvoha,643.0,51.0,65.0
15,3D Reconstruction,28.0,automatic 3d reconstruction from multi-date satellite images,5.0,201.0,1.0,35.0,5.0,3.4,99.3,16,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w18/papers/Facciolo_Automatic_3D_Reconstruction_CVPR_2017_paper.pdf,"We propose an algorithm for computing a 3D model from several satellite images of the same site. The method works even if the images were taken at different dates with important lighting and vegetation differences. We show that with a large number of input images the resulting 3D models can be as accurate as those obtained from a single same-date stereo pair. To deal with seasonal vegetation changes, we propose a strategy that accounts for the multi-modal nature of 3D models computed from multi-date images. Our method uses a local affine camera approximation and thus focuses on the 3D reconstruction of small areas. This is a common setup in urgent cartography for emergency management, for which abundant multi-date imagery can be immediately available to build a reference 3D model. A preliminary implementation of this method was used to win the IARPA Multi-View Stereo 3D Mapping Challenge 2016. Experiments on the challenge dataset are used to substantiate our claims.",3au3drefrmusaim,54.0,35.0,5.0
16,3D Reconstruction,1.0,stereoscan: dense 3d reconstruction in real-time,5.0,201.0,1.0,54.0,4.0,3.1,96.9,17,http://ww.cvlibs.net/publications/Geiger2011IV.pdf,"Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed. This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while - at the same time - we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.",3stde3dreinre,955.0,27.0,156.0
17,3D Reconstruction,5.0,multi-image 3d reconstruction data evaluation,5.0,201.0,1.0,53.0,4.0,3.1,97.8,18,http://dsp.ee.duth.gr/~chamzas/chamzas_pdfs/publications/201305_3D_EVALUATE_elsevier_culture.pdf,"Abstract A number of software solutions based on the Structure-From-Motion (SFM) and Dense Multi-View 3D Reconstruction (DMVR) algorithms have been made recently available. They allow the production of high quality 3D models by using unordered image collections that depict a scene or an object from different viewpoints. In this work, we question the quality of the data produced by a commercial SFM-DMVR software. An Ottoman monument located in the region of Xanthi, Greece has been selected as a case study. We attempted to quantify the quality of the SFM-DMVR data in relation to the data produced by a Time-of-Flight terrestrial 3D range scanner. We have implemented a number of comparisons between different parts of the monument in order to assess the mesh deviations and the reconstruction's accuracy. In order to further ensure the validity of our evaluation phase, we performed additional distance measurements between feature points on the monument's surface by using a total station and empirical measurements. The applicability of the SFM-DMVR method was questioned by creating a complete 3D digital replica of the monument.",3mu3dredaev,269.0,23.0,8.0
18,3D Reconstruction,59.0,deep local shapes: learning local sdf priors for detailed 3d reconstruction,4.0,55.0,4.0,201.0,1.0,3.1,100.0,19,https://arxiv.org/pdf/2003.10983,"We propose a novel framework for fine-grained object recognition that learns to recover object variation in 3D space from a single image, trained on an image collection without using any ground-truth 3D annotation. We accomplish this by representing an object as a composition of 3D shape and its appearance, while eliminating the effect of camera viewpoint, in a canonical configuration. Unlike conventional methods modeling spatial variation in 2D images only, our method is capable of reconfiguring the appearance feature in a canonical 3D space, thus enabling the subsequent object classifier to be invariant under 3D geometric variation. Our representation also allows us to go beyond existing methods, by incorporating 3D shape variation as an additional cue for object recognition. To learn the model without ground-truth 3D annotation, we deploy a differentiable renderer in an analysis-by-synthesis framework. By incorporating 3D shape and appearance jointly in a deep representation, our method learns the discriminative representation of the object and achieves competitive performance on fine-grained image recognition and vehicle re-identification. We also demonstrate that the performance of 3D shape reconstruction is improved by learning fine-grained shape deformation in a boosting manner.",3deloshlelosdprfode3dre,79.0,66.0,10.0
19,3D Reconstruction,51.0,killingfusion: non-rigid 3d reconstruction without correspondences,4.0,201.0,1.0,24.0,5.0,3.1,102.9,20,http://openaccess.thecvf.com/content_cvpr_2017/papers/Slavcheva_KillingFusion_Non-Rigid_3D_CVPR_2017_paper.pdf,"We introduce a geometry-driven approach for real-time 3D reconstruction of deforming surfaces from a single RGB-D stream without any templates or shape priors. To this end, we tackle the problem of non-rigid registration by level set evolution without explicit correspondence search. Given a pair of signed distance fields (SDFs) representing the shapes of interest, we estimate a dense deformation field that aligns them. It is defined as a displacement vector field of the same resolution as the SDFs and is determined iteratively via variational minimization. To ensure it generates plausible shapes, we propose a novel regularizer that imposes local rigidity by requiring the deformation to be a smooth and approximately Killing vector field, i.e. generating nearly isometric motions. Moreover, we enforce that the level set property of unity gradient magnitude is preserved over iterations. As a result, KillingFusion reliably reconstructs objects that are undergoing topological changes and fast inter-frame motion. In addition to incrementally building a model from scratch, our system can also deform complete surfaces. We demonstrate these capabilities on several public datasets and introduce our own sequences that permit both qualitative and quantitative comparison to related approaches.",3kino3drewico,117.0,60.0,9.0
20,3D Reconstruction,34.0,orbslam-based endoscope tracking and 3d reconstruction,5.0,201.0,1.0,41.0,4.0,3.1,102.9,21,https://arxiv.org/pdf/1608.08149,"We aim to track the endoscope location inside the surgical scene and provide 3D reconstruction, in real-time, from the sole input of the image sequence captured by the monocular endoscope. This information offers new possibilities for developing surgical navigation and augmented reality applications. The main benefit of this approach is the lack of extra tracking elements which can disturb the surgeon performance in the clinical routine. It is our first contribution to exploit ORBSLAM, one of the best performing monocular SLAM algorithms, to estimate both of the endoscope location, and 3D structure of the surgical scene. However, the reconstructed 3D map poorly describe textureless soft organ surfaces such as liver. It is our second contribution to extend ORBSLAM to be able to reconstruct a semi-dense map of soft organs. Experimental results on in-vivo pigs, shows a robust endoscope tracking even with organs deformations and partial instrument occlusions. It also shows the reconstruction density, and accuracy against ground truth surface obtained from CT.",3orentran3dre,56.0,18.0,4.0
21,3D Reconstruction,10.0,3d reconstruction from accidental motion,5.0,201.0,1.0,75.0,4.0,3.1,105.9,22,http://openaccess.thecvf.com/content_cvpr_2014/papers/Yu_3D_Reconstruction_from_2014_CVPR_paper.pdf,"We have discovered that 3D reconstruction can be achieved from asingle still photographic capture due to accidental motions of thephotographer, even while attempting to hold the camera still. Although these motions result in little baseline and therefore high depth uncertainty, in theory, we can combine many such measurements over the duration of the capture process (a few seconds) to achieve usable depth estimates. Wepresent a novel 3D reconstruction system tailored for this problemthat produces depth maps from short video sequences from standard cameraswithout the need for multi-lens optics, active sensors, or intentionalmotions by the photographer. This result leads to the possibilitythat depth maps of sufficient quality for RGB-D photography applications likeperspective change, simulated aperture, and object segmentation, cancome ""for free"" for a significant fraction of still photographsunder reasonable conditions.",33drefracmo,77.0,38.0,17.0
22,3D Reconstruction,29.0,3d modeling on the go: interactive 3d reconstruction of large-scale scenes on mobile devices,5.0,201.0,1.0,59.0,4.0,3.1,106.8,23,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.706.9171&rep=rep1&type=pdf,"This paper presents a system for 3D reconstruction of large-scale outdoor scenes based on monocular motion stereo. Ours is the first such system to run at interactive frame rates on a mobile device (Google Project Tango Tablet), thus allowing a user to reconstruct scenes ""on the go"" by simply walking around them. We utilize the device's GPU to compute depth maps using plane sweep stereo. We then fuse the depth maps into a global model of the environment represented as a truncated signed distance function in a spatially hashed voxel grid. We observe that in contrast to reconstructing objects in a small volume of interest, or using the near outlier-free data provided by depth sensors, one can rely less on free-space measurements for suppressing outliers in unbounded large-scale scenes. Consequently, we propose a set of simple filtering operations to remove unreliable depth estimates and experimentally demonstrate the benefit of strongly filtering depth maps. We extensively evaluate the system with real as well as synthetic datasets.",33dmoonthgoin3dreoflasconmode,91.0,38.0,5.0
23,3D Reconstruction,27.0,optical sensors and methods for underwater 3d reconstruction,5.0,201.0,1.0,62.0,4.0,3.1,107.1,24,https://www.mdpi.com/1424-8220/15/12/29864/pdf,"This paper presents a survey on optical sensors and methods for 3D reconstruction in underwater environments. The techniques to obtain range data have been listed and explained, together with the different sensor hardware that makes them possible. The literature has been reviewed, and a classification has been proposed for the existing solutions. New developments, commercial solutions and previous reviews in this topic have also been gathered and considered.",3opseanmefoun3dre,91.0,188.0,4.0
24,3D Reconstruction,32.0,structured light-based 3d reconstruction system for plants,5.0,201.0,1.0,61.0,4.0,3.1,108.3,25,https://www.mdpi.com/1424-8220/15/8/18587/pdf,"Camera-based 3D reconstruction of physical objects is one of the most popular computer vision trends in recent years. Many systems have been built to model different real-world subjects, but there is lack of a completely robust system for plants.This paper presents a full 3D reconstruction system that incorporates both hardware structures (including the proposed structured light system to enhance textures on object surfaces) and software algorithms (including the proposed 3D point cloud registration and plant feature measurement). This paper demonstrates the ability to produce 3D models of whole plants created from multiple pairs of stereo images taken at different viewing angles, without the need to destructively cut away any parts of a plant. The ability to accurately predict phenotyping features, such as the number of leaves, plant height, leaf size and internode distances, is also demonstrated. Experimental results show that, for plants having a range of leaf sizes and a distance between leaves appropriate for the hardware design, the algorithms successfully predict phenotyping features in the target crops, with a recall of 0.97 and a precision of 0.89 for leaf detection and less than a 13-mm error for plant size, leaf size and internode distance.",3stli3dresyfopl,99.0,47.0,0.0
25,3D Reconstruction,23.0,real-time 3d reconstruction in dynamic scenes using point-based fusion,5.0,201.0,1.0,70.0,4.0,3.1,108.3,26,http://web4.cs.ucl.ac.uk/staff/t.weyrich/projects/kinect/keller13realtime.pdf,"Real-time or online 3D reconstruction has wide applicability and receives further interest due to availability of consumer depth cameras. Typical approaches use a moving sensor to accumulate depth measurements into a single model which is continuously refined. Designing such systems is an intricate balance between reconstruction quality, speed, spatial scale, and scene assumptions. Existing online methods either trade scale to achieve higher quality reconstructions of small objects/scenes. Or handle larger scenes by trading real-time performance and/or quality, or by limiting the bounds of the active reconstruction. Additionally, many systems assume a static scene, and cannot robustly handle scene motion or reconstructions that evolve to reflect scene changes. We address these limitations with a new system for real-time dense reconstruction with equivalent quality to existing online methods, but with support for additional spatial scale and robustness in dynamic scenes. Our system is designed around a simple and flat point-Based representation, which directly works with the input acquired from range/depth sensors, without the overhead of converting between representations. The use of points enables speed and memory efficiency, directly leveraging the standard graphics pipeline for all central operations, i.e., camera pose estimation, data association, outlier removal, fusion of depth maps into a single denoised model, and detection and update of dynamic objects. We conclude with qualitative and quantitative results that highlight robust tracking and high quality reconstructions of a diverse set of scenes at varying scales.",3re3dreindyscuspofu,292.0,30.0,41.0
26,3D Reconstruction,66.0,real-time large-scale dense 3d reconstruction with loop closure,4.0,201.0,1.0,36.0,5.0,3.1,111.0,27,http://arxiv.org/pdf/2105.09188v1,"In the highly active research field of dense 3D reconstruction and modelling, loop closure is still a largely unsolved problem. While a number of previous works show how to accumulate keyframes, globally optimize their pose on closure, and compute a dense 3D model as a post-processing step, in this paper we propose an online framework which delivers a consistent 3D model to the user in real time. This is achieved by splitting the scene into submaps, and adjusting the poses of the submaps as and when required. We present a novel technique for accumulating relative pose constraints between the submaps at very little computational cost, and demonstrate how to maintain a lightweight, scalable global optimization of submap poses. In contrast to previous works, the number of submaps grows with the observed 3D scene surface, rather than with time. In addition to loop closure, the paper incorporates relocalization and provides a novel way of assessing tracking quality.",3relade3drewilocl,83.0,38.0,12.0
27,3D Reconstruction,37.0,3d reconstruction of small sized objects from a sequence of multi-focused images,5.0,201.0,1.0,81.0,4.0,3.1,115.8,28,http://arxiv.org/pdf/1301.1595v2,"Abstract 3D reconstructions of small objects are more and more frequently employed in several disciplines such as medicine, archaeology, restoration of cultural heritage, forensics, etc. The capability of performing accurate analyses directly on a three-dimensional surface allows for a significant improvement in the accuracy of the measurements, which are otherwise performed on 2D images acquired through a microscope. In this work we present a new methodology for the 3D reconstruction of small sized objects based on a multi-view passive stereo technique applied on a sequence of macro images. The resolving power of macro lenses makes them ideal for photogrammetric applications, but the very small depth of field is their biggest limit. Our approach solves this issue by using an image fusion algorithm to extend the depth of field of the images used in the photogrammetric process. The paper aims to overcome the problems related to the use of macro lenses in photogrammetry, showing how it is possible to retrieve the camera calibration parameters of the sharp images by using an open source Structure from Motion software. Our approach has been tested on two case studies, on objects with a bounding box diagonal ranging from 13.5 mm to 41 mm. The accuracy analysis, performed on certified gauge blocks, demonstrates that the experimental setup returns a 3D model with an accuracy that can reach the 0.05% of the bounding box diagonal.",33dreofsmsiobfraseofmuim,75.0,17.0,2.0
28,3D Reconstruction,401.0,learning free-form deformations for 3d object reconstruction,1.0,100.0,4.0,76.0,4.0,3.1,183.1,29,http://arxiv.org/pdf/2108.04628v1,"Representing 3D shape in deep learning frameworks in an accurate, efficient and compact manner still remains an open challenge. Most existing work addresses this issue by employing voxel-based representations. While these approaches benefit greatly from advances in computer vision by generalizing 2D convolutions to the 3D setting, they also have several considerable drawbacks. The computational complexity of voxel-encodings grows cubically with the resolution thus limiting such representations to low-resolution 3D reconstruction. In an attempt to solve this problem, point cloud representations have been proposed. Although point clouds are more efficient than voxel representations as they only cover surfaces rather than volumes, they do not encode detailed geometric information about relationships between points. In this paper we propose a method to learn free-form deformations (FFD) for the task of 3D reconstruction from a single image. By learning to deform points sampled from a high-quality mesh, our trained model can be used to produce arbitrarily dense point clouds or meshes with fine-grained geometry. We evaluate our proposed framework on both synthetic and real-world data and achieve state-of-the-art results on point-cloud and volumetric metrics. Additionally, we qualitatively demonstrate its applicability to label transferring for 3D semantic segmentation.",3lefrdefo3dobre,53.0,43.0,4.0
29,3D Reconstruction,109.0,"learning single-image 3d reconstruction by generative modelling of shape, pose and shading",3.0,42.0,4.0,201.0,1.0,2.8,109.8,30,http://arxiv.org/pdf/1901.06447v2,"We present a unified framework tackling two problems: class-specific 3D reconstruction from a single image, and generation of new 3D shape samples. These tasks have received considerable attention recently; however, most existing approaches rely on 3D supervision, annotation of 2D images with keypoints or poses, and/or training with multiple views of each object instance. Our framework is very general: it can be trained in similar settings to existing approaches, while also supporting weaker supervision. Importantly, it can be trained purely from 2D images, without pose annotations, and with only a single view per instance. We employ meshes as an output representation, instead of voxels used in most prior work. This allows us to reason over lighting parameters and exploit shading information during training, which previous 2D-supervised methods cannot. Thus, our method can learn to generate and reconstruct concave object classes. We evaluate our approach in various settings, showing that: (i) it learns to disentangle shape from pose and lighting; (ii) using shading in the loss improves performance compared to just silhouettes; (iii) when using a standard single white light, our model outperforms state-of-the-art 2D-supervised methods, both with and without pose supervision, thanks to exploiting shading cues; (iv) performance improves further when using multiple coloured lights, even approaching that of state-of-the-art 3D-supervised methods; (v) shapes produced by our model capture smooth surfaces and fine details better than voxel-based approaches; and (vi) our approach supports concave classes such as bathtubs and sofas, which methods based on silhouettes cannot learn.",3lesi3drebygemoofshpoansh,63.0,78.0,1.0
30,3D Reconstruction,47.0,"automated as-built 3d reconstruction of civil infrastructure using computer vision: achievements, opportunities, and challenges",4.0,201.0,1.0,58.0,4.0,2.8,111.9,31,http://users.ics.forth.gr/~lourakis/publ/2015_ADVEI.pdf,"Image-based 3D reconstruction of civil infrastructure is an emerging topic that is gaining significant interest both in the scientific and commercial sectors of the construction industry. Reliable computer vision-based algorithms have become available over the last decade and they can now be applied to solve real-life problems in uncontrolled environments. While a large number of such algorithms have been developed by the computer vision and photogrammetry communities, relatively little work has been done to study their performance in the context of infrastructure. This paper aims to analyze the state-of-the-art in image-based 3D reconstruction and categorize existing algorithms according to different metrics that are important for the given purpose. An ideal solution is portrayed to show what the ultimate goal is. This will be followed by identifying gaps in knowledge and highlighting future research topics that could contribute to the widespread adoption of this technology in the construction industry. Finally, a list of practical constraints that make the 3D reconstruction of infrastructure a challenging task is presented.",3auas3dreofciinuscoviacopanch,118.0,137.0,2.0
31,3D Reconstruction,43.0,from single image query to detailed 3d reconstruction,4.0,201.0,1.0,78.0,4.0,2.8,116.70000000000002,32,https://openaccess.thecvf.com/content_cvpr_2015/papers/Schonberger_From_Single_Image_2015_CVPR_paper.pdf,"Structure-from-Motion for unordered image collections has significantly advanced in scale over the last decade. This impressive progress can be in part attributed to the introduction of efficient retrieval methods for those systems. While this boosts scalability, it also limits the amount of detail that the large-scale reconstruction systems are able to produce. In this paper, we propose a joint reconstruction and retrieval system that maintains the scalability of large-scale Structure-from-Motion systems while also recovering the often lost ability of reconstructing fine details of the scene. We demonstrate our proposed method on a large-scale dataset of 7.4 million images downloaded from the Internet.",3frsiimqutode3dre,110.0,36.0,2.0
32,3D Reconstruction,75.0,a symmetry prior for convex variational 3d reconstruction,4.0,201.0,1.0,50.0,4.0,2.8,117.9,33,http://arxiv.org/abs/1509.04309v3,We propose a novel prior for variational 3D reconstruction that favors symmetric solutions when dealing with noisy or incomplete data. We detect symmetries from incomplete data while explicitly handling unexplored areas to allow for plausible scene completions. The set of detected symmetries is then enforced on their respective support domain within a variational reconstruction framework. This formulation also handles multiple symmetries sharing the same support. The proposed approach is able to denoise and complete surface geometry and even hallucinate large scene parts. We demonstrate in several experiments the benefit of harnessing symmetries when regularizing a surface.,3asyprfocova3dre,39.0,29.0,1.0
33,3D Reconstruction,19.0,modeling kinect sensor noise for improved 3d reconstruction and tracking,5.0,201.0,1.0,108.0,3.0,2.8,118.5,34,https://www.academia.edu/download/41158360/Modeling_Kinect_Sensor_Noise_for_Improve20160114-17514-v7iwdp.pdf20160114-19908-19yxmcc.pdf,"We contribute an empirically derived noise model for the Kinect sensor. We systematically measure both lateral and axial noise distributions, as a function of both distance and angle of the Kinect to an observed surface. The derived noise model can be used to filter Kinect depth maps for a variety of applications. Our second contribution applies our derived noise model to the KinectFusion system to extend filtering, volumetric fusion, and pose estimation within the pipeline. Qualitative results show our method allows reconstruction of finer details and the ability to reconstruct smaller objects and thinner surfaces. Quantitative results also show our method improves pose estimation accuracy.",3mokisenofoim3dreantr,309.0,12.0,28.0
34,3D Reconstruction,22.0,multiview 3d reconstruction in geosciences,5.0,201.0,1.0,117.0,3.0,2.8,122.1,35,https://www.earth-prints.org/bitstream/2122/7480/1/CG_PP_Favalli_etal_2011.pdf,"Multiview three-dimensional (3D) reconstruction is a technology that allows the creation of 3D models of a given scenario from a series of overlapping pictures taken using consumer-grade digital cameras. This type of 3D reconstruction is facilitated by freely available software, which does not require expert-level skills. This technology provides a 3D working environment, which integrates sample/field data visualization and measurement tools. In this study, we test the potential of this method for 3D reconstruction of decimeter-scale objects of geological interest. We generated 3D models of three different outcrops exposed in a marble quarry and two solids: a volcanic bomb and a stalagmite. Comparison of the models obtained in this study using the presented method with those obtained using a precise laser scanner shows that multiview 3D reconstruction yields models that present a root mean square error/average linear dimensions between 0.11 and 0.68%. Thus this technology turns out to be an extremely promising tool, which can be fruitfully applied in geosciences.",3mu3dreinge,84.0,26.0,5.0
35,3D Reconstruction,53.0,descriptor-based methodology for statistical characterization and 3d reconstruction of microstructural materials,4.0,201.0,1.0,91.0,4.0,2.8,123.6,36,http://arxiv.org/pdf/2102.02407v1,"Abstract 3D reconstructions of heterogeneous microstructures are important for assessing material properties using advanced simulation techniques such as finite element analysis (FEA). Nevertheless, for many materials systems like polymer nanocomposites, only 2D microstructural images are available even with the state-of-the-art imaging techniques. This paper proposes a new descriptor-based methodology for reconstructing 3D particle-based heterogeneous microstructures based on 2D images. The proposed methodology characterizes a 2D microstructural morphology using a small set of microstructure descriptors covering features including material composition, dispersion status, and phase geometry, and then reconstructs statistically equivalent microstructures in a 3D space based on the 3D descriptors derived from 2D characterization and a few reasonable assumptions. Our approach is the most useful when the direct 3D microstructure analysis, such as 3D tomography, is not available due to either high cost or difficulties in sample preparations. Other practical features of descriptor-based characterization include low dimensionality, which enables optimal parametric design of microstructures, as well as physically meaningful mapping of processing related material parameters. In reconstruction, the proposed algorithm is capable to generate large size 3D structures at a low computational cost. Furthermore, since the algorithm is stochastic, it can be used to construct both Representative Volume Element (RVE) and Statistical Volume Element (SVE) for FEA studies. We demonstrate the proposed methodology by characterizing and reconstructing polymer nanocomposites.",3demefostchan3dreofmima,100.0,50.0,4.0
36,3D Reconstruction,35.0,performance evaluation of a multi-image 3d reconstruction software on a low-feature artefact,5.0,201.0,1.0,111.0,3.0,2.8,124.2,37,http://arxiv.org/abs/1708.01419v1,"Abstract Nowadays, multi-image 3D reconstruction is an active research field and a number of commercial and free software tools have been already made available to the public. These provide methods for the 3D reconstruction of real world objects by matching feature points and retrieving depth information from a set of unordered digital images. This is achieved by exploiting computer vision algorithms such as Structure-From-Motion (SFM) and Dense Multi-View 3D Reconstruction (DMVR). In this work, we evaluate the performance of a low-cost commercial SFM–DMVR software by digitising a Cycladic woman figurine. Although the surface properties of the specific artefact are considered 3D laser scanner friendly, its almost featureless white-grey surface composes a challenging digitisation candidate for image based methodologies as no strong feature points are available. We quantify the quality of the 3D data produced by the SFM–DMVR software in relation to the data produced by a high accuracy 3D laser scanner in terms of surface deviation and topological errors. We question the applicability and efficiency of two digitisation pipelines (SFM–DMVR and laser scanner) in relation to hardware requirements, background knowledge and man-hours. This is achieved by producing a complete 3D digital replica of the Cycladic artefact by following both pipelines.",3peevofamu3dresoonaloar,67.0,15.0,5.0
37,3D Reconstruction,116.0,"recognition, location, measurement, and 3d reconstruction of concealed cracks using convolutional neural networks",3.0,201.0,1.0,33.0,5.0,2.8,125.1,38,http://arxiv.org/abs/1909.02410v3,"Abstract Concealed cracks in asphalt pavement are the cracks that originate below the surface of the pavement. These cracks are a major contributing factor to pavement damage, in addition to being a major contributing factor to the formation of reflection cracks. The detection of a concealed crack is considered challenging because the location of the crack is, by definition, difficult to find. Therefore, the research on the utilization of ground penetrating radar (GPR) to locate concealed cracks has gained significant interest in recent years. However, the manually processed GPR image used for the recognition, location, and measurement of concealed cracks is inefficient and inaccurate. This project presents an application of convolutional neural networks (CNNs) to GPR images that automatically recognizes, locates, measures, and produces a 3D reconstruction of concealed cracks. In this project, three different CNNs (recognition, location, and feature extraction) were established to accomplish the aforementioned tasks automatically. Each CNN is developed through processes of structural design, training, and testing. The recognition CNN was designed to distinguish concealed cracks from other types of damage in a GPR image, the location CNN determined the location and length measurement of concealed crack images based on the results provided by the recognition CNN, and crack feature points were extracted by the feature extraction CNN to establish the 3D reconstruction models of the concealed cracks. The 3D reconstruction models were then used to calculate crack volume and predict the growth tendency of cracks. The results indicated that the recognition CNN is able to distinguish concealed cracks from other types of damages in 6482 GPR images with zero errors. In addition, the length recognition results calculated from the location CNN possess a 0.2543 cm mean squared error, a 0.978 cm maximum length error, and a 0.504 cm average error in the test samples. Meanwhile, the feature extraction CNN is able to provide feature points for a 3D reconstruction model. The results of this study suggest that the CNNs could be accurately used for the recognition, location, and 3D reconstruction of concealed cracks in asphalt pavement in real-world applications.",3relomean3dreofcocrusconene,61.0,36.0,1.0
38,3D Reconstruction,72.0,3d reconstruction of as-built industrial instrumentation models from laser-scan data and a 3d cad database based on prior knowledge,4.0,201.0,1.0,88.0,4.0,2.8,128.4,39,http://arxiv.org/pdf/2009.09633v1,"Abstract In the operation and maintenance phase of industrial plants, it is important to ensure that the as-built condition of every piece of installed instrumentation, and any changes that may subsequently occur, are properly recorded and adjusted in interpretive documents. However, the manual creation of industrial instrumentation models from an acquired set of three-dimensional (3D) point clouds is difficult. This paper proposes an approach for the reconstruction of 3D models of as-built industrial instrumentation from terrestrial laser-scan data and a 3D computer-aided design (CAD) database based on prior knowledge. An experimental evaluation of an actual industrial plant demonstrates that incorporating prior knowledge into the reconstruction process greatly reduces the manual interaction required, facilitating the process of reconstructing 3D models of as-built industrial instrumentation from unstructured (raw) laser-scan data. The proposed approach could be successfully utilized to assist in the reconstruction of 3D models of as-built industrial instrumentation during the operation and maintenance phase of an industrial plant.",33dreofasininmofrladaana3dcadabaonprkn,60.0,49.0,1.0
39,3D Reconstruction,82.0,omnidirectional 3d reconstruction in augmented manhattan worlds,4.0,201.0,1.0,86.0,4.0,2.8,130.8,40,http://t.cvlibs.net/publications/Schoenbein2014IROS.pdf,"This paper proposes a method for high-quality omnidirectional 3D reconstruction of augmented Manhattan worlds from catadioptric stereo video sequences. In contrast to existing works we do not rely on constructing virtual perspective views, but instead propose to optimize depth jointly in a unified omnidirectional space. Furthermore, we show that plane-based prior models can be applied even though planes in 3D do not project to planes in the omnidirectional domain. Towards this goal, we propose an omnidirectional slanted-plane Markov random field model which relies on plane hypotheses extracted using a novel voting scheme for 3D planes in omnidirectional space. To quantitatively evaluate our method we introduce a dataset which we have captured using our autonomous driving platform AnnieWAY which we equipped with two horizontally aligned catadioptric cameras and a Velodyne HDL-64E laser scanner for precise ground truth depth measurements. As evidenced by our experiments, the proposed method clearly benefits from the unified view and significantly outperforms existing stereo matching techniques both quantitatively and qualitatively. Furthermore, our method is able to reduce noise and the obtained depth maps can be represented very compactly by a small number of image segments and plane parameters.",3om3dreinaumawo,59.0,46.0,5.0
40,3D Reconstruction,80.0,video pop-up: monocular 3d reconstruction of dynamic scenes,4.0,201.0,1.0,90.0,4.0,2.8,131.4,41,https://link.springer.com/content/pdf/10.1007/978-3-319-10584-0_38.pdf,"Consider a video sequence captured by a single camera observing a complex dynamic scene containing an unknown mixture of multiple moving and possibly deforming objects. In this paper we propose an unsupervised approach to the challenging problem of simultaneously segmenting the scene into its constituent objects and reconstructing a 3D model of the scene. The strength of our approach comes from the ability to deal with real-world dynamic scenes and to handle seamlessly different types of motion: rigid, articulated and non-rigid. We formulate the problem as hierarchical graph-cut based segmentation where we decompose the whole scene into background and foreground objects and model the complex motion of non-rigid or articulated objects as a set of overlapping rigid parts. We evaluate the motion segmentation functionality of our approach on the Berkeley Motion Segmentation Dataset. In addition, to validate the capability of our approach to deal with real-world scenes we provide 3D reconstructions of some challenging videos from the YouTube-Objects dataset.",3vipomo3dreofdysc,105.0,56.0,5.0
41,3D Reconstruction,20.0,online 3d reconstruction using convex optimization,5.0,201.0,1.0,152.0,3.0,2.8,132.0,42,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.381.5668&rep=rep1&type=pdf,"We present a system that is capable of interactively reconstructing a scene from a single live camera. We use a dense volumetric representation of the surface, which means there are no constraints concerning the 3D-scene topology. Reconstruction is based on range image fusion using a total variation formulation, where the surface is represented implicitly by a signed distance function. The final 3D-model is obtained by minimizing a global convex energy and extracting the zero level-set of the solution. The whole reconstruction process is designed to be online. Users can constantly inspect the current reconstruction and adapt camera movement to get the desired amount of detail for specific parts of the reconstructed scene.",3on3dreuscoop,79.0,11.0,6.0
42,3D Reconstruction,139.0,large-scale semantic 3d reconstruction: an adaptive multi-resolution model for multi-class volumetric labeling,3.0,201.0,1.0,37.0,5.0,2.8,133.2,43,https://openaccess.thecvf.com/content_cvpr_2016/papers/Blaha_Large-Scale_Semantic_3D_CVPR_2016_paper.pdf,"We propose an adaptive multi-resolution formulation of semantic 3D reconstruction. Given a set of images of a scene, semantic 3D reconstruction aims to densely reconstruct both the 3D shape of the scene and a segmentation into semantic object classes. Jointly reasoning about shape and class allows one to take into account class-specific shape priors (e.g., building walls should be smooth and vertical, and vice versa smooth, vertical surfaces are likely to be building walls), leading to improved reconstruction results. So far, semantic 3D reconstruction methods have been limited to small scenes and low resolution, because of their large memory footprint and computational cost. To scale them up to large scenes, we propose a hierarchical scheme which refines the reconstruction only in regions that are likely to contain a surface, exploiting the fact that both high spatial resolution and high numerical precision are only required in those regions. Our scheme amounts to solving a sequence of convex optimizations while progressively removing constraints, in such a way that the energy, in each iteration, is the tightest possible approximation of the underlying energy at full resolution. In our experiments the method saves up to 98% memory and 95% computation time, without any loss of accuracy.",3lase3dreanadmumofomuvola,87.0,48.0,4.0
43,3D Reconstruction,157.0,learning priors for semantic 3d reconstruction,3.0,201.0,1.0,26.0,5.0,2.8,135.3,44,http://openaccess.thecvf.com/content_ECCV_2018/papers/Ian_Cherabier_Learning_Priors_for_ECCV_2018_paper.pdf,"We present a novel semantic 3D reconstruction framework which embeds variational regularization into a neural network. Our network performs a fixed number of unrolled multi-scale optimization iterations with shared interaction weights. In contrast to existing variational methods for semantic 3D reconstruction, our model is end-to-end trainable and captures more complex dependencies between the semantic labels and the 3D geometry. Compared to previous learning-based approaches to 3D reconstruction, we integrate powerful long-range dependencies using variational coarse-to-fine optimization. As a result, our network architecture requires only a moderate number of parameters while keeping a high level of expressiveness which enables learning from very little data. Experiments on real and synthetic datasets demonstrate that our network achieves higher accuracy compared to a purely variational approach while at the same time requiring two orders of magnitude less iterations to converge. Moreover, our approach handles ten times more semantic class labels using the same computational resources.",3leprfose3dre,25.0,43.0,2.0
44,3D Reconstruction,14.0,from 3d reconstruction to virtual reality: a complete methodology for digital archaeological exhibition,5.0,201.0,1.0,172.0,3.0,2.8,136.20000000000002,45,https://www.academia.edu/download/50363049/From_3D_reconstruction_to_virtual_realit20161116-7958-1vqhze0.pdf,"Abstract For nearly two decades, virtual reality (VR) technologies have been employed in the field of cultural heritage for various purposes. The safeguard, the protection and the fruition of the remains of the past have gained a powerful tool, thanks to the potentialities of immersive visualization and 3D reconstruction of archaeological sites and finds. VR applications based on videogame technologies are known for their realism and fluid interactivity, but the choice of the fittest technologies remains a complex task because there is an ample number of hardware devices and software development kits. Moreover the design of a VR application for cultural heritage requires several different professional skills and presents a certain complexity in coordination and management. This paper presents strategies to overcome these problems, by suggesting some guidelines for the development of VR systems for cultural heritage. It illustrates a complete methodology to create a virtual exhibition system, based on realistic high-quality 3D models of archaeological finds (reconstructed using a 3D Scanner and a high definition camera) and a low-cost multimedia stereoscopic system called MNEME, which allows the user to interact in a free and easy way with a rich collection of archaeological finds. The solution we propose is intended to be easy to transport and fully usable by different user typologies, without any external assistance or supervision.",3fr3dretovireacomefodiarex,290.0,26.0,10.0
45,3D Reconstruction,25.0,3d reconstruction of histological sections: application to mammary gland tissue,5.0,201.0,1.0,166.0,3.0,2.8,137.7,46,https://www.academia.edu/download/39276879/3D_reconstruction_of_histological_sections._Application_to_mammary_gland_tissue.pdf,"In this article, we present a novel method for the automatic 3D reconstruction of thick tissue blocks from 2D histological sections. The algorithm completes a high‐content (multiscale, multifeature) imaging system for simultaneous morphological and molecular analysis of thick tissue samples. This computer‐based system integrates image acquisition, annotation, registration, and three‐dimensional reconstruction. We present an experimental validation of this tool using both synthetic and real data. In particular, we present the 3D reconstruction of an entire mouse mammary gland and demonstrate the integration of high‐resolution molecular data. Microsc. Res. Tech. 73:1019–1029, 2010. © 2010 Wiley‐Liss, Inc.",33dreofhiseaptomaglti,426.0,47.0,19.0
46,3D Reconstruction,13.0,3d reconstruction of underwater structures,5.0,201.0,1.0,184.0,3.0,2.8,139.5,47,https://upcommons.upc.edu/bitstream/handle/2117/12344/borrar%20Ila%20drac2.pdf%3Bjsessionid%3D9D5CB5A1F18BCC48FA7022798B297F53?sequence%3D1,"Environmental change is a growing international concern, calling for the regular monitoring, studying and preserving of detailed information about the evolution of underwater ecosystems. For example, fragile coral reefs are exposed to various sources of hazards and potential destruction, and need close observation. Computer vision offers promising technologies to build 3D models of an environment from two-dimensional images. The state of the art techniques have enabled high-quality digital reconstruction of large-scale structures, e.g., buildings and urban environments, but only sparse representations or dense reconstruction of small objects have been obtained from underwater video and still imagery. The application of standard 3D reconstruction methods to challenging underwater environments typically produces unsatisfactory results. Accurate, full camera trajectories are needed to serve as the basis for dense 3D reconstruction. A highly accurate sparse 3D reconstruction is the ideal foundation on which to base subsequent dense reconstruction algorithms. In our application the models are constructed from synchronized high definition videos collected using a wide baseline stereo rig. The rig can be hand-held, attached to a boat, or even to an autonomous underwater vehicle. We solve this problem by employing a smoothing and mapping toolkit developed in our lab specifically for this type of application. The result of our technique is a highly accurate sparse 3D reconstruction of underwater structures such as corals.",33dreofunst,85.0,18.0,4.0
47,3D Reconstruction,11.0,3d reconstruction using an n-layer heightmap,5.0,201.0,1.0,187.0,3.0,2.8,139.8,48,http://www-oldurls.inf.ethz.ch/personal/pomarc/pubs/GallupDAGM10.pdf,"We present a novel method for 3D reconstruction of urban scenes extending a recently introduced heightmap model. Our model has several advantages for 3D modeling of urban scenes: it naturally enforces vertical surfaces, has no holes, leads to an efficient algorithm, and is compact in size. We remove the major limitation of the heightmap by enabling modeling of overhanging structures. Our method is based on an an n-layer heightmap with each layer representing a surface between full and empty space. The configuration of layers can be computed optimally using a dynamic programming method. Our cost function is derived from probabilistic occupancy, and incorporates the Bayesian Information Criterion (BIC) for selecting the number of layers to use at each pixel. 3D surface models are extracted from the heightmap. We show results from a variety of datasets including Internet photo collections. Our method runs on the GPU and the complete system processes video at 13 Hz.",33dreusann-he,57.0,23.0,1.0
48,3D Reconstruction,199.0,mixed reality remote collaboration combining 360 video and 3d reconstruction,3.0,201.0,1.0,7.0,5.0,2.8,142.2,49,http://library.usc.edu.ph/ACM/CHI2019/1proc/paper201.pdf,"Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement.",3mirerecoco36vian3dre,47.0,43.0,2.0
49,3D Reconstruction,169.0,im2avatar: colorful 3d reconstruction from a single image,3.0,82.0,4.0,201.0,1.0,2.8,143.8,50,https://arxiv.org/pdf/1804.06375,"The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 20,240 clean and realistic synthetic images of 5,000 different rooms. There are 9,992 unique detailed 3D instances of furniture with high-resolution textures. Experienced designers developed the room scenes, and the 3D CAD shapes in the scene are used for industrial production. Given the well-organized 3D-FUTURE, we provide baseline experiments on several widely studied tasks, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, and texture recovery for 3D shapes, to facilitate related future researches on our database.",3imco3drefrasiim,20.0,27.0,2.0
50,3D Reconstruction,401.0,structure-from-motion revisited,1.0,1.0,5.0,201.0,1.0,2.6,181.0,51,http://arxiv.org/pdf/astro-ph/0412218v1,"Comments on the article ""Pulsar dynamics: magnetic dipole model revisited"".",3stre,1358.0,68.0,220.0
51,3D Reconstruction,401.0,large-scale 3d shape reconstruction and segmentation from shapenet core55,1.0,2.0,5.0,201.0,1.0,2.6,181.4,52,http://arxiv.org/pdf/1710.06104v2,"We introduce a large-scale 3D shape understanding benchmark using data and annotation from ShapeNet 3D object database. The benchmark consists of two tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view images. Ten teams have participated in the challenge and the best performing teams have outperformed state-of-the-art approaches on both tasks. A few novel deep learning architectures have been proposed on various 3D representations on both tasks. We report the techniques used by each team and the corresponding performances. In addition, we summarize the major discoveries from the reported results and possible trends for the future work in the field.",3la3dshreansefrshco,48.0,22.0,3.0
52,3D Reconstruction,401.0,"3d dynamic scene graphs: actionable spatial perception with places, objects, and humans",1.0,7.0,5.0,201.0,1.0,2.6,183.4,53,http://arxiv.org/pdf/2002.06289v2,"We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g. objects, walls, rooms), and edges represent relations (e.g. inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g. humans, robots), and to include actionable information that supports planning and decision-making (e.g. spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first fully automatic Spatial PerceptIon eNgine(SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g. places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction. A video abstract is available at https://youtu.be/SWbofjhyPzI",33ddyscgracsppewiplobanhu,37.0,125.0,3.0
53,3D Reconstruction,401.0,incremental visual-inertial 3d mesh generation with structural regularities,1.0,9.0,5.0,201.0,1.0,2.6,184.2,54,http://arxiv.org/pdf/1903.01067v2,"Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud representation of the scene that does not model the topology of the environment. A 3D mesh instead offers a richer, yet lightweight, model. Nevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks triangulated by a VIO algorithm often results in a mesh that does not fit the real scene. In order to regularize the mesh, previous approaches decouple state estimation from the 3D mesh regularization step, and either limit the 3D mesh to the current frame or let the mesh grow indefinitely. We propose instead to tightly couple mesh regularization and state estimation by detecting and enforcing structural regularities in a novel factor-graph formulation. We also propose to incrementally build the mesh by restricting its extent to the time-horizon of the VIO optimization; the resulting 3D mesh covers a larger portion of the scene than a per-frame approach while its memory usage and computational complexity remain bounded. We show that our approach successfully regularizes the mesh, while improving localization accuracy, when structural regularities are present, and remains operational in scenes without regularities.",3invi3dmegewistre,18.0,28.0,2.0
54,3D Reconstruction,401.0,"expressive body capture: 3d hands, face, and body from a single image",1.0,10.0,5.0,201.0,1.0,2.6,184.6,55,http://arxiv.org/pdf/2105.05301v2,"Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First, existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence. All part experts can contribute to the whole, using SMPL-X's shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer ""gendered"" 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates more accurate whole-body shape and detailed face shape than the state of the art. Models and code are available at https://pixie.is.tue.mpg.de.",3exboca3dhafaanbofrasiim,296.0,84.0,56.0
55,3D Reconstruction,401.0,superhuman accuracy on the snemi3d connectomics challenge,1.0,11.0,5.0,201.0,1.0,2.6,185.0,56,http://arxiv.org/pdf/1706.00120v1,"For the past decade, convolutional networks have been used for 3D reconstruction of neurons from electron microscopic (EM) brain images. Recent years have seen great improvements in accuracy, as evidenced by submissions to the SNEMI3D benchmark challenge. Here we report the first submission to surpass the estimate of human accuracy provided by the SNEMI3D leaderboard. A variant of 3D U-Net is trained on a primary task of predicting affinities between nearest neighbor voxels, and an auxiliary task of predicting long-range affinities. The training data is augmented by simulated image defects. The nearest neighbor affinities are used to create an oversegmentation, and then supervoxels are greedily agglomerated based on mean affinity. The resulting SNEMI3D score exceeds the estimate of human accuracy by a large margin. While one should be cautious about extrapolating from the SNEMI3D benchmark to real-world accuracy of large-scale neural circuit reconstruction, our result inspires optimism that the goal of full automation may be realizable in the future.",3suaconthsncoch,124.0,37.0,22.0
56,3D Reconstruction,401.0,unpaired motion style transfer from video to animation,1.0,12.0,5.0,201.0,1.0,2.6,185.4,57,http://arxiv.org/abs/2005.05751v1,"Transferring the motion style from one animation clip to another, while preserving the motion content of the latter, has been a long-standing problem in character animation. Most existing data-driven approaches are supervised and rely on paired data, where motions with the same content are performed in different styles. In addition, these approaches are limited to transfer of styles that were seen during training. In this paper, we present a novel data-driven framework for motion style transfer, which learns from an unpaired collection of motions with style labels, and enables transferring motion styles not observed during training. Furthermore, our framework is able to extract motion styles directly from videos, bypassing 3D reconstruction, and apply them to the 3D input motion. Our style transfer network encodes motions into two latent codes, for content and for style, each of which plays a different role in the decoding (synthesis) process. While the content code is decoded into the output motion by several temporal convolutional layers, the style code modifies deep features via temporally invariant adaptive instance normalization (AdaIN). Moreover, while the content code is encoded from 3D joint rotations, we learn a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos. Our results are comparable to the state-of-the-art, despite not requiring paired training data, and outperform other methods when transferring previously unseen styles. To our knowledge, we are the first to demonstrate style transfer directly from videos to 3D animations - an ability which enables one to extend the set of style examples far beyond motions captured by MoCap systems.",3unmosttrfrvitoan,21.0,60.0,5.0
57,3D Reconstruction,401.0,learning continuous image representation with local implicit image function,1.0,14.0,5.0,201.0,1.0,2.6,186.2,58,http://arxiv.org/pdf/2012.09161v2,"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.",3lecoimrewiloimimfu,27.0,55.0,6.0
58,3D Reconstruction,401.0,learning implicit surface light fields,1.0,15.0,5.0,201.0,1.0,2.6,186.6,59,http://arxiv.org/pdf/2003.12406v1,"Implicit representations of 3D objects have recently achieved impressive results on learning-based 3D reconstruction tasks. While existing works use simple texture models to represent object appearance, photo-realistic image synthesis requires reasoning about the complex interplay of light, geometry and surface properties. In this work, we propose a novel implicit representation for capturing the visual appearance of an object in terms of its surface light field. In contrast to existing representations, our implicit model represents surface light fields in a continuous fashion and independent of the geometry. Moreover, we condition the surface light field with respect to the location and color of a small light source. Compared to traditional surface light field models, this allows us to manipulate the light source and relight the object using environment maps. We further demonstrate the capabilities of our model to predict the visual appearance of an unseen object from a single real RGB image and corresponding 3D shape information. As evidenced by our experiments, our model is able to infer rich visual appearance including shadows and specular reflections. Finally, we show that the proposed representation can be embedded into a variational auto-encoder for generating novel appearances that conform to the specified illumination conditions.",3leimsulifi,19.0,74.0,1.0
59,3D Reconstruction,401.0,convolutional occupancy networks,1.0,16.0,5.0,201.0,1.0,2.6,187.0,60,http://arxiv.org/pdf/2003.04618v2,"Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.",3coocne,119.0,56.0,16.0
60,3D Reconstruction,401.0,geo-supervised visual depth prediction,1.0,21.0,5.0,201.0,1.0,2.6,189.0,61,http://arxiv.org/pdf/1810.01011v1,"Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.",3gevidepr,34.0,56.0,2.0
61,3D Reconstruction,401.0,on learning 3d face morphable model from in-the-wild images,1.0,26.0,5.0,201.0,1.0,2.6,191.0,62,http://arxiv.org/abs/1808.09560v2,"As a classic statistical model of 3D facial shape and albedo, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of 3D face scans with associated well-controlled 2D face images, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of in-the-wild face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, lighting, shape and albedo parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape and albedo, respectively. With the projection parameter, lighting, 3D shape, and albedo, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment, 3D reconstruction, and face editing.",3onle3dfamomofrinim,67.0,89.0,8.0
62,3D Reconstruction,401.0,nonlinear 3d face morphable model,1.0,27.0,5.0,201.0,1.0,2.6,191.4,63,http://arxiv.org/pdf/1804.03786v3,"As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction.",3no3dfamomo,182.0,47.0,17.0
63,3D Reconstruction,401.0,common objects in 3d: large-scale learning and evaluation of real-life 3d category reconstruction,1.0,29.0,5.0,201.0,1.0,2.6,192.2,64,http://arxiv.org/pdf/2109.00512v1,"Traditional approaches for learning 3D object categories have been predominantly trained and evaluated on synthetic datasets due to the unavailability of real 3D-annotated category-centric data. Our main goal is to facilitate advances in this field by collecting real-world data in a magnitude similar to the existing synthetic counterparts. The principal contribution of this work is thus a large-scale dataset, called Common Objects in 3D, with real multi-view images of object categories annotated with camera poses and ground truth 3D point clouds. The dataset contains a total of 1.5 million frames from nearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such, it is significantly larger than alternatives both in terms of the number of categories and objects. We exploit this new dataset to conduct one of the first large-scale ""in-the-wild"" evaluations of several new-view-synthesis and category-centric 3D reconstruction methods. Finally, we contribute NerFormer - a novel neural rendering method that leverages the powerful Transformer to reconstruct an object given a small number of its views. The CO3D dataset is available at https://github.com/facebookresearch/co3d .",3coobin3dlaleanevofre3dcare,2.0,66.0,0.0
64,3D Reconstruction,401.0,pixel-perfect structure-from-motion with featuremetric refinement,1.0,34.0,5.0,201.0,1.0,2.6,194.2,65,http://arxiv.org/pdf/2108.08291v1,"Finding local features that are repeatable across multiple views is a cornerstone of sparse 3D reconstruction. The classical image matching paradigm detects keypoints per-image once and for all, which can yield poorly-localized features and propagate large errors to the final geometry. In this paper, we refine two key steps of structure-from-motion by a direct alignment of low-level image information from multiple views: we first adjust the initial keypoint locations prior to any geometric estimation, and subsequently refine points and camera poses as a post-processing. This refinement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network. This significantly improves the accuracy of camera poses and scene geometry for a wide range of keypoint detectors, challenging viewing conditions, and off-the-shelf deep features. Our system easily scales to large image collections, enabling pixel-perfect crowd-sourced localization at scale. Our code is publicly available at https://github.com/cvg/pixel-perfect-sfm as an add-on to the popular SfM software COLMAP.",3pistwifere,0.0,104.0,0.0
65,3D Reconstruction,401.0,graph-based parallel large scale structure from motion,1.0,37.0,5.0,201.0,1.0,2.6,195.4,66,http://arxiv.org/pdf/2103.09523v1,"SLAM allows a robot to continuously perceive the surrounding environment and locate itself correctly. However, its high computational complexity limits the practical use of SLAM in resource-constrained computing platforms. We propose a resource-efficient FPGA-based accelerator and apply it to two major SLAM methods: particle filter-based and graph-based SLAM. We compare their performances in terms of the latency, throughput gain, and memory consumption, considering their algorithmic characteristics, and confirm that the accelerator removes the bottleneck without compromising the accuracy in both methods.",3grpalascstfrmo,5.0,62.0,0.0
66,3D Reconstruction,401.0,learning character-agnostic motion for motion retargeting in 2d,1.0,38.0,5.0,201.0,1.0,2.6,195.8,67,http://arxiv.org/abs/1905.01680v1,"Analyzing human motion is a challenging task with a wide variety of applications in computer vision and in graphics. One such application, of particular importance in computer animation, is the retargeting of motion from one performer to another. While humans move in three dimensions, the vast majority of human motions are captured using video, requiring 2D-to-3D pose and camera recovery, before existing retargeting approaches may be applied. In this paper, we present a new method for retargeting video-captured motion between different human performers, without the need to explicitly reconstruct 3D poses and/or camera parameters. In order to achieve our goal, we learn to extract, directly from a video, a high-level latent motion representation, which is invariant to the skeleton geometry and the camera view. Our key idea is to train a deep neural network to decompose temporal sequences of 2D poses into three components: motion, skeleton, and camera view-angle. Having extracted such a representation, we are able to re-combine motion with novel skeletons and camera views, and decode a retargeted temporal sequence, which we compare to a ground truth from a synthetic dataset. We demonstrate that our framework can be used to robustly extract human motion from videos, bypassing 3D reconstruction, and outperforming existing retargeting methods, when applied to videos in-the-wild. It also enables additional applications, such as performance cloning, video-driven cartoons, and motion retrieval.",3lechmofomorein2d,40.0,55.0,4.0
67,3D Reconstruction,401.0,single-image piece-wise planar 3d reconstruction via associative embedding,1.0,40.0,5.0,201.0,1.0,2.6,196.6,68,http://arxiv.org/pdf/1902.09777v3,"Single-image piece-wise planar 3D reconstruction aims to simultaneously segment plane instances and recover 3D plane parameters from an image. Most recent approaches leverage convolutional neural networks (CNNs) and achieve promising results. However, these methods are limited to detecting a fixed number of planes with certain learned order. To tackle this problem, we propose a novel two-stage method based on associative embedding, inspired by its recent success in instance segmentation. In the first stage, we train a CNN to map each pixel to an embedding space where pixels from the same plane instance have similar embeddings. Then, the plane instances are obtained by grouping the embedding vectors in planar regions via an efficient mean shift clustering algorithm. In the second stage, we estimate the parameter for each plane instance by considering both pixel-level and instance-level consistencies. With the proposed method, we are able to detect an arbitrary number of planes. Extensive experiments on public datasets validate the effectiveness and efficiency of our method. Furthermore, our method runs at 30 fps at the testing time, thus could facilitate many real-time applications such as visual SLAM and human-robot interaction. Code is available at https://github.com/svip-lab/PlanarReconstruction.",3sipipl3dreviasem,32.0,37.0,10.0
68,3D Reconstruction,114.0,point cloud noise and outlier removal for image-based 3d reconstruction,3.0,201.0,1.0,42.0,4.0,2.5,127.2,69,http://disneyresearch.s3.amazonaws.com/wp-content/uploads/20161014174414/Point-Cloud-Noise-and-Outlier-Removal-for-Image-Based-3D-Reconstruction-Paper.pdf,"Point sets generated by image-based 3D reconstruction techniques are often much noisier than those obtained using active techniques like laser scanning. Therefore, they pose greater challenges to the subsequent surface reconstruction (meshing) stage. We present a simple and effective method for removing noise and outliers from such point sets. Our algorithm uses the input images and corresponding depth maps to remove pixels which are geometrically or photometrically inconsistent with the colored surface implied by the input. This allows standard surface reconstruction methods (such as Poisson surface reconstruction) to perform less smoothing and thus achieve higher quality surfaces with more features. Our algorithm is efficient, easy to implement, and robust to varying amounts of noise. We demonstrate the benefits of our algorithm in combination with a variety of state-of-the-art depth and surface reconstruction methods.",3poclnoanourefoim3dre,59.0,54.0,3.0
69,3D Reconstruction,108.0,refractive 3d reconstruction on underwater images,3.0,201.0,1.0,57.0,4.0,2.5,129.9,70,https://openreview.net/pdf?id=YfefLcNpiW,"Highlights 
 
• Complete 3D reconstruction system from images and videos. 
• Refractive Structure from Motion for flat port underwater cameras. 
• Eliminates systematic modeling error caused by using perspective camera model. 
 
Cameras can be considered measurement devices complementary to acoustic sensors when it comes to surveying marine environments. When calibrated and used correctly, these visual sensors are well-suited for automated detection, quantification, mapping, and monitoring applications and when aiming at high-accuracy 3D models or change detection. In underwater scenarios, cameras are often set up in pressure housings with a flat glass window, a flat port, which allows them to observe the environment. In this contribution, a geometric model for image formation is discussed that explicitly considers refraction at the interface under realistic assumptions like a slightly misaligned camera (w.r.t. the glass normal) and thick glass ports as common for deep sea applications. Then, starting from camera calibration, a complete, fully automated 3D reconstruction system is discussed that takes an image sequence and produces a 3D model. Newly derived refractive estimators for sparse two-view geometry, pose estimation, bundle adjustment, and dense depth estimation are discussed and evaluated in detail.",3re3dreonunim,29.0,59.0,2.0
70,3D Reconstruction,124.0,image-based 3d reconstruction for posthurricane residential building damage assessment,3.0,201.0,1.0,47.0,4.0,2.5,131.7,71,http://arxiv.org/pdf/2010.15303v1,"AbstractStreet-level storm damage photos are an essential type of data used in postdisaster damage assessment. However, the existing approaches only leverage such data in a two-dimensional context. The research reported in this paper leveraged the photos collected during Hurricane Sandy to explore image-based three-dimensional (3D) reconstruction for posthurricane residential building damage assessment. Specifically, two commonly used image reconstruction pipelines are employed to reconstruct several impacted residential buildings to evaluate their performances regarding key measurement needs in posthurricane damage assessment. Damage data recorded by a mobile light detection and ranging (LIDAR) system were used as the ground truth for performance evaluation. The study results suggest that image-based 3D reconstruction can adequately support hurricane damage assessment needs for residential buildings. However, for damage assessment tasks which rely on very accurate estimate (generally <1  cm) of displacem...",3im3drefoporebudaas,40.0,51.0,1.0
71,3D Reconstruction,87.0,3d reconstruction of multiple stained histology images,4.0,201.0,1.0,112.0,3.0,2.5,140.1,72,http://arxiv.org/pdf/2104.14873v3,"Context: Three dimensional (3D) tissue reconstructions from the histology images with different stains allows the spatial alignment of structural and functional elements highlighted by different stains for quantitative study of many physiological and pathological phenomena. This has significant potential to improve the understanding of the growth patterns and the spatial arrangement of diseased cells, and enhance the study of biomechanical behavior of the tissue structures towards better treatments (e.g. tissue-engineering applications). Methods: This paper evaluates three strategies for 3D reconstruction from sets of two dimensional (2D) histological sections with different stains, by combining methods of 2D multi-stain registration and 3D volumetric reconstruction from same stain sections. Setting and Design: The different strategies have been evaluated on two liver specimens (80 sections in total) stained with Hematoxylin and Eosin (H and E), Sirius Red, and Cytokeratin (CK) 7. Results and Conclusion: A strategy of using multi-stain registration to align images of a second stain to a volume reconstructed by same-stain registration results in the lowest overall error, although an interlaced image registration approach may be more robust to poor section quality.",33dreofmusthiim,68.0,7.0,2.0
72,3D Reconstruction,70.0,a role of three-dimensional (3d)-reconstruction in the classification of lung adenocarcinoma,4.0,201.0,1.0,132.0,3.0,2.5,141.0,73,https://downloads.hindawi.com/journals/acp/2012/684751.pdf,"Background: Three-dimensional (3D)-reconstruction from paraffin embedded sections has been considered laborious and time-consuming. However, the high-resolution images of large object areas and different fields of view obtained by 3D-reconstruction make one wonder whether it can add a new insight into lung adenocarcinoma, the most frequent histology type of lung cancer characterized by its morphological heterogeneity. Objective: In this work, we tested whether an automated tissue sectioning machine and slide scanning system could generate precise 3D-reconstruction of microanatomy of the lung and help us better understand and define histologic subtypes of lung adenocarcinoma. Methods: Four formalin-fixed human lung adenocarcinoma resections were studied. Paraffin embedded tissues were sectioned with Kurabo-Automated tissue sectioning machine and serial sections were automatically stained and scanned with a Whole Slide Imaging system. The resulting stacks of images were 3D reconstructed by Pannoramic Viewer software. Results: Two of the four specimens contained islands of tumor cells detached in alveolar spaces that had not been described in any of the existing adenocarcinoma classifications. 3D-reconstruction revealed the details of spatial distribution and structural interaction of the tumor that could hardly be observed by 2D light microscopy studies. The islands of tumor cells extended into a deeper aspect of the tissue, and were interconnected with each other and with the main tumor with a solid pattern that was surrounded by the islands. The finding raises the question whether the islands of tumor cells should be classified into a solid pattern in the current classification. Conclusion: The combination of new technologies enabled us to build an effective 3D-reconstruction of resected lung adenocarcinomas. 3D-reconstruction may help us refine the classification of lung adenocarcinoma by adding detailed spatial/structural information to 2D light microscopy evaluation.",3aroofth(3inthclofluad,33.0,9.0,1.0
73,3D Reconstruction,105.0,improving sparse 3d models for man-made environments using line-based 3d reconstruction,3.0,201.0,1.0,97.0,4.0,2.5,141.0,74,https://www.researchgate.net/profile/Manuel-Hofer-4/publication/267763566_Improving_Sparse_3D_Models_for_Man-Made_Environments_Using_Line-Based_3D_Reconstruction/links/545a0f7d0cf2bccc4912feb0/Improving-Sparse-3D-Models-for-Man-Made-Environments-Using-Line-Based-3D-Reconstruction.pdf,"Traditional Structure-from-Motion (SfM) approaches work well for richly textured scenes with a high number of distinctive feature points. Since man-made environments often contain texture less objects, the resulting point cloud suffers from a low density in corresponding scene parts. The missing 3D information heavily affects all kinds of subsequent post-processing tasks (e.g. Meshing), and significantly decreases the visual appearance of the resulting 3D model. We propose a novel 3D reconstruction approach, which uses the output of conventional SfM pipelines to generate additional complementary 3D information, by exploiting line segments. We use appearance-less epipolar guided line matching to create a potentially large set of 3D line hypotheses, which are then verified using a global graph clustering procedure. We show that our proposed method outperforms the current state-of-the-art in terms of runtime and accuracy, as well as visual appearance of the resulting reconstructions.",3imsp3dmofomaenusli3dre,36.0,41.0,3.0
74,3D Reconstruction,46.0,the perivascular astroglial sheath provides a complete covering of the brain microvessels: an electron microscopic 3d reconstruction,4.0,201.0,1.0,162.0,3.0,2.5,142.8,75,https://synapseweb.clm.utexas.edu/sites/default/files/synapseweb/files/2010gliamathiisenottersentheperivascastrosheath.pdf,"The unravelling of the polarized distribution of AQP4 in perivascular astrocytic endfeet has revitalized the interest in the role of astrocytes in controlling water and ion exchange at the brain–blood interface. The importance of the endfeet is based on the premise that they constitute a complete coverage of the vessel wall. Despite a number of studies based on different microscopic techniques this question has yet to be resolved. We have made an electron microscopic 3D reconstruction of perivascular endfeet in CA1 (stratum moleculare) of rat hippocampus. The endfeet interdigitate and overlap, leaving no slits between them. Only in a few sites do processes—tentatively classified as processes of microglia—extend through the perivascular glial sheath to establish direct contact with the endothelial basal lamina. In contrast to the endfoot covering of the endothelial tube, the endfoot covering of the pericyte is incomplete, allowing neuropil elements to touch the basal lamina that enwraps this type of cell. The 3D reconstruction also revealed large bundles of mitochondria in the endfoot processes that came in close apposition to the perivascular endfoot membrane. Our data support the idea that in pathophysiological conditions, the perivascular astrocytic covering may control the exchange of water and solutes between blood and brain and that free diffusion is limited to narrow clefts between overlapping endfeet. © 2010 Wiley‐Liss, Inc.",3thpeasshpracocoofthbrmianelmi3dre,572.0,89.0,19.0
75,3D Reconstruction,159.0,a laser line auto-scanning system for underwater 3d reconstruction,3.0,201.0,1.0,56.0,4.0,2.5,144.9,76,https://www.mdpi.com/1424-8220/16/9/1534/pdf,"In this study, a laser line auto-scanning system was designed to perform underwater close-range 3D reconstructions with high accuracy and resolution. The system changes the laser plane direction with a galvanometer to perform automatic scanning and obtain continuous laser strips for underwater 3D reconstruction. The system parameters were calibrated with the homography constraints between the target plane and image plane. A cost function was defined to optimize the galvanometer’s rotating axis equation. Compensation was carried out for the refraction of the incident and emitted light at the interface. The accuracy and the spatial measurement capability of the system were tested and analyzed with standard balls under laboratory underwater conditions, and the 3D surface reconstruction for a sealing cover of an underwater instrument was proved to be satisfactory.",3alaliausyfoun3dre,28.0,55.0,1.0
76,3D Reconstruction,100.0,skeleton-based 3d reconstruction of as-built pipelines from laser-scan data,4.0,201.0,1.0,120.0,3.0,2.5,146.4,77,https://www.academia.edu/download/34508369/9780784412343.0031.pdf,"Abstract There has been a growing demand for an as-built 3D pipeline model. Although several studies on automation of the process of reconstruction of as-built 3D pipelines have been carried out, previous approaches have been limited to the generation of only a portion of an entire 3D pipeline. The aim of this study was to propose an automated approach to the generation of as-built 3D pipeline models of entire pipelines composed of straight pipes, elbows, and tee pipes from laser-scan data. First, the skeletons of individual pipelines are extracted. Then the extracted skeletons are segmented into their individual components, and a set of parameters for them are calculated. The experimental results show that the proposed approach is robust to incompleteness of the laser-scan data, as well as to noise and to density variations in the data. As a result, the proposed method enables the generation of reliable as-built 3D pipeline models.",3sk3dreofaspifrlada,81.0,41.0,0.0
77,3D Reconstruction,88.0,fast and robust laser stripe extraction for 3d reconstruction in industrial environments,4.0,201.0,1.0,181.0,3.0,2.5,161.10000000000002,78,https://www.academia.edu/download/50055886/s00138-010-0288-620161102-6716-haozm.pdf,"The use of 3D reconstruction based on active laser triangulation techniques is very complex in industrial environments. The main problem is that most of these techniques are based on laser stripe extraction methods which are highly sensitive to noise, which is virtually inevitable in these conditions. In industrial environments, variable luminance, reflections which show up in the images as noise, and uneven surfaces are common. These factors modify the shape of the laser profile. This work proposes a fast, accurate, and robust method to extract laser stripes in industrial environments. Specific procedures are proposed to extract the laser stripe projected on the background, using a boundary linking process, and on the foreground, using an improved Split-and-Merge approach with different approximation functions including linear, quadratic, and Akima splines. Also, a novel procedure to automatically define the region of interest in the image is proposed. The real-time performance of the proposed method is analyzed by measuring the time taken by the tasks involved in their application. Finally, the proposed extraction method is applied to two real applications: 3D reconstruction of steel strips and weld seam tracking.",3faanrolastexfo3dreininen,104.0,31.0,4.0
78,3D Reconstruction,178.0,piecewise‐planar 3d reconstruction with edge and corner regularization,3.0,201.0,1.0,95.0,4.0,2.5,162.3,79,https://www.boulch.eu/files/2014_sgp_boulch.pdf,"This paper presents a method for the 3D reconstruction of a piecewise‐planar surface from range images, typically laser scans with millions of points. The reconstructed surface is a watertight polygonal mesh that conforms to observations at a given scale in the visible planar parts of the scene, and that is plausible in hidden parts. We formulate surface reconstruction as a discrete optimization problem based on detected and hypothesized planes. One of our major contributions, besides a treatment of data anisotropy and novel surface hypotheses, is a regularization of the reconstructed surface w.r.t. the length of edges and the number of corners. Compared to classical area‐based regularization, it better captures surface complexity and is therefore better suited for man‐made environments, such as buildings. To handle the underlying higher‐order potentials, that are problematic for MRF optimizers, we formulate minimization as a sparse mixed‐integer linear programming problem and obtain an approximate solution using a simple relaxation. Experiments show that it is fast and reaches near‐optimal solutions.",3pi3drewiedancore,48.0,29.0,3.0
79,3D Reconstruction,401.0,3d object reconstruction from hand-object interactions,1.0,155.0,3.0,149.0,3.0,2.4000000000000004,227.0,80,http://arxiv.org/pdf/1712.01359v1,"Recent advances have enabled 3d object reconstruction approaches using a single off-the-shelf RGB-D camera. Although these approaches are successful for a wide range of object classes, they rely on stable and distinctive geometric or texture features. Many objects like mechanical parts, toys, household or decorative articles, however, are textureless and characterized by minimalistic shapes that are simple and symmetric. Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features. In this work, we show that extracting 3d hand motion for in-hand scanning effectively facilitates the reconstruction of even featureless and highly symmetric objects and we present an approach that fuses the rich additional information of hands into a 3d reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.",33dobrefrhain,47.0,45.0,5.0
80,3D Reconstruction,125.0,learning single-view 3d reconstruction with limited pose supervision,3.0,113.0,3.0,201.0,1.0,2.4,143.0,81,https://openaccess.thecvf.com/content_ECCV_2018/papers/Guandao_Yang_A_Unified_Framework_ECCV_2018_paper.pdf,"This paper presents a method to reconstruct dense semantic trajectory stream of human interactions in 3D from synchronized multiple videos. The interactions inherently introduce self-occlusion and illumination/appearance/shape changes, resulting in highly fragmented trajectory reconstruction with noisy and coarse semantic labels. Our conjecture is that among many views, there exists a set of views that can confidently recognize the visual semantic label of a 3D trajectory. We introduce a new representation called 3D semantic map---a probability distribution over the semantic labels per trajectory. We construct the 3D semantic map by reasoning about visibility and 2D recognition confidence based on view-pooling, i.e., finding the view that best represents the semantics of the trajectory. Using the 3D semantic map, we precisely infer all trajectory labels jointly by considering the affinity between long range trajectories via estimating their local rigid transformations. This inference quantitatively outperforms the baseline approaches in terms of predictive validity, representation robustness, and affinity effectiveness. We demonstrate that our algorithm can robustly compute the semantic labels of a large scale trajectory set involving real-world human interactions with object, scenes, and people.",3lesi3drewiliposu,31.0,27.0,4.0
81,3D Reconstruction,111.0,carfusion: combining point tracking and part detection for dynamic 3d reconstruction of vehicles,3.0,160.0,3.0,201.0,1.0,2.4,157.6,82,http://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf,"Despite significant research in the area, reconstruction of multiple dynamic rigid objects (eg. vehicles) observed from wide-baseline, uncalibrated and unsynchronized cameras, remains hard. On one hand, feature tracking works well within each view but is hard to correspond across multiple cameras with limited overlap infields of view or due to occlusions. On the other hand, advances in deep learning have resulted in strong detectors that work across different viewpoints but are still not precise enough for triangulation-based reconstruction. In this work, we develop a framework to fuse both the single-view feature tracks and multiview detected part locations to significantly improve the detection, localization and reconstruction of moving vehicles, even in the presence of strong occlusions. We demonstrate our framework at a busy traffic intersection by reconstructing over 62 vehicles passing within a 3-minute window. We evaluate the different components within our framework and compare to alternate approaches such as reconstruction using tracking-by-detection.",3cacopotranpadefody3dreofve,30.0,54.0,2.0
82,3D Reconstruction,127.0,road surface 3d reconstruction based on dense subpixel disparity map estimation,3.0,175.0,3.0,201.0,1.0,2.4,168.39999999999998,83,https://arxiv.org/pdf/1807.01874,"Various 3D reconstruction methods have enabled civil engineers to detect damage on a road surface. To achieve the millimeter accuracy required for road condition assessment, a disparity map with subpixel resolution needs to be used. However, none of the existing stereo matching algorithms are specially suitable for the reconstruction of the road surface. Hence in this paper, we propose a novel dense subpixel disparity estimation algorithm with high computational efficiency and robustness. This is achieved by first transforming the perspective view of the target frame into the reference view, which not only increases the accuracy of the block matching for the road surface but also improves the processing speed. The disparities are then estimated iteratively using our previously published algorithm, where the search range is propagated from three estimated neighboring disparities. Since the search range is obtained from the previous iteration, errors may occur when the propagated search range is not sufficient. Therefore, a correlation maxima verification is performed to rectify this issue, and the subpixel resolution is achieved by conducting a parabola interpolation enhancement. Furthermore, a novel disparity global refinement approach developed from the Markov random fields and fast bilateral stereo is introduced to further improve the accuracy of the estimated disparity map, where disparities are updated iteratively by minimizing the energy function that is related to their interpolated correlation polynomials. The algorithm is implemented in C language with a near real-time performance. The experimental results illustrate that the absolute error of the reconstruction varies from 0.1 to 3 mm.",3rosu3drebaondesudimaes,60.0,48.0,8.0
83,3D Reconstruction,172.0,amodal 3d reconstruction for robotic manipulation via stability and connectivity,3.0,159.0,3.0,201.0,1.0,2.4,175.5,84,https://proceedings.mlr.press/v155/agnew21a/agnew21a.pdf,"Learning-based 3D object reconstruction enables single- or few-shot estimation of 3D object models. For robotics, this holds the potential to allow model-based methods to rapidly adapt to novel objects and scenes. Existing 3D reconstruction techniques optimize for visual reconstruction fidelity, typically measured by chamfer distance or voxel IOU. We find that when applied to realistic, cluttered robotics environments, these systems produce reconstructions with low physical realism, resulting in poor task performance when used for model-based control. We propose ARM, an amodal 3D reconstruction system that introduces (1) a stability prior over object shapes, (2) a connectivity prior, and (3) a multi-channel input representation that allows for reasoning over relationships between groups of objects. By using these priors over the physical properties of objects, our system improves reconstruction quality not just by standard visual metrics, but also performance of model-based control on a variety of robotics manipulation tasks in challenging, cluttered environments. Code is available at this http URL.",3am3dreforomavistanco,6.0,39.0,0.0
84,3D Reconstruction,2.0,web-based 3d reconstruction service,5.0,201.0,1.0,201.0,1.0,2.2,141.3,85,https://homes.esat.kuleuven.be/~visit3d/webservice/v2/webservice.pdf,"Recovering the 3D shape of transparent objects using a small number of unconstrained natural images is an ill-posed problem. Complex light paths induced by refraction and reflection have prevented both traditional and deep multiview stereo from solving this challenge. We propose a physically-based network to recover 3D shape of transparent objects using a few images acquired with a mobile phone camera, under a known but arbitrary environment map. Our novel contributions include a normal representation that enables the network to model complex light transport through local computation, a rendering layer that models refractions and reflections, a cost volume specifically designed for normal refinement of transparent shapes and a feature mapping based on predicted normals for 3D point cloud reconstruction. We render a synthetic dataset to encourage the model to learn refractive light transport across different views. Our experiments show successful recovery of high-quality 3D geometry for complex transparent shapes using as few as 5-12 natural images. Code and data are publicly released.",3we3drese,259.0,18.0,4.0
85,3D Reconstruction,3.0,real time localization and 3d reconstruction,5.0,201.0,1.0,201.0,1.0,2.2,141.60000000000002,86,https://hal.archives-ouvertes.fr/hal-00091145/file/Cvpr06.pdf,"In this paper we describe a method that estimates the motion of a calibrated camera (settled on an experimental vehicle) and the tridimensional geometry of the environment. The only data used is a video input. In fact, interest points are tracked and matched between frames at video rate. Robust estimates of the camera motion are computed in real-time, key-frames are selected and permit the features 3D reconstruction. The algorithm is particularly appropriate to the reconstruction of long images sequences thanks to the introduction of a fast and local bundle adjustment method that ensures both good accuracy and consistency of the estimated camera poses along the sequence. It also largely reduces computational complexity compared to a global bundle adjustment. Experiments on real data were carried out to evaluate speed and robustness of the method for a sequence of about one kilometer long. Results are also compared to the ground truth measured with a differential GPS.",3retiloan3dre,418.0,30.0,33.0
86,3D Reconstruction,6.0,detailed real-time urban 3d reconstruction from video,5.0,201.0,1.0,201.0,1.0,2.2,142.5,87,http://cvg-pub.inf.ethz.ch/WebBIB/papers/1900/002_fulltext.pdf,"Abstract
The paper presents a system for automatic, geo-registered, real-time 3D reconstruction from video of urban scenes. The system collects video streams, as well as GPS and inertia measurements in order to place the reconstructed models in geo-registered coordinates. It is designed using current state of the art real-time modules for all processing steps. It employs commodity graphics hardware and standard CPU’s to achieve real-time performance. We present the main considerations in designing the system and the steps of the processing pipeline. Our system extends existing algorithms to meet the robustness and variability necessary to operate out of the lab. To account for the large dynamic range of outdoor videos the processing pipeline estimates global camera gain changes in the feature tracking stage and efficiently compensates for these in stereo estimation without impacting the real-time performance. The required accuracy for many applications is achieved with a two-step stereo reconstruction process exploiting the redundancy across frames. We show results on real video sequences comprising hundreds of thousands of frames.
",3dereur3drefrvi,784.0,94.0,36.0
87,3D Reconstruction,8.0,efficient 3d reconstruction for face recognition,5.0,201.0,1.0,201.0,1.0,2.2,143.10000000000002,88,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.3670&rep=rep1&type=pdf,"Face recognition with variant pose, illumination and expression (PIE) is a challenging problem. In this paper, we propose an analysis-by-synthesis framework for face recognition with variant PIE. First, an efficient two-dimensional (2D)-to-three-dimensional (3D) integrated face reconstruction approach is introduced to reconstruct a personalized 3D face model from a single frontal face image with neutral expression and normal illumination. Then, realistic virtual faces with different PIE are synthesized based on the personalized 3D face to characterize the face subspace. Finally, face recognition is conducted based on these representative virtual faces. Compared with other related work, this framework has following advantages: (1) only one single frontal face is required for face recognition, which avoids the burdensome enrollment work; (2) the synthesized face samples provide the capability to conduct recognition under difficult conditions like complex PIE; and (3) compared with other 3D reconstruction approaches, our proposed 2D-to-3D integrated face reconstruction approach is fully automatic and more efficient. The extensive experimental results show that the synthesized virtual faces significantly improve the accuracy of face recognition with changing PIE.",3ef3drefofare,255.0,60.0,14.0
88,3D Reconstruction,12.0,3d-reconstruction of blood vessels by ultramicroscopy,5.0,201.0,1.0,201.0,1.0,2.2,144.3,89,http://arxiv.org/abs/1312.4027v1,"Application of computation in many fields are growing fast in last two decades. Increasing on computation performance helps researchers to understand natural phenomena in many fields of science and technology including in life sciences. Computational fluid dynamic is one of numerical methods which is very popular used to describe those phenomena. In this paper we propose moving particle semi-implicit (MPS) and molecular dynamics (MD) to describe different phenomena in blood vessel. The effect of increasing the blood pressure on vessel wall will be calculate using MD methods, while the two fluid blending dynamics will be discussed using MPS. Result from the first phenomenon shows that around 80% of constriction on blood vessel make blood vessel increase and will start to leak on vessel wall, while from the second phenomenon the result shows the visualization of two fluids mixture (drugs and blood) influenced by ratio of drugs debit to blood debit.   Keywords: molecular dynamic, blood vessel, fluid dynamic, moving particle semi implicit.",33dofblvebyul,82.0,16.0,1.0
89,3D Reconstruction,15.0,procedural 3d reconstruction of puuc buildings in xkipché.,5.0,201.0,1.0,201.0,1.0,2.2,145.2,90,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.670.9222&rep=rep1&type=pdf,"This paper examines how architectural shape grammars can be used to procedurally generate 3D reconstructions of an archaeological site. The Puuc-style buildings found in Xkipche, Mexico, were used as a test-case. We first introduce the ancient Mayan site of Xkipche and give an overview of the building types as distinguished by the archaeologists, based on excavations and surveys of the building remains at the surface. Secondly, we outline the elements of the building design that are characteristic of the Puuc architecture. For the creation of the actual building geometries, we further determine the shape grammar rules for the different architectural parts. The modeling system can then be used to reconstruct the whole site based on various GIS (Geographical Information Systems) data given as input, such as building footprints, architectural information, and elevation. The results demonstrate that our modeling system is, in contrast to traditional 3D modeling, able to efficiently construct a large number of high quality geometric models at low cost.",3pr3dreofpubuinxk,89.0,20.0,8.0
90,3D Reconstruction,17.0,the effect of overabundant projection directions on 3d reconstruction algorithms,5.0,201.0,1.0,201.0,1.0,2.2,145.8,91,https://www.academia.edu/download/48828355/The_effect_of_overabundant_projection_di20160914-16580-3g842k.pdf,"The experimental process of collecting images from macromolecules in an electron microscope is such that it does not allow for prior specification of the angular distribution of the projection images. As a consequence, an uneven distribution of projection directions may occur. Concerns have been raised recently about the behavior of 3D reconstruction algorithms for the case of unevenly distributed projections. It has been illustrated on experimental data that in the case of a heavily uneven distribution of projection directions some algorithms tend to elongate the reconstructed volumes along the overloaded direction so much as to make a quantitative biological analysis impossible. In answer to these concerns we have developed a strategy for quantitative comparison and optimization of 3D reconstruction algorithms. We apply this strategy to quantitatively analyze algebraic reconstruction techniques (ART) with blobs, simultaneous iterative reconstruction techniques (SIRT) with voxels, and weighted backprojection (WBP). We show that the elongation artifacts that had been previously reported can be strongly reduced. With our specific choices for the free parameters of the three algorithms, WBP reconstructions tend to be inferior to those obtained with either SIRT or ART and the results obtained with ART are comparable to those with SIRT, but at a very small fraction of the computational cost of SIRT.",3thefofovprdion3dreal,73.0,20.0,4.0
91,3D Reconstruction,18.0,automatic 3d reconstruction: an exploration of the state of the art,5.0,201.0,1.0,201.0,1.0,2.2,146.10000000000002,92,http://dl6.globalstf.org/index.php/joc/article/viewFile/854/2137,"Presented here in the form of case study examples are the results from a number of practical exercises to explore the state of the art of automatic 3D reconstructions. That is, deriving the underlying geometry of an object or place based only upon photographs. There is a wide range of applications for this technology; traditionally it has been used for landscape/terrain modeling, geology and by the mining industry. The interest here is in capturing geometric data in archaeology, providing a new data format suited to a richer exploration compared to the more traditional photography. Examples of the use of this 3D geometric representation include the population of virtual worlds and gaming engines. The manual generation of such assets is normally both time consuming and can involve an element interpretation on the part of a human modeler.",3au3dreanexofthstofthar,23.0,6.0,0.0
92,3D Reconstruction,104.0,detection and 3d reconstruction of traffic signs from multiple view color images,3.0,201.0,1.0,116.0,3.0,2.2,146.4,93,http://arxiv.org/pdf/1805.04424v1,"Abstract 3D reconstruction of traffic signs is of great interest in many applications such as image-based localization and navigation. In order to reflect the reality, the reconstruction process should meet both accuracy and precision. In order to reach such a valid reconstruction from calibrated multi-view images, accurate and precise extraction of signs in every individual view is a must. This paper presents first an automatic pipeline for identifying and extracting the silhouette of signs in every individual image. Then, a multi-view constrained 3D reconstruction algorithm provides an optimum 3D silhouette for the detected signs. The first step called detection, tackles with a color-based segmentation to generate ROIs (Region of Interests) in image. The shape of every ROI is estimated by fitting an ellipse, a quadrilateral or a triangle to edge points. A ROI is rejected if none of the three shapes can be fitted sufficiently precisely. Thanks to the estimated shape the remained candidates ROIs are rectified to remove the perspective distortion and then matched with a set of reference signs using textural information. Poor matches are rejected and the types of remained ones are identified. The output of the detection algorithm is a set of identified road signs whose silhouette in image plane is represented by and ellipse, a quadrilateral or a triangle. The 3D reconstruction process is based on a hypothesis generation and verification. Hypotheses are generated by a stereo matching approach taking into account epipolar geometry and also the similarity of the categories. The hypotheses that are plausibly correspond to the same 3D road sign are identified and grouped during this process. Finally, all the hypotheses of the same group are merged to generate a unique 3D road sign by a multi-view algorithm integrating a priori knowledges about 3D shape of road signs as constraints. The algorithm is assessed on real and synthetic images and reached and average accuracy of 3.5cm for position and 4.5° for orientation.",3dean3dreoftrsifrmuvicoim,60.0,45.0,4.0
93,3D Reconstruction,21.0,a review of techniques for 3d reconstruction of indoor environments,5.0,201.0,1.0,201.0,1.0,2.2,147.0,94,https://www.mdpi.com/2220-9964/9/5/330/pdf,"Convolutional neural networks are the most widely used deep learning algorithms for traffic signal classification till date but they fail to capture pose, view, orientation of the images because of the intrinsic inability of max pooling layer.This paper proposes a novel method for Traffic sign detection using deep learning architecture called capsule networks that achieves outstanding performance on the German traffic sign dataset.Capsule network consists of capsules which are a group of neurons representing the instantiating parameters of an object like the pose and orientation by using the dynamic routing and route by agreement algorithms.unlike the previous approaches of manual feature extraction,multiple deep neural networks with many parameters,our method eliminates the manual effort and provides resistance to the spatial variances.CNNs can be fooled easily using various adversary attacks and capsule networks can overcome such attacks from the intruders and can offer more reliability in traffic sign detection for autonomous vehicles.Capsule network have achieved the state-of-the-art accuracy of 97.6% on German Traffic Sign Recognition Benchmark dataset (GTSRB).",3areoftefo3dreofinen,15.0,187.0,0.0
94,3D Reconstruction,26.0,bundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration,5.0,201.0,1.0,201.0,1.0,2.2,148.5,95,https://arxiv.org/pdf/1604.01093,"Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results but suffer from (1) needing minutes to perform online correction, preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation, resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real time to ensure global consistency, all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results.1",3bureglco3dreusonsure,458.0,77.0,61.0
95,3D Reconstruction,102.0,pothole properties measurement through visual 2d recognition and 3d reconstruction,3.0,201.0,1.0,125.0,3.0,2.2,148.5,96,https://www.researchgate.net/profile/Ioannis_Brilakis/publication/269691329_Pothole_Properties_Measurement_through_Visual_2D_Recognition_and_3D_Reconstruction/links/56326b7b08ae911fcd490f40.pdf?inViewer=true&pdfJsDownload=true&disableCoverPage=true&origin=publication_detail,"Current pavement condition assessment methods are predominantly manual and time consuming. Existing pothole recognition and assessment methods rely on 3D surface reconstruction that requires high equipment and computational costs or relies on acceleration data which provides preliminary results. This paper presents an inexpensive solution that automatically detects and assesses the severity of potholes using vision-based data for both 2D recognition and for 3D reconstruction. The combination of these two techniques is used to improve recognition results by using visual and spatial characteristics of potholes and measure properties (width, number, and depth) that are used to assess severity of potholes. The number of potholes is deduced with 2D recognition whereas the width and depth of the potholes is obtained with 3D reconstruction. The proposed method is validated on several actual potholes. The results show that the proposed inexpensive and visual method holds promise to improve automated pothole detection and severity assessment.",3poprmethvi2drean3dre,60.0,13.0,0.0
96,3D Reconstruction,31.0,a dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image,5.0,201.0,1.0,201.0,1.0,2.2,150.0,97,http://arxiv.org/pdf/1907.00939v2,"In this work we present a method to train a plane-aware convolutional neural network for dense depth and surface normal estimation as well as plane boundaries from a single indoor $360^\circ$ image. Using our proposed loss function, our network outperforms existing methods for single-view, indoor, omnidirectional depth estimation and provides an initial benchmark for surface normal prediction from $360^\circ$ images. Our improvements are due to the use of a novel plane-aware loss that leverages principal curvature as an indicator of planar boundaries. We also show that including geodesic coordinate maps as network priors provides a significant boost in surface normal prediction accuracy. Finally, we demonstrate how we can combine our network's outputs to generate high quality 3D ""pop-up"" models of indoor scenes.",3adybanemofoau3drefrasiinim,235.0,23.0,9.0
97,3D Reconstruction,36.0,3d reconstruction and characterization of polycrystalline microstructures using a fib–sem system,5.0,201.0,1.0,201.0,1.0,2.2,151.5,98,https://cmrl.jhu.edu/wp-content/uploads/2015/11/2006_MaterialsCharacterization_57.pdf,"Abstract A novel methodology is described in this paper which is a step towards three-dimensional representation of grain structures for microstructure characterization and processing microstructural data for subsequent computational analysis. It facilitates evaluation of stereological parameters of grain structures from a series of two-dimensional (2D) electron backscatter diffraction (EBSD) maps. Crystallographic orientation maps of consecutive serial sections of a micron-size specimen are collected in an automated manner using a dual-beam focused ion beam–scanning electron microscope (FIB–SEM) outfitted with an EBSD system. Analysis of the serial-sectioning data is accomplished using a special purpose software program called “Micro-Imager”. Micro-Imager is able to output characterization parameters such as the distribution of grain size, number of neighboring grains, and grain orientation and misorientation for every 2D section. Some of these data can be compared with results from stereological exercises. Stacking the 2D statistical information obtained from the analysis of the serial-sectioning data provides a means to quantify the variability of grain structure in 3D.",33dreanchofpomiusafisy,243.0,15.0,6.0
98,3D Reconstruction,38.0,"3d reconstruction methods, a survey",5.0,201.0,1.0,201.0,1.0,2.2,152.10000000000002,99,https://www.scitepress.org/Papers/2006/13697/13697.pdf,"3D reconstruction technologies have evolved over the years. In this paper we try to highlight the evolution of the scanning technologies. The idea of a survey came up with our decision to look at 3D reconstruction methods. Little has been written about the methods in general, yet many developments have taken place in this area. This survey will prove useful for those intending to embark on research in 3D reconstruction technologies or are considering acquiring a 3D scanner. The survey takes a look at the major reconstruction methods, which are; Laser triangulation, Stereoscopy, Conoscopic holography and Moiré Interferometry. A review of the major producers of scanning technology for 3D reconstruction is also carried out.",33dremeasu,6.0,7.0,0.0
99,3D Reconstruction,39.0,detailed 3d reconstruction of monuments using multiple techniques,5.0,201.0,1.0,201.0,1.0,2.2,152.4,100,https://www.academia.edu/download/41518062/Detailed_3D_Reconstruction_of_Monuments_20160124-19967-f63m23.pdf,"The use of 3D digitization and modeling in documenting heritage sites has increased significantly over the past few years. This is mainly due to advances in laser scanning techniques, 3D modeling software, image-based-modeling techniques, computer power, and virtual reality. There are many approaches currently available. The most common remains based on surveying and CAD tools and/or traditional photogrammetry with control points and a human operator. This is very time consuming and can be tedious and sustained effort. Lately, modeling methods based on scanners data and more automated image-based technique are becoming available. We will discuss each approach and point out its advantages and disadvantages. We will then present our approach, which is a combination of several technologies. The approach presented in this paper uses both interactive and automatic techniques, each where it is best suited, to accurately and completely model heritage objects and sites. A highly detailed structure or site can be modeled at various levels of detail. Image-based modeling may be used for the basic shape and main structural elements, and laser scanning for fine details and sculpted surfaces. The results of applying this approach were very encouraging and several models were created from sites all over the world. Modeling of the Abbey of Pomposa near Ferrara, Italy, will be presented as an example.",3de3dreofmousmute,79.0,19.0,1.0
421,Action Recognition,8.0,temporal segment networks: towards good practices for deep action recognition,5.0,13.0,5.0,4.0,5.0,5.0,8.799999999999999,1,http://arxiv.org/pdf/1608.00859v1,"Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 (\( 69.4\,\% \)) and UCF101 (\( 94.2\,\% \)). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks).",Atesenetogoprfodeacre,2139.0,43.0,382.0
422,Action Recognition,27.0,convolutional two-stream network fusion for video action recognition,5.0,19.0,5.0,10.0,5.0,5.0,18.7,2,https://openaccess.thecvf.com/content_cvpr_2016/papers/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.pdf,"Recent applications of Convolutional Neural Networks (ConvNets) for human action recognition in videos have proposed different solutions for incorporating the appearance and motion information. We study a number of ways of fusing ConvNet towers both spatially and temporally in order to best take advantage of this spatio-temporal information. We make the following findings: (i) that rather than fusing at the softmax layer, a spatial and temporal network can be fused at a convolution layer without loss of performance, but with a substantial saving in parameters, (ii) that it is better to fuse such networks spatially at the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost accuracy, finally (iii) that pooling of abstract convolutional features over spatiotemporal neighbourhoods further boosts performance. Based on these studies we propose a new ConvNet architecture for spatiotemporal fusion of video snippets, and evaluate its performance on standard benchmarks where this architecture achieves state-of-the-art results.",Acotwnefufoviacre,1776.0,42.0,155.0
423,Action Recognition,44.0,spatial temporal graph convolutional networks for skeleton-based action recognition,4.0,10.0,5.0,8.0,5.0,4.7,19.6,3,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17135/16343,"Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.",Asptegrconefoskacre,1056.0,39.0,244.0
424,Action Recognition,67.0,large-scale weakly-supervised pre-training for video action recognition,4.0,4.0,5.0,33.0,5.0,4.7,31.6,4,https://openaccess.thecvf.com/content_CVPR_2019/papers/Ghadiyaram_Large-Scale_Weakly-Supervised_Pre-Training_for_Video_Action_Recognition_CVPR_2019_paper.pdf,"Current fully-supervised video datasets consist of only a few hundred thousand videos and fewer than a thousand domain-specific labels. This hinders the progress towards advanced video architectures. This paper presents an in-depth study of using large volumes of web videos for pre-training video models for the task of action recognition. Our primary empirical finding is that pre-training at a very large scale (over 65 million videos), despite on noisy social-media videos and hashtags, substantially improves the state-of-the-art on three challenging public action recognition datasets. Further, we examine three questions in the construction of weakly-supervised video action datasets. First, given that actions involve interactions with objects, how should one construct a verb-object pre-training label space to benefit transfer learning the most? Second, frame-based models perform quite well on action recognition; is pre-training for good image features sufficient or is pre-training for spatio-temporal features valuable for optimal transfer learning? Finally, actions are generally less well-localized in long videos vs. short videos; since action labels are provided at a video level, how should one choose video clips for best performance, given some fixed budget of number or minutes of videos?",Alaweprfoviacre,126.0,73.0,16.0
425,Action Recognition,9.0,representation flow for action recognition,5.0,37.0,5.0,50.0,4.0,4.7,32.5,5,https://openaccess.thecvf.com/content_CVPR_2019/papers/Piergiovanni_Representation_Flow_for_Action_Recognition_CVPR_2019_paper.pdf,"In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the `flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning `flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. The code is publicly available.",Areflfoacre,68.0,33.0,2.0
426,Action Recognition,21.0,two-stream convolutional networks for action recognition in videos,5.0,58.0,4.0,1.0,5.0,4.6,29.800000000000004,6,https://arxiv.org/pdf/1406.2199,"We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. 
 
Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",Atwconefoacreinvi,5051.0,37.0,765.0
427,Action Recognition,135.0,mars: motion-augmented rgb stream for action recognition,3.0,63.0,4.0,36.0,5.0,4.0,76.5,7,http://openaccess.thecvf.com/content_CVPR_2019/papers/Crasto_MARS_Motion-Augmented_RGB_Stream_for_Action_Recognition_CVPR_2019_paper.pdf,"Most state-of-the-art methods for action recognition consist of a two-stream architecture with 3D convolutions: an appearance stream for RGB frames and a motion stream for optical flow frames. Although combining flow with RGB improves the performance, the cost of computing accurate optical flow is high, and increases action recognition latency. This limits the usage of two-stream approaches in real-world applications requiring low latency. In this paper, we introduce two learning approaches to train a standard 3D CNN, operating on RGB frames, that mimics the motion stream, and as a result avoids flow computation at test time. First, by minimizing a feature-based loss compared to the Flow stream, we show that the network reproduces the motion stream with high fidelity. Second, to leverage both appearance and motion information effectively, we train with a linear combination of the feature-based loss and the standard cross-entropy loss for action recognition. We denote the stream trained using this combined loss as Motion-Augmented RGB Stream (MARS). As a single stream, MARS performs better than RGB or Flow alone, for instance with 72.7% accuracy on Kinetics compared to 72.0% and 65.6% with RGB and Flow streams respectively.",Amamorgstfoacre,107.0,48.0,20.0
428,Action Recognition,61.0,end-to-end video-level representation learning for action recognition,4.0,97.0,4.0,98.0,4.0,4.0,86.5,8,https://arxiv.org/pdf/1711.04161,"From the frame/clip-level feature learning to the video-level representation building, deep learning methods in action recognition have developed rapidly in recent years. However, current methods suffer from the confusion caused by partial observation training, or without end-to-end learning, or restricted to single temporal scale modeling and so on. In this paper, we build upon two-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling (DTPP), an end-to-end video-level representation learning approach, to address these problems. Specifically, at first, RGB images and optical flow stacks are sparsely sampled across the whole video. Then a temporal pyramid pooling layer is used to aggregate the frame-level features which consist of spatial and temporal cues. Lastly, the trained model has compact video-level representation with multiple temporal scales, which is both global and sequence-aware. Experimental results show that DTPP achieves the state-of-the-art performance on two challenging video action datasets: UCF101 and HMDB51, either by ImageNet pre-training or Kinetics pre-training.",Aenvirelefoacre,73.0,56.0,3.0
429,Action Recognition,401.0,a closer look at spatiotemporal convolutions for action recognition,1.0,5.0,5.0,16.0,5.0,3.8,127.1,9,http://arxiv.org/pdf/1711.11248v3,"In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block ""R(2+1)D"" which produces CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101, and HMDB51.",Aaclloatspcofoacre,1076.0,50.0,153.0
430,Action Recognition,401.0,temporal segment networks for action recognition in videos,1.0,11.0,5.0,9.0,5.0,3.8,127.4,10,http://arxiv.org/pdf/1708.03280v2,"We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structure with a new segment-based sampling and aggregation scheme. This unique design enables the TSN framework to efficiently learn action models by using the whole video. The learned models could be easily deployed for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the implementation of the TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on five challenging action recognition benchmarks: HMDB51 (71.0 percent), UCF101 (94.9 percent), THUMOS14 (80.1 percent), ActivityNet v1.2 (89.6 percent), and Kinetics400 (75.7 percent). In addition, using the proposed RGB difference as a simple motion representation, our method can still achieve competitive accuracy on UCF101 (91.0 percent) while running at 340 FPS. Furthermore, based on the proposed TSN framework, we won the video classification track at the ActivityNet challenge 2016 among 24 teams.",Atesenefoacreinvi,255.0,83.0,45.0
431,Action Recognition,401.0,two-stream adaptive graph convolutional networks for skeleton-based action recognition,1.0,15.0,5.0,7.0,5.0,3.8,128.4,11,http://arxiv.org/abs/1912.06971v1,"In skeleton-based action recognition, graph convolutional networks (GCNs), which model the human body skeletons as spatiotemporal graphs, have achieved remarkable performance. However, in existing GCN-based methods, the topology of the graph is set manually, and it is fixed over all layers and input samples. This may not be optimal for the hierarchical GCN and diverse samples in action recognition tasks. In addition, the second-order information (the lengths and directions of bones) of the skeleton data, which is naturally more informative and discriminative for action recognition, is rarely investigated in existing methods. In this work, we propose a novel two-stream adaptive graph convolutional network (2s-AGCN) for skeleton-based action recognition. The topology of the graph in our model can be either uniformly or individually learned by the BP algorithm in an end-to-end manner. This data-driven method increases the flexibility of the model for graph construction and brings more generality to adapt to various data samples. Moreover, a two-stream framework is proposed to model both the first-order and the second-order information simultaneously, which shows notable improvement for the recognition accuracy. Extensive experiments on the two large-scale datasets, NTU-RGBD and Kinetics-Skeleton, demonstrate that the performance of our model exceeds the state-of-the-art with a significant margin.",Atwadgrconefoskacre,266.0,34.0,71.0
433,Action Recognition,401.0,actional-structural graph convolutional networks for skeleton-based action recognition,1.0,38.0,5.0,24.0,5.0,3.8,142.7,12,http://arxiv.org/pdf/1805.06184v2,"Action recognition with skeleton data has recently attracted much attention in computer vision. Previous studies are mostly based on fixed skeleton graphs, only capturing local physical dependencies among joints, which may miss implicit joint correlations. To capture richer dependencies, we introduce an encoder-decoder structure, called A-link inference module, to capture action-specific latent dependencies, i.e. actional links, directly from actions. We also extend the existing skeleton graphs to represent higher-order dependencies, i.e. structural links. Combing the two types of links into a generalized skeleton graph, We further propose the actional-structural graph convolution network (AS-GCN), which stacks actional-structural graph convolution and temporal convolution as a basic building block, to learn both spatial and temporal features for action recognition. A future pose prediction head is added in parallel to the recognition head to help capture more detailed action patterns through self-supervision. We validate AS-GCN in action recognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed AS-GCN achieves consistently large improvement compared to the state-of-the-art methods. As a side product, AS-GCN also shows promising results for future pose prediction.",Aacgrconefoskacre,216.0,35.0,28.0
434,Action Recognition,100.0,im2flow: motion hallucination from static images for action recognition,4.0,115.0,3.0,96.0,4.0,3.6000000000000005,104.8,13,https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.pdf,"Existing methods to recognize actions in static images take the images at their face value, learning the appearances-objects, scenes, and body poses-that distinguish each action class. However, such models are deprived of the rich dynamic structure and motions that also define human activity. We propose an approach that hallucinates the unobserved future motion implied by a single snapshot to help static-image action recognition. The key idea is to learn a prior over short-term dynamics from thousands of unlabeled videos, infer the anticipated optical flow on novel static images, and then train discriminative models that exploit both streams of information. Our main contributions are twofold. First, we devise an encoder-decoder convolutional neural network and a novel optical flow encoding that can translate a static image into an accurate flow map. Second, we show the power of hallucinated flow for recognition, successfully transferring the learned motion into a standard two-stream network for activity recognition. On seven datasets, we demonstrate the power of the approach. It not only achieves state-of-the-art accuracy for dense optical flow prediction, but also consistently enhances recognition of actions and dynamic scenes.",Aimmohafrstimfoacre,64.0,96.0,11.0
435,Action Recognition,114.0,d3d: distilled 3d networks for video action recognition,3.0,123.0,3.0,13.0,5.0,3.6,87.30000000000001,14,https://openaccess.thecvf.com/content_WACV_2020/papers/Stroud_D3D_Distilled_3D_Networks_for_Video_Action_Recognition_WACV_2020_paper.pdf,"State-of-the-art methods for action recognition commonly use two networks: the spatial stream, which takes RGB frames as input, and the temporal stream, which takes optical flow as input. In recent work, both streams are 3D Convolutional Neural Networks, which use spatiotemporal filters. These filters can respond to motion, and therefore should allow the network to learn motion representations, removing the need for optical flow. However, we still see significant benefits in performance by feeding optical flow into the temporal stream, indicating that the spatial stream is ""missing"" some of the signal that the temporal stream captures. In this work, we first investigate whether motion representations are indeed missing in the spatial stream, and show that there is significant room for improvement. Second, we demonstrate that these motion representations can be improved using distillation, that is, by tuning the spatial stream to mimic the temporal stream, effectively combining both models into a single stream. Finally, we show that our Distilled 3D Network (D3D) achieves performance on par with the two-stream approach, with no need to compute optical flow during inference.",Ad3di3dnefoviacre,66.0,49.0,4.0
436,Action Recognition,20.0,action recognition with trajectory-pooled deep-convolutional descriptors,5.0,147.0,3.0,113.0,3.0,3.6,98.70000000000002,15,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wang_Action_Recognition_With_2015_CVPR_paper.pdf,"Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features [31] and deep-learned features [24]. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMD-B51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features [31] and deep-learned features [24]. Our method also achieves superior performance to the state of the art on these datasets.",Aacrewitrdede,970.0,60.0,89.0
437,Action Recognition,401.0,compressed video action recognition,1.0,24.0,5.0,60.0,4.0,3.5,147.9,16,http://arxiv.org/pdf/1712.00636v2,"Training robust deep video representations has proven to be much more challenging than learning deep image representations. This is in part due to the enormous size of raw video streams and the high temporal redundancy; the true and interesting signal is often drowned in too much irrelevant data. Motivated by that the superfluous information can be reduced by up to two orders of magnitude by video compression (using H.264, HEVC, etc.), we propose to train a deep network directly on the compressed video. This representation has a higher information density, and we found the training to be easier. In addition, the signals in a compressed video provide free, albeit noisy, motion information. We propose novel techniques to use them effectively. Our approach is about 4.6 times faster than Res3D and 2.7 times faster than ResNet-152. On the task of action recognition, our approach outperforms all the other methods on the UCF-101, HMDB-51, and Charades dataset.",Acoviacre,177.0,53.0,33.0
438,Action Recognition,401.0,real-time action recognition with enhanced motion vector cnns,1.0,22.0,5.0,75.0,4.0,3.5,151.6,17,http://arxiv.org/pdf/2105.09188v1,"The deep two-stream architecture [23] exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.",Areacrewienmovecn,308.0,41.0,30.0
439,Action Recognition,401.0,hidden two-stream convolutional networks for action recognition,1.0,27.0,5.0,70.0,4.0,3.5,152.1,18,http://arxiv.org/pdf/1704.00389v4,"Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.",Ahitwconefoacre,203.0,81.0,21.0
440,Action Recognition,401.0,2d/3d pose estimation and action recognition using multitask deep learning,1.0,33.0,5.0,66.0,4.0,3.5,153.3,19,http://arxiv.org/pdf/1802.09232v2,"Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.",A2dpoesanacreusmudele,257.0,64.0,23.0
441,Action Recognition,401.0,co-occurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation,1.0,43.0,4.0,27.0,5.0,3.4000000000000004,145.6,20,http://arxiv.org/pdf/1804.06055v1,"Skeleton-based human action recognition has recently drawn increasing attentions with the availability of large-scale skeleton datasets. The most crucial factors for this task lie in two aspects: the intra-frame representation for joint co-occurrences and the inter-frame representation for skeletons' temporal evolutions. In this paper we propose an end-to-end convolutional co-occurrence feature learning framework. The co-occurrence features are learned with a hierarchical methodology, in which different levels of contextual information are aggregated gradually. Firstly point-level information of each joint is encoded independently. Then they are assembled into semantic representation in both spatial and temporal domains. Specifically, we introduce a global spatial aggregation scheme, which is able to learn superior joint co-occurrence features over local aggregation. Besides, raw skeleton coordinates as well as their temporal difference are integrated with a two-stream paradigm. Experiments show that our approach consistently outperforms other state-of-the-arts on action recognition and detection benchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD.",Acofelefrskdafoacreandewihiag,216.0,27.0,42.0
443,Action Recognition,401.0,untrimmednets for weakly supervised action recognition and detection,1.0,59.0,4.0,30.0,5.0,3.4000000000000004,152.9,21,http://arxiv.org/pdf/1703.03329v2,"Current action recognition methods heavily rely on trimmed videos for model training. However, it is expensive and time-consuming to acquire a large-scale trimmed video dataset. This paper presents a new weakly supervised architecture, called UntrimmedNet, which is able to directly learn action recognition models from untrimmed videos without the requirement of temporal annotations of action instances. Our UntrimmedNet couples two important components, the classification module and the selection module, to learn the action models and reason about the temporal duration of action instances, respectively. These two components are implemented with feed-forward networks, and UntrimmedNet is therefore an end-to-end trainable architecture. We exploit the learned models for action recognition (WSR) and detection (WSD) on the untrimmed video datasets of THUMOS14 and ActivityNet. Although our UntrimmedNet only employs weak supervision, our method achieves performance superior or comparable to that of those strongly supervised approaches on these two datasets.",Aunfowesuacreande,273.0,66.0,50.0
444,Action Recognition,401.0,semantics-guided neural networks for efficient skeleton-based human action recognition,1.0,71.0,4.0,18.0,5.0,3.4000000000000004,154.1,22,http://arxiv.org/pdf/1608.07876v1,"Skeleton-based human action recognition has attracted great interest thanks to the easy accessibility of the human skeleton data. Recently, there is a trend of using very deep feedforward neural networks to model the 3D coordinates of joints without considering the computational efficiency. In this paper, we propose a simple yet effective semantics-guided neural network (SGN) for skeleton-based action recognition. We explicitly introduce the high level semantics of joints (joint type and frame index) into the network to enhance the feature representation capability. In addition, we exploit the relationship of joints hierarchically through two modules, i.e., a joint-level module for modeling the correlations of joints in the same frame and a framelevel module for modeling the dependencies of frames by taking the joints in the same frame as a whole. A strong baseline is proposed to facilitate the study of this field. With an order of magnitude smaller model size than most previous works, SGN achieves the state-of-the-art performance on the NTU60, NTU120, and SYSU datasets.",Asenenefoefskhuacre,58.0,70.0,11.0
445,Action Recognition,401.0,long-term temporal convolutions for action recognition,1.0,85.0,4.0,25.0,5.0,3.4000000000000004,161.8,23,http://arxiv.org/pdf/2001.04335v2,"Typical human actions last several seconds and exhibit characteristic spatio-temporal structure. Recent methods attempt to capture this structure and learn action representations with convolutional neural networks. Such representations, however, are typically learned at the level of a few video frames failing to model actions at their full temporal extent. In this work we learn video representations using neural networks with long-term temporal convolutions (LTC). We demonstrate that LTC-CNN models with increased temporal extents improve the accuracy of action recognition. We also study the impact of different low-level representations, such as raw values of video pixels and optical flow vector fields and demonstrate the importance of high-quality optical flow estimation for learning accurate action models. We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).",Alotecofoacre,659.0,43.0,50.0
446,Action Recognition,401.0,view adaptive neural networks for high performance skeleton-based human action recognition,1.0,79.0,4.0,35.0,5.0,3.4000000000000004,162.4,24,http://arxiv.org/pdf/1703.08274v2,"Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in action recognition lies in the large variations of action representations when they are captured from different viewpoints. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints over the course of an action in a learning based data driven manner. Instead of re-positioning the skeletons using a fixed human-defined prior criterion, we design two view adaptive neural networks, i.e., VA-RNN and VA-CNN, which are respectively built based on the recurrent neural network (RNN) with the Long Short-term Memory (LSTM) and the convolutional neural network (CNN). For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various views to much more consistent virtual viewpoints. Therefore, the models largely eliminate the influence of the viewpoints, enabling the networks to focus on the learning of action-specific features and thus resulting in superior performance. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the final prediction, obtaining enhanced performance. Moreover, random rotation of skeleton sequences is employed to improve the robustness of view adaptation models and alleviate overfitting during training. Extensive experimental evaluations on five challenging benchmarks demonstrate the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches.",Aviadnenefohipeskhuacre,124.0,61.0,12.0
447,Action Recognition,2.0,"quo vadis, action recognition? a new model and the kinetics dataset",5.0,201.0,1.0,3.0,5.0,3.4,81.9,25,https://openaccess.thecvf.com/content_cvpr_2017/papers/Carreira_Quo_Vadis_Action_CVPR_2017_paper.pdf,"The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.2% on HMDB-51 and 97.9% on UCF-101.",Aquvaacreanemoanthkida,3087.0,42.0,804.0
448,Action Recognition,1.0,action recognition with improved trajectories,5.0,201.0,1.0,6.0,5.0,3.4,82.5,26,https://openaccess.thecvf.com/content_iccv_2013/papers/Wang_Action_Recognition_with_2013_ICCV_paper.pdf,"Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.",Aacrewiimtr,2709.0,44.0,512.0
449,Action Recognition,10.0,3d convolutional neural networks for human action recognition,5.0,201.0,1.0,2.0,5.0,3.4,84.0,27,https://openreview.net/pdf?id=HJZhKsb_-B,"We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.",A3dconenefohuacre,3950.0,67.0,229.0
450,Action Recognition,11.0,temporal pyramid network for action recognition,5.0,201.0,1.0,15.0,5.0,3.4,88.2,28,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Temporal_Pyramid_Network_for_Action_Recognition_CVPR_2020_paper.pdf,"Visual tempo characterizes the dynamics and the temporal scale of an action. Modeling such visual tempos of different actions facilitates their recognition. Previous works often capture the visual tempo through sampling raw videos at multiple rates and constructing an input-level frame pyramid, which usually requires a costly multi-branch network to handle. In this work we propose a generic Temporal Pyramid Network (TPN) at the feature-level, which can be flexibly integrated into 2D or 3D backbone networks in a plug-and-play manner. Two essential components of TPN, the source of features and the fusion of features, form a feature hierarchy for the backbone so that it can capture action instances at various tempos. TPN also shows consistent improvements over other challenging baselines on several action recognition datasets. Specifically, when equipped with TPN, the 3D ResNet-50 with dense sampling obtains a 2\% gain on the validation set of Kinetics-400. A further analysis also reveals that TPN gains most of its improvements on action classes that have large variances in their visual tempos, validating the effectiveness of TPN.",Atepynefoacre,72.0,39.0,11.0
451,Action Recognition,17.0,stm: spatiotemporal and motion encoding for action recognition,5.0,201.0,1.0,34.0,5.0,3.4,95.7,29,https://openaccess.thecvf.com/content_ICCV_2019/papers/Jiang_STM_SpatioTemporal_and_Motion_Encoding_for_Action_Recognition_ICCV_2019_paper.pdf,"Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose a STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.",Astspanmoenfoacre,121.0,43.0,20.0
452,Action Recognition,39.0,hierarchical recurrent neural network for skeleton based action recognition,5.0,201.0,1.0,32.0,5.0,3.4,101.7,30,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.pdf,"Human actions can be represented by the trajectories of skeleton joints. Traditional methods generally model the spatial structure and temporal dynamics of human skeleton with hand-crafted features and recognize human actions by well-designed classifiers. In this paper, considering that recurrent neural network (RNN) can model the long-term contextual information of temporal sequences well, we propose an end-to-end hierarchical RNN for skeleton based action recognition. Instead of taking the whole skeleton as the input, we divide the human skeleton into five parts according to human physical structure, and then separately feed them to five subnets. As the number of layers increases, the representations extracted by the subnets are hierarchically fused to be the inputs of higher layers. The final representations of the skeleton sequences are fed into a single-layer perceptron, and the temporally accumulated output of the perceptron is the final decision. We compare with five other deep RNN architectures derived from our model to verify the effectiveness of the proposed network, and also compare with several other methods on three publicly available datasets. Experimental results demonstrate that our model achieves the state-of-the-art performance with high computational efficiency.",Ahirenenefoskbaacre,1201.0,41.0,155.0
453,Action Recognition,127.0,lsta: long short-term attention for egocentric action recognition,3.0,135.0,3.0,53.0,4.0,3.3,108.0,31,http://openaccess.thecvf.com/content_CVPR_2019/papers/Sudhakaran_LSTA_Long_Short-Term_Attention_for_Egocentric_Action_Recognition_CVPR_2019_paper.pdf,"Egocentric activity recognition is one of the most challenging tasks in video analysis. It requires a fine-grained discrimination of small objects and their manipulation. While some methods base on strong supervision and attention mechanisms, they are either annotation consuming or do not take spatio-temporal patterns into account. In this paper we propose LSTA as a mechanism to focus on features from spatial relevant parts while attention is being tracked smoothly across the video sequence. We demonstrate the effectiveness of LSTA on egocentric activity recognition with an end-to-end trainable two-stream architecture, achieving state-of-the-art performance on four standard benchmarks.",Alsloshatfoegacre,72.0,48.0,11.0
454,Action Recognition,177.0,modality distillation with multiple stream networks for action recognition,3.0,177.0,3.0,99.0,4.0,3.3,153.6,32,https://openaccess.thecvf.com/content_ECCV_2018/papers/Nuno_Garcia_Modality_Distillation_with_ECCV_2018_paper.pdf,"Diverse input data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance. However, while a (training) dataset could be accurately designed to include a variety of sensory inputs, it is often the case that not all modalities are available in real life (testing) scenarios, where a model has to be deployed. This raises the challenge of how to learn robust representations leveraging multimodal data in the training stage, while considering limitations at test time, such as noisy or missing modalities. This paper presents a new approach for multimodal video action recognition, developed within the unified frameworks of distillation and privileged information, named generalized distillation. Particularly, we consider the case of learning representations from depth and RGB videos, while relying on RGB data only at test time. We propose a new approach to train an hallucination network that learns to distill depth features through multiplicative connections of spatiotemporal representations, leveraging soft labels and hard labels, as well as distance between feature maps. We report state-of-the-art results on video action classification on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the UWA3DII and Northwestern-UCLA.",Amodiwimustnefoacre,71.0,38.0,4.0
455,Action Recognition,401.0,attentional pooling for action recognition,1.0,36.0,5.0,102.0,3.0,3.2,165.29999999999998,33,http://arxiv.org/pdf/1711.01467v3,"We introduce a simple yet surprisingly powerful model to incorporate attention in action recognition and human object interaction tasks. Our proposed attention module can be trained with or without extra supervision, and gives a sizable boost in accuracy while keeping the network size and computational cost nearly the same. It leads to significant improvements over state of the art base architecture on three standard action recognition benchmarks across still images and videos, and establishes new state of the art on MPII dataset with 12.5% relative improvement. We also perform an extensive analysis of our attention module both empirically and analytically. In terms of the latter, we introduce a novel derivation of bottom-up and top-down attention as low-rank approximations of bilinear pooling methods (typically used for fine-grained classification). From this perspective, our attention formulation suggests a novel characterization of action recognition as a fine-grained recognition problem.",Aatpofoacre,230.0,64.0,23.0
456,Action Recognition,401.0,action recognition using visual attention,1.0,29.0,5.0,114.0,3.0,3.2,166.1,34,http://arxiv.org/pdf/1812.02707v2,"We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.",Aacreusviat,540.0,41.0,46.0
457,Action Recognition,6.0,timeception for complex action recognition,5.0,201.0,1.0,49.0,4.0,3.1,96.9,35,https://openaccess.thecvf.com/content_CVPR_2019/papers/Hussein_Timeception_for_Complex_Action_Recognition_CVPR_2019_paper.pdf,"This paper focuses on the temporal aspect for recognizing human activities in videos; an important visual cue that has long been undervalued. We revisit the conventional definition of activity and restrict it to Complex Action: a set of one-actions with a weak temporal pattern that serves a specific purpose. Related works use spatiotemporal 3D convolutions with fixed kernel size, too rigid to capture the varieties in temporal extents of complex actions, and too short for long-range temporal modeling. In contrast, we use multi-scale temporal convolutions, and we reduce the complexity of 3D convolutions. The outcome is Timeception convolution layers, which reasons about minute-long temporal patterns, a factor of 8 longer than best related works. As a result, Timeception achieves impressive accuracy in recognizing the human activities of Charades, Breakfast Actions and MultiTHUMOS. Further, we demonstrate that Timeception learns long-range temporal dependencies and tolerate temporal extents of complex actions.",Atifocoacre,92.0,52.0,11.0
458,Action Recognition,13.0,a review of convolutional-neural-network-based action recognition,5.0,201.0,1.0,43.0,4.0,3.1,97.20000000000002,36,http://arxiv.org/abs/1906.09955v1,"Abstract Video action recognition is widely applied in video indexing, intelligent surveillance, multimedia understanding, and other fields. Recently, it was greatly improved by incorporating the learning of deep information using Convolutional Neural Network (CNN). This motivated us to review the notable CNN-based action recognition works. Because CNN is primarily designed to extract 2D spatial features from still image and videos are naturally viewed as 3D spatiotemporal signals, the core issue of extending the CNN from image to video is temporal information exploitation. We divide the solutions for exploiting temporal information exploration into three strategies: 1) 3D CNN; 2) taking the motion-related information as the CNN input; and 3) fusion. In this paper, we present a comprehensive review of the CNN-based action recognition methods according to these strategies. We also discuss the action recognition performance on recent large-scale benchmarks and the limitations and future research directions of CNN-based action recognition. This paper offers an objective and clear review of CNN-based action recognition and provides a guide for future research.",Aareofcoacre,88.0,101.0,1.0
459,Action Recognition,19.0,mining actionlet ensemble for action recognition with depth cameras,5.0,201.0,1.0,65.0,4.0,3.1,105.6,37,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/06247813.pdf,"Human action recognition is an important yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.",Amiacenfoacrewideca,1325.0,26.0,209.0
460,Action Recognition,14.0,view invariant human action recognition using histograms of 3d joints,5.0,201.0,1.0,77.0,4.0,3.1,107.7,38,https://groups.io/g/miners/attachment/38/0/view_inv_hoj.pdf,"In this paper, we present a novel approach for human action recognition with histograms of 3D joint locations (HOJ3D) as a compact representation of postures. We extract the 3D skeletal joint locations from Kinect depth maps using Shotton et al.'s method [6]. The HOJ3D computed from the action depth sequences are reprojected using LDA and then clustered into k posture visual words, which represent the prototypical poses of actions. The temporal evolutions of those visual words are modeled by discrete hidden Markov models (HMMs). In addition, due to the design of our spherical coordinate system and the robust 3D skeleton estimation from Kinect, our method demonstrates significant view invariance on our 3D action dataset. Our dataset is composed of 200 3D sequences of 10 indoor activities performed by 10 individuals in varied views. Our method is real-time and achieves superior results on the challenging 3D action dataset. We also tested our algorithm on the MSR Action 3D dataset and our algorithm outperforms Li et al. [25] on most of the cases.",Aviinhuacreushiof3djo,1192.0,45.0,180.0
461,Action Recognition,54.0,scsampler: sampling salient clips from video for efficient action recognition,4.0,201.0,1.0,40.0,5.0,3.1,108.6,39,https://openaccess.thecvf.com/content_ICCV_2019/papers/Korbar_SCSampler_Sampling_Salient_Clips_From_Video_for_Efficient_Action_Recognition_ICCV_2019_paper.pdf,"While many action recognition datasets consist of collections of brief, trimmed videos each containing a relevant action, videos in the real-world (e.g., on YouTube) exhibit very different properties: they are often several minutes long, where brief relevant clips are often interleaved with segments of extended duration containing little change. Applying densely an action recognition system to every temporal clip within such videos is prohibitively expensive. Furthermore, as we show in our experiments, this results in suboptimal recognition accuracy as informative predictions from relevant clips are outnumbered by meaningless classification outputs over long uninformative sections of the video. In this paper we introduce a lightweight ``clip-sampling'' model that can efficiently identify the most salient temporal clips within a long video. We demonstrate that the computational cost of action recognition on untrimmed videos can be dramatically reduced by invoking recognition only on these most salient clips. Furthermore, we show that this yields significant gains in recognition accuracy compared to analysis of all clips or randomly selected clips. On Sports1M, our clip sampling scheme elevates the accuracy of an already state-of-the-art action classifier by 7% and reduces by more than 15 times its computational cost.",Ascsasaclfrvifoefacre,75.0,65.0,15.0
462,Action Recognition,18.0,dense trajectories and motion boundary descriptors for action recognition,5.0,201.0,1.0,79.0,4.0,3.1,109.5,40,https://hal.inria.fr/hal-00803241/document,"This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the context of action classification on nine datasets, namely KTH, YouTube, Hollywood2, UCF sports, IXMAS, UIUC, Olympic Sports, UCF50 and HMDB51. On all datasets our approach outperforms current state-of-the-art results.",Adetranmobodefoacre,1484.0,73.0,256.0
463,Action Recognition,15.0,on the integration of optical flow and action recognition,5.0,201.0,1.0,90.0,4.0,3.1,111.9,41,https://arxiv.org/pdf/1712.08416,"Most of the top performing action recognition methods use optical flow as a ""black box"" input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.",Aonthinofopflanacre,98.0,43.0,5.0
464,Action Recognition,16.0,resound: towards action recognition without representation bias,5.0,201.0,1.0,92.0,4.0,3.1,112.8,42,https://openaccess.thecvf.com/content_ECCV_2018/papers/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.pdf,"While large datasets have proven to be a key enabler for progress in computer vision, they can have biases that lead to erroneous conclusions. The notion of the representation bias of a dataset is proposed to combat this problem. It captures the fact that representations other than the ground-truth representation can achieve good performance on any given dataset. When this is the case, the dataset is said not to be well calibrated. Dataset calibration is shown to be a necessary condition for the standard state-of-the-art evaluation practice to converge to the ground-truth representation. A procedure, RESOUND, is proposed to quantify and minimize representation bias. Its application to the problem of action recognition shows that current datasets are biased towards static representations (objects, scenes and people). Two versions of RESOUND are studied. An Explicit RESOUND procedure is proposed to assemble new datasets by sampling existing datasets. An implicit RESOUND procedure is used to guide the creation of a new dataset, Diving48, of over 18,000 video clips of competitive diving actions, spanning 48 fine-grained dive classes. Experimental evaluation confirms the effectiveness of RESOUND to reduce the static biases of current datasets.",Aretoacrewirebi,84.0,26.0,20.0
465,Action Recognition,22.0,actionflownet: learning motion representation for action recognition,5.0,201.0,1.0,94.0,4.0,3.1,115.2,43,https://arxiv.org/pdf/1612.03052,"We present a data-efficient representation learning approach to learn video representation with small amount of labeled data. We propose a multitask learning model ActionFlowNet to train a single stream network directly from raw pixels to jointly estimate optical flow while recognizing actions with convolutional neural networks, capturing both appearance and motion in a single model. Our model effectively learns video representation from motion information on unlabeled videos. Our model significantly improves action recognition accuracy by a large margin (23.6%) compared to state-of-the-art CNN-based unsupervised representation learning methods trained without external large scale data and additional optical flow input. Without pretraining on large external labeled datasets, our model, by well exploiting the motion information, achieves competitive recognition accuracy to the models trained with large labeled datasets such as ImageNet and Sport-1M.",Aaclemorefoacre,78.0,43.0,7.0
466,Action Recognition,95.0,epic-fusion: audio-visual temporal binding for egocentric action recognition,4.0,201.0,1.0,37.0,5.0,3.1,120.0,44,https://openaccess.thecvf.com/content_ICCV_2019/papers/Kazakos_EPIC-Fusion_Audio-Visual_Temporal_Binding_for_Egocentric_Action_Recognition_ICCV_2019_paper.pdf,"We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multi-modal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities -- RGB, Flow and Audio -- and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities. We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.",Aepautebifoegacre,90.0,49.0,15.0
467,Action Recognition,40.0,t-c3d: temporal convolutional 3d network for real-time action recognition,5.0,201.0,1.0,97.0,4.0,3.1,121.5,45,https://ojs.aaai.org/index.php/AAAI/article/view/12333/12192,"Video-based action recognition with deep neural networks has shown remarkable progress. However, most of the existing approaches are too computationally expensive due to the complex network architecture. To address these problems, we propose a new real-time action recognition architecture, called Temporal Convolutional 3D Network (T-C3D), which learns video action representations in a hierarchical multi-granularity manner. Specifically, we combine a residual 3D convolutional neural network which captures complementary information on the appearance of a single frame and the motion between consecutive frames with a new temporal encoding method to explore the temporal dynamics of the whole video. Thus heavy calculations are avoided when doing the inference, which enables the method to be capable of real-time processing. On two challenging benchmark datasets, UCF101 and HMDB51, our method is significantly better than state-of-the-art real-time methods by over 5.4% in terms of accuracy and 2 times faster in terms of inference speed (969 frames per second), demonstrating comparable recognition performance to the state-of-the-art methods. The source code for the complete system as well as the pre-trained models are publicly available at https://github.com/tc3d.",At-teco3dneforeacre,71.0,29.0,3.0
468,Action Recognition,401.0,dynamic image networks for action recognition,1.0,54.0,4.0,55.0,4.0,3.1,158.4,46,http://arxiv.org/pdf/1612.00738v2,"We introduce the concept of dynamic image, a novel compact representation of videos useful for video analysis especially when convolutional neural networks (CNNs) are used. The dynamic image is based on the rank pooling concept and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. This idea is simple but powerful as it enables the use of existing CNN models directly on video data with fine-tuning. We present an efficient and effective approximate rank pooling operator, speeding it up orders of magnitude compared to rank pooling. Our new approximate rank pooling CNN layer allows us to generalize dynamic images to dynamic feature maps and we demonstrate the power of our new representations on standard benchmarks in action recognition achieving state-of-the-art performance.",Adyimnefoacre,431.0,34.0,64.0
469,Action Recognition,401.0,spatiotemporal multiplier networks for video action recognition,1.0,55.0,4.0,56.0,4.0,3.1,159.10000000000002,47,http://arxiv.org/pdf/1903.01038v1,This paper presents a general ConvNet architecture for video action recognition based on multiplicative interactions of spacetime features. Our model combines the appearance and motion pathways of a two-stream architecture by motion gating and is trained end-to-end. We theoretically motivate multiplicative gating functions for residual networks and empirically study their effect on classification accuracy. To capture long-term dependencies we inject identity mapping kernels for learning temporal relationships. Our architecture is fully convolutional in spacetime and able to evaluate a video in a single forward pass. Empirical investigation reveals that our model produces state-of-the-art results on two standard action recognition datasets.,Aspmunefoviacre,379.0,44.0,31.0
470,Action Recognition,401.0,spatiotemporal residual networks for video action recognition,1.0,56.0,4.0,63.0,4.0,3.1,161.6,48,http://arxiv.org/pdf/1611.02155v1,"Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping these with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.",Asprenefoviacre,525.0,32.0,46.0
471,Action Recognition,401.0,optical flow guided feature: a fast and robust motion representation for video action recognition,1.0,46.0,4.0,81.0,4.0,3.1,163.0,49,http://arxiv.org/pdf/1711.11152v2,"Motion representation plays a vital role in human action recognition in videos. In this study, we introduce a novel compact motion representation for video action recognition, named Optical Flow guided Feature (OFF), which enables the network to distill temporal information through a fast and robust approach. The OFF is derived from the definition of optical flow and is orthogonal to the optical flow. The derivation also provides theoretical support for using the difference between two frames. By directly calculating pixel-wise spatio-temporal gradients of the deep feature maps, the OFF could be embedded in any existing CNN based video action recognition framework with only a slight additional cost. It enables the CNN to extract spatiotemporal information, especially the temporal information between frames simultaneously. This simple but powerful idea is validated by experimental results. The network with OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on UCF-101, which is comparable with the result obtained by two streams (RGB and optical flow), but is 15 times faster in speed. Experimental results also show that OFF is complementary to other motion modalities such as optical flow. When the proposed method is plugged into the state-of-the-art video action recognition framework, it has 96.0% and 74.2% accuracy on UCF-101 and HMDB-51 respectively. The code for this project is available at: https://github.com/kevin-ssy/Optical-Flow-Guided-Feature",Aopflgufeafaanromorefoviacre,174.0,56.0,8.0
473,Action Recognition,401.0,learning spatio-temporal features with 3d residual networks for action recognition,1.0,80.0,4.0,73.0,4.0,3.1,174.20000000000002,50,http://arxiv.org/pdf/1708.07632v1,"Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have an ability to directly extract spatiotemporal features from videos for action recognition. Although the 3D kernels tend to overfit because of a large number of their parameters, the 3D CNNs are greatly improved by using recent huge video databases. However, the architecture of3D CNNs is relatively shallow against to the success of very deep neural networks in 2D-based CNNs, such as residual networks (ResNets). In this paper, we propose a 3D CNNs based on ResNets toward a better action representation. We describe the training procedure of our 3D ResNets in details. We experimentally evaluate the 3D ResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the Kinetics did not suffer from overfitting despite the large number of parameters of the model, and achieved better performance than relatively shallow networks, such as C3D. Our code and pretrained models (e.g. Kinetics and ActivityNet) are publicly available at https://github.com/kenshohara/3D-ResNets.",Alespfewi3drenefoacre,233.0,23.0,31.0
474,Action Recognition,76.0,collaborative spatiotemporal feature learning for video action recognition,4.0,201.0,1.0,47.0,4.0,2.8,117.3,51,http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Collaborative_Spatiotemporal_Feature_Learning_for_Video_Action_Recognition_CVPR_2019_paper.pdf,"Spatiotemporal feature learning is of central importance for action recognition in videos. Existing deep neural network models either learn spatial and temporal features independently (C2D) or jointly with unconstrained parameters (C3D). In this paper, we propose a novel neural operation which encodes spatiotemporal features collaboratively by imposing a weight-sharing constraint on the learnable parameters. In particular, we perform 2D convolution along three orthogonal views of volumetric video data, which learns spatial appearance and temporal motion cues respectively. By sharing the convolution kernels of different views, spatial and temporal features are collaboratively learned and thus benefit from each other. The complementary features are subsequently fused by a weighted summation whose coefficients are learned end-to-end. Our approach achieves state-of-the-art performance on large-scale benchmarks and won the 1st place in the Moments in Time Challenge 2018. Moreover, based on the learned coefficients of different views, we are able to quantify the contributions of spatial and temporal features. This analysis sheds light on interpretability of the model and may also guide the future design of algorithm for video recognition.",Acospfelefoviacre,44.0,39.0,3.0
475,Action Recognition,82.0,action-stage emphasized spatiotemporal vlad for video action recognition,4.0,201.0,1.0,54.0,4.0,2.8,121.2,52,http://47.93.31.198/thesis/1_Action-Stage%20Emphasized%20Spatio-Temporal%20VLAD%20for%20Video%20Action%20Recognition_TIP2019.pdf,"Despite outstanding performance in image recognition, convolutional neural networks (CNNs) do not yet achieve the same impressive results on action recognition in videos. This is partially due to the inability of CNN for modeling long-range temporal structures especially those involving individual action stages that are critical to human action recognition. In this paper, we propose a novel action-stage (ActionS) emphasized spatiotemporal vector of locally aggregated descriptors (ActionS-ST-VLAD) method to aggregate informative deep features across the entire video according to adaptive video feature segmentation and adaptive segment feature sampling (AVFS-ASFS). In our ActionS-ST-VLAD encoding approach, by using AVFS-ASFS, the keyframe features are chosen and the corresponding deep features are automatically split into segments with the features in each segment belonging to a temporally coherent ActionS. Then, based on the extracted keyframe feature in each segment, a flow-guided warping technique is introduced to detect and discard redundant feature maps, while the informative ones are aggregated by using our exploited similarity weight. Furthermore, we exploit an RGBF modality to capture motion salient regions in the RGB images corresponding to action activity. Extensive experiments are conducted on four public benchmarks—HMDB51, UCF101, Kinetics, and ActivityNet for evaluation. Results show that our method is able to effectively pool useful deep features spatiotemporally, leading to the state-of-the-art performance for video-based action recognition.",Aacemspvlfoviacre,43.0,62.0,0.0
476,Action Recognition,91.0,adaptive fusion and category-level dictionary learning model for multiview human action recognition,4.0,201.0,1.0,45.0,4.0,2.8,121.2,53,http://arxiv.org/abs/2010.13302v1,"Human actions are often captured by multiple cameras (or sensors) to overcome the significant variations in viewpoints, background clutter, object speed, and motion patterns in video surveillance, and action recognition systems often benefit from fusing multiple types of cameras (sensors). Therefore, adaptive fusion of the information from multiple domains is mandatory for multiview human action recognition. Two widely applied fusion schemes are feature-level fusion and score-level fusion. We point out that limitations still exist and there is tremendous room for improvement, including the separate computation of feature fusion and action recognition, or the fixed weights for each action and each camera. However, previous fusion methods cannot accomplish them. In this paper, inspired by nature, the above limitations are addressed for multiview action recognition by developing a novel adaptive fusion and category-level dictionary learning model (abbreviated to AFCDL). It can jointly learn the adaptive weight for each camera and optimize the reconstruction of samples toward the action recognition task. To induce the dictionary learning and the reconstruction of query set (or test samples), the induced set for each category is built, and the corresponding induced regularization term is designed for the objective function. Extensive experiments on four public multiview action benchmarks show that AFCDL can significantly outperforms the state-of-the-art methods with 3% to 10% improvement in recognition accuracy.",Aadfuancadilemofomuhuacre,89.0,64.0,0.0
477,Action Recognition,5.0,a survey on vision-based human action recognition,5.0,201.0,1.0,137.0,3.0,2.8,123.0,54,https://www.academia.edu/download/35315898/A-survey-on-vision-based-human-action-recognition_Image-and-Vision-Computing_2010.pdf,"Vision-based human action recognition is the process of labeling image sequences with action labels. Robust solutions to this problem have applications in domains such as visual surveillance, video retrieval and human-computer interaction. The task is challenging due to variations in motion performance, recording settings and inter-personal differences. In this survey, we explicitly address these challenges. We provide a detailed overview of current advances in the field. Image representations and the subsequent classification process are discussed separately to focus on the novelties of recent research. Moreover, we discuss limitations of the state of the art and outline promising directions of research.",Aasuonvihuacre,2039.0,199.0,66.0
478,Action Recognition,35.0,deep local video feature for action recognition,5.0,201.0,1.0,135.0,3.0,2.8,131.4,55,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w14/papers/Lan_Deep_Local_Video_CVPR_2017_paper.pdf,"We investigate the problem of representing an entire video using CNN features for human action recognition. End-to-end learning of CNN/RNNs is currently not possible for whole videos due to GPU memory limitations and so a common practice is to use sampled frames as inputs along with the video labels as supervision. However, the global video labels might not be suitable for all of the temporally local samples as the videos often contain content besides the action of interest. We therefore propose to instead treat the deep networks trained on local inputs as local feature extractors. The local features are then aggregated to form global features which are used to assign video-level labels through a second classification stage. We investigate a number of design choices for this local feature approach. Experimental results on the HMDB51 and UCF101 datasets show that a simple maximum pooling on the sparsely sampled local features leads to significant performance improvement.",Adelovifefoacre,78.0,34.0,5.0
479,Action Recognition,4.0,action recognition based on a bag of 3d points,5.0,201.0,1.0,172.0,3.0,2.8,133.20000000000002,56,https://ro.uow.edu.au/cgi/viewcontent.cgi?article=2502&context=infopapers,"This paper presents a method to recognize human actions from sequences of depth maps. Specifically, we employ an action graph to model explicitly the dynamics of the actions and a bag of 3D points to characterize a set of salient postures that correspond to the nodes in the action graph. In addition, we propose a simple, but effective projection based sampling scheme to sample the bag of 3D points from the depth maps. Experimental results have shown that over 90% recognition accuracy were achieved by sampling only about 1% 3D points from the depth maps. Compared to the 2D silhouette based recognition, the recognition errors were halved. In addition, we demonstrate the potential of the bag of points posture model to deal with occlusions through simulation.",Aacrebaonabaof3dpo,1211.0,22.0,286.0
480,Action Recognition,3.0,evaluation of local spatio-temporal features for action recognition,5.0,201.0,1.0,174.0,3.0,2.8,133.5,57,https://hal.inria.fr/docs/00/43/97/69/PDF/paper.pdf,"Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.",Aevoflospfefoacre,1438.0,31.0,172.0
481,Action Recognition,94.0,action recognition based on joint trajectory maps with convolutional neural networks,4.0,201.0,1.0,88.0,4.0,2.8,135.0,58,http://arxiv.org/pdf/1612.09401v1,"Abstract Convolutional Neural Networks (ConvNets) have recently shown promising performance in many computer vision tasks, especially image-based recognition. How to effectively apply ConvNets to sequence-based data is still an open problem. This paper proposes an effective yet simple method to represent spatio-temporal information carried in 3 D skeleton sequences into three 2 D images by encoding the joint trajectories and their dynamics into color distribution in the images, referred to as Joint Trajectory Maps (JTM), and adopts ConvNets to learn the discriminative features for human action recognition. Such an image-based representation enables us to fine-tune existing ConvNets models for the classification of skeleton sequences without training the networks afresh. The three JTMs are generated in three orthogonal planes and provide complimentary information to each other. The final recognition is further improved through multiplicative score fusion of the three JTMs. The proposed method was evaluated on four public benchmark datasets, the large NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset and UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the state-of-the-art results.",Aacrebaonjotrmawiconene,106.0,96.0,12.0
482,Action Recognition,133.0,actionclip: a new paradigm for video action recognition,3.0,98.0,4.0,201.0,1.0,2.8,139.39999999999998,59,https://arxiv.org/pdf/2109.08472,"Convolutional Neural Networks (ConvNets) have recently shown promising performance in many computer vision tasks, especially image-based recognition. How to effectively apply ConvNets to sequence-based data is still an open problem. This paper proposes an effective yet simple method to represent spatio-temporal information carried in $3D$ skeleton sequences into three $2D$ images by encoding the joint trajectories and their dynamics into color distribution in the images, referred to as Joint Trajectory Maps (JTM), and adopts ConvNets to learn the discriminative features for human action recognition. Such an image-based representation enables us to fine-tune existing ConvNets models for the classification of skeleton sequences without training the networks afresh. The three JTMs are generated in three orthogonal planes and provide complimentary information to each other. The final recognition is further improved through multiply score fusion of the three JTMs. The proposed method was evaluated on four public benchmark datasets, the large NTU RGB+D Dataset, MSRC-12 Kinect Gesture Dataset (MSRC-12), G3D Dataset and UTD Multimodal Human Action Dataset (UTD-MHAD) and achieved the state-of-the-art results.",Aacanepafoviacre,0.0,54.0,0.0
483,Action Recognition,38.0,learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis,5.0,201.0,1.0,181.0,3.0,2.8,146.10000000000002,60,https://research.google/pubs/pub44932.pdf,"Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/∼wzou/",Alehiinspfefoacrewiinsuan,1045.0,50.0,81.0
484,Action Recognition,28.0,review of action recognition and detection methods,5.0,201.0,1.0,192.0,3.0,2.8,146.4,61,https://arxiv.org/pdf/1610.06906,"In computer vision, action recognition refers to the act of classifying an action that is present in a given video and action detection involves locating actions of interest in space and/or time. Videos, which contain photometric information (e.g. RGB, intensity values) in a lattice structure, contain information that can assist in identifying the action that has been imaged. The process of action recognition and detection often begins with extracting useful features and encoding them to ensure that the features are specific to serve the task of action recognition and detection. Encoded features are then processed through a classifier to identify the action class and their spatial and/or temporal locations. In this report, a thorough review of various action recognition and detection algorithms in computer vision is provided by analyzing the two-step process of a typical action recognition and detection algorithm: (i) extraction and encoding of features, and (ii) classifying features into action classes. In efforts to ensure that computer vision-based algorithms reach the capabilities that humans have of identifying actions irrespective of various nuisance variables that may be present within the field of view, the state-of-the-art methods are reviewed and some remaining problems are addressed in the final chapter.",Areofacreandeme,41.0,227.0,4.0
485,Action Recognition,401.0,asynchronous temporal fields for action recognition,1.0,44.0,4.0,162.0,3.0,2.8,186.5,62,http://arxiv.org/pdf/1612.06371v2,"Actions are more than just movements and trajectories: we cook to eat and we hold a cup to drink from it. A thorough understanding of videos requires going beyond appearance modeling and necessitates reasoning about the sequence of activities, as well as the higher-level constructs such as intentions. But how do we model and reason about these? We propose a fully-connected temporal CRF model for reasoning over various aspects of activities that includes objects, actions, and intentions, where the potentials are predicted by a deep network. End-to-end training of such structured models is a challenging endeavor: For inference and learning we need to construct mini-batches consisting of whole videos, leading to mini-batches with only a few videos. This causes high-correlation between data points leading to breakdown of the backprop algorithm. To address this challenge, we present an asynchronous variational inference method that allows efficient end-to-end training. Our method achieves a classification mAP of 22.4% on the Charades [42] benchmark, outperforming the state-of-the-art (17.2% mAP), and offers equal gains on the task of temporal localization.",Aastefifoacre,131.0,70.0,19.0
486,Action Recognition,401.0,contextual action recognition with r*cnn,1.0,64.0,4.0,173.0,3.0,2.8,197.8,63,http://arxiv.org/pdf/1505.01197v3,"There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.",Acoacrewir*,326.0,42.0,31.0
487,Action Recognition,401.0,out-of-distribution detection for generalized zero-shot action recognition,1.0,124.0,3.0,46.0,4.0,2.7,183.7,64,http://arxiv.org/pdf/1903.01092v1,"Generalized zero-shot action recognition is a challenging problem, where the task is to recognize new action categories that are unavailable during the training stage, in addition to the seen action categories. Existing approaches suffer from the inherent bias of the learned classifier towards the seen action categories. As a consequence, unseen category samples are incorrectly classified as belonging to one of the seen action categories. In this paper, we set out to tackle this issue by arguing for a separate treatment of seen and unseen action categories in generalized zero-shot action recognition. We introduce an out-of-distribution detector that determines whether the video features belong to a seen or unseen action category. To train our out-of-distribution detector, video features for unseen action categories are synthesized using generative adversarial networks trained on seen action category features. To the best of our knowledge, we are the first to propose an out-of-distribution detector based GZSL framework for action recognition in videos. Experiments are performed on three action recognition datasets: Olympic Sports, HMDB51 and UCF101. For generalized zero-shot action recognition, our proposed approach outperforms the baseline with absolute gains (in classification accuracy) of 7.0%, 3.4%, and 4.9%, respectively, on these datasets.",Aoudefogezeacre,50.0,41.0,9.0
488,Action Recognition,401.0,"videolstm convolves, attends and flows for action recognition",1.0,173.0,3.0,48.0,4.0,2.7,203.9,65,http://arxiv.org/pdf/1607.01794v1,"We present a new architecture for end-to-end sequence learning of actions in video, we call VideoLSTM. Rather than adapting the video to the peculiarities of established recurrent or convolutional architectures, we adapt the architecture to fit the requirements of the video medium. Starting from the soft-Attention LSTM, VideoLSTM makes three novel contributions. First, video has a spatial layout. To exploit the spatial correlation we hardwire convolutions in the soft-Attention LSTM architecture. Second, motion not only informs us about the action content, but also guides better the attention towards the relevant spatio-temporal locations. We introduce motion-based attention. And finally, we demonstrate how the attention from VideoLSTM can be used for action localization by relying on just the action class label. Experiments and comparisons on challenging datasets for action classification and localization support our claims.",Avicoatanflfoacre,305.0,61.0,31.0
489,Action Recognition,401.0,human action recognition by representing 3d skeletons as points in a lie group,1.0,190.0,3.0,74.0,4.0,2.7,218.5,66,http://arxiv.org/pdf/1406.2282v1,"Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of Shotton et al. [16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE(3), the proposed skeletal representation lies in the Lie group SE(3)×.. .×SE(3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.",Ahuacrebyre3dskaspoinaligr,1012.0,33.0,126.0
490,Action Recognition,401.0,sparse 3d convolutional neural networks,1.0,12.0,5.0,201.0,1.0,2.6,185.4,67,http://arxiv.org/pdf/1706.01307v1,"Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include pen-strokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard ""dense"" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than ""dilating"" the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation.",Asp3dconene,99.0,18.0,9.0
491,Action Recognition,401.0,actor-centric relation network,1.0,14.0,5.0,201.0,1.0,2.6,186.2,68,http://arxiv.org/abs/0806.2274v2,"Many, if not most network analysis algorithms have been designed specifically for single-relational networks; that is, networks in which all edges are of the same type. For example, edges may either represent ""friendship,"" ""kinship,"" or ""collaboration,"" but not all of them together. In contrast, a multi-relational network is a network with a heterogeneous set of edge labels which can represent relationships of various types in a single data structure. While multi-relational networks are more expressive in terms of the variety of relationships they can capture, there is a need for a general framework for transferring the many single-relational network analysis algorithms to the multi-relational domain. It is not sufficient to execute a single-relational network analysis algorithm on a multi-relational network by simply ignoring edge labels. This article presents an algebra for mapping multi-relational networks to single-relational networks, thereby exposing them to single-relational network analysis algorithms.",Aacrene,101.0,66.0,9.0
492,Action Recognition,401.0,what makes training multi-modal classification networks hard?,1.0,16.0,5.0,201.0,1.0,2.6,187.0,69,http://arxiv.org/pdf/1905.12681v5,"Consider end-to-end training of a multi-modal vs. a single-modal network on a task with multiple input modalities: the multi-modal network receives more information, so it should match or outperform its single-modal counterpart. In our experiments, however, we observe the opposite: the best single-modal network always outperforms the multi-modal network. This observation is consistent across different combinations of modalities and on different tasks and benchmarks.   This paper identifies two main causes for this performance drop: first, multi-modal networks are often prone to overfitting due to increased capacity. Second, different modalities overfit and generalize at different rates, so training them jointly with a single optimization strategy is sub-optimal. We address these two problems with a technique we call Gradient Blending, which computes an optimal blend of modalities based on their overfitting behavior. We demonstrate that Gradient Blending outperforms widely-used baselines for avoiding overfitting and achieves state-of-the-art accuracy on various tasks including human action recognition, ego-centric action recognition, and acoustic event detection.",Awhmatrmuclneha,60.0,70.0,7.0
493,Action Recognition,401.0,multivariate lstm-fcns for time series classification,1.0,17.0,5.0,201.0,1.0,2.6,187.4,70,http://arxiv.org/pdf/1409.8211v2,"Kernel-based approaches for sequence classification have been successfully applied to a variety of domains, including the text categorization, image classification, speech analysis, biological sequence analysis, time series and music classification, where they show some of the most accurate results.   Typical kernel functions for sequences in these domains (e.g., bag-of-words, mismatch, or subsequence kernels) are restricted to {\em discrete univariate} (i.e. one-dimensional) string data, such as sequences of words in the text analysis, codeword sequences in the image analysis, or nucleotide or amino acid sequences in the DNA and protein sequence analysis. However, original sequence data are often of real-valued multivariate nature, i.e. are not univariate and discrete as required by typical $k$-mer based sequence kernel functions.   In this work, we consider the problem of the {\em multivariate} sequence classification such as classification of multivariate music sequences, or multidimensional protein sequence representations. To this end, we extend {\em univariate} kernel functions typically used in sequence analysis and propose efficient {\em multivariate} similarity kernel method (MVDFQ-SK) based on (1) a direct feature quantization (DFQ) of each sequence dimension in the original {\em real-valued} multivariate sequences and (2) applying novel multivariate discrete kernel measures on these multivariate discrete DFQ sequence representations to more accurately capture similarity relationships among sequences and improve classification performance.   Experiments using the proposed MVDFQ-SK kernel method show excellent classification performance on three challenging music classification tasks as well as protein sequence classification with significant 25-40% improvements over univariate kernel methods and existing state-of-the-art sequence classification methods.",Amulsfotisecl,213.0,68.0,31.0
494,Action Recognition,401.0,ts-lstm and temporal-inception: exploiting spatiotemporal dynamics for activity recognition,1.0,18.0,5.0,201.0,1.0,2.6,187.8,71,http://arxiv.org/pdf/1703.10667v1,"Recent two-stream deep Convolutional Neural Networks (ConvNets) have made significant progress in recognizing human actions in videos. Despite their success, methods extending the basic two-stream ConvNet have not systematically explored possible network architectures to further exploit spatiotemporal dynamics within video sequences. Further, such networks often use different baseline two-stream networks. Therefore, the differences and the distinguishing factors between various methods using Recurrent Neural Networks (RNN) or convolutional networks on temporally-constructed feature vectors (Temporal-ConvNet) are unclear. In this work, we first demonstrate a strong baseline two-stream ConvNet using ResNet-101. We use this baseline to thoroughly examine the use of both RNNs and Temporal-ConvNets for extracting spatiotemporal information. Building upon our experimental results, we then propose and investigate two different networks to further integrate spatiotemporal information: 1) temporal segment RNN and 2) Inception-style Temporal-ConvNet. We demonstrate that using both RNNs (using LSTMs) and Temporal-ConvNets on spatiotemporal feature matrices are able to exploit spatiotemporal dynamics to improve the overall performance. However, each of these methods require proper care to achieve state-of-the-art performance; for example, LSTMs require pre-segmented data or else they cannot fully exploit temporal information. Our analysis identifies specific limitations for each method that could form the basis of future work. Our experimental results on UCF101 and HMDB51 datasets achieve state-of-the-art performances, 94.1% and 69.0%, respectively, without requiring extensive temporal augmentation.",Atsanteexspdyfoacre,140.0,59.0,5.0
495,Action Recognition,401.0,you only watch once: a unified cnn architecture for real-time spatiotemporal action localization,1.0,20.0,5.0,201.0,1.0,2.6,188.6,72,http://arxiv.org/pdf/1911.06644v5,"Spatiotemporal action localization requires the incorporation of two sources of information into the designed architecture: (1) temporal information from the previous frames and (2) spatial information from the key frame. Current state-of-the-art approaches usually extract these information with separate networks and use an extra mechanism for fusion to get detections. In this work, we present YOWO, a unified CNN architecture for real-time spatiotemporal action localization in video streams. YOWO is a single-stage architecture with two branches to extract temporal and spatial information concurrently and predict bounding boxes and action probabilities directly from video clips in one evaluation. Since the whole architecture is unified, it can be optimized end-to-end. The YOWO architecture is fast providing 34 frames-per-second on 16-frames input clips and 62 frames-per-second on 8-frames input clips, which is currently the fastest state-of-the-art architecture on spatiotemporal action localization task. Remarkably, YOWO outperforms the previous state-of-the art results on J-HMDB-21 and UCF101-24 with an impressive improvement of ~3% and ~12%, respectively. Moreover, YOWO is the first and only single-stage architecture that provides competitive results on AVA dataset. We make our code and pretrained models publicly available.",Ayoonwaonauncnarforespaclo,22.0,61.0,7.0
496,Action Recognition,401.0,a pursuit of temporal accuracy in general activity detection,1.0,21.0,5.0,201.0,1.0,2.6,189.0,73,http://arxiv.org/pdf/1703.02716v1,"Detecting activities in untrimmed videos is an important but challenging task. The performance of existing methods remains unsatisfactory, e.g., they often meet difficulties in locating the beginning and end of a long complex action. In this paper, we propose a generic framework that can accurately detect a wide variety of activities from untrimmed videos. Our first contribution is a novel proposal scheme that can efficiently generate candidates with accurate temporal boundaries. The other contribution is a cascaded classification pipeline that explicitly distinguishes between relevance and completeness of a candidate instance. On two challenging temporal activity detection datasets, THUMOS14 and ActivityNet, the proposed framework significantly outperforms the existing state-of-the-art methods, demonstrating superior accuracy and strong adaptivity in handling activities with various temporal structures.",Aapuofteacingeacde,87.0,44.0,11.0
497,Action Recognition,401.0,towards good practices for very deep two-stream convnets,1.0,23.0,5.0,201.0,1.0,2.6,189.8,74,http://arxiv.org/pdf/1507.02159v1,"Deep convolutional networks have achieved great success for object recognition in still images. However, for action recognition in videos, the improvement of deep convolutional networks is not so evident. We argue that there are two reasons that could probably explain this result. First the current network architectures (e.g. Two-stream ConvNets) are relatively shallow compared with those very deep models in image domain (e.g. VGGNet, GoogLeNet), and therefore their modeling capacity is constrained by their depth. Second, probably more importantly, the training dataset of action recognition is extremely small compared with the ImageNet dataset, and thus it will be easy to over-fit on the training dataset.   To address these issues, this report presents very deep two-stream ConvNets for action recognition, by adapting recent very deep architectures into video domain. However, this extension is not easy as the size of action recognition is quite small. We design several good practices for the training of very deep two-stream ConvNets, namely (i) pre-training for both spatial and temporal nets, (ii) smaller learning rates, (iii) more data augmentation techniques, (iv) high drop out ratio. Meanwhile, we extend the Caffe toolbox into Multi-GPU implementation with high computational efficiency and low memory consumption. We verify the performance of very deep two-stream ConvNets on the dataset of UCF101 and it achieves the recognition accuracy of $91.4\%$.",Atogoprfovedetwco,351.0,30.0,61.0
498,Action Recognition,401.0,a comprehensive study of deep video action recognition,1.0,26.0,5.0,201.0,1.0,2.6,191.0,75,http://arxiv.org/pdf/2012.06567v1,"Video action recognition is one of the representative tasks for video understanding. Over the last decade, we have witnessed great advancements in video action recognition thanks to the emergence of deep learning. But we also encountered new challenges, including modeling long-range temporal information in videos, high computation costs, and incomparable results due to datasets and evaluation protocol variances. In this paper, we provide a comprehensive survey of over 200 existing papers on deep learning for video action recognition. We first introduce the 17 video action recognition datasets that influenced the design of models. Then we present video action recognition models in chronological order: starting with early attempts at adapting deep learning, then to the two-stream networks, followed by the adoption of 3D convolutional kernels, and finally to the recent compute-efficient models. In addition, we benchmark popular methods on several representative datasets and release code for reproducibility. In the end, we discuss open problems and shed light on opportunities for video action recognition to facilitate new research ideas.",Aacostofdeviacre,12.0,294.0,0.0
499,Action Recognition,401.0,ntu rgb+d 120: a large-scale benchmark for 3d human activity understanding,1.0,28.0,5.0,201.0,1.0,2.6,191.8,76,http://arxiv.org/abs/1905.04757v2,"Research on depth-based human activity analysis achieved outstanding performance and demonstrated the effectiveness of 3D representation for action recognition. The existing depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of large-scale training samples, realistic number of distinct class categories, diversity in camera views, varied environmental conditions, and variety of human subjects. In this work, we introduce a large-scale dataset for RGB+D human action recognition, which is collected from 106 distinct subjects and contains more than 114 thousand video samples and 8 million frames. This dataset contains 120 different action classes including daily, mutual, and health-related activities. We evaluate the performance of a series of existing 3D activity analysis methods on this dataset, and show the advantage of applying deep learning methods for 3D-based human action recognition. Furthermore, we investigate a novel one-shot 3D activity recognition problem on our dataset, and a simple yet effective Action-Part Semantic Relevance-aware (APSR) framework is proposed for this task, which yields promising results for recognition of the novel action classes. We believe the introduction of this large-scale dataset will enable the community to apply, adapt, and develop various data-hungry learning techniques for depth-based and RGB+D-based human activity understanding. [The dataset is available at: http://rose1.ntu.edu.sg/Datasets/actionRecognition.asp]",Antrg12alabefo3dhuacun,218.0,118.0,56.0
500,Action Recognition,401.0,moments in time dataset: one million videos for event understanding,1.0,30.0,5.0,201.0,1.0,2.6,192.6,77,http://arxiv.org/pdf/1801.03150v3,"We present the Moments in Time Dataset, a large-scale human-annotated collection of one million short videos corresponding to dynamic events unfolding within three seconds. Modeling the spatial-audio-temporal dynamics even for actions occurring in 3 second videos poses many challenges: meaningful events do not include only people, but also objects, animals, and natural phenomena; visual and auditory events can be symmetrical in time (""opening"" is ""closing"" in reverse), and either transient or sustained. We describe the annotation process of our dataset (each video is tagged with one action or activity label among 339 different classes), analyze its scale and diversity in comparison to other large-scale video datasets for action recognition, and report results of several baseline models addressing separately, and jointly, three modalities: spatial, temporal and auditory. The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis.",Amointidaonmivifoevun,258.0,75.0,40.0
501,Action Recognition,401.0,unsupervised learning of video representations using lstms,1.0,31.0,5.0,201.0,1.0,2.6,193.0,78,http://arxiv.org/pdf/1502.04681v3,"We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (""percepts"") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.",Aunleofvireusls,1806.0,41.0,202.0
502,Action Recognition,401.0,graph convolutional networks for temporal action localization,1.0,32.0,5.0,201.0,1.0,2.6,193.4,79,http://arxiv.org/pdf/2012.08804v1,"Graph Convolutional Networks (GCNs), which model skeleton data as graphs, have obtained remarkable performance for skeleton-based action recognition. Particularly, the temporal dynamic of skeleton sequence conveys significant information in the recognition task. For temporal dynamic modeling, GCN-based methods only stack multi-layer 1D local convolutions to extract temporal relations between adjacent time steps. With the repeat of a lot of local convolutions, the key temporal information with non-adjacent temporal distance may be ignored due to the information dilution. Therefore, these methods still remain unclear how to fully explore temporal dynamic of skeleton sequence. In this paper, we propose a Temporal Enhanced Graph Convolutional Network (TE-GCN) to tackle this limitation. The proposed TE-GCN constructs temporal relation graph to capture complex temporal dynamic. Specifically, the constructed temporal relation graph explicitly builds connections between semantically related temporal features to model temporal relations between both adjacent and non-adjacent time steps. Meanwhile, to further explore the sufficient temporal dynamic, multi-head mechanism is designed to investigate multi-kinds of temporal relations. Extensive experiments are performed on two widely used large-scale datasets, NTU-60 RGB+D and NTU-120 RGB+D. And experimental results show that the proposed model achieves the state-of-the-art performance by making contribution to temporal modeling for action recognition.",Agrconefoteaclo,160.0,56.0,21.0
503,Action Recognition,401.0,delving deeper into convolutional networks for learning video representations,1.0,34.0,5.0,201.0,1.0,2.6,194.2,80,http://arxiv.org/pdf/1511.06432v4,"We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call ""percepts"" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations.   We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.",Adedeinconefolevire,392.0,46.0,56.0
504,Action Recognition,401.0,describing videos by exploiting temporal structure,1.0,35.0,5.0,201.0,1.0,2.6,194.6,81,http://arxiv.org/pdf/1502.08029v5,"Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.",Adevibyextest,842.0,57.0,139.0
505,Action Recognition,401.0,temporal action localization in untrimmed videos via multi-stage cnns,1.0,39.0,5.0,201.0,1.0,2.6,196.2,82,http://arxiv.org/pdf/1601.02129v2,"We address temporal action localization in untrimmed long videos. This is important because videos in real applications are usually unconstrained and contain multiple action instances plus video content of background scenes or other activities. To address this challenging issue, we exploit the effectiveness of deep networks in temporal action localization via three segment-based 3D ConvNets: (1) a proposal network identifies candidate segments in a long video that may contain actions; (2) a classification network learns one-vs-all action classification model to serve as initialization for the localization network; and (3) a localization network fine-tunes on the learned classification network to localize each action instance. We propose a novel loss function for the localization network to explicitly consider temporal overlap and therefore achieve high temporal localization accuracy. Only the proposal network and the localization network are used during prediction. On two large-scale benchmarks, our approach achieves significantly superior performances compared with other state-of-the-art systems: mAP increases from 1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014, when the overlap threshold for evaluation is set to 0.5.",Ateacloinunvivimucn,575.0,50.0,82.0
506,Action Recognition,55.0,yolo based human action recognition and localization,4.0,201.0,1.0,110.0,3.0,2.5,129.9,83,https://www.sciencedirect.com/science/article/pii/S1877050918310652/pdf?md5=a8f0aea2e42cf5b00fbc2a636d0ea147&pid=1-s2.0-S1877050918310652-main.pdf,"Abstract Human action recognition in video analytics has been widely studied in recent years. Yet, most of these methods assign a single action label to video after either analyzing a complete video or using classifier for each frame. But when compared to human vision strategy, it can be deduced that we (human) require just an instance of visual data for recognition of scene. It turns out that small group of frames or even single frame from the video are enough for precise recognition. In this paper, we present an approach to detect, localize and recognize actions of interest in almost real-time from frames obtained by a continuous stream of video data that can be captured from a surveillance camera. The model takes input frames after a specified period and is able to give action label based on a single frame. Combining results over specific time we predicted the action label for the stream of video. We demonstrate that YOLO is effective method and comparatively fast for recognition and localization in Liris Human Activities dataset.",Ayobahuacreanlo,49.0,5.0,1.0
507,Action Recognition,42.0,learning features combination for human action recognition from skeleton sequences,4.0,201.0,1.0,136.0,3.0,2.5,133.8,84,https://hal.archives-ouvertes.fr/hal-01515376/file/root.pdf,"Abstract Human action recognition is a challenging task due to the complexity of human movements and to the variety among the same actions performed by distinct subjects. Recent technologies provide the skeletal representation of human body extracted in real time from depth maps, which is a high discriminant information for efficient action recognition. In this context, we present a new framework for human action recognition from skeleton sequences. We propose extracting sets of spatial and temporal local features from subgroups of joints, which are aggregated by a robust method based on the VLAD algorithm and a pool of clusters. Several feature vectors are then combined by a metric learning method inspired by the LMNN algorithm with the objective to improve the classification accuracy using the nonparametric k-NN classifier. We evaluated our method on three public datasets, including the MSR-Action3D, the UTKinect-Action3D, and the Florence 3D Actions dataset. As a result, the proposed framework performance overcomes the methods in the state of the art on all the experiments.",Alefecofohuacrefrskse,71.0,34.0,7.0
508,Action Recognition,57.0,human action recognition using transfer learning with deep representations,4.0,201.0,1.0,148.0,3.0,2.5,141.9,85,https://eprints.lancs.ac.uk/id/document/56604,"Human action recognition is an imperative research area in the field of computer vision due to its numerous applications. Recently, with the emergence and successful deployment of deep learning techniques for image classification, object recognition, and speech recognition, more research is directed from traditional handcrafted to deep learning techniques. This paper presents a novel method for human action recognition based on a pre-trained deep CNN model for feature extraction & representation followed by a hybrid Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) classifier for action recognition. It has been observed that already learnt CNN based representations on large-scale annotated dataset could be transferred to action recognition task with limited training dataset. The proposed method is evaluated on two well-known action datasets, i.e., UCF sports and KTH. The comparative analysis confirms that the proposed method achieves superior performance over state-of-the-art methods in terms of accuracy.",Ahuacreustrlewidere,61.0,55.0,3.0
509,Action Recognition,156.0,action recognition with spatio–temporal visual attention on skeleton image sequences,3.0,201.0,1.0,52.0,4.0,2.5,142.8,86,https://arxiv.org/pdf/1801.10304,"Action recognition with 3D skeleton sequences became popular due to its speed and robustness. The recently proposed convolutional neural networks (CNNs)-based methods show a good performance in learning spatio–temporal representations for skeleton sequences. Despite the good recognition accuracy achieved by previous CNN-based methods, there existed two problems that potentially limit the performance. First, previous skeleton representations were generated by chaining joints with a fixed order. The corresponding semantic meaning was unclear and the structural information among the joints was lost. Second, previous models did not have an ability to focus on informative joints. The attention mechanism was important for skeleton-based action recognition because different joints contributed unequally toward the correct recognition. To solve these two problems, we proposed a novel CNN-based method for skeleton-based action recognition. We first redesigned the skeleton representations with a depth-first tree traversal order, which enhanced the semantic meaning of skeleton images and better preserved the associated structural information. We then proposed the general two-branch attention architecture that automatically focused on spatio–temporal key stages and filtered out unreliable joint predictions. Based on the proposed general architecture, we designed a global long-sequence attention network with refined branch structures. Furthermore, in order to adjust the kernel’s spatio–temporal aspect ratios and better capture long-term dependencies, we proposed a sub-sequence attention network (SSAN) that took sub-image sequences as inputs. We showed that the two-branch attention architecture could be combined with the SSAN to further improve the performance. Our experiment results on the NTU RGB+D data set and the SBU kinetic interaction data set outperformed the state of the art. The model was further validated on noisy estimated poses from the subsets of the UCF101 data set and the kinetics data set.",Aacrewispviatonskimse,64.0,63.0,2.0
510,Action Recognition,45.0,multimodal multipart learning for action recognition in depth videos,4.0,201.0,1.0,164.0,3.0,2.5,143.1,87,https://arxiv.org/pdf/1507.08761,"The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy.",Amumulefoacreindevi,103.0,57.0,2.0
511,Action Recognition,93.0,a real-time human action recognition system using depth and inertial sensor fusion,4.0,201.0,1.0,149.0,3.0,2.5,153.0,88,https://tarjomefa.com/wp-content/uploads/2017/11/8143-English-TarjomeFa.pdf,"This paper presents a human action recognition system that runs in real time and simultaneously uses a depth camera and an inertial sensor based on a previously developed sensor fusion method. Computationally efficient depth image features and inertial signals features are fed into two computationally efficient collaborative representative classifiers. A decision-level fusion is then performed. The developed real-time system is evaluated using a publicly available multimodal human action recognition data set by considering a comprehensive set of human actions. The overall classification rate of the developed real-time system is shown to be >97%, which is at least 9% higher than when each sensing modality is used individually. The results from both offline and real-time experimentations demonstrate the effectiveness of the system and its real-time throughputs.",Aarehuacresyusdeaninsefu,123.0,32.0,7.0
512,Action Recognition,165.0,learning clip representations for skeleton-based 3d action recognition,3.0,201.0,1.0,89.0,4.0,2.5,156.6,89,http://arxiv.org/pdf/2001.00294v1,"This paper presents a new representation of skeleton sequences for 3D action recognition. Existing methods based on hand-crafted features or recurrent neural networks cannot adequately capture the complex spatial structures and the long-term temporal dynamics of the skeleton sequences, which are very important to recognize the actions. In this paper, we propose to transform each channel of the 3D coordinates of a skeleton sequence into a clip. Each frame of the generated clip represents the temporal information of the entire skeleton sequence and one particular spatial relationship between the skeleton joints. The entire clip incorporates multiple frames with different spatial relationships, which provide useful spatial structural information of the human skeleton. We also propose a multitask convolutional neural network (MTCNN) to learn the generated clips for action recognition. The proposed MTCNN processes all the frames of the generated clips in parallel to explore the spatial and temporal information of the skeleton sequences. The proposed method has been extensively tested on six challenging benchmark datasets. Experimental results consistently demonstrate the superiority of the proposed clip representation and the feature learning method for 3D action recognition compared to the existing techniques.",Aleclrefosk3dacre,103.0,77.0,12.0
513,Action Recognition,166.0,real-time action recognition with deeply transferred motion vector cnns,3.0,201.0,1.0,93.0,4.0,2.5,158.1,90,https://wanglimin.github.io/papers/ZhangWWQW_TIP18.pdf,"The two-stream CNNs prove very successful for video-based action recognition. However, the classical two-stream CNNs are time costly, mainly due to the bottleneck of calculating optical flows (OFs). In this paper, we propose a two-stream-based real-time action recognition approach by using motion vector (MV) to replace OF. MVs are encoded in video stream and can be extracted directly without extra calculation. However, directly training CNN with MVs degrades accuracy severely due to the noise and the lack of fine details in MVs. In order to relieve this problem, we propose four training strategies which leverage the knowledge learned from OF CNN to enhance the accuracy of MV CNN. Our insight is that MV and OF share inherent similar structures which allow us to transfer knowledge from one domain to another. To fully utilize the knowledge learned in OF domain, we develop deeply transferred MV CNN. Experimental results on various datasets show the effectiveness of our training strategies. Our approach is significantly faster than OF based approaches and achieves processing speed of 390.7 frames per second, surpassing real-time requirement. We release our model and code to facilitate further research.11https://github.com/zbwglory/MV-release",Areacrewidetrmovecn,81.0,55.0,6.0
514,Action Recognition,85.0,vlad3: encoding dynamics of deep features for action recognition,4.0,201.0,1.0,175.0,3.0,2.5,158.4,91,http://openaccess.thecvf.com/content_cvpr_2016/papers/Li_VLAD3_Encoding_Dynamics_CVPR_2016_paper.pdf,"Previous approaches to action recognition with deep features tend to process video frames only within a small temporal region, and do not model long-range dynamic information explicitly. However, such information is important for the accurate recognition of actions, especially for the discrimination of complex activities that share sub-actions, and when dealing with untrimmed videos. Here, we propose a representation, VLAD for Deep Dynamics (VLAD3), that accounts for different levels of video dynamics. It captures short-term dynamics with deep convolutional neural network features, relying on linear dynamic systems (LDS) to model medium-range dynamics. To account for long-range inhomogeneous dynamics, a VLAD descriptor is derived for the LDS and pooled over the whole video, to arrive at the final VLAD3 representation. An extensive evaluation was performed on Olympic Sports, UCF101 and THUMOS15, where the use of the VLAD3 representation leads to state-of-the-art results.",Avlendyofdefefoacre,66.0,39.0,7.0
516,Action Recognition,401.0,memory attention networks for skeleton-based action recognition,1.0,171.0,3.0,106.0,3.0,2.4000000000000004,220.5,92,http://arxiv.org/pdf/1804.08254v2,"Skeleton-based action recognition task is entangled with complex spatio-temporal variations of skeleton joints, and remains challenging for Recurrent Neural Networks (RNNs). In this work, we propose a temporal-then-spatial recalibration scheme to alleviate such complex variations, resulting in an end-to-end Memory Attention Networks (MANs) which consist of a Temporal Attention Recalibration Module (TARM) and a Spatio-Temporal Convolution Module (STCM). Specifically, the TARM is deployed in a residual learning module that employs a novel attention learning network to recalibrate the temporal attention of frames in a skeleton sequence. The STCM treats the attention calibrated skeleton joint sequences as images and leverages the Convolution Neural Networks (CNNs) to further model the spatial and temporal information of skeleton data. These two modules (TARM and STCM) seamlessly form a single network architecture that can be trained in an end-to-end fashion. MANs significantly boost the performance of skeleton-based action recognition and achieve the best results on four challenging benchmark datasets: NTU RGB+D, HDM05, SYSU-3D and UT-Kinect.",Ameatnefoskacre,56.0,33.0,7.0
517,Action Recognition,401.0,spatio-temporal naive-bayes nearest-neighbor (st-nbnn) for skeleton-based action recognition,1.0,200.0,3.0,160.0,3.0,2.4000000000000004,248.3,93,http://openaccess.thecvf.com/content_cvpr_2017/papers/Weng_Spatio-Temporal_Naive-Bayes_Nearest-Neighbor_CVPR_2017_paper.pdf,"Motivated by previous success of using non-parametric methods to recognize objects, e.g., NBNN [2], we extend it to recognize actions using skeletons. Each 3D action is presented by a sequence of 3D poses. Similar to NBNN, our proposed Spatio-Temporal-NBNN applies stage-to-class distance to classify actions. However, ST-NBNN takes the spatio-temporal structure of 3D actions into consideration and relaxes the Naive Bayes assumption of NBNN. Specifically, ST-NBNN adopts bilinear classifiers [19] to identify both key temporal stages as well as spatial joints for action classification. Although only using a linear classifier, experiments on three benchmark datasets show that by combining the strength of both non-parametric and parametric models, ST-NBNN can achieve competitive performance compared with state-of-the-art results using sophisticated models such as deep learning. Moreover, by identifying key skeleton joints and temporal stages for each action class, our ST-NBNN can capture the essential spatio-temporal patterns that play key roles of recognizing actions, which is not always achievable by using end-to-end models.",Aspnane(sfoskacre,78.0,36.0,8.0
518,Action Recognition,171.0,learning self-similarity in space and time as generalized motion for video action recognition,3.0,180.0,3.0,201.0,1.0,2.4,183.6,94,https://openaccess.thecvf.com/content/ICCV2021/papers/Kwon_Learning_Self-Similarity_in_Space_and_Time_As_Generalized_Motion_for_ICCV_2021_paper.pdf,"Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. The proposed neural block, dubbed SELFY, can be easily inserted into neural architectures and trained end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition. Our experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, SomethingSomething-V1 & V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.",Aleseinspantiasgemofoviacre,0.0,0.0,0.0
519,Action Recognition,7.0,action recognition with actons,5.0,201.0,1.0,201.0,1.0,2.2,142.8,95,https://openaccess.thecvf.com/content_iccv_2013/papers/Zhu_Action_Recognition_with_2013_ICCV_paper.pdf,"Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the benefits of formulating actions as a combination of atomic-actions have shown promise in improving action understanding with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple viewpoints and multiple modalities of data for representation learning. To promote research in this direction, we introduce Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels. Leveraging rich multi-modal and multi-view settings, we propose Cooperative Compositional Action Understanding (CCAU), a cooperative learning framework for hierarchical action recognition that is aware of compositional action elements. CCAU shows consistent performance improvements across all modalities. Furthermore, we demonstrate the utility of co-learning compositions in few-shot action recognition by achieving 28.6% mAP with just a single sample.",Aacrewiac,100.0,36.0,3.0
520,Action Recognition,108.0,coding kendall's shape trajectories for 3d action recognition,3.0,201.0,1.0,112.0,3.0,2.2,146.4,96,https://openaccess.thecvf.com/content_cvpr_2018/papers/Tanfous_Coding_Kendalls_Shape_CVPR_2018_paper.pdf,"Suitable shape representations as well as their temporal evolution, termed trajectories, often lie to non-linear manifolds. This puts an additional constraint (i.e., non-linearity) in using conventional machine learning techniques for the purpose of classification, event detection, prediction, etc. This paper accommodates the well-known Sparse Coding and Dictionary Learning to the Kendall's shape space and illustrates effective coding of 3D skeletal sequences for action recognition. Grounding on the Riemannian geometry of the shape space, an intrinsic sparse coding and dictionary learning formulation is proposed for static skeletal shapes to overcome the inherent non-linearity of the manifold. As a main result, initial trajectories give rise to sparse code functions with suitable computational properties, including sparsity and vector space representation. To achieve action recognition, two different classification schemes were adopted. A bi-directional LSTM is directly performed on sparse code functions, while a linear SVM is applied after representing sparse code functions using Fourier temporal pyramid. Experiments conducted on three publicly available datasets show the superiority of the proposed approach compared to existing Riemannian representations and its competitiveness with respect to other recently-proposed approaches. When the benefits of invariance are maintained from the Kendall's shape representation, our approach not only overcomes the problem of non-linearity but also yields to discriminative sparse code functions.",Acokeshtrfo3dacre,37.0,44.0,3.0
521,Action Recognition,23.0,action mach a spatio-temporal maximum average correlation height filter for action recognition,5.0,201.0,1.0,201.0,1.0,2.2,147.60000000000002,97,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.407.9213&rep=rep1&type=pdf,"Convolutional Neural Networks (CNNs) are successful deep learning models in the field of computer vision. To get the maximum advantage of CNN model for Human Action Recognition (HAR) using inertial sensor data, in this paper, we use 4 types of spatial domain methods for transforming inertial sensor data to activity images, which are then utilized in a novel fusion framework. These four types of activity images are Signal Images (SI), Gramian Angular Field (GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP) Images. Furthermore, for creating a multimodal fusion framework and to exploit activity image, we made each type of activity images multimodal by convolving with two spatial domain filters : Prewitt filter and High-boost filter. Resnet-18, a CNN model, is used to learn deep features from multi-modalities. Learned features are extracted from the last pooling layer of each ReNet and then fused by canonical correlation based fusion (CCF) for improving the accuracy of human action recognition. These highly informative features are served as input to a multiclass Support Vector Machine (SVM). Experimental results on three publicly available inertial datasets show the superiority of the proposed method over the current state-of-the-art.",Aacmaaspmaavcohefifoacre,1219.0,23.0,170.0
522,Action Recognition,24.0,learning the viewpoint manifold for action recognition,5.0,201.0,1.0,201.0,1.0,2.2,147.9,98,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.409.3560&rep=rep1&type=pdf,"Recognizing human actions from unknown and unseen (novel) views is a challenging problem. We propose a Robust Non-Linear Knowledge Transfer Model (R-NKTM) for human action recognition from novel views. The proposed R-NKTM is a deep fully-connected neural network that transfers knowledge of human actions from any unknown view to a shared high-level virtual view by finding a non-linear virtual path that connects the views. The R-NKTM is learned from dense trajectories of synthetic 3D human models fitted to real motion capture data and generalizes to real videos of human actions. The strength of our technique is that we learn a single R-NKTM for all actions and all viewpoints for knowledge transfer of any real human action video without the need for re-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to incorporate new action classes. R-NKTM is learned with dummy labels and does not require knowledge of the camera viewpoint at any stage. Experiments on three benchmark cross-view human action datasets show that our method outperforms existing state-of-the-art.",Alethvimafoacre,96.0,16.0,2.0
523,Action Recognition,25.0,a human action recognition system for embedded computer vision application,5.0,201.0,1.0,201.0,1.0,2.2,148.2,99,https://www.cs.york.ac.uk/arch/publications/byyear/2007/a-human-action-recognition-system-for-embedded-/at_download/A%20Human%20Action%20Recognition%20System%20for%20Embedded%20Computer%20Vision%20Applications.pdf,"Most of the existing work on automatic facial expression analysis focuses on discrete emotion recognition, or facial action unit detection. However, facial expressions do not always fall neatly into pre-defined semantic categories. Also, the similarity between expressions measured in the action unit space need not correspond to how humans perceive expression similarity. Different from previous work, our goal is to describe facial expressions in a continuous fashion using a compact embedding space that mimics human visual preferences. To achieve this goal, we collect a large-scale faces-in-the-wild dataset with human annotations in the form: Expressions A and B are visually more similar when compared to expression C, and use this dataset to train a neural network that produces a compact (16-dimensional) expression embedding. We experimentally demonstrate that the learned embedding can be successfully used for various applications such as expression retrieval, photo album summarization, and emotion recognition. We also show that the embedding learned using the proposed dataset performs better than several other embeddings learned using existing emotion or action unit datasets.",Aahuacresyfoemcoviap,101.0,24.0,4.0
524,Action Recognition,26.0,free viewpoint action recognition using motion history volumes,5.0,201.0,1.0,201.0,1.0,2.2,148.5,100,https://hal.inria.fr/docs/00/54/46/29/PDF/cviu_motion_history_volumes.pdf,"Generating free-viewpoint videos is critical for immersive VR/AR experience but recent neural advances still lack the editing ability to manipulate the visual perception for large dynamic scenes. To fill this gap, in this paper we propose the first approach for editable photo-realistic free-viewpoint video generation for large-scale dynamic scenes using only sparse 16 cameras. The core of our approach is a new layered neural representation, where each dynamic entity including the environment itself is formulated into a space-time coherent neural layered radiance representation called ST-NeRF. Such layered representation supports fully perception and realistic manipulation of the dynamic scene whilst still supporting a free viewing experience in a wide range. In our ST-NeRF, the dynamic entity/layer is represented as continuous functions, which achieves the disentanglement of location, deformation as well as the appearance of the dynamic entity in a continuous and self-supervised manner. We propose a scene parsing 4D label map tracking to disentangle the spatial information explicitly, and a continuous deform module to disentangle the temporal motion implicitly. An object-aware volume rendering scheme is further introduced for the re-assembling of all the neural layers. We adopt a novel layered loss and motion-aware ray sampling strategy to enable efficient training for a large dynamic scene with multiple performers, Our framework further enables a variety of editing functions, i.e., manipulating the scale and location, duplicating or retiming individual neural layers to create numerous visual effects while preserving high realism. Extensive experiments demonstrate the effectiveness of our approach to achieve high-quality, photo-realistic, and editable free-viewpoint video generation for dynamic scenes.",Afrviacreusmohivo,903.0,26.0,99.0
857,Autonomous vehicles,15.0,airsim: high-fidelity visual and physical simulation for autonomous vehicles,5.0,1.0,5.0,3.0,5.0,5.0,5.800000000000001,1,https://arxiv.org/pdf/1705.05065.pdf%20http://arxiv.org/abs/1705.05065,"Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.",Aaihivianphsifoauve,761.0,35.0,92.0
858,Autonomous vehicles,4.0,"preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations",5.0,201.0,1.0,24.0,5.0,3.4,88.80000000000001,2,https://www.caee.utexas.edu/prof/kockelman/public_html/TRB14EnoAVs.pdf,"Autonomous vehicles (AVs) represent a potentially disruptive yet beneficial change to the transportation system. This new technology has the potential to impact vehicle safety, congestion, and travel behavior. All told, major social AV impacts in the form of crash savings, travel time reduction, fuel efficiency and parking benefits are estimated to approach $2,000 to per year per AV, and may eventually approach nearly $4,000 when comprehensive crash costs are accounted for. Yet barriers to implementation and mass-market penetration remain. Initial costs will likely be unaffordable. Licensing and testing standards in the U.S. are being developed at the state level, rather than nationally, which may lead to inconsistencies across states. Liability details remain undefined, security concerns linger, and without new privacy standards, a default lack of privacy for personal travel may become the norm. The impacts and interactions with other components of the transportation system, as well as implementation details, remain uncertain. To address these concerns, the federal government should expand research in these areas and create a nationally recognized licensing framework for AVs, determining appropriate standards for liability, security, and data privacy.",Apranafoauveopbaanpore,1649.0,64.0,80.0
859,Autonomous vehicles,28.0,autonomous vehicles that interact with pedestrians: a survey of theory and practice,5.0,201.0,1.0,2.0,5.0,3.4,89.4,3,https://ieeexplore.ieee.org/iel7/6979/4358928/08667866.pdf,"One of the major challenges that autonomous cars are facing today is driving in urban environments. To make it a reality, autonomous vehicles require the ability to communicate with other road users and understand their intentions. Such interactions are essential between vehicles and pedestrians, the most vulnerable road users. Understanding pedestrian behavior, however, is not intuitive and depends on various factors, such as demographics of the pedestrians, traffic dynamics, environmental conditions, and so on. In this paper, we identify these factors by surveying pedestrian behavior studies, both the classical works on pedestrian–driver interaction and the modern ones that involve autonomous vehicles. To this end, we will discuss various methods of studying pedestrian behavior and analyze how the factors identified in the literature are interrelated. We will also review the practical applications aimed at solving the interaction problem, including design approaches for autonomous vehicles that communicate with pedestrians and visual perception and reasoning algorithms tailored to understanding pedestrian intention. Based on our findings, we will discuss the open problems and propose future research directions.",Aauvethinwipeasuofthanpr,196.0,161.0,5.0
860,Autonomous vehicles,19.0,"autonomous vehicles: challenges, opportunities, and future implications for transportation policies",5.0,201.0,1.0,12.0,5.0,3.4,89.7,4,http://arxiv.org/pdf/2001.09763v1,Autonomous vehicles are future of Smart Cities. In this paper challenges faced by autonomous vehicles are discussed. What are the future implications for transportation policies we can opt? How artificial intelligence is helping in autonomous vehicles. With the advancement of technology autonomous vehicles can be used safely in future. What technology is being used today in autonomous vehicles? What can be used in future? This paper also discuss benefits of autonomous vehicles.,Aauvechopanfuimfotrpo,86.0,0.0,1.0
861,Autonomous vehicles,2.0,planning and decision-making for autonomous vehicles,5.0,201.0,1.0,34.0,5.0,3.4,91.2,5,http://arxiv.org/pdf/1810.05766v1,"In this review, we provide an overview of emerging trends and challenges in the field of intelligent and autonomous, or self-driving, vehicles. Recent advances in the field of perception, planning,...",Aplandefoauve,205.0,153.0,4.0
862,Autonomous vehicles,3.0,user preferences regarding autonomous vehicles,5.0,201.0,1.0,36.0,5.0,3.4,92.1,6,http://arxiv.org/pdf/2103.03496v2,"Abstract This study gains insight into individual motivations for choosing to own and use autonomous vehicles and develops a model for autonomous vehicle long-term choice decisions. A stated preference questionnaire is distributed to 721 individuals living across Israel and North America. Based on the characteristics of their current commutes, individuals are presented with various scenarios and asked to choose the car they would use for their commute. A vehicle choice model which includes three options is estimated: (1) Continue to commute using a regular car that you have in your possession. (2) Buy and shift to commuting using a privately-owned autonomous vehicle (PAV). (3) Shift to using a shared-autonomous vehicle (SAV), from a fleet of on-demand cars for your commute. A factor analysis determined five relevant latent variables describing the individuals’ attitudes: technology interest, environmental concern, enjoy driving, public transit attitude, and pro-AV sentiments. The effects that the characteristics of the individual and the autonomous vehicle have on use and acceptance are quantified through random utility models including logit kernel model taking into account panel effects. Currently, large overall hesitations towards autonomous vehicle adoption exist, with 44% of choice decisions remaining regular vehicles. Early AV adopters will likely be young, students, more educated, and spend more time in vehicles. Even if the SAV service were to be completely free, only 75% of individuals would currently be willing to use SAVs. The study also found various differences regarding the preferences of individuals in Israel and North America, namely that Israelis are overall more likely to shift to autonomous vehicles. Methods to encourage SAV use include increasing the costs for regular cars as well as educating the public about the benefits of shared autonomous vehicles.",Ausprreauve,364.0,17.0,14.0
863,Autonomous vehicles,26.0,localization requirements for autonomous vehicles,5.0,201.0,1.0,20.0,5.0,3.4,94.2,7,https://arxiv.org/pdf/1906.01061,"Autonomous vehicles require precise knowledge of their position and orientation in all weather and traffic conditions for path planning, perception, control, and general safe operation. Here we derive these requirements for autonomous vehicles based on first principles. We begin with the safety integrity level, defining the allowable probability of failure per hour of operation based on desired improvements on road safety today. This draws comparisons with the localization integrity levels required in aviation and rail where similar numbers are derived at 10^-8 probability of failure per hour of operation. We then define the geometry of the problem, where the aim is to maintain knowledge that the vehicle is within its lane and to determine what road level it is on. Longitudinal, lateral, and vertical localization error bounds (alert limits) and 95% accuracy requirements are derived based on US road geometry standards (lane width, curvature, and vertical clearance) and allowable vehicle dimensions. For passenger vehicles operating on freeway roads, the result is a required lateral error bound of 0.57 m (0.20 m, 95%), a longitudinal bound of 1.40 m (0.48 m, 95%), a vertical bound of 1.30 m (0.43 m, 95%), and an attitude bound in each direction of 1.50 deg (0.51 deg, 95%). On local streets, the road geometry makes requirements more stringent where lateral and longitudinal error bounds of 0.29 m (0.10 m, 95%) are needed with an orientation requirement of 0.50 deg (0.17 deg, 95%).",Alorefoauve,46.0,52.0,1.0
864,Autonomous vehicles,8.0,the social dilemma of autonomous vehicles,5.0,201.0,1.0,38.0,5.0,3.4,94.20000000000002,8,https://arxiv.org/pdf/1510.03346,"Codes of conduct in autonomous vehicles When it becomes possible to program decision-making based on moral principles into machines, will self-interest or the public good predominate? In a series of surveys, Bonnefon et al. found that even though participants approve of autonomous vehicles that might sacrifice passengers to save others, respondents would prefer not to ride in such vehicles (see the Perspective by Greene). Respondents would also not approve regulations mandating self-sacrifice, and such regulations would make them less willing to buy an autonomous vehicle. Science, this issue p. 1573; see also p. 1514 Programming an acceptable morality into driverless cars presents large challenges. Autonomous vehicles (AVs) should reduce traffic accidents, but they will sometimes have to choose between two evils, such as running over pedestrians or sacrificing themselves and their passenger to save the pedestrians. Defining the algorithms that will help AVs make these moral decisions is a formidable challenge. We found that participants in six Amazon Mechanical Turk studies approved of utilitarian AVs (that is, AVs that sacrifice their passengers for the greater good) and would like others to buy them, but they would themselves prefer to ride in AVs that protect their passengers at all costs. The study participants disapprove of enforcing utilitarian regulations for AVs and would be less willing to buy such an AV. Accordingly, regulating for utilitarian algorithms may paradoxically increase casualties by postponing the adoption of a safer technology.",Athsodiofauve,761.0,33.0,23.0
865,Autonomous vehicles,34.0,autonomous vehicles and the future of urban tourism,5.0,201.0,1.0,13.0,5.0,3.4,94.5,9,http://arxiv.org/abs/2008.11578v1,"Abstract Connected and autonomous vehicles (CAVs) have the potential to disrupt all industries tied to transport, including tourism. This conceptual paper breaks new ground by providing an in-depth imaginings approach to the potential future far-reaching implications of CAVs for urban tourism. Set against key debates in urban studies and urban tourism, we discuss the enchantments and apprehensions surrounding CAVs and how they may impact cities in terms of tourism transport mode use, spatial changes, tourism employment and the night-time visitor economy, leading to new socio-economic opportunities and a range of threats and inequities. We provide a concluding agenda that sets the foundation for a new research sub-field on CAVs and tourism, of relevance to urban planners, policymakers and the tourism industry.",Aauveanthfuofurto,73.0,54.0,1.0
866,Autonomous vehicles,14.0,social behavior for autonomous vehicles,5.0,201.0,1.0,35.0,5.0,3.4,95.1,10,https://www.pnas.org/content/pnas/116/50/24972.full.pdf,"Significance We present a framework that integrates social psychology tools into controller design for autonomous vehicles. Our key insight utilizes Social Value Orientation (SVO), quantifying an agent’s degree of selfishness or altruism, which allows us to better predict driver behavior. We model interactions between human and autonomous agents with game theory and the principle of best response. Our unified algorithm estimates driver SVOs and incorporates their predicted trajectories into the autonomous vehicle’s control while respecting safety constraints. We study common-yet-difficult traffic scenarios: highway merging and unprotected left turns. Incorporating SVO reduces error in predictions by 25%, validated on 92 human driving merges. Furthermore, we find that merging drivers are more competitive than nonmerging drivers. Deployment of autonomous vehicles on public roads promises increased efficiency and safety. It requires understanding the intent of human drivers and adapting to their driving styles. Autonomous vehicles must also behave in safe and predictable ways without requiring explicit communication. We integrate tools from social psychology into autonomous-vehicle decision making to quantify and predict the social behavior of other drivers and to behave in a socially compliant way. A key component is Social Value Orientation (SVO), which quantifies the degree of an agent’s selfishness or altruism, allowing us to better predict how the agent will interact and cooperate with others. We model interactions between agents as a best-response game wherein each agent negotiates to maximize their own utility. We solve the dynamic game by finding the Nash equilibrium, yielding an online method of predicting multiagent interactions given their SVOs. This approach allows autonomous vehicles to observe human drivers, estimate their SVOs, and generate an autonomous control policy in real time. We demonstrate the capabilities and performance of our algorithm in challenging traffic scenarios: merging lanes and unprotected left turns. We validate our results in simulation and on human driving data from the NGSIM dataset. Our results illustrate how the algorithm’s behavior adapts to social preferences of other drivers. By incorporating SVO, we improve autonomous performance and reduce errors in human trajectory predictions by 25%.",Asobefoauve,22.0,58.0,0.0
867,Autonomous vehicles,30.0,a safety standard approach for fully autonomous vehicles,5.0,201.0,1.0,27.0,5.0,3.4,97.5,11,http://users.ece.cmu.edu/~koopman/pubs/Koopman19_WAISE_UL4600.pdf,"Assuring the safety of self-driving cars and other fully autonomous vehicles presents significant challenges to traditional software safety standards both in terms of content and approach. We propose a safety standard approach for fully autonomous vehicles based on setting scope requirements for an overarching safety case. A viable approach requires feedback paths to ensure that both the safety case and the standard itself co-evolve with the technology and accumulated experience. An external assessment process must be part of this approach to ensure lessons learned are captured, as well as to ensure transparency. This approach forms the underlying basis for the UL 4600 initial draft standard.",Aasastapfofuauve,33.0,14.0,2.0
868,Autonomous vehicles,35.0,"perceptions of autonomous vehicles: relationships with road users, risk, gender and age",5.0,201.0,1.0,30.0,5.0,3.4,99.9,12,http://gala.gre.ac.uk/id/eprint/17846/1/17846%20HULSE_Perceptions_Of_Autonomous_Vehicles_2017.pdf,"Abstract Fully automated self-driving cars, with expected benefits including improved road safety, are closer to becoming a reality. Thus, attention has turned to gauging public perceptions of these autonomous vehicles. To date, surveys have focused on the public as potential passengers of autonomous cars, overlooking other road users who would interact with them. Comparisons with perceptions of other existing vehicles are also lacking. This study surveyed almost 1000 participants on their perceptions, particularly with regards to safety and acceptance of autonomous vehicles. Overall, results revealed that autonomous cars were perceived as a “somewhat low risk“ form of transport and, while concerns existed, there was little opposition to the prospect of their use on public roads. However, compared to human-operated cars, autonomous cars were perceived differently depending on the road user perspective: more risky when a passenger yet less risky when a pedestrian. Autonomous cars were also perceived as more risky than existing autonomous trains. Gender, age and risk-taking had varied relationships with the perceived risk of different vehicle types and general attitudes towards autonomous cars. For instance, males and younger adults displayed greater acceptance. Whilst their adoption of this autonomous technology would seem societally beneficial – due to these groups’ greater propensity for taking road user risks, behaviours linked with poorer road safety – other results suggested it might be premature to draw conclusions on risk-taking and user acceptance. Future studies should therefore continue to investigate people’s perceptions from multiple perspectives, taking into account various road user viewpoints and individual characteristics.",Apeofauverewirousrigeanag,240.0,75.0,6.0
869,Autonomous vehicles,31.0,preferences for shared autonomous vehicles,5.0,201.0,1.0,40.0,5.0,3.4,101.7,13,https://drive.google.com/file/d/1aohPEpbt5i4jjZKMYlsaej-kwqkDCNtQ/view,"Shared autonomous vehicles (SAVs) could provide inexpensive mobility on-demand services. In addition, the autonomous vehicle technology could facilitate the implementation of dynamic ride-sharing (DRS). The widespread adoption of SAVs could provide benefits to society, but also entail risks. For the design of effective policies aiming to realize the advantages of SAVs, a better understanding of how SAVs may be adopted is necessary. This article intends to advance future research about the travel behavior impacts of SAVs, by identifying the characteristics of users who are likely to adopt SAV services and by eliciting willingness to pay measures for service attributes. For this purpose, a stated choice survey was conducted and analyzed, using a mixed logit model. The results show that service attributes including travel cost, travel time and waiting time may be critical determinants of the use of SAVs and the acceptance of DRS. Differences in willingness to pay for service attributes indicate that SAVs with DRS and SAVs without DRS are perceived as two distinct mobility options. The results imply that the adoption of SAVs may differ across cohorts, whereby young individuals and individuals with multimodal travel patterns may be more likely to adopt SAVs. The methodological limitations of the study are also acknowledged. Despite a potential hypothetical bias, the results capture the directionality and relative importance of the attributes of interest. Language: en",Aprfoshauve,392.0,37.0,23.0
870,Autonomous vehicles,32.0,influence of connected and autonomous vehicles on traffic flow stability and throughput,5.0,201.0,1.0,39.0,5.0,3.4,101.7,14,http://arxiv.org/abs/1904.02209v2,"The introduction of connected and autonomous vehicles will bring changes to the highway driving environment. Connected vehicle technology provides real-time information about the surrounding traffic condition and the traffic management center’s decisions. Such information is expected to improve drivers’ efficiency, response, and comfort while enhancing safety and mobility. Connected vehicle technology can also further increase efficiency and reliability of autonomous vehicles, though these vehicles could be operated solely with their on-board sensors, without communication. While several studies have examined the possible effects of connected and autonomous vehicles on the driving environment, most of the modeling approaches in the literature do not distinguish between connectivity and automation, leaving many questions unanswered regarding the implications of different contemplated deployment scenarios. There is need for a comprehensive acceleration framework that distinguishes between these two technologies while modeling the new connected environment. This study presents a framework that utilizes different models with technology-appropriate assumptions to simulate different vehicle types with distinct communication capabilities. The stability analysis of the resulting traffic stream behavior using this framework is presented for different market penetration rates of connected and autonomous vehicles. The analysis reveals that connected and autonomous vehicles can improve string stability. Moreover, automation is found to be more effective in preventing shockwave formation and propagation under the model’s assumptions. In addition to stability, the effects of these technologies on throughput are explored, suggesting substantial potential throughput increases under certain penetration scenarios.",Ainofcoanauveontrflstanth,539.0,35.0,24.0
871,Autonomous vehicles,42.0,object scene flow for autonomous vehicles,4.0,201.0,1.0,5.0,5.0,3.1,94.5,15,http://openaccess.thecvf.com/content_cvpr_2015/papers/Menze_Object_Scene_Flow_2015_CVPR_paper.pdf,"This paper proposes a novel model and dataset for 3D scene flow estimation with an application to autonomous driving. Taking advantage of the fact that outdoor scenes often decompose into a small number of independently moving objects, we represent each element in the scene by its rigid motion parameters and each superpixel by a 3D plane as well as an index to the corresponding object. This minimal representation increases robustness and leads to a discrete-continuous CRF where the data term decomposes into pairwise potentials between superpixels and objects. Moreover, our model intrinsically segments the scene into its constituting dynamic components. We demonstrate the performance of our model on existing benchmarks as well as a novel realistic dataset with scene flow ground truth. We obtain this dataset by annotating 400 dynamic scenes from the KITTI raw data collection using detailed 3D CAD models for all vehicles in motion. Our experiments also reveal novel challenges which cannot be handled by existing methods.",Aobscflfoauve,1202.0,47.0,261.0
872,Autonomous vehicles,5.0,the impact of autonomous vehicles on cities: a review,5.0,201.0,1.0,45.0,4.0,3.1,95.4,16,http://arxiv.org/pdf/2108.01615v1,"ABSTRACT Autonomous vehicles (AVs) are starting to hit our roads. It is only a matter of time until the technological challenges still facing full AV implementation are solved, and legal, social, and transport issues related to AVs become part of the public discussion. AVs have the potential to become a major catalyst for urban transformation. To explore some of these transformations, first, we discuss the possibility of decoupling the many functions of urban vehicles from the form factor (without drivers, do cars need to look like they look today?). Second, we question whether AVs will lead to more or fewer cars on the roads, highlighting the synergies between AVs and ride-sharing schemes. Third, with AVs as part of multimodal and sharing-mobility systems, millions of square kilometers currently used for parking spaces might be liberated, or even change the way we design road space. Fourth, freed from the fatigue related to traffic, we question whether AVs would make people search for home locations farther from cities, increasing urban sprawl, or would rather attract more residents to city centers, also freed from congestion and pollution. Fifth, depending on responses to the previous questions and innovative traffic algorithms, we ask whether AVs will demand more or less road infrastructure. We conclude by suggesting that AVs offer the first opportunity to rethink urban life and city design since cars replaced horse-powered traffic and changed the design of cities for a hundred years.",Athimofauveonciare,100.0,91.0,1.0
873,Autonomous vehicles,12.0,autoware on board: enabling autonomous vehicles with embedded systems,5.0,201.0,1.0,41.0,4.0,3.1,96.3,17,https://www.researchgate.net/profile/Takuya-Azumi/publication/327198306_Autoware_on_Board_Enabling_Autonomous_Vehicles_with_Embedded_Systems/links/5c9085da45851564fae6dcd0/Autoware-on-Board-Enabling-Autonomous-Vehicles-with-Embedded-Systems.pdf,"This paper presents Autoware on Board, a new profile of Autoware, especially designed to enable autonomous vehicles with embedded systems. Autoware is a popular open-source software project that provides a complete set of self-driving modules, including localization, detection, prediction, planning, and control. We customize and extend the software stack of Autoware to accommodate embedded computing capabilities. In particular, we use DRIVE PX2 as a reference computing platform, which is manufactured by NVIDIA Corporation for development of autonomous vehicles, and evaluate the performance of Autoware on ARM-based embedded processing cores and Tegra-based embedded graphics processing units (GPUs). Given that low-power CPUs are often preferred over high-performance GPUs, from the functional safety point of view, this paper focuses on the application of Autoware on ARM cores rather than Tegra ones. However, some Autoware modules still need to be executed on the Tegra cores to achieve load balancing and real-time processing. The experimental results show that the execution latency imposed on the DRIVE PX2 platform is capped at about three times as much as that on a high-end laptop computer. We believe that this observed computing performance is even acceptable for real-world production of autonomous vehicles in certain scenarios.",Aauonboenauvewiemsy,155.0,25.0,15.0
874,Autonomous vehicles,13.0,"perception, planning, control, and coordination for autonomous vehicles",5.0,201.0,1.0,44.0,4.0,3.1,97.5,18,https://www.mdpi.com/2075-1702/5/1/6/pdf,"Autonomous vehicles are expected to play a key role in the future of urban transportation systems, as they offer potential for additional safety, increased productivity, greater accessibility, better road efﬁciency, and positive impact on the environment. Research in autonomous systems has seen dramatic advances in recent years, due to the increases in available computing power and reduced cost in sensing and computing technologies, resulting in maturing technological readiness level of fully autonomous vehicles. The objective of this paper is to provide a general overview of the recent developments in the realm of autonomous vehicle software systems. Fundamental components of autonomous vehicle software are reviewed, and recent developments in each area are discussed.",Apeplcoancofoauve,250.0,252.0,10.0
875,Autonomous vehicles,63.0,hierarchical game-theoretic planning for autonomous vehicles,4.0,201.0,1.0,9.0,5.0,3.1,102.0,19,https://ieeexplore.ieee.org/iel7/8780387/8793254/08794007.pdf,"The actions of an autonomous vehicle on the road affect and are affected by those of other drivers, whether overtaking, negotiating a merge, or avoiding an accident. This mutual dependence, best captured by dynamic game theory, creates a strong coupling between the vehicle’s planning and its predictions of other drivers’ behavior, and constitutes an open problem with direct implications on the safety and viability of autonomous driving technology. Unfortunately, dynamic games are too computationally demanding to meet the real-time constraints of autonomous driving in its continuous state and action space. In this paper, we introduce a novel game-theoretic trajectory planning algorithm for autonomous driving, that enables real-time performance by hierarchically decomposing the underlying dynamic game into a long-horizon “strategic” game with simplified dynamics and full information structure, and a short-horizon “tactical” game with full dynamics and a simplified information structure. The value of the strategic game is used to guide the tactical planning, implicitly extending the planning horizon, pushing the local trajectory optimization closer to global solutions, and, most importantly, quantitatively accounting for the autonomous vehicle and the human driver’s ability and incentives to influence each other. In addition, our approach admits non-deterministic models of human decision-making, rather than relying on perfectly rational predictions. Our results showcase richer, safer, and more effective autonomous behavior in comparison to existing techniques.",Ahigaplfoauve,78.0,31.0,5.0
876,Autonomous vehicles,64.0,an overview of lidar imaging systems for autonomous vehicles,4.0,201.0,1.0,10.0,5.0,3.1,102.6,20,https://www.mdpi.com/2076-3417/9/19/4093/pdf,"Lidar imaging systems are one of the hottest topics in the optronics industry. The need to sense the surroundings of every autonomous vehicle has pushed forward a race dedicated to deciding the final solution to be implemented. However, the diversity of state-of-the-art approaches to the solution brings a large uncertainty on the decision of the dominant final solution. Furthermore, the performance data of each approach often arise from different manufacturers and developers, which usually have some interest in the dispute. Within this paper, we intend to overcome the situation by providing an introductory, neutral overview of the technology linked to lidar imaging systems for autonomous vehicles, and its current state of development. We start with the main single-point measurement principles utilized, which then are combined with different imaging strategies, also described in the paper. An overview of the features of the light sources and photodetectors specific to lidar imaging systems most frequently used in practice is also presented. Finally, a brief section on pending issues for lidar development in autonomous vehicles has been included, in order to present some of the problems which still need to be solved before implementation may be considered as final. The reader is provided with a detailed bibliography containing both relevant books and state-of-the-art papers for further progress in the subject.",Aanovofliimsyfoauve,52.0,172.0,3.0
877,Autonomous vehicles,10.0,autonomous vehicles: the next jump in accessibilities?,5.0,201.0,1.0,70.0,4.0,3.1,104.4,21,https://www.sciencedirect.com/science/article/am/pii/S0739885917300021,"Autonomous vehicles are expected to offer a higher comfort of traveling at lower prices and at the same time to increase road capacity - a pattern recalling the rise of the private car and later of motorway construction. Using the Swiss national transport model, this research simulates the impact of autonomous vehicles on accessibility of the Swiss municipalities. The results show that autonomous vehicles could cause another quantum leap in accessibility. Moreover, the spatial distribution of the accessibility impacts implies that autonomous vehicles favor urban sprawl and may render public transport superfluous except for dense urban areas.",Aauvethnejuinac,197.0,52.0,3.0
878,Autonomous vehicles,33.0,humanlike driving: empirical decision-making system for autonomous vehicles,5.0,201.0,1.0,49.0,4.0,3.1,105.0,22,http://arxiv.org/abs/2004.09044v1,"The autonomous vehicle, as an emerging and rapidly growing field, has received extensive attention for its futuristic driving experiences. Although the fast developing depth sensors and machine learning methods have given a huge boost to self-driving research, existing autonomous driving vehicles do meet with several avoidable accidents during their road testings. The major cause is the misunderstanding between self-driving systems and human drivers. To solve this problem, we propose a humanlike driving system in this paper to give autonomous vehicles the ability to make decisions like a human. In our method, a convolutional neural network model is used to detect, recognize, and abstract the information in the input road scene, which is captured by the on-board sensors. And then a decision-making system calculates the specific commands to control the vehicles based on the abstractions. The biggest advantage of our work is that we implement a decision-making system which can well adapt to real-life road conditions, in which a massive number of human drivers exist. In addition, we build our perception system with only the depth information, rather than the unstable RGB data. The experimental results give a good demonstration of the efficiency and robustness of the proposed method.",Ahudremdesyfoauve,88.0,37.0,0.0
879,Autonomous vehicles,57.0,"autonomous vehicles can be shared, but a feeling of ownership is important: examination of the influential factors for intention to use autonomous vehicles",4.0,201.0,1.0,26.0,5.0,3.1,105.3,23,http://arxiv.org/pdf/1602.01718v1,"Abstract Autonomous vehicles are expected to be commercialized within a few years, and researchers have investigated various factors that influence their adoption. However, only a few studies have considered comparative and psychological perspectives that can affect user-vehicle relationships. Focusing on this limitation, this study investigates influential factors on the use of autonomous vehicles in terms of a technology acceptance model (which considers perceived ease of use, perceived usefulness, and intention to use) and factors for autonomous vehicle use (e.g., perceived risk, relative advantage, self-efficacy, and psychological ownership (i.e., feeling of ownership)). Our results show that self-efficacy positively affects the perceived ease of use and intention to use, while the relative advantage affects perceived usefulness. Psychological ownership affects the intention to use but not the perceived usefulness. This implies that encouraging a consumer to form a psychological bond (i.e., psychological ownership) with an autonomous vehicle may be an effective strategy for promoting the use of autonomous vehicles.",Aauvecabeshbuafeofowisimexofthinfafointousauve,41.0,63.0,1.0
880,Autonomous vehicles,98.0,dissipation of stop-and-go waves via control of autonomous vehicles: field experiments,4.0,201.0,1.0,37.0,5.0,3.1,120.9,24,https://www.sciencedirect.com/science/article/am/pii/S0968090X18301517,"Abstract Traffic waves are phenomena that emerge when the vehicular density exceeds a critical threshold. Considering the presence of increasingly automated vehicles in the traffic stream, a number of research activities have focused on the influence of automated vehicles on the bulk traffic flow. In the present article, we demonstrate experimentally that intelligent control of an autonomous vehicle is able to dampen stop-and-go waves that can arise even in the absence of geometric or lane changing triggers. Precisely, our experiments on a circular track with more than 20 vehicles show that traffic waves emerge consistently, and that they can be dampened by controlling the velocity of a single vehicle in the flow. We compare metrics for velocity, braking events, and fuel economy across experiments. These experimental findings suggest a paradigm shift in traffic management: flow control will be possible via a few mobile actuators (less than 5%) long before a majority of vehicles have autonomous capabilities.",Adiofstwavicoofauvefiex,297.0,84.0,19.0
881,Autonomous vehicles,36.0,"autonomous vehicles: disengagements, accidents and reaction times",5.0,201.0,1.0,99.0,4.0,3.1,120.9,25,http://arxiv.org/abs/1712.09227v1,"Autonomous vehicles are being viewed with scepticism in their ability to improve safety and the driving experience. A critical issue with automated driving at this stage of its development is that it is not yet reliable and safe. When automated driving fails, or is limited, the autonomous mode disengages and the drivers are expected to resume manual driving. For this transition to occur safely, it is imperative that drivers react in an appropriate and timely manner. Recent data released from the California trials provide compelling insights into the current factors influencing disengagements of autonomous mode. Here we show that the number of accidents observed has a significantly high correlation with the autonomous miles travelled. The reaction times to take control of the vehicle in the event of a disengagement was found to have a stable distribution across different companies at 0.83 seconds on average. However, there were differences observed in reaction times based on the type of disengagements, type of roadway and autonomous miles travelled. Lack of trust caused by the exposure to automated disengagements was found to increase the likelihood to take control of the vehicle manually. Further, with increased vehicle miles travelled the reaction times were found to increase, which suggests an increased level of trust with more vehicle miles travelled. We believe that this research would provide insurers, planners, traffic management officials and engineers fundamental insights into trust and reaction times that would help them design and engineer their systems.",Aauvediacanreti,139.0,15.0,3.0
882,Autonomous vehicles,41.0,designing parking facilities for autonomous vehicles,4.0,201.0,1.0,56.0,4.0,2.8,109.5,26,https://www.itscanada.ca/files/ACGM18/3A_Sina%20Bahrami%20MRoorda%20s2.0-S0191261517305866-main.pdf,"Autonomous vehicles will have a major impact on parking facility designs in the future. Compared to regular car-parks that have only two rows of vehicles in each island, future car-parks (for autonomous vehicles) can have multiple rows of vehicles stacked behind each other. Although this multi-row layout reduces parking space, it can cause blockage if a certain vehicle is barricaded by other vehicles and cannot leave the facility. To release barricaded vehicles, the car-park operator has to relocate some of the vehicles to create a clear pathway for the blocked vehicle to exit. The extent of vehicle relocation depends on the layout design of the car-park. To find the optimal car-park layout with minimum relocations, we present a mixed-integer non-linear program that treats each island in the car-park as a queuing system. We solve the problem using Benders decomposition for an exact answer and we present a heuristic algorithm to find a reasonable upper-bound of the mathematical model. We show that autonomous vehicle car-parks can decrease the need for parking space by an average of 62% and a maximum of 87%. This revitalization of space that was previously used for parking can be socially beneficial if car-parks are converted into commercial and residential land-uses.",Adepafafoauve,67.0,48.0,1.0
883,Autonomous vehicles,51.0,adaptive stress testing for autonomous vehicles,4.0,201.0,1.0,58.0,4.0,2.8,113.1,27,https://arxiv.org/pdf/1902.01909,"This paper presents a method for testing the decision making systems of autonomous vehicles. Our approach involves perturbing stochastic elements in the vehicle's environment until the vehicle is involved in a collision. Instead of applying direct Monte Carlo sampling to find collision scenarios, we formulate the problem as a Markov decision process and use reinforcement learning algorithms to find the most likely failure scenarios. This paper presents Monte Carlo Tree Search (MCTS) and Deep Reinforcement Learning (DRL) solutions that can scale to large environments. We show that DRL can find more likely failure scenarios than MCTS with fewer calls to the simulator. A simulation scenario involving a vehicle approaching a crosswalk is used to validate the framework. Our proposed approach is very general and can be easily applied to other scenarios given the appropriate models of the vehicle and the environment.",Aadsttefoauve,50.0,26.0,3.0
884,Autonomous vehicles,115.0,rrpn: radar region proposal network for object detection in autonomous vehicles,3.0,56.0,4.0,201.0,1.0,2.8,117.2,28,https://arxiv.org/pdf/1905.00526,"Due to increasing urban population and growing number of motor vehicles, traffic congestion is becoming a major problem of the 21st century. One of the main reasons behind traffic congestion is accidents which can not only result in casualties and losses for the participants, but also in wasted and lost time for the others that are stuck behind the wheels. Early detection of an accident can save lives, provides quicker road openings, hence decreases wasted time and resources, and increases efficiency. In this study, we propose a preliminary real-time autonomous accident-detection system based on computational intelligence techniques. Istanbul City traffic-flow data for the year 2015 from various sensor locations are populated using big data processing methodologies. The extracted features are then fed into a nearest neighbor model, a regression tree, and a feed-forward neural network model. For the output, the possibility of an occurrence of an accident is predicted. The results indicate that even though the number of false alarms dominates the real accident cases, the system can still provide useful information that can be used for status verification and early reaction to possible accidents.",Arrrareprnefoobdeinauve,39.0,22.0,4.0
885,Autonomous vehicles,54.0,developing policy for urban autonomous vehicles: impact on congestion,4.0,201.0,1.0,69.0,4.0,2.8,117.3,29,https://www.mdpi.com/2413-8851/2/2/33/pdf,"An important problem for surface transport is road traffic congestion, which is ubiquitous and difficult to mitigate. Accordingly, a question for policymakers is the possible impact on congestion of autonomous vehicles. It seems likely that the main impact of vehicle automation will not be seen until driverless vehicles are sufficiently safe for use amid general traffic on urban streets. Shared use driverless vehicles could reduce the cost of taxis and a wider range of public transport vehicles could be economic. Individually owned autonomous vehicles would have the ability to travel unoccupied and may need to be regulated where this might add to congestion. It is possible that autonomous vehicles could provide mobility services at lower cost and wider scope, such that private car use in urban areas could decline and congestion reduce. City authorities should be alert to these possibilities in developing transport policy.",Adepofourauveimonco,41.0,35.0,0.0
886,Autonomous vehicles,1.0,an open approach to autonomous vehicles,5.0,201.0,1.0,126.0,3.0,2.8,118.5,30,http://cs.furman.edu/~tallen/csc271/source/openAppr.pdf,"Autonomous vehicles are an emerging application of automotive technology. They can recognize the scene, plan the path, and control the motion by themselves while interacting with drivers. Although they receive considerable attention, components of autonomous vehicles are not accessible to the public but instead are developed as proprietary assets. To facilitate the development of autonomous vehicles, this article introduces an open platform using commodity vehicles and sensors. Specifically, the authors present algorithms, software libraries, and datasets required for scene recognition, path planning, and vehicle control. This open platform allows researchers and developers to study the basis of autonomous vehicles, design new algorithms, and test their performance using the common interface.",Aanopaptoauve,240.0,13.0,23.0
887,Autonomous vehicles,25.0,"pedestrians, autonomous vehicles, and cities",5.0,201.0,1.0,103.0,3.0,2.8,118.8,31,https://journals.sagepub.com/doi/pdf/10.1177/0739456X16675674,"Autonomous vehicles, popularly known as self-driving cars, have the potential to transform travel behavior. However, existing analyses have ignored strategic interactions with other road users. In this article, I use game theory to analyze the interactions between pedestrians and autonomous vehicles, with a focus on yielding at crosswalks. Because autonomous vehicles will be risk-averse, the model suggests that pedestrians will be able to behave with impunity, and autonomous vehicles may facilitate a shift toward pedestrian-oriented urban neighborhoods. At the same time, autonomous vehicle adoption may be hampered by their strategic disadvantage that slows them down in urban traffic.",Apeauveanci,145.0,29.0,2.0
888,Autonomous vehicles,11.0,the release of autonomous vehicles,5.0,201.0,1.0,125.0,3.0,2.8,121.2,32,http://arxiv.org/pdf/2004.13822v1,"In the future, the functions of autonomous driving could fundamentally change all road traffic; to do so, it would have to be implemented on a large scale, in series production.",Athreofauve,108.0,20.0,15.0
889,Autonomous vehicles,95.0,modeling connected and autonomous vehicles in heterogeneous traffic flow,4.0,201.0,1.0,46.0,4.0,2.8,122.7,33,https://nagoya.repo.nii.ac.jp/?action=repository_action_common_download&item_id=25086&item_no=1&attribute_id=17&file_no=1,"The objective of this study was to develop a heterogeneous traffic-flow model to study the possible impact of connected and autonomous vehicles (CAVs) on the traffic flow. Based on a recently proposed two-state safe-speed model (TSM), a two-lane cellular automaton (CA) model was developed, wherein both the CAVs and conventional vehicles were incorporated in the heterogeneous traffic flow. In particular, operation rules for CAVs are established considering the new characteristics of this emerging technology, including autonomous driving through the adaptive cruise control and inter-vehicle connection via short-range communication. Simulations were conducted under various CAV-penetration rates in the heterogeneous flow. The impact of CAVs on the road capacity was numerically investigated. The simulation results indicate that the road capacity increases with an increase in the CAV-penetration rate within the heterogeneous flow. Up to a CAV-penetration rate of 30%, the road capacity increases gradually; the effect of the difference in the CAV capability on the growth rate is insignificant. When the CAV-penetration rate exceeds 30%, the growth rate is largely decided by the capability of the CAV. The greater the capability, the higher the road-capacity growth rate. The relationship between the CAV-penetration rate and the road capacity is numerically analyzed, providing some insights into the possible impact of the CAVs on traffic systems.",Amocoanauveinhetrfl,88.0,16.0,5.0
890,Autonomous vehicles,79.0,are consumers willing to pay to let cars drive for them? analyzing response to autonomous vehicles,4.0,201.0,1.0,66.0,4.0,2.8,123.9,34,http://arxiv.org/pdf/2106.11025v1,"Abstract Autonomous vehicles use sensing and communication technologies to navigate safely and efficiently with little or no input from the driver. These driverless technologies will create an unprecedented revolution in how people move, and policymakers will need appropriate tools to plan for and analyze the large impacts of novel navigation systems. In this paper we derive semiparametric estimates of the willingness to pay for automation. We use data from a nationwide online panel of 1260 individuals who answered a vehicle-purchase discrete choice experiment focused on energy efficiency and autonomous features. Several models were estimated with the choice microdata, including a conditional logit with deterministic consumer heterogeneity, a parametric random parameter logit, and a semiparametric random parameter logit. We draw three key results from our analysis. First, we find that the average household is willing to pay a significant amount for automation: about $3500 for partial automation and $4900 for full automation. Second, we estimate substantial heterogeneity in preferences for automation, where a significant share of the sample is willing to pay above $10,000 for full automation technology while many are not willing to pay any positive amount for the technology. Third, our semiparametric random parameter logit estimates suggest that the demand for automation is split approximately evenly between high, modest and no demand, highlighting the importance of modeling flexible preferences for emerging vehicle technology.",Aarcowitopatolecadrfothanretoauve,187.0,39.0,7.0
891,Autonomous vehicles,87.0,distributed conflict resolution for connected autonomous vehicles,4.0,201.0,1.0,61.0,4.0,2.8,124.8,35,http://arxiv.org/abs/2110.08127v1,"This paper proposes a novel communication-enabled distributed conflict resolution mechanism in order for a group of connected autonomous vehicles to navigate safely and efficiently in intersections without any traffic manager. The conflict resolution strategy for individual vehicle is decoupled temporally. In a decision maker, the vehicle computes the desired time slots to pass the conflict zones by solving a conflict graph locally based on the broadcasted information from other vehicles. In a motion planner, the vehicle computes the desired speed profile by solving a temporal optimization problem constrained in the desired time slot. The estimated time to occupy the conflict zones given the new speed profile is then broadcasted again. It is proved that the aggregation of these local decisions solves the conflicts globally. Theoretically, this method provides an efficient parallel mechanism to obtain local optimal solutions of a large-scale optimization problem (e.g., multivehicle navigation). Application-wise, as demonstrated by extensive simulation, this mechanism increases the efficiency of autonomous vehicles in terms of smaller delay time, as well as the efficiency of the traffic in terms of larger throughput when there is no traffic manager to mediate the conflicts.",Adicorefocoauve,47.0,20.0,3.0
892,Autonomous vehicles,23.0,learning driving styles for autonomous vehicles from demonstration,5.0,201.0,1.0,133.0,3.0,2.8,127.20000000000002,36,http://ais.informatik.uni-freiburg.de/publications/papers/kuderer15icra.pdf,"It is expected that autonomous vehicles capable of driving without human supervision will be released to market within the next decade. For user acceptance, such vehicles should not only be safe and reliable, they should also provide a comfortable user experience. However, individual perception of comfort may vary considerably among users. Whereas some users might prefer sporty driving with high accelerations, others might prefer a more relaxed style. Typically, a large number of parameters such as acceleration profiles, distances to other cars, speed during lane changes, etc., characterize a human driver's style. Manual tuning of these parameters may be a tedious and error-prone task. Therefore, we propose a learning from demonstration approach that allows the user to simply demonstrate the desired style by driving the car manually. We model the individual style in terms of a cost function and use feature-based inverse reinforcement learning to find the model parameters that fit the observed style best. Once the model has been learned, it can be used to efficiently compute trajectories for the vehicle in autonomous mode. We show that our approach is capable of learning cost functions and reproducing different driving styles using data from real drivers.",Aledrstfoauvefrde,285.0,22.0,12.0
893,Autonomous vehicles,76.0,collision avoidance and stabilization for autonomous vehicles in emergency scenarios,4.0,201.0,1.0,88.0,4.0,2.8,129.6,37,https://ieeexplore.ieee.org/iel7/87/7945304/07585053.pdf,"Emergency scenarios may necessitate autonomous vehicle maneuvers up to their handling limits in order to avoid collisions. In these scenarios, vehicle stabilization becomes important to ensure that the vehicle does not lose control. However, stabilization actions may conflict with those necessary for collision avoidance, potentially leading to a collision. This paper presents a new control structure that integrates path tracking, vehicle stabilization, and collision avoidance and mediates among these sometimes conflicting objectives by prioritizing collision avoidance. It can even temporarily violate vehicle stabilization criteria if needed to avoid a collision. The framework is implemented using model predictive and feedback controllers. Incorporating tire nonlinearities into the model allows the controller to use all of the vehicle’s performance capability to meet the objectives. A prediction horizon comprised of variable length time steps integrates the different time scales associated with stabilization and collision avoidance. Experimental data from an autonomous vehicle demonstrate the controller safely driving at the vehicle’s handling limits and avoiding an obstacle suddenly introduced in the middle of a turn.",Acoavanstfoauveinemsc,186.0,22.0,4.0
894,Autonomous vehicles,86.0,"the travel and environmental implications of shared autonomous vehicles, using agent-based model scenarios",4.0,201.0,1.0,86.0,4.0,2.8,132.0,38,https://www.icscarsharing.it/wp-content/uploads/2019/02/2013-The-travel-and-environmental-implications-of-shared-autonomous-vehicles-using-agent-based-model-scenarios.pdf,"Car sharing programs like Car2Go and ZipCar have quickly expanded, with the number of United States (US) users doubling every one to two years over the past decade. Such programs seek to shift personal transportation choices from an owned asset to a service used on demand. The advent of autonomous vehicles will address many current car-sharing barriers, including users’ travel to access available vehicles. This work describes the design of an agent-based model for Shared Autonomous Vehicle (SAV) operations, the results of many case study applications using this model, and the estimated environmental benefits of such settings, versus conventional vehicle ownership and use settings. The model operates by generating trips throughout a grid-based urban area, with each trip assigned an origin, destination and departure time, to mimic realistic travel profiles. A preliminary model run estimates the SAV fleet size required to reasonably service all trips. Next, the model is run over one hundred days with driverless vehicles ferrying travelers from one destination to the next. During each 5-minute interval, some unused SAVs relocate to shorten wait times for next-period travelers. Case studies vary trip generation rates, trip distribution patterns, network congestion levels, service area size, vehicle relocation strategies, and fleet size. Preliminary results indicate that each SAV can replace around eleven conventional vehicles, but adds up to 10% more travel distance than comparable non-SAV trips, resulting in overall beneficial emissions impacts, once fleet-efficiency changes and embodied versus in-use emissions are assessed.",Athtranenimofshauveusagmosc,719.0,30.0,42.0
895,Autonomous vehicles,94.0,applied artificial intelligence and trust—the case of autonomous vehicles and medical assistance devices,4.0,201.0,1.0,90.0,4.0,2.8,135.60000000000002,39,http://arxiv.org/abs/1902.03442v2,"Automation with inherent artificial intelligence (AI) is increasingly emerging in diverse applications, for instance, autonomous vehicles and medical assistance devices. However, despite their growing use, there is still noticeable skepticism in society regarding these applications. Drawing an analogy from human social interaction, the concept of trust provides a valid foundation for describing the relationship between humans and automation. Accordingly, this paper explores how firms systematically foster trust regarding applied AI. Based on empirical analysis using nine case studies in the transportation and medical technology industries, our study illustrates the dichotomous constitution of trust in applied AI. Concretely, we emphasize the symbiosis of trust in the technology as well as in the innovating firm and its communication about the technology. In doing so, we provide tangible approaches to increase trust in the technology and illustrate the necessity of a democratic development process for applied AI.",Aaparinantrcaofauveanmeasde,235.0,63.0,3.0
896,Autonomous vehicles,18.0,autonomous vehicles: no drivers required,5.0,201.0,1.0,181.0,3.0,2.8,140.10000000000002,40,http://www.umc.edu.dz/images/518020a.pdf,Automation is one of the hottest topics in transportation research and could yield completely driverless cars in less than a decade.,Aauvenodrre,117.0,0.0,2.0
897,Autonomous vehicles,20.0,autonomous vehicles: the future of automobiles,5.0,201.0,1.0,185.0,3.0,2.8,141.9,41,http://arxiv.org/pdf/2108.05805v1,"Autonomous cars are the future smart cars anticipated to be driver less, efficient and crash avoiding ideal urban car of the future. To reach this goal automakers have started working in this area to realized the potential and solve the challenges currently in this area to reach the expected outcome. In this regard the first challenge would be to customize and imbibe existing technology in conventional vehicle to translate them to a near expected autonomous car. This transition of conventional vehicles into an autonomous vehicle by adopting and implementing different upcoming technologies is discussed in this paper. This includes the objectives of autonomous vehicles and their implementation difficulties. The paper also touches upon the existing standards for the same and compares the introduction of autonomous vehicles in Indian market in comparison to other markets. There after the acceptance approach in Indian market scenarios is discussed for autonomous vehicles.",Aauvethfuofau,26.0,4.0,1.0
898,Autonomous vehicles,401.0,accelerating 3d deep learning with pytorch3d,1.0,2.0,5.0,201.0,1.0,2.6,181.4,42,http://arxiv.org/pdf/2007.08501v1,"Deep learning has significantly improved 2D image recognition. Extending into 3D may advance many new applications including autonomous vehicles, virtual and augmented reality, authoring 3D content, and even improving 2D recognition. However despite growing interest, 3D deep learning remains relatively underexplored. We believe that some of this disparity is due to the engineering challenges involved in 3D deep learning, such as efficiently processing heterogeneous data and reframing graphics operations to be differentiable. We address these challenges by introducing PyTorch3D, a library of modular, efficient, and differentiable operators for 3D deep learning. It includes a fast, modular differentiable renderer for meshes and point clouds, enabling analysis-by-synthesis approaches. Compared with other differentiable renderers, PyTorch3D is more modular and efficient, allowing users to more easily extend it while also gracefully scaling to large meshes and images. We compare the PyTorch3D operators and renderer with other implementations and demonstrate significant speed and memory improvements. We also use PyTorch3D to improve the state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D images on ShapeNet. PyTorch3D is open-source and we hope it will help accelerate research in 3D deep learning.",Aac3ddelewipy,105.0,69.0,12.0
899,Autonomous vehicles,401.0,commands 4 autonomous vehicles (c4av) workshop summary,1.0,3.0,5.0,201.0,1.0,2.6,181.8,43,http://arxiv.org/pdf/2009.08792v1,"The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the \emph{Commands for Autonomous Vehicles} (C4AV) challenge based on the recent \emph{Talk2Car} dataset (URL: https://www.aicrowd.com/challenges/eccv-2020-commands-4-autonomous-vehicles). This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work.",Aco4auve(cwosu,1.0,59.0,0.0
900,Autonomous vehicles,401.0,lgsvl simulator: a high fidelity simulator for autonomous driving,1.0,5.0,5.0,201.0,1.0,2.6,182.6,44,http://arxiv.org/pdf/2005.03778v3,"Testing autonomous driving algorithms on real autonomous vehicles is extremely costly and many researchers and developers in the field cannot afford a real car and the corresponding sensors. Although several free and open-source autonomous driving stacks, such as Autoware and Apollo are available, choices of open-source simulators to use with them are limited. In this paper, we introduce the LGSVL Simulator which is a high fidelity simulator for autonomous driving. The simulator engine provides end-to-end, full-stack simulation which is ready to be hooked up to Autoware and Apollo. In addition, simulator tools are provided with the core simulation engine which allow users to easily customize sensors, create new types of controllable objects, replace some modules in the core simulator, and create digital twins of particular environments.",Algsiahifisifoaudr,39.0,36.0,7.0
901,Autonomous vehicles,401.0,neural circuit policies enabling auditable autonomy,1.0,6.0,5.0,201.0,1.0,2.6,183.0,45,http://arxiv.org/pdf/2108.04214v1,"Safety is a critical concern for the next generation of autonomy that is likely to rely heavily on deep neural networks for perception and control. Formally verifying the safety and robustness of well-trained DNNs and learning-enabled systems under attacks, model uncertainties, and sensing errors is essential for safe autonomy. This research proposes a framework to repair unsafe DNNs in safety-critical systems with reachability analysis. The repair process is inspired by adversarial training which has demonstrated high effectiveness in improving the safety and robustness of DNNs. Different from traditional adversarial training approaches where adversarial examples are utilized from random attacks and may not be representative of all unsafe behaviors, our repair process uses reachability analysis to compute the exact unsafe regions and identify sufficiently representative examples to enhance the efficacy and efficiency of the adversarial training.   The performance of our framework is evaluated on two types of benchmarks without safe models as references. One is a DNN controller for aircraft collision avoidance with access to training data. The other is a rocket lander where our framework can be seamlessly integrated with the well-known deep deterministic policy gradient (DDPG) reinforcement learning algorithm. The experimental results show that our framework can successfully repair all instances on multiple safety specifications with negligible performance degradation. In addition, to increase the computational and memory efficiency of the reachability analysis algorithm, we propose a depth-first-search algorithm that combines an existing exact analysis method with an over-approximation approach based on a new set representation. Experimental results show that our method achieves a five-fold improvement in runtime and a two-fold improvement in memory usage compared to exact analysis.",Anecipoenauau,30.0,76.0,0.0
902,Autonomous vehicles,401.0,learning interaction-aware guidance policies for motion planning in dense traffic scenarios,1.0,7.0,5.0,201.0,1.0,2.6,183.4,46,http://arxiv.org/pdf/2107.04538v1,"Autonomous navigation in dense traffic scenarios remains challenging for autonomous vehicles (AVs) because the intentions of other drivers are not directly observable and AVs have to deal with a wide range of driving behaviors. To maneuver through dense traffic, AVs must be able to reason how their actions affect others (interaction model) and exploit this reasoning to navigate through dense traffic safely. This paper presents a novel framework for interaction-aware motion planning in dense traffic scenarios. We explore the connection between human driving behavior and their velocity changes when interacting. Hence, we propose to learn, via deep Reinforcement Learning (RL), an interaction-aware policy providing global guidance about the cooperativeness of other vehicles to an optimization-based planner ensuring safety and kinematic feasibility through constraint satisfaction. The learned policy can reason and guide the local optimization-based planner with interactive behavior to pro-actively merge in dense traffic while remaining safe in case the other vehicles do not yield. We present qualitative and quantitative results in highly interactive simulation environments (highway merging and unprotected left turns) against two baseline approaches, a learning-based and an optimization-based method. The presented results demonstrate that our method significantly reduces the number of collisions and increases the success rate with respect to both learning-based and optimization-based baselines.",Aleingupofomoplindetrsc,0.0,61.0,0.0
903,Autonomous vehicles,401.0,deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic,1.0,8.0,5.0,201.0,1.0,2.6,183.8,47,http://arxiv.org/pdf/2105.05701v1,"On-ramp merging is a challenging task for autonomous vehicles (AVs), especially in mixed traffic where AVs coexist with human-driven vehicles (HDVs). In this paper, we formulate the mixed-traffic highway on-ramp merging problem as a multi-agent reinforcement learning (MARL) problem, where the AVs (on both merge lane and through lane) collaboratively learn a policy to adapt to HDVs to maximize the traffic throughput. We develop an efficient and scalable MARL framework that can be used in dynamic traffic where the communication topology could be time-varying. Parameter sharing and local rewards are exploited to foster inter-agent cooperation while achieving great scalability. An action masking scheme is employed to improve learning efficiency by filtering out invalid/unsafe actions at each step. In addition, a novel priority-based safety supervisor is developed to significantly reduce collision rate and greatly expedite the training process. A gym-like simulation environment is developed and open-sourced with three different levels of traffic densities. We exploit curriculum learning to efficiently learn harder tasks from trained models under simpler settings. Comprehensive experimental results show the proposed MARL framework consistently outperforms several state-of-the-art benchmarks.",Ademurelefohionmeinmitr,0.0,52.0,0.0
904,Autonomous vehicles,401.0,lidar-camera calibration using 3d-3d point correspondences,1.0,10.0,5.0,201.0,1.0,2.6,184.6,48,http://arxiv.org/abs/1803.08181v2,"3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet",Alicaus3dpoco,73.0,2.0,4.0
905,Autonomous vehicles,401.0,on the importance of stereo for accurate depth estimation: an efficient semi-supervised deep neural network approach,1.0,11.0,5.0,201.0,1.0,2.6,185.0,49,http://arxiv.org/pdf/1803.09719v4,"We revisit the problem of visual depth estimation in the context of autonomous vehicles. Despite the progress on monocular depth estimation in recent years, we show that the gap between monocular and stereo depth accuracy remains large$-$a particularly relevant result due to the prevalent reliance upon monocular cameras by vehicles that are expected to be self-driving. We argue that the challenges of removing this gap are significant, owing to fundamental limitations of monocular vision. As a result, we focus our efforts on depth estimation by stereo. We propose a novel semi-supervised learning approach to training a deep stereo neural network, along with a novel architecture containing a machine-learned argmax layer and a custom runtime (that will be shared publicly) that enables a smaller version of our stereo DNN to run on an embedded GPU. Competitive results are shown on the KITTI 2015 stereo dataset. We also evaluate the recent progress of stereo algorithms by measuring the impact upon accuracy of various design criteria.",Aonthimofstfoacdeesanefsedeneneap,55.0,34.0,3.0
906,Autonomous vehicles,401.0,joint 3d proposal generation and object detection from view aggregation,1.0,12.0,5.0,201.0,1.0,2.6,185.4,50,http://arxiv.org/pdf/1712.02294v4,"We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod",Ajo3dprgeanobdefrviag,633.0,32.0,118.0
907,Autonomous vehicles,401.0,simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles,1.0,13.0,5.0,201.0,1.0,2.6,185.8,51,http://arxiv.org/pdf/1812.06120v2,"Using deep reinforcement learning, we train control policies for autonomous vehicles leading a platoon of vehicles onto a roundabout. Using Flow, a library for deep reinforcement learning in micro-simulators, we train two policies, one policy with noise injected into the state and action space and one without any injected noise. In simulation, the autonomous vehicle learns an emergent metering behavior for both policies in which it slows to allow for smoother merging. We then directly transfer this policy without any tuning to the University of Delaware Scaled Smart City (UDSSC), a 1:25 scale testbed for connected and automated vehicles. We characterize the performance of both policies on the scaled city. We show that the noise-free policy winds up crashing and only occasionally metering. However, the noise-injected policy consistently performs the metering behavior and remains collision-free, suggesting that the noise helps with the zero-shot policy transfer. Additionally, the transferred, noise-injected policy leads to a 5% reduction of average travel time and a reduction of 22% in maximum travel time in the UDSSC. Videos of the controllers can be found at https://sites.google.com/view/iccps-policy-transfer.",Asitosccizepotrfotrcoviauve,31.0,42.0,0.0
908,Autonomous vehicles,401.0,gaussian yolov3: an accurate and fast object detector using localization uncertainty for autonomous driving,1.0,15.0,5.0,201.0,1.0,2.6,186.6,52,http://arxiv.org/pdf/1904.04620v2,"The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.",Agayoanacanfaobdeuslounfoaudr,121.0,31.0,7.0
909,Autonomous vehicles,401.0,joint monocular 3d vehicle detection and tracking,1.0,17.0,5.0,201.0,1.0,2.6,187.4,53,http://arxiv.org/pdf/1811.10742v3,"Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.",Ajomo3dvedeantr,70.0,61.0,6.0
910,Autonomous vehicles,401.0,benchmarking 6dof outdoor visual localization in changing conditions,1.0,19.0,5.0,201.0,1.0,2.6,188.2,54,http://arxiv.org/pdf/1707.09092v3,"Visual localization enables autonomous vehicles to navigate in their surroundings and augmented reality applications to link virtual to real worlds. Practical visual localization approaches need to be robust to a wide variety of viewing condition, including day-night changes, as well as weather and seasonal variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera pose estimates. In this paper, we introduce the first benchmark datasets specifically designed for analyzing the impact of such factors on visual localization. Using carefully created ground truth poses for query images taken under a wide variety of conditions, we evaluate the impact of various factors on 6DOF camera pose estimation accuracy through extensive experiments with state-of-the-art localization approaches. Based on our results, we draw conclusions about the difficulty of different conditions, showing that long-term localization is far from solved, and propose promising avenues for future work, including sequence-based localization approaches and the need for better local features. Our benchmark is available at visuallocalization.net.",Abe6douviloinchco,270.0,85.0,39.0
911,Autonomous vehicles,401.0,end-to-end lane shape prediction with transformers,1.0,22.0,5.0,201.0,1.0,2.6,189.4,55,http://arxiv.org/pdf/2011.04233v2,"Lane detection, the process of identifying lane markings as approximated curves, is widely used for lane departure warning and adaptive cruise control in autonomous vehicles. The popular pipeline that solves it in two steps -- feature extraction plus post-processing, while useful, is too inefficient and flawed in learning the global context and lanes' long and thin structures. To tackle these issues, we propose an end-to-end method that directly outputs parameters of a lane shape model, using a network built with a transformer to learn richer structures and context. The lane shape model is formulated based on road structures and camera pose, providing physical interpretation for parameters of network output. The transformer models non-local interactions with a self-attention mechanism to capture slender structures and global context. The proposed method is validated on the TuSimple benchmark and shows state-of-the-art accuracy with the most lightweight model size and fastest speed. Additionally, our method shows excellent adaptability to a challenging self-collected lane detection dataset, showing its powerful deployment potential in real applications. Codes are available at https://github.com/liuruijin17/LSTR.",Aenlashprwitr,22.0,26.0,3.0
912,Autonomous vehicles,401.0,keep your eyes on the lane: real-time attention-guided lane detection,1.0,23.0,5.0,201.0,1.0,2.6,189.8,56,http://arxiv.org/pdf/2010.12035v2,"Modern lane detection methods have achieved remarkable performances in complex real-world scenarios, but many have issues maintaining real-time efficiency, which is important for autonomous vehicles. In this work, we propose LaneATT: an anchor-based deep lane detection model, which, akin to other generic deep object detectors, uses the anchors for the feature pooling step. Since lanes follow a regular pattern and are highly correlated, we hypothesize that in some cases global information may be crucial to infer their positions, especially in conditions such as occlusion, missing lane markers, and others. Thus, this work proposes a novel anchor-based attention mechanism that aggregates global information. The model was evaluated extensively on three of the most widely used datasets in the literature. The results show that our method outperforms the current state-of-the-art methods showing both higher efficacy and efficiency. Moreover, an ablation study is performed along with a discussion on efficiency trade-off options that are useful in practice.",Akeyoeyonthlareatlade,8.0,29.0,2.0
913,Autonomous vehicles,401.0,sparse and noisy lidar completion with rgb guidance and uncertainty,1.0,25.0,5.0,201.0,1.0,2.6,190.6,57,http://arxiv.org/pdf/1902.05356v1,"This work proposes a new method to accurately complete sparse LiDAR maps guided by RGB images. For autonomous vehicles and robotics the use of LiDAR is indispensable in order to achieve precise depth predictions. A multitude of applications depend on the awareness of their surroundings, and use depth cues to reason and react accordingly. On the one hand, monocular depth prediction methods fail to generate absolute and precise depth maps. On the other hand, stereoscopic approaches are still significantly outperformed by LiDAR based approaches. The goal of the depth completion task is to generate dense depth predictions from sparse and irregular point clouds which are mapped to a 2D plane. We propose a new framework which extracts both global and local information in order to produce proper depth maps. We argue that simple depth completion does not require a deep network. However, we additionally propose a fusion method with RGB guidance from a monocular camera in order to leverage object information and to correct mistakes in the sparse input. This improves the accuracy significantly. Moreover, confidence masks are exploited in order to take into account the uncertainty in the depth predictions from each modality. This fusion method outperforms the state-of-the-art and ranks first on the KITTI depth completion benchmark. Our code with visualizations is available.",Aspannolicowirgguanun,92.0,33.0,22.0
915,Autonomous vehicles,401.0,embedded real-time stereo estimation via semi-global matching on the gpu,1.0,28.0,5.0,201.0,1.0,2.6,191.8,58,http://arxiv.org/pdf/2006.03250v4,"Fast and accurate depth estimation, or stereo matching, is essential in embedded stereo vision systems, requiring substantial design effort to achieve an appropriate balance among accuracy, speed and hardware cost. To reduce the design effort and achieve the right balance, we propose FP-Stereo for building high-performance stereo matching pipelines on FPGAs automatically. FP-Stereo consists of an open-source hardware-efficient library, allowing designers to obtain the desired implementation instantly. Diverse methods are supported in our library for each stage of the stereo matching pipeline and a series of techniques are developed to exploit the parallelism and reduce the resource overhead. To improve the usability, FP-Stereo can generate synthesizable C code of the FPGA accelerator with our optimized HLS templates automatically. To guide users for the right design choice meeting specific application requirements, detailed comparisons are performed on various configurations of our library to investigate the accuracy/speed/cost trade-off. Experimental results also show that FP-Stereo outperforms the state-of-the-art FPGA design from all aspects, including 6.08% lower error, 2x faster speed, 30% less resource usage and 40% less energy consumption. Compared to GPU designs, FP-Stereo achieves the same accuracy at a competitive speed while consuming much less energy.",Aemrestesvisemaonthgp,90.0,18.0,13.0
916,Autonomous vehicles,401.0,ford multi-av seasonal dataset,1.0,31.0,5.0,201.0,1.0,2.6,193.0,59,http://arxiv.org/abs/2003.07969v1,"This paper presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times during 2017-18. The vehicles traversed an average route of 66 km in Michigan that included a mix of driving scenarios such as the Detroit Airport, freeways, city-centers, university campus and suburban neighbourhoods, etc. Each vehicle used in this data collection is a Ford Fusion outfitted with an Applanix POS-LV GNSS system, four HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged on the rooftop for 360-degree coverage and 1 Pointgrey 5 MP camera mounted behind the windshield for the forward field of view. We present the seasonal variation in weather, lighting, construction and traffic conditions experienced in dynamic urban environments. This dataset can help design robust algorithms for autonomous vehicles and multi-agent systems. Each log in the dataset is time-stamped and contains raw data from all the sensors, calibration values, pose trajectory, ground truth pose, and 3D maps. All data is available in Rosbag format that can be visualized, modified and applied using the open-source Robot Operating System (ROS). We also provide the output of state-of-the-art reflectivity-based localization for bench-marking purposes. The dataset can be freely downloaded at our website.",Afomuseda,16.0,72.0,0.0
917,Autonomous vehicles,401.0,counterfactual multi-agent policy gradients,1.0,32.0,5.0,201.0,1.0,2.6,193.4,60,http://arxiv.org/pdf/1705.08926v2,"Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.",Acomupogr,714.0,58.0,132.0
918,Autonomous vehicles,401.0,self-supervised 3d keypoint learning for ego-motion estimation,1.0,34.0,5.0,201.0,1.0,2.6,194.2,61,http://arxiv.org/pdf/2104.08027v2,"Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years. However, previous work has indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective universal lexical and sentence encoders even without any additional data and without any supervision. We propose an extremely simple, fast and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds without any additional external knowledge. Mirror-BERT relies on fully identical or slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in both lexical-level and sentence-level tasks, across different domains and different languages. Notably, in the standard sentence semantic similarity (STS) tasks, our self-supervised Mirror-BERT model even matches the performance of the task-tuned Sentence-BERT models from prior work. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple approach can yield effective universal lexical and sentence encoders.",Ase3dkelefoeges,5.0,61.0,0.0
919,Autonomous vehicles,401.0,multi-scale interaction for real-time lidar data segmentation on an embedded platform,1.0,35.0,5.0,201.0,1.0,2.6,194.6,62,http://arxiv.org/pdf/2008.09162v1,"Real-time semantic segmentation of LiDAR data is crucial for autonomously driving vehicles, which are usually equipped with an embedded platform and have limited computational resources. Approaches that operate directly on the point cloud use complex spatial aggregation operations, which are very expensive and difficult to optimize for embedded platforms. They are therefore not suitable for real-time applications with embedded systems. As an alternative, projection-based methods are more efficient and can run on embedded platforms. However, the current state-of-the-art projection-based methods do not achieve the same accuracy as point-based methods and use millions of parameters. In this paper, we therefore propose a projection-based method, called Multi-scale Interaction Network (MINet), which is very efficient and accurate. The network uses multiple paths with different scales and balances the computational resources between the scales. Additional dense interactions between the scales avoid redundant computations and make the network highly efficient. The proposed network outperforms point-based, image-based, and projection-based methods in terms of accuracy, number of parameters, and runtime. Moreover, the network processes more than 24 scans per second on an embedded platform, which is higher than the framerates of LiDAR sensors. The network is therefore suitable for autonomous vehicles.",Amuinforelidaseonanempl,7.0,52.0,0.0
920,Autonomous vehicles,401.0,expecting the unexpected: training detectors for unusual pedestrians with adversarial imposters,1.0,36.0,5.0,201.0,1.0,2.6,195.0,63,http://arxiv.org/pdf/1703.06283v2,"As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail"" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To allow for large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right ""priors"" or parameters for synthesis: we would like realistic data with poses and object configurations that mimic true Precarious Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem the Adversarial Imposters. We demonstrate that this simple pipeline allows one to synthesize realistic training data by making use of rendering/animation engines within a GAN framework. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Adversarial Imposters can also be used for ""in-the-tail"" validation at test-time, a notoriously difficult challenge for real-world deployment.",Aexthuntrdefounpewiadim,43.0,54.0,2.0
921,Autonomous vehicles,401.0,deep dual-resolution networks for real-time and accurate semantic segmentation of road scenes,1.0,37.0,5.0,201.0,1.0,2.6,195.4,64,http://arxiv.org/pdf/2101.06085v2,"Semantic segmentation is a key technology for autonomous vehicles to understand the surrounding scenes. The appealing performances of contemporary models usually come at the expense of heavy computations and lengthy inference time, which is intolerable for self-driving. Using light-weight architectures (encoder-decoder or two-pathway) or reasoning on low-resolution images, recent methods realize very fast scene parsing, even running at more than 100 FPS on a single 1080Ti GPU. However, there is still a significant gap in performance between these real-time methods and the models based on dilation backbones. To tackle this problem, we proposed a family of efficient backbones specially designed for real-time semantic segmentation. The proposed deep dual-resolution networks (DDRNets) are composed of two deep branches between which multiple bilateral fusions are performed. Additionally, we design a new contextual information extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to enlarge effective receptive fields and fuse multi-scale context based on low-resolution feature maps. Our method achieves a new state-of-the-art trade-off between accuracy and speed on both Cityscapes and CamVid dataset. In particular, on a single 2080Ti GPU, DDRNet-23-slim yields 77.4% mIoU at 102 FPS on Cityscapes test set and 74.7% mIoU at 230 FPS on CamVid test set. With widely used test augmentation, our method is superior to most state-of-the-art models and requires much less computation. Codes and trained models are available online.",Adeduneforeanacseseofrosc,13.0,61.0,0.0
922,Autonomous vehicles,401.0,categorical depth distribution network for monocular 3d object detection,1.0,38.0,5.0,201.0,1.0,2.6,195.8,65,http://arxiv.org/pdf/2103.01100v2,"Monocular 3D object detection is a key problem for autonomous vehicles, as it provides a solution with simple configuration compared to typical multi-sensor systems. The main challenge in monocular 3D detection lies in accurately predicting object depth, which must be inferred from object and scene cues due to the lack of direct range measurement. Many methods attempt to directly estimate depth to assist in 3D detection, but show limited performance as a result of depth inaccuracy. Our proposed solution, Categorical Depth Distribution Network (CaDDN), uses a predicted categorical depth distribution for each pixel to project rich contextual feature information to the appropriate depth interval in 3D space. We then use the computationally efficient bird's-eye-view projection and single-stage detector to produce the final output bounding boxes. We design CaDDN as a fully differentiable end-to-end approach for joint depth estimation and object detection. We validate our approach on the KITTI 3D object detection benchmark, where we rank 1st among published monocular methods. We also provide the first monocular 3D detection results on the newly released Waymo Open Dataset. We provide a code release for CaDDN which is made available.",Acadedinefomo3dobde,8.0,71.0,2.0
923,Autonomous vehicles,401.0,"lift, splat, shoot: encoding images from arbitrary camera rigs by implicitly unprojecting to 3d",1.0,39.0,5.0,201.0,1.0,2.6,196.2,66,http://arxiv.org/pdf/2008.05711v1,"The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single ""bird's-eye-view"" coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird's-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to ""lift"" each image individually into a frustum of features for each camera, then ""splat"" all frustums into a rasterized bird's-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird's-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by ""shooting"" template trajectories into a bird's-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot .",Alispshenimfrarcaribyimunto3d,24.0,45.0,5.0
924,Autonomous vehicles,401.0,persistent map saving for visual localization for autonomous vehicles: an orb-slam extension,1.0,40.0,5.0,201.0,1.0,2.6,196.6,67,http://arxiv.org/pdf/2005.07429v1,"Electric vhicles and autonomous driving dominate current research efforts in the automotive sector. The two topics go hand in hand in terms of enabling safer and more environmentally friendly driving. One fundamental building block of an autonomous vehicle is the ability to build a map of the environment and localize itself on such a map. In this paper, we make use of a stereo camera sensor in order to perceive the environment and create the map. With live Simultaneous Localization and Mapping (SLAM), there is a risk of mislocalization, since no ground truth map is used as a reference and errors accumulate over time. Therefore, we first build up and save a map of visual features of the environment at low driving speeds with our extension to the ORB-SLAM\,2 package. In a second run, we reload the map and then localize on the previously built-up map. Loading and localizing on a previously built map can improve the continuous localization accuracy for autonomous vehicles in comparison to a full SLAM. This map saving feature is missing in the original ORB-SLAM\,2 implementation.   We evaluate the localization accuracy for scenes of the KITTI dataset against the built up SLAM map. Furthermore, we test the localization on data recorded with our own small scale electric model car. We show that the relative translation error of the localization stays under 1\% for a vehicle travelling at an average longitudinal speed of 36 m/s in a feature-rich environment. The localization mode contributes to a better localization accuracy and lower computational load compared to a full SLAM. The source code of our contribution to the ORB-SLAM2 will be made public at: https://github.com/TUMFTM/orbslam-map-saving-extension.",Apemasafovilofoauveanorex,2.0,25.0,0.0
925,Autonomous vehicles,49.0,killing by autonomous vehicles and the legal doctrine of necessity,4.0,201.0,1.0,121.0,3.0,2.5,131.4,68,https://core.ac.uk/download/pdf/190003190.pdf,"How should autonomous vehicles (aka self-driving cars) be programmed to behave in the event of an unavoidable accident in which the only choice open is one between causing different damages or losses to different objects or persons? This paper addresses this ethical question starting from the normative principles elaborated in the law to regulate difficult choices in other emergency scenarios. In particular, the paper offers a rational reconstruction of some major principles and norms embedded in the Anglo-American jurisprudence and case law on the “doctrine of necessity”; and assesses which, if any, of these principles and norms can be utilized to find reasonable guidelines for solving the ethical issue of the regulation of the programming of autonomous vehicles in emergency situations. The paper covers the following topics: the distinction between “justification” and “excuse”, the legal prohibition of intentional killing outside self-defence, the incommensurability of goods, and the legal constrains to the use of lethal force set by normative positions: obligations, responsibility, rights, and authority. For each of these principles and constrains the possible application to the programming of autonomous vehicles is discussed. Based on the analysis, some practical suggestions are offered.",Akibyauveanthledoofne,36.0,37.0,1.0
926,Autonomous vehicles,61.0,a rawlsian algorithm for autonomous vehicles,4.0,201.0,1.0,116.0,3.0,2.5,133.5,69,http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=13881957&AN=123024542&h=qUkh%2FJfSOiU4ilVnJyfrE9F7hW%2F0QK8QzYYKu8l7TKYx%2BFMjfAWNmKNscF0gHTlZMXMZyBwW5j0Dhz9q7sf0sw%3D%3D&crl=f,"Autonomous vehicles must be programmed with procedures for dealing with trolley-style dilemmas where actions result in harm to either pedestrians or passengers. This paper outlines a Rawlsian algorithm as an alternative to the Utilitarian solution. The algorithm will gather the vehicle’s estimation of probability of survival for each person in each action, then calculate which action a self-interested person would agree to if he or she were in an original bargaining position of fairness. I will employ Rawls’ assumption that the Maximin procedure is what self-interested agents would use from an original position, and then show how the Maximin procedure can be operationalized to produce unique outputs over probabilities of survival.",Aaraalfoauve,46.0,17.0,1.0
927,Autonomous vehicles,109.0,situation awareness in future autonomous vehicles: beware of the unexpected,3.0,201.0,1.0,68.0,4.0,2.5,133.5,70,https://www.researchgate.net/profile/Mica-Endsley/publication/325298406_Situation_Awareness_in_Future_Autonomous_Vehicles_Beware_of_the_Unexpected/links/5b0445d44585154aeb07eed0/Situation-Awareness-in-Future-Autonomous-Vehicles-Beware-of-the-Unexpected.pdf,"Vehicle autonomy is being heavily promoted as a means of improving transportation safety on the roadways. This goal, however, is highly dependent on the ability of human drivers to maintain situation awareness and intervene in circumstances that the automation cannot handle. While autonomy software is improving, it remains far less capable than human drivers. The automation conundrum shows that even as it improves, system autonomy is increasingly likely to reduce the ability of drivers to provide needed oversight. The Human-Automation System Oversight (HASO) model provides guidance on the design of vehicle autonomy to facilitate effective human-autonomy design for semi-autonomous vehicles.",Asiawinfuauvebeofthun,42.0,21.0,1.0
928,Autonomous vehicles,105.0,a field study of pedestrians and autonomous vehicles,3.0,201.0,1.0,79.0,4.0,2.5,135.6,71,https://dl.acm.org/doi/pdf/10.1145/3239060.3239064,"Autonomous vehicles have been in development for nearly thirty years and recently have begun to operate in real-world, uncontrolled settings. With such advances, more widespread research and evaluation of human interaction with autonomous vehicles (AV) is necessary. Here, we present an interview study of 32 pedestrians who have interacted with Uber AVs. Our findings are focused on understanding and trust of AVs, perceptions of AVs and artificial intelligence, and how the perception of a brand affects these constructs. We found an inherent relationship between favorable perceptions of technology and feelings of trust toward AVs. Trust in AVs was also influenced by a favorable interpretation of the company's brand and facilitated by knowledge about what AV technology is and how it might fit into everyday life. To our knowledge, this paper is the first to surface AV-related interview data from pedestrians in a natural, real-world setting.",Aafistofpeanauve,34.0,64.0,2.0
929,Autonomous vehicles,138.0,a new car-following model for autonomous vehicles flow with mean expected velocity field,3.0,201.0,1.0,48.0,4.0,2.5,136.20000000000002,72,https://www.researchgate.net/profile/Li-Dong-Zhang-2/publication/321629662_A_new_car-following_model_for_autonomous_vehicles_flow_with_mean_expected_velocity_field/links/60497d35299bf1f5d83d92cd/A-new-car-following-model-for-autonomous-vehicles-flow-with-mean-expected-velocity-field.pdf,"Abstract Due to the development of the modern scientific technology, autonomous vehicles may realize to connect with each other and share the information collected from each vehicle. An improved forward considering car-following model was proposed with mean expected velocity field to describe the autonomous vehicles flow behavior. The new model has three key parameters: adjustable sensitivity, strength factor and mean expected velocity field size. Two lemmas and one theorem were proven as criteria for judging the stability of homogeneousautonomous vehicles flow. Theoretical results show that the greater parameters means larger stability regions. A series of numerical simulations were carried out to check the stability and fundamental diagram of autonomous flow. From the numerical simulation results, the profiles, hysteresis loop and density waves of the autonomous vehicles flow were exhibited. The results show that with increased sensitivity, strength factor or field size the traffic jam was suppressed effectively which are well in accordance with the theoretical results. Moreover, the fundamental diagrams corresponding to three parameters respectively were obtained. It demonstrates that these parameters play almost the same role on traffic flux: i.e. before the critical density the bigger parameter is, the greater flux is and after the critical density, the opposite tendency is. In general, the three parameters have a great influence on the stability and jam state of the autonomous vehicles flow.",Aanecamofoauveflwimeexvefi,83.0,36.0,4.0
930,Autonomous vehicles,117.0,pedestrian perception of autonomous vehicles with external interacting features,3.0,201.0,1.0,74.0,4.0,2.5,137.7,73,http://arxiv.org/abs/2006.12906v2,"The increasing number of autonomous vehicles has raised questions regarding pedestrian interaction with autonomous vehicles. Researchers have studied external interfaces designed for vehicle operators and other road-users (e.g., pedestrians). Most past studies have considered the interaction between pedestrians and autonomous vehicles with no visible operator. However, pedestrian-autonomous vehicle interaction may be complicated when there is a human sitting in the conventional driver’s seat of an autonomous vehicle. Such a scenario may cause some pedestrians to look to the passenger for cues when they should be looking for cues from the vehicle. The objective of the current study was to investigate pedestrians’ perspective of autonomous vehicles based on the interaction effect between passenger status and external features on the vehicle. Sixteen pedestrians completed a VR experiment. The results provided important insight into the important question of pedestrian-autonomous vehicle interaction when passengers are present in the driver seat of the vehicle.",Apepeofauvewiexinfe,36.0,3.0,2.0
931,Autonomous vehicles,102.0,autonomous vehicles: developing a public health research agenda to frame the future of transportation policy,3.0,201.0,1.0,98.0,4.0,2.5,140.4,74,https://publicpolicy.unc.edu/wp-content/uploads/sites/107/2017/09/Meier_Crayton-article-9.2017.pdf,"Recent advancements in autonomous vehicle technology have led to projections that fully autonomous vehicles could define the transportation network within the coming years. In preparation for this disruptive innovation in transportation technology, transportation scholars have started to assess the potential impacts of autonomous vehicles, and transportation policymakers have started to formulate policy recommendations and regulatory guidance concerning their deployment. However, there has been little analysis of the public health implications arising from the widespread adoption of fully autonomous vehicles. We examine these prospective public health impacts—both benefits and harms to individual and population health—and analyze how they can be considered in the development of transportation policy. In this manuscript, we discuss the evolving relationship between technological innovations in transportation and public health, conceptualize automated transportation as a disruptive technology necessitating a public policy response, and define a research agenda to examine the public health implications of autonomous vehicle policy, as seen through existing evidence on road casualties, environmental health, aging populations, non-communicable disease, land use, and labor markets. We conclude that such a public health research agenda would provide a basis to frame autonomous vehicle policies that best support the public's health, realize the United Nations Sustainable Development Goals to ensure healthy lives and create sustainable cities, and provide a basis for public health participation in transportation policy reforms.",Aauvedeapuhereagtofrthfuoftrpo,64.0,82.0,2.0
932,Autonomous vehicles,60.0,autonomous vehicles and energy impacts: a scenario analysis,4.0,201.0,1.0,143.0,3.0,2.5,141.3,75,https://www.sciencedirect.com/science/article/pii/S187661021736410X/pdf?md5=881ffaa6c5c520362a9297db6c71b1da&pid=1-s2.0-S187661021736410X-main.pdf,"Abstract Autonomous vehicles will have tremendous impact on our cities and regions. This rapidly emerging technology will affect the transport system in its entirety including changes in energy consumption; increased safety, climate change impacts, efficiency of transport operations and the platooning of trucks carrying freight. Primary questions remain. What are reasonable expectations of the impact of autonomous and connected vehicles on travel demand, energy consumption, and emissions? Can vehicle to vehicle communication have a significant impact on congestion and vehicle movements that will result in smoother traffic flow and accompanying reductions in energy and emissions? What policies and regulations guiding the operation of autonomous and connected vehicles will be enacted and to what extent will autonomous vehicles penetrate the market over what time period? This article attempts to identify and quantify the impact of autonomous vehicles on energy through development of scenarios that gauge the potential range and contribution of this emerging technology. The scenarios reflect an assessment of the state of practice and current research conducted in both the public and the private sector. Three scenarios are examined in details including energy impacts based on partial or full automation and personal versus a shared vehicle future for autonomous vehicles. There are numerous possible scenarios that may unfold but each will have to be responsive to our multimodal transport system with an objective to optimize modal, technological, financial and energy resources now and in the future.",Aauveanenimascan,29.0,3.0,0.0
933,Autonomous vehicles,75.0,privacy implications and liability issues of autonomous vehicles,4.0,201.0,1.0,131.0,3.0,2.5,142.2,76,http://eprints.kingston.ac.uk/37341/1/Collingwood-L-37341-AAM.pdf,"Autonomous vehicles have the potential for a variety of societal benefits. Individual mobility can be expanded to parties including the physically challenged, the elderly and the young. However, this article will consider two associated aspects of autonomous driving namely, privacy implications and issues of liability. Despite the many advantages of autonomous or connected vehicles, the downside in respect of privacy is that the ability to move about in relative anonymity will be lost. A secret rendezvous with a lover will be a thing of the past because the data bank associated with such vehicles will include information regarding exactly who is riding, where the passengers were picked up and dropped off, at what time and what route was taken. This information is a legitimate (and potentially very valuable!) business asset of the companies that own and operate autonomous vehicle fleets, who rely on such data to analyse how many vehicles are needed, in which locations and when they should be charged or re-fuelled, but the consequences on privacy (and the susceptibility of cyberattack) are tangible. Similarly, whilst another advantage of autonomous driving is that traffic accidents may be virtually eliminated, some people will nevertheless die or be injured in accidents involving autonomous vehicles. Therefore, in autonomous driving, a key question is that of liability and, specifically, where liability should reside in the event of such accident. This article considers how best to exploit autonomous vehicle innovation whilst, at the same time, securing the type of regulation appropriate to deal with the issues raised above.",Aprimanliisofauve,34.0,8.0,2.0
934,Autonomous vehicles,127.0,safe driving envelopes for path tracking in autonomous vehicles,3.0,201.0,1.0,81.0,4.0,2.5,142.8,77,https://dynamicdesignlab.sites.stanford.edu/sites/g/files/sbiybj9456/f/brown_safe_driving_envelopes_2016.pdf,"Abstract One approach to motion control of autonomous vehicles is to divide control between path planning and path tracking. This paper introduces an alternative control framework that integrates local path planning and path tracking using model predictive control (MPC). The controller plans trajectories, consisting of position and velocity states, that best follow a desired path while remaining within two safe envelopes. One envelope corresponds to conditions for stability and the other to obstacle avoidance. This enables the controller to safely and minimally deviate from a nominal path if necessary to avoid spinning out or colliding with an obstacle. A long prediction horizon allows action in the present to avoid a dangerous situation in the future. This motivates the use of a first-order hold discretization method that maintains model fidelity and computational feasibility. The controller is implemented in real-time on an experimental vehicle for several driving scenarios.",Asadrenfopatrinauve,146.0,21.0,3.0
935,Autonomous vehicles,82.0,p2v and v2p communication for pedestrian warning on the basis of autonomous vehicles,4.0,201.0,1.0,132.0,3.0,2.5,144.6,78,http://arxiv.org/pdf/1808.09023v3,"The use of smartphones in a road context by drivers and Vulnerable Road Users (VRU) is rapidly increasing. To reduce the risks related to the influence of smartphone usage in a situation where traffic needs to be considered, a collision prediction algorithm is proposed based on Pedestrian to Vehicle (P2V) and Vehicle to Pedestrian (V2P) communication technologies, which increases the visual situational awareness of VRU regarding the nearby location of both autonomous and manually-controlled vehicles in a user-friendly form. The proposed application broadcasts the device's position to the vehicles nearby, and reciprocally, the vehicles nearby broadcast their position to the device in use, supporting pedestrians and other VRU to minimize potential dangers and increase the acceptance of autonomous vehicles on our roads. Results regarding the evaluation of the proposed approach showed a good performance and high detection rate, as well as a high user satisfaction derived from the interaction with the system.",Ap2anv2cofopewaonthbaofauve,64.0,35.0,4.0
936,Autonomous vehicles,50.0,autonomous intersection management for semi-autonomous vehicles,4.0,201.0,1.0,170.0,3.0,2.5,146.4,79,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.1947&rep=rep1&type=pdf,"Recent advances in autonomous vehicle technology will open the door to highly efficient transportation systems in the future, as demonstrated by Autonomous Intersection Management, an intersection control protocol designed for fully autonomous vehicles. We, however, anticipate there will be a long transition period during which most vehicles have some but not all capabilities of fully autonomous vehicles. This paper introduces a new protocol called Semi-Autonomous Intersection Management, which allows vehicles with partially-autonomous features such as adaptive cruise control to enter an intersection from different directions simultaneously. Our experiments show that this protocol can greatly decrease traffic delay when most vehicles are semi-autonomous. Our incremental deployment study reveals that traffic delay keeps decreasing as more and more vehicles employ features of autonomy.",Aauinmafoseve,67.0,24.0,1.0
937,Autonomous vehicles,65.0,responsibility for crashes of autonomous vehicles: an ethical analysis,4.0,201.0,1.0,156.0,3.0,2.5,146.7,80,http://arxiv.org/pdf/1706.02513v1,"A number of companies including Google and BMW are currently working on the development of autonomous cars. But if fully autonomous cars are going to drive on our roads, it must be decided who is to be held responsible in case of accidents. This involves not only legal questions, but also moral ones. The first question discussed is whether we should try to design the tort liability for car manufacturers in a way that will help along the development and improvement of autonomous vehicles. In particular, Patrick Lin’s concern that any security gain derived from the introduction of autonomous cars would constitute a trade-off in human lives will be addressed. The second question is whether it would be morally permissible to impose liability on the user based on a duty to pay attention to the road and traffic and to intervene when necessary to avoid accidents. Doubts about the moral legitimacy of such a scheme are based on the notion that it is a form of defamation if a person is held to blame for causing the death of another by his inattention if he never had a real chance to intervene. Therefore, the legitimacy of such an approach would depend on the user having an actual chance to do so. The last option discussed in this paper is a system in which a person using an autonomous vehicle has no duty (and possibly no way) of interfering, but is still held (financially, not criminally) responsible for possible accidents. Two ways of doing so are discussed, but only one is judged morally feasible.",Arefocrofauveanetan,168.0,27.0,6.0
938,Autonomous vehicles,59.0,stip: spatio-temporal intersection protocols for autonomous vehicles,4.0,201.0,1.0,183.0,3.0,2.5,153.0,81,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.710.1542&rep=rep1&type=pdf,"Autonomous driving is likely to be the heart of urban transportation in the future. Autonomous vehicles have the potential to increase the safety of passengers and also to make road trips shorter and more enjoyable. As the first steps toward these goals, many car manufacturers are investing in designing and equipping their vehicles with advanced driver-assist systems. Road intersections are considered to be serious bottlenecks of urban transportation, as more than 44% of all reported crashes in U.S. occur within intersection areas which in turn lead to 8,500 fatalities and approximately 1 million injuries every year. Furthermore, the impact of road intersections on traffic delays leads to enormous waste of human and natural resources. In this paper, we therefore focus on intersection management in Intelligent Transportation Systems (ITS) research. In the future, when dealing with autonomous vehicles, it is critical to address safety and throughput concerns that arise from autonomous driving through intersections and roundabouts. Our goal is to provide vehicles with a safe and efficient passage method through intersections and roundabouts. We have been investigating vehicle-to-vehicle (V2V) communications as a part of co-operative driving in the context of autonomous driving. We have designed and developed efficient and reliable intersection protocols to avoid vehicle collisions at intersections and increase traffic throughput. In this paper, we introduce new V2V intersection protocols to achieve the above goals. We show that, in addition to intersections, these protocols are also applicable to vehicle crossings at roundabouts. Additionally, we study the effects of position inaccuracy of commonly-used GPS devices on some of our V2V intersection protocols and suggest required modifications to guarantee their safety and efficiency despite these impairments. Our simulation results show that we are able to avoid collisions and also increase the throughput of the intersections up to 87.82% compared to common traffic-light signalized intersections.",Astspinprfoauve,60.0,16.0,5.0
939,Autonomous vehicles,91.0,sue my car not me: products liability and accidents involving autonomous vehicles,4.0,201.0,1.0,194.0,3.0,2.5,165.9,82,http://illinoisjltp.com/journal/wp-content/uploads/2013/12/Gurney.pdf,"Autonomous vehicles will revolutionize society in the near future. Computers, however, are not perfect, and accidents will occur while the vehicle is in autonomous mode. This Article answers the question of who should be liable when an accident is caused in autonomous mode. This Article addresses the liability of autonomous vehicle by examining products liability through the use of four scenarios: the Distracted Driver; the Diminished Capabilities Driver; the Disabled Driver; and the Attentive Driver.Based on those scenarios, this Article suggests that the autonomous technology manufacturer should be liable for accidents caused in autonomous mode because the autonomous vehicle probably caused the accident. Liability should shift back to the “driver” depending on the nature of the driver and the ability of that person to prevent the accident. Thus, this Article argues that an autonomous vehicle manufacturer should be liable for accidents caused in autonomous mode for the Disabled Driver and partially for the Diminished Capabilities Driver and the Distracted Driver. This Article argues the Attentive Driver should be liable for most accidents caused in autonomous vehicle. Currently, products liability does not currently allocate the financial responsibility of an accident to the party that is responsible the accident, and this Article suggests that courts and legislatures need to address tort liability for accidents caused in autonomous mode to ensure that the responsible party bears responsibility for accidents.",Asumycanomeprlianacinauve,85.0,0.0,3.0
940,Autonomous vehicles,165.0,youtube av 50k: an annotated corpus for comments in autonomous vehicles,3.0,183.0,3.0,201.0,1.0,2.4,183.0,83,https://arxiv.org/pdf/1807.11227,"As intelligent systems are increasingly making decisions that directly affect society, perhaps the most important upcoming research direction in AI is to rethink the ethical implications of their actions. Means are needed to integrate moral, societal and legal values with technological developments in AI, both during the design process as well as part of the deliberation algorithms employed by these systems. In this paper, we describe leading ethics theories and propose alternative ways to ensure ethical behavior by artificial systems. Given that ethics are dependent on the socio-cultural context and are often only implicit in deliberation processes, methodologies are needed to elicit the values held by designers and stakeholders, and to make these explicit leading to better understanding and trust on artificial autonomous systems.",Ayoav50anancofocoinauve,18.0,67.0,0.0
941,Autonomous vehicles,6.0,"computer vision for autonomous vehicles: problems, datasets and state of the art",5.0,201.0,1.0,201.0,1.0,2.2,142.5,84,http://arxiv.org/pdf/1704.05519v3,"Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This book attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.",Acovifoauveprdaanstofthar,304.0,480.0,11.0
942,Autonomous vehicles,7.0,on the future of transportation in an era of automated and autonomous vehicles,5.0,201.0,1.0,201.0,1.0,2.2,142.8,85,https://www.pnas.org/content/pnas/116/16/7684.full.pdf,"Automated vehicles (AVs) already navigate US highways and those of many other nations around the world. Current questions about AVs do not now revolve around whether such technologies should or should not be implemented; they are already with us. Rather, such questions are more and more focused on how such technologies will impact evolving transportation systems, our social world, and the individuals who live within it and whether such systems ought to be fully automated or remain under some form of direct human control. More importantly, how will mobility itself change as these independent operational vehicles first share and then dominate our roadways? How will the public be kept apprised of their evolving capacities, and what will be the impact of science and the communication of scientific advances across the varying forms of social media on these developments? We look here to address these issues and to provide some suggestions for the problems that are currently emerging.",Aonthfuoftrinanerofauanauve,80.0,111.0,0.0
943,Autonomous vehicles,9.0,the coming collision between autonomous vehicles and the liability system,5.0,201.0,1.0,201.0,1.0,2.2,143.4,86,http://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=2731&context=lawreview,,Athcocobeauveanthlisy,115.0,0.0,6.0
944,Autonomous vehicles,16.0,an integrated architecture for autonomous vehicles simulation,5.0,201.0,1.0,201.0,1.0,2.2,145.5,87,http://paginas.fe.up.pt/~niadr/PUBLICATIONS/2012/2_ACM-SAC_056_1886.pdf,"Modeling and simulation tools are being increasingly acclaimed in the research field of autonomous vehicles systems, as they provide suitable test beds for the development and evaluation of such complex systems. However, these tools still do not account for some integration capabilities amongst several state-of-the-art Intelligent Transportation Systems, e.g. to study autonomous driving behaviors in human-steered urban traffic scenarios, which are crucial to the Future Urban Transport paradigm.
 In this paper we describe the modeling and implementation of an integration architecture of two types of simulators, namely a robotics and a traffic simulator. This integration should enable autonomous vehicles to be deployed in a rather realistic traffic flow as an agent entity (on the traffic simulator), at the same time it simulates all its sensors and actuators (on the robotics counterpart). Also, the statistical tools available in the traffic simulator will allow practitioners to infer what kind of advantages such a novel technology will bring to our everyday's lives. Furthermore, an architecture for the integration of the aforementioned simulators is proposed and implemented in the light of the most desired features of such software environments.
 To assess the usefulness of the platform architecture towards the expected realistic simulation facility, a comprehensive system evaluation is performed and critically reviewed, leveraging the feasibility of the integration. Further developments and future perspectives are also suggested.",Aaninarfoauvesi,57.0,121.0,0.0
945,Autonomous vehicles,17.0,autonomous vehicles in support of naval operations,5.0,201.0,1.0,201.0,1.0,2.2,145.8,88,http://arxiv.org/pdf/2011.13528v2,"With the gradual maturity of 5G technology,autonomous driving technology has attracted moreand more attention among the research commu-nity. Autonomous driving vehicles rely on the co-operation of artificial intelligence, visual comput-ing, radar, monitoring equipment and GPS, whichenables computers to operate motor vehicles auto-matically and safely without human interference.However, the large-scale dataset for training andsystem evaluation is still a hot potato in the devel-opment of robust perception models. In this paper,we present the NEOLIX dataset and its applica-tions in the autonomous driving area. Our datasetincludes about 30,000 frames with point cloud la-bels, and more than 600k 3D bounding boxes withannotations. The data collection covers multipleregions, and various driving conditions, includingday, night, dawn, dusk and sunny day. In orderto label this complete dataset, we developed vari-ous tools and algorithms specified for each task tospeed up the labelling process. It is expected thatour dataset and related algorithms can support andmotivate researchers for the further developmentof autonomous driving in the field of computer vi-sion.",Aauveinsuofnaop,11.0,3.0,0.0
946,Autonomous vehicles,21.0,sensor technology in autonomous vehicles: a review,5.0,201.0,1.0,201.0,1.0,2.2,147.0,89,http://arxiv.org/pdf/1912.02596v1,"Energy Autonomous Wearable Sensors (EAWS) have attracted a large interest due to their potential to provide reliable measurements and continuous bioelectric signals, which help to reduce health risk factors early on, ongoing assessment for disease prevention, and maintaining optimum, lifelong health quality. This review paper presents recent developments and state-of-the-art research related to three critical elements that enable an EAWS. The first element is wearable sensors, which monitor human body physiological signals and activities. Emphasis is given on explaining different types of transduction mechanisms presented, and emerging materials and fabrication techniques. The second element is the flexible and wearable energy storage device to drive low-power electronics and the software needed for automatic detection of unstable physiological parameters. The third is the flexible and stretchable energy harvesting module to recharge batteries for continuous operation of wearable sensors. We conclude by discussing some of the technical challenges in realizing energy-autonomous wearable sensing technologies and possible solutions for overcoming them.",Aseteinauveare,36.0,19.0,2.0
947,Autonomous vehicles,22.0,autonomous vehicles control in the vislab intercontinental autonomous challenge,5.0,201.0,1.0,201.0,1.0,2.2,147.3,90,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.591.3670&rep=rep1&type=pdf,"Autonomous driving is one of the most interesting fields of research, with a number of important applications, like agricultural, military and, most significantly, safety. This paper addresses the problem of designing a general purpose path planner and its associated low level control for autonomous vehicles operating in unknown environments. Different kinds of inputs, like the results of obstacle detection, ditch localization, lane detection, and global path planning information are merged together using potential fields to build a representation of the environment in real-time; kinematically feasible trajectories, based on vehicle dynamics, are generated on a cost map. This approach demonstrated both flexibility and reliability for vehicle driving in very different environments, including extreme road conditions. This controller was extensively tested during VIAC, the VisLab Intercontinental Autonomous Challenge, a 13,000 km long test for intelligent vehicle applications. The results, collected during the development stage and the experiment itself, are presented in the final part of this article.",Aauvecointhviinauch,101.0,26.0,1.0
948,Autonomous vehicles,24.0,planning long dynamically feasible maneuvers for autonomous vehicles,5.0,201.0,1.0,201.0,1.0,2.2,147.9,91,https://repository.upenn.edu/cgi/viewcontent.cgi?article=1027&context=grasp_papers,"In this paper, we present an algorithm for generating complex dynamically-feasible maneuvers for autonomous vehicles traveling at high speeds over large distances. Our approach is based on performing anytime incremental search on a multiresolution, dynamically-feasible lattice state space. The resulting planner provides real-time performance and guarantees on and control of the suboptimality of its solution. We provide theoretical properties and experimental results from an implementation on an autonomous passenger vehicle that competed in, and won, the Urban Challenge competition.",Apllodyfemafoauve,188.0,0.0,22.0
949,Autonomous vehicles,27.0,people must retain control of autonomous vehicles,5.0,201.0,1.0,201.0,1.0,2.2,148.8,92,http://arxiv.org/pdf/1807.04414v2,"The emerging technology enabling autonomy in vehicles has led to a variety of new problems in transportation networks, such as planning and perception for autonomous vehicles. Other works consider social objectives such as decreasing fuel consumption and travel time by platooning. However, these strategies are limited by the actions of the surrounding human drivers. In this paper, we consider proactively achieving these social objectives by influencing human behavior through planned interactions. Our key insight is that we can use these social objectives to design local interactions that influence human behavior to achieve these goals. To this end, we characterize the increase in road capacity afforded by platooning, as well as the vehicle configuration that maximizes road capacity. We present a novel algorithm that uses a low-level control framework to leverage local interactions to optimally rearrange vehicles. We showcase our algorithm using a simulated road shared between autonomous and human-driven vehicles, in which we illustrate the reordering in action.",Apemurecoofauve,35.0,8.0,0.0
950,Autonomous vehicles,116.0,path planning for autonomous vehicles using model predictive control,3.0,201.0,1.0,112.0,3.0,2.2,148.8,93,https://users.soe.ucsc.edu/~habhakta/MPC_pathPlanning.pdf,"Path planning for autonomous vehicles in dynamic environments is an important but challenging problem, due to the constraints of vehicle dynamics and existence of surrounding vehicles. Typical trajectories of vehicles involve different modes of maneuvers, including lane keeping, lane change, ramp merging, and intersection crossing. There exist prior arts using the rule-based high-level decision making approaches to decide the mode switching. Instead of using explicit rules, we propose a unified path planning approach using Model Predictive Control (MPC), which automatically decides the mode of maneuvers. To ensure safety, we model surrounding vehicles as polygons and develop a type of constraints in MPC to enforce the collision avoidance between the ego vehicle and surrounding vehicles. To achieve comfortable and natural maneuvers, we include a lane-associated potential field in the objective function of the MPC. We have simulated the proposed method in different test scenarios and the results demonstrate the effectiveness of the proposed approach in automatically generating reasonable maneuvers while guaranteeing the safety of the autonomous vehicle.",Apaplfoauveusmoprco,56.0,26.0,1.0
951,Autonomous vehicles,29.0,some pitfalls in the promises of automated and autonomous vehicles,5.0,201.0,1.0,201.0,1.0,2.2,149.4,94,http://arxiv.org/pdf/2102.05897v1,"Automated driving has become a major topic of interest not only in the active research community but also in mainstream media reports. Visual perception of such intelligent vehicles has experienced large progress in the last decade thanks to advances in deep learning techniques but some challenges still remain. One such challenge is the detection of corner cases. They are unexpected and unknown situations that occur while driving. Conventional visual perception methods are often not able to detect them because corner cases have not been witnessed during training. Hence, their detection is highly safety-critical, and detection methods can be applied to vast amounts of collected data to select suitable training data. A reliable detection of corner cases will not only further automate the data selection procedure and increase safety in autonomous driving but can thereby also affect the public acceptance of the new technology in a positive manner. In this work, we continue a previous systematization of corner cases on different levels by an extended set of examples for each level. Moreover, we group detection approaches into different categories and link them with the corner case levels. Hence, we give directions to showcase specific corner cases and basic guidelines on how to technically detect them.",Asopiinthprofauanauve,49.0,101.0,5.0
952,Autonomous vehicles,37.0,cooperative control of dynamical systems: applications to autonomous vehicles,5.0,201.0,1.0,201.0,1.0,2.2,151.8,95,http://arxiv.org/pdf/1907.07643v1,"Autonomous driving is a safety critical application of sensing and decision-making technologies. Communication technologies extend the awareness capabilities of vehicles, beyond what is achievable with the on-board systems only. Nonetheless, issues typically related to wireless networking must be taken into account when designing safe and reliable autonomous systems. The aim of this work is to present a control algorithm and a communication paradigm over 5G networks for negotiating traffic junctions in urban areas. The proposed control framework has been shown to converge in a finite time and the supporting communication software has been designed with the objective of minimising communication delays. At the same time, the underlying network guarantees reliability of the communication. The proposed framework has been successfully deployed and tested, in partnership with Ericsson AB, at the AstaZero proving ground in Goteborg, Sweden. In our experiments, three autonomous vehicles successfully drove through an intersection of 235 square meters in a urban scenario.",Acocoofdysyaptoauve,822.0,64.0,52.0
953,Autonomous vehicles,38.0,simulation in development and testing of autonomous vehicles,5.0,201.0,1.0,201.0,1.0,2.2,152.10000000000002,96,https://www.researchgate.net/profile/Hans-Peter-Schoener/publication/330486072_The_Role_of_Simulation_in_Development_and_Testing_of_Autonomous_Vehicles/links/5b511c0caca27217ffa670f7/The-Role-of-Simulation-in-Development-and-Testing-of-Autonomous-Vehicles.pdf,"On the first glance, autonomous vehicles seem to be just a simple continuation of the development of assistance systems which help the driver keeping the lane, holding the distance to other vehicles and avoiding accidents, with the vision of avoiding 80% of all accidents, because they are mainly caused by human errors. However, there is huge challenge with respect to the requirements on system performance and reliability for this step. As Herrtwich mentioned in [1], human drivers do quite well in driving a vehicle without accident, with statistically 7.5 million km between accidents on the German Autobahn network; if an assistance system helps a driver to avoid such accidents in (just for example) 9 out of 10 times, it does a good job by reducing the number of accidents by a factor of ten. However, autonomous vehicles with SAE level 3 or higher face the challenge to avoid or control any critical situation within a statistical distance of 75 million km between accidents, in order to achieve a similar performance compared to a level 2 (driver assisted) system. That includes many situations, which have traditionally been handled by human drivers easily, but might be difficult for automation.",Asiindeanteofauve,6.0,0.0,0.0
954,Autonomous vehicles,39.0,toward reliable off road autonomous vehicles operating in challenging environments,5.0,201.0,1.0,201.0,1.0,2.2,152.4,97,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.2383&rep=rep1&type=pdf,"The DARPA PerceptOR program has implemented a rigorous evaluative test program which fosters the development of field relevant outdoor mobile robots. Autonomous ground vehicles were deployed on diverse test courses throughout the USA and quantitatively evaluated on such factors as autonomy level, waypoint acquisition, failure rate, speed, and communications bandwidth. Our efforts over the three year program have produced new approaches in planning, perception, localization, and control which have been driven by the quest for reliable operation in challenging environments. This paper focuses on some of the most unique aspects of the systems developed by the CMU PerceptOR team, the lessons learned during the effort, and the most immediate challenges that remain to be addressed.",Atoreofroauveopinchen,161.0,119.0,5.0
955,Autonomous vehicles,40.0,"coordinating hundreds of cooperative, autonomous vehicles in warehouses",5.0,201.0,1.0,201.0,1.0,2.2,152.7,98,https://ojs.aaai.org/index.php/aimagazine/article/view/2082/1981,"The Kiva warehouse management system creates a new paradigm for pick-pack-and-ship warehouses that significantly improves worker productivity. The Kiva system uses movable storage shelves that can be lifted by small, autonomous robots. By bringing the product to the worker, productivity is increased by a factor of two or more, while simultaneously improving accountability and flexibility. A Kiva installation for a large distribution center may require 500 or more vehicles. As such, the Kiva system represents the first commercially available, large-scale autonomous robot system. The first permanent installation of a Kiva system was deployed in the summer of 2006.",Acohuofcoauveinwa,608.0,20.0,22.0
956,Autonomous vehicles,113.0,use of iot technology to drive the automotive industry from connected to full autonomous vehicles,3.0,201.0,1.0,141.0,3.0,2.2,156.60000000000002,99,http://arxiv.org/pdf/2008.04379v1,"Abstract: The automotive industry has been around for quite some time and it has evolved ever since, but the major transformation that is happening now from vehicles driven by humans to vehicles driven by themselves will have a long term impact on society. Today's cars are already connected and have been connected for some time, since they can link to smartphones, offer emergency roadside assistance, register real-time traffic alerts etc., but this evolution is about to change. The automobile industry is on the brink of a revolution, to move to self-driving automobile industry, and the driving force behind this is the fast developing technology, the Internet of Things (IoT). IoT will transform the automobile industry and at the same time, the automobile industry will provide a big boost to IoT. The potential and the prospects of this technology is astonishing. This paper examines the market and technical trends towards Autonomous Vehicles, evolution stages from early cars to fully autonomous, the importance of IoT in driving this industry ecosystem, advantages and disadvantages of Autonomous Vehincles, key issues and challenges faced by the industry, standards activities around this industry and finally the deployment use cases. The focus of this paper is more based on an industrial push to identify issues and challenges of Autonomous Vehicles and less on any academic research activity. The intention of this paper is to bring these issues and challenges to the attention of IFAC technical committee and trigger some debate on the opportunities for IFAC research in international stability.",Ausofiotetodrthauinfrcotofuauve,60.0,1.0,3.0
957,Autonomous vehicles,154.0,risk analysis of autonomous vehicles in mixed traffic streams,3.0,201.0,1.0,118.0,3.0,2.2,162.0,100,http://arxiv.org/pdf/2109.07211v1,"The introduction of autonomous vehicles in the surface transportation system could improve traffic safety and reduce traffic congestion and negative environmental effects. Although the continuous evolution in computing, sensing, and communication technologies can improve the performance of autonomous vehicles, the new combination of autonomous automotive and electronic communication technologies will present new challenges, such as interaction with other nonautonomous vehicles, which must be addressed before implementation. The objective of this study was to identify the risks associated with the failure of an autonomous vehicle in mixed traffic streams. To identify the risks, the autonomous vehicle system was first disassembled into vehicular components and transportation infrastructure components, and then a fault tree model was developed for each system. The failure probabilities of each component were estimated by reviewing the published literature and publicly available data sources. This analysis resulted in a failure probability of about 14% resulting from a sequential failure of the autonomous vehicular components alone in the vehicle’s lifetime, particularly the components responsible for automation. After the failure probability of autonomous vehicle components was combined with the failure probability of transportation infrastructure components, an overall failure probability related to vehicular or infrastructure components was found: 158 per 1 million mi of travel. The most critical combination of events that could lead to failure of autonomous vehicles, known as minimal cut-sets, was also identified. Finally, the results of fault tree analysis were compared with real-world data available from the California Department of Motor Vehicles autonomous vehicle testing records.",Arianofauveinmitrst,42.0,136.0,1.0
1297,Denoising,13.0,unprocessing images for learned raw denoising,5.0,5.0,5.0,25.0,5.0,5.0,13.4,1,http://openaccess.thecvf.com/content_CVPR_2019/papers/Brooks_Unprocessing_Images_for_Learned_Raw_Denoising_CVPR_2019_paper.pdf,"Machine learning techniques work best when the data used for training resembles the data used for evaluation. This holds true for learned single-image denoising algorithms, which are applied to real raw camera sensor readings but, due to practical constraints, are often trained on synthetic image data. Though it is understood that generalizing from synthetic to real images requires careful consideration of the noise properties of camera sensors, the other aspects of an image processing pipeline (such as gain, color correction, and tone mapping) are often overlooked, despite their significant effect on how raw measurements are transformed into finished images. To address this, we present a technique to “unprocess” images by inverting each step of an image processing pipeline, thereby allowing us to synthesize realistic raw sensor measurements from commonly available Internet photos. We additionally model the relevant components of an image processing pipeline when evaluating our loss function, which allows training to be aware of all relevant photometric processing that will occur after denoising. By unprocessing and processing training data and model outputs in this way, we are able to train a simple convolutional neural network that has 14%-38% lower error rates and is 9×-18× faster than the previous state of the art on the Darmstadt Noise Dataset, and generalizes to sensors outside of that dataset as well.",Dunimfolerade,141.0,53.0,22.0
1298,Denoising,33.0,denoising diffusion probabilistic models,5.0,10.0,5.0,5.0,5.0,5.0,15.4,2,https://arxiv.org/pdf/2006.11239,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL",Ddediprmo,128.0,74.0,47.0
1299,Denoising,40.0,beyond a gaussian denoiser: residual learning of deep cnn for image denoising,5.0,20.0,5.0,2.0,5.0,5.0,20.6,3,https://arxiv.org/pdf/1608.03981.pdf).,"The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",Dbeagadereleofdecnfoimde,3164.0,49.0,572.0
1300,Denoising,47.0,toward convolutional blind denoising of real photographs,4.0,32.0,5.0,10.0,5.0,4.7,29.9,4,https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_Toward_Convolutional_Blind_Denoising_of_Real_Photographs_CVPR_2019_paper.pdf,"While deep convolutional neural networks (CNNs) have achieved impressive success in image denoising with additive white Gaussian noise (AWGN), their performance remains limited on real-world noisy photographs. The main reason is that their learned models are easy to overfit on the simplified AWGN model which deviates severely from the complicated real-world noise model. In order to improve the generalization ability of deep CNN denoisers, we suggest training a convolutional blind denoising network (CBDNet) with more realistic noise model and real-world noisy-clean image pairs. On the one hand, both signal-dependent noise and in-camera signal processing pipeline is considered to synthesize realistic noisy images. On the other hand, real-world noisy photographs and their nearly noise-free counterparts are also included to train our CBDNet. To further provide an interactive strategy to rectify denoising result conveniently, a noise estimation subnetwork with asymmetric learning to suppress under-estimation of noise level is embedded into CBDNet. Extensive experimental results on three datasets of real-world noisy pho- tographs clearly demonstrate the superior performance of CBDNet over state-of-the-arts in terms of quantitative met- rics and visual quality. The code has been made available at https://github.com/GuoShi28/CBDNet.",Dtocobldeofreph,291.0,73.0,54.0
1301,Denoising,35.0,noise2self: blind denoising by self-supervision,5.0,50.0,4.0,16.0,5.0,4.6,35.3,5,http://proceedings.mlr.press/v97/batson19a/batson19a.pdf,"We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (""$\mathcal{J}$-invariant""), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate $\mathcal{J}$-invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.",Dnobldebyse,176.0,31.0,35.0
1302,Denoising,139.0,speech denoising with deep feature losses,3.0,35.0,5.0,32.0,5.0,4.4,65.3,6,https://arxiv.org/pdf/1806.10522,"We present an end-to-end deep learning approach to denoising speech signals by processing the raw waveform directly. Given input audio containing speech corrupted by an additive background signal, the system aims to produce a processed signal that contains only the speech content. Recent approaches have shown promising results using various deep network architectures. In this paper, we propose to train a fully-convolutional context aggregation network using a deep feature loss. That loss is based on comparing the internal feature activations in a different network, trained for acoustic environment detection and domestic audio tagging. Our approach outperforms the state-of-the-art in objective speech quality metrics and in large-scale perceptual experiments with human listeners. It also outperforms an identical network trained using traditional regression losses. The advantage of the new approach is particularly pronounced for the hardest data with the most intrusive background noise, for which denoising is most needed and most challenging.",Dspdewidefelo,94.0,46.0,21.0
1303,Denoising,50.0,real image denoising with feature attention,4.0,52.0,4.0,28.0,5.0,4.3,44.2,7,http://openaccess.thecvf.com/content_ICCV_2019/papers/Anwar_Real_Image_Denoising_With_Feature_Attention_ICCV_2019_paper.pdf,"Deep convolutional neural networks perform better on images containing spatially invariant noise (synthetic noise); however, its performance is limited on real-noisy photographs and requires multiple stage network modeling. To advance the practicability of the denoising algorithms, this paper proposes a novel single-stage blind real image denoising network (RIDNet) by employing a modular architecture. We use residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality on three synthetic and four real noisy datasets against 19 state-of-the-art algorithms demonstrate the superiority of our RIDNet.",Dreimdewifeat,118.0,70.0,24.0
1304,Denoising,24.0,from denoising to compressed sensing,5.0,83.0,4.0,46.0,4.0,4.3,54.2,8,https://arxiv.org/pdf/1406.4175,"A denoising algorithm seeks to remove noise, errors, or perturbations from a signal. Extensive research has been devoted to this arena over the last several decades, and as a result, todays denoisers can effectively remove large amounts of additive white Gaussian noise. A compressed sensing (CS) reconstruction algorithm seeks to recover a structured signal acquired using a small number of randomized measurements. Typical CS reconstruction algorithms can be cast as iteratively estimating a signal from a perturbed observation. This paper answers a natural question: How can one effectively employ a generic denoiser in a CS reconstruction algorithm? In response, we develop an extension of the approximate message passing (AMP) framework, called denoising-based AMP (D-AMP), that can integrate a wide class of denoisers within its iterations. We demonstrate that, when used with a high-performance denoiser for natural images, D-AMP offers the state-of-the-art CS recovery performance while operating tens of times faster than competing methods. We explain the exceptional performance of D-AMP by analyzing some of its theoretical features. A key element in D-AMP is the use of an appropriate Onsager correction term in its iterations, which coerces the signal perturbation at each iteration to be very close to the white Gaussian noise that denoisers are typically designed to remove.",Dfrdetocose,398.0,98.0,80.0
1305,Denoising,93.0,burst denoising with kernel prediction networks,4.0,53.0,4.0,60.0,4.0,4.0,67.1,9,https://openaccess.thecvf.com/content_cvpr_2018/papers/Mildenhall_Burst_Denoising_With_CVPR_2018_paper.pdf,"We present a technique for jointly denoising bursts of images taken from a handheld camera. In particular, we propose a convolutional neural network architecture for predicting spatially varying kernels that can both align and denoise frames, a synthetic data generation approach based on a realistic noise formation model, and an optimization guided by an annealed loss function to avoid undesirable local minima. Our model matches or outperforms the state-of-the-art across a wide range of noise levels on both real and synthetic data.",Dbudewikeprne,187.0,30.0,21.0
1306,Denoising,66.0,the little engine that could: regularization by denoising (red),4.0,125.0,3.0,34.0,5.0,3.9,80.0,10,https://arxiv.org/pdf/1611.02862,"Removal of noise from an image is an extensively studied problem in image processing. Indeed, the recent advent of sophisticated and highly effective denoising algorithms has led some to believe that existing methods are touching the ceiling in terms of noise removal performance. Can we leverage this impressive achievement to treat other tasks in image processing? Recent work has answered this question positively, in the form of the Plug-and-Play Prior ($P^3$) method, showing that any inverse problem can be handled by sequentially applying image denoising steps. This relies heavily on the ADMM optimization technique in order to obtain this chained denoising interpretation. Is this the only way in which tasks in image processing can exploit the image denoising engine? In this paper we provide an alternative, more powerful, and more flexible framework for achieving the same goal. As opposed to the $P^3$ method, we offer Regularization by Denoising (RED): using the denoising engine in defining the regulariza...",Dthlienthcorebyde(r,329.0,109.0,51.0
1307,Denoising,401.0,"bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",1.0,2.0,5.0,1.0,5.0,3.8,121.4,11,https://arxiv.org/pdf/1910.13461,"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",Dbadeseprfonalagetranco,1339.0,34.0,333.0
1308,Denoising,401.0,multilingual denoising pre-training for neural machine translation,1.0,1.0,5.0,4.0,5.0,3.8,121.9,12,http://arxiv.org/pdf/2110.10472v1,"Abstract This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1",Dmudeprfonematr,321.0,118.0,87.0
1309,Denoising,114.0,real-world noisy image denoising: a new benchmark,3.0,76.0,4.0,85.0,4.0,3.7,90.1,13,https://arxiv.org/pdf/1804.02603,"Most of previous image denoising methods focus on additive white Gaussian noise (AWGN). However,the real-world noisy image denoising problem with the advancing of the computer vision techiniques. In order to promote the study on this problem while implementing the concurrent real-world image denoising datasets, we construct a new benchmark dataset which contains comprehensive real-world noisy images of different natural scenes. These images are captured by different cameras under different camera settings. We evaluate the different denoising methods on our new dataset as well as previous datasets. Extensive experimental results demonstrate that the recently proposed methods designed specifically for realistic noise removal based on sparse or low rank theories achieve better denoising performance and are more robust than other competing methods, and the newly proposed dataset is more challenging. The constructed dataset of real photographs is publicly available at \url{this https URL} for researchers to investigate new real-world image denoising methods. We will add more analysis on the noise statistics in the real photographs of our new dataset in the next version of this article.",Drenoimdeanebe,79.0,34.0,10.0
1310,Denoising,401.0,noise2void - learning denoising from single noisy images,1.0,58.0,4.0,9.0,5.0,3.4000000000000004,146.2,14,http://arxiv.org/abs/1906.00651v2,"The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as Noise2Noise (N2N). Here, we introduce Noise2Void (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of Noise2Void drops in moderation and compares favorably to training-free denoising methods.",Dno-ledefrsinoim,280.0,29.0,61.0
1311,Denoising,401.0,high-quality self-supervised deep image denoising,1.0,49.0,4.0,30.0,5.0,3.4000000000000004,148.9,15,http://papers.nips.cc/paper/8920-high-quality-self-supervised-deep-image-denoising.pdf,"We describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images, or explicit pairs of corrupted images, and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a ""blind spot"" in the receptive field, and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise, and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data.",Dhisedeimde,104.0,39.0,20.0
1312,Denoising,401.0,variational denoising network: toward blind noise modeling and removal,1.0,80.0,4.0,40.0,5.0,3.4000000000000004,164.3,16,http://arxiv.org/pdf/1908.11314v4,"Blind image denoising is an important yet very challenging problem in computer vision due to the complicated acquisition process of real images. In this work we propose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.",Dvadenetoblnomoanre,61.0,51.0,24.0
1313,Denoising,2.0,"a review of image denoising algorithms, with a new one",5.0,201.0,1.0,18.0,5.0,3.4,86.4,17,https://hal.archives-ouvertes.fr/hal-00271141/file/061602r.pdf,"The search for efficient image denoising methods is still a valid challenge at the crossing of functional analysis and statistics. In spite of the sophistication of the recently proposed methods, most algorithms have not yet attained a desirable level of applicability. All show an outstanding performance when the image model corresponds to the algorithm assumptions but fail in general and create artifacts or remove image fine structures. The main focus of this paper is, first, to define a general mathematical and experimental methodology to compare and classify classical image denoising algorithms and, second, to propose a nonlocal means (NL-means) algorithm addressing the preservation of structure in a digital image. The mathematical analysis is based on the analysis of the ""method noise,"" defined as the difference between a digital image and its denoised version. The NL-means algorithm is proven to be asymptotically optimal under a generic statistical image model. The denoising performance of all consid...",Dareofimdealwianeon,3872.0,58.0,329.0
1314,Denoising,8.0,a non-local algorithm for image denoising,5.0,201.0,1.0,13.0,5.0,3.4,86.70000000000002,18,http://audio.rightmark.org/lukin/msu/NonLocal.pdf,"We propose a new measure, the method noise, to evaluate and compare the performance of digital image denoising methods. We first compute and analyze this method noise for a wide class of denoising algorithms, namely the local smoothing filters. Second, we propose a new algorithm, the nonlocal means (NL-means), based on a nonlocal averaging of all pixels in the image. Finally, we present some experiments comparing the NL-means algorithm and the local smoothing filters.",Danoalfoimde,5525.0,22.0,518.0
1315,Denoising,15.0,deep learning on image denoising: an overview,5.0,201.0,1.0,12.0,5.0,3.4,88.5,19,https://arxiv.org/pdf/1912.13171,"Deep learning techniques have received much attention in the area of image denoising. However, there are substantial differences in the various types of deep learning methods dealing with image denoising. Specifically, discriminative learning based on deep learning can ably address the issue of Gaussian noise. Optimization models based on deep learning are effective in estimating the real noise. However, there has thus far been little related research to summarize the different deep learning techniques for image denoising. In this paper, we offer a comparative study of deep techniques in image denoising. We first classify the deep convolutional neural networks (CNNs) for additive white noisy images; the deep CNNs for real noisy images; the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images, which represents the combination of noisy, blurred and low-resolution images. Then, we analyze the motivations and principles of the different types of deep learning methods. Next, we compare the state-of-the-art methods on public denoising datasets in terms of quantitative and qualitative analyses. Finally, we point out some potential challenges and directions of future research.",Ddeleonimdeanov,120.0,330.0,1.0
1316,Denoising,20.0,image denoising review: from classical to state-of-the-art approaches,5.0,201.0,1.0,19.0,5.0,3.4,92.1,20,http://arxiv.org/pdf/1408.2427v1,"Abstract At the crossing of the statistical and functional analysis, there exists a relentless quest for an efficient image denoising algorithm. In terms of greyscale imaging, a plethora of denoising algorithms have been documented in the literature, in spite of which the level of functionality of these algorithms still holds margin to acquire desired level of applicability. Quite often noise affecting the pixels in image is Gaussian in nature and uniformly deters information pixels in image. Based on some specific set of assumptions all methods work optimally, however they tend to create artefacts and remove fine structural details under general conditions. This article focuses on classifying and comparing some of the significant works in the field of denoising.",Dimderefrcltostap,74.0,276.0,0.0
1317,Denoising,11.0,brief review of image denoising techniques,5.0,201.0,1.0,37.0,5.0,3.4,94.8,21,http://arxiv.org/pdf/1809.03298v1,"With the explosion in the number of digital images taken every day, the demand for more accurate and visually pleasing images is increasing. However, the images captured by modern cameras are inevitably degraded by noise, which leads to deteriorated visual image quality. Therefore, work is required to reduce noise without losing image features (edges, corners, and other sharp structures). So far, researchers have already proposed various methods for decreasing noise. Each method has its own advantages and disadvantages. In this paper, we summarize some important research in the field of image denoising. First, we give the formulation of the image denoising problem, and then we present several image denoising techniques. In addition, we discuss the characteristics of these techniques. Finally, we provide several promising directions for future research.",Dbrreofimdete,77.0,160.0,4.0
1318,Denoising,155.0,when image denoising meets high-level vision tasks: a deep learning approach,3.0,120.0,3.0,67.0,4.0,3.3,114.6,22,https://arxiv.org/pdf/1706.04284,"Conventionally, image denoising and high-level vision tasks are handled separately in computer vision. In this paper, we cope with the two jointly and explore the mutual influence between them. First we propose a convolutional neural network for image denoising which achieves the state-of-the-art performance. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We demonstrate that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network can generate more visually appealing results. To the best of our knowledge, this is the first work investigating the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning.",Dwhimdemehivitaadeleap,130.0,52.0,6.0
1319,Denoising,195.0,denoising adversarial autoencoders,3.0,199.0,3.0,47.0,4.0,3.3,152.20000000000002,23,https://ieeexplore.ieee.org/iel7/5962385/8668600/08438540.pdf,"Unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabeled data to learn useful representations for inference. Autoencoders, a form of generative model, may be trained by learning to reconstruct unlabeled input data from a latent representation space. More robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones. Representations may be further improved by introducing regularization during training to shape the distribution of the encoded data in the latent space. We suggest denoising adversarial autoencoders (AAEs), which combine denoising and regularization, shaping the distribution of latent space using adversarial training. We introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of AAEs. Experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis. Our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance and can synthesize samples that are more consistent with the input data than those trained without a corruption process.",Ddeadau,60.0,41.0,2.0
1320,Denoising,16.0,benchmarking denoising algorithms with real photographs,5.0,201.0,1.0,45.0,4.0,3.1,98.7,24,http://openaccess.thecvf.com/content_cvpr_2017/papers/Plotz_Benchmarking_Denoising_Algorithms_CVPR_2017_paper.pdf,"Lacking realistic ground truth data, image denoising techniques are traditionally evaluated on images corrupted by synthesized i.i.d. Gaussian noise. We aim to obviate this unrealistic setting by developing a methodology for benchmarking denoising techniques on real photographs. We capture pairs of images with different ISO values and appropriately adjusted exposure times, where the nearly noise-free low-ISO image serves as reference. To derive the ground truth, careful post-processing is needed. We correct spatial misalignment, cope with inaccuracies in the exposure parameters through a linear intensity transform based on a novel heteroscedastic Tobit regression model, and remove residual low-frequency bias that stems, e.g., from minor illumination changes. We then capture a novel benchmark dataset, the Darmstadt Noise Dataset (DND), with consumer cameras of differing sensor sizes. One interesting finding is that various recent techniques that perform well on synthetic noise are clearly outperformed by BM3D on photographs with real noise. Our benchmark delineates realistic evaluation scenarios that deviate strongly from those commonly used in the scientific literature.",Dbedealwireph,242.0,39.0,68.0
1321,Denoising,54.0,feature denoising for improving adversarial robustness,4.0,201.0,1.0,8.0,5.0,3.1,99.0,25,https://openaccess.thecvf.com/content_CVPR_2019/papers/Xie_Feature_Denoising_for_Improving_Adversarial_Robustness_CVPR_2019_paper.pdf,"Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ~10%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training.",Dfedefoimadro,443.0,33.0,79.0
1322,Denoising,48.0,ffdnet: toward a fast and flexible solution for cnn-based image denoising,4.0,201.0,1.0,17.0,5.0,3.1,99.9,26,https://arxiv.org/pdf/1710.04026,"Due to the fast inference and good performance, discriminative learning methods have been widely studied in image denoising. However, these methods mostly learn a specific model for each noise level, and require multiple models for denoising images with different noise levels. They also lack flexibility to deal with spatially variant noise, limiting their applications in practical denoising. To address these issues, we present a fast and flexible denoising convolutional neural network, namely FFDNet, with a tunable noise level map as the input. The proposed FFDNet works on downsampled sub-images, achieving a good trade-off between inference speed and denoising performance. In contrast to the existing discriminative denoisers, FFDNet enjoys several desirable properties, including: 1) the ability to handle a wide range of noise levels (i.e., [0, 75]) effectively with a single network; 2) the ability to remove spatially variant noise by specifying a non-uniform noise level map; and 3) faster speed than benchmark BM3D even on CPU without sacrificing denoising performance. Extensive experiments on synthetic and real noisy images are conducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The results show that FFDNet is effective and efficient, making it highly attractive for practical denoising applications.",Dfftoafaanflsofocnimde,749.0,68.0,131.0
1323,Denoising,57.0,extracting and composing robust features with denoising autoencoders,4.0,201.0,1.0,11.0,5.0,3.1,100.8,27,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.569.2442&rep=rep1&type=pdf,"Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",Dexancorofewideau,4978.0,38.0,396.0
1324,Denoising,76.0,image denoising via sparse and redundant representations over learned dictionaries,4.0,201.0,1.0,14.0,5.0,3.1,107.4,28,https://www.egr.msu.edu/~aviyente/elad06.pdf,"We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods",Dimdevispanrereovledi,4840.0,51.0,405.0
1325,Denoising,37.0,image denoising: can plain neural networks compete with bm3d?,5.0,201.0,1.0,75.0,4.0,3.1,114.0,29,https://hcburger.com/files/neuraldenoising.pdf,"Image denoising can be described as the problem of mapping from a noisy image to a noise-free image. The best currently available denoising methods approximate this mapping with cleverly engineered algorithms. In this work we attempt to learn this mapping directly with a plain multi layer perceptron (MLP) applied to image patches. While this has been done before, we will show that by training on large image databases we are able to compete with the current state-of-the-art image denoising methods. Furthermore, our approach is easily adapted to less extensively studied types of noise (by merely exchanging the training data), for which we achieve excellent results as well.",Dimdecaplnenecowibm,953.0,30.0,96.0
1326,Denoising,86.0,a high-quality denoising dataset for smartphone cameras,4.0,201.0,1.0,33.0,5.0,3.1,116.1,30,https://openaccess.thecvf.com/content_cvpr_2018/papers/Abdelhamed_A_High-Quality_Denoising_CVPR_2018_paper.pdf,"The last decade has seen an astronomical shift from imaging with DSLR and point-and-shoot cameras to imaging with smartphone cameras. Due to the small aperture and sensor size, smartphone images have notably more noise than their DSLR counterparts. While denoising for smartphone images is an active research area, the research community currently lacks a denoising image dataset representative of real noisy images from smartphone cameras with high-quality ground truth. We address this issue in this paper with the following contributions. We propose a systematic procedure for estimating ground truth for noisy images that can be used to benchmark denoising performance for smartphone cameras. Using this procedure, we have captured a dataset - the Smartphone Image Denoising Dataset (SIDD) - of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images. We used this dataset to benchmark a number of denoising algorithms. We show that CNN-based methods perform better when trained on our high-quality dataset than when trained using alternative strategies, such as low-ISO images used as a proxy for ground truth data.",Dahidedafosmca,187.0,37.0,67.0
1327,Denoising,26.0,medical image denoising using convolutional denoising autoencoders,5.0,201.0,1.0,97.0,4.0,3.1,117.3,31,https://arxiv.org/pdf/1608.04667,"Image denoising is an important pre-processing step in medical image analysis. Different algorithms have been proposed in past three decades with varying denoising performances. More recently, having outperformed all conventional methods, deep learning based models have shown a great promise. These methods are however limited for requirement of large training sample size and high computational costs. In this paper we show that using small sample size, denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images. Heterogeneous images can be combined to boost sample size for increased denoising performance. Simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye.",Dmeimdeuscodeau,255.0,40.0,5.0
1328,Denoising,401.0,videnn: deep blind video denoising,1.0,85.0,4.0,56.0,4.0,3.1,171.10000000000002,32,http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Claus_ViDeNN_Deep_Blind_Video_Denoising_CVPRW_2019_paper.pdf,"We propose ViDeNN: a CNN for Video Denoising without prior knowledge on the noise distribution (blind denoising). The CNN architecture uses a combination of spatial and temporal filtering, learning to spatially denoise the frames first and at the same time how to combine their temporal information, handling objects motion, brightness changes, low-light conditions and temporal inconsistencies. We demonstrate the importance of the data used for CNNs training, creating for this purpose a specific dataset for low-light conditions. We test ViDeNN on common benchmarks and on self-collected data, achieving good results comparable with the state-of-the-art.",Dvideblvide,34.0,39.0,3.0
1329,Denoising,401.0,connecting image denoising and high-level vision tasks via deep learning,1.0,118.0,3.0,24.0,5.0,3.0,174.7,33,https://arxiv.org/pdf/1809.01826,"Image denoising and high-level vision tasks are usually handled independently in the conventional practice of computer vision, and their connection is fragile. In this paper, we cope with the two jointly and explore the mutual influence between them with the focus on two questions, namely (1) how image denoising can help improving high-level vision tasks, and (2) how the semantic information from high-level vision tasks can be used to guide image denoising. First for image denoising we propose a convolutional neural network in which convolutions are conducted in various spatial resolutions via downsampling and upsampling operations in order to fuse and exploit contextual information on different scales. Second we propose a deep neural network solution that cascades two modules for image denoising and various high-level tasks, respectively, and use the joint loss for updating only the denoising network via back-propagation. We experimentally show that on one hand, the proposed denoiser has the generality to overcome the performance degradation of different high-level vision tasks. On the other hand, with the guidance of high-level vision information, the denoising network produces more visually appealing results. Extensive experiments demonstrate the benefit of exploiting image semantics simultaneously for image denoising and high-level vision tasks via deep learning. The code is available online: https://github.com/Ding-Liu/DeepDenoising",Dcoimdeanhivitavidele,43.0,55.0,1.0
1330,Denoising,67.0,adaptive wavelet thresholding for image denoising and compression,4.0,201.0,1.0,41.0,4.0,2.8,112.8,34,https://infoscience.epfl.ch/record/33854/files/ChangYV00a.pdf,"The first part of this paper proposes an adaptive, data-driven threshold for image denoising via wavelet soft-thresholding. The threshold is derived in a Bayesian framework, and the prior used on the wavelet coefficients is the generalized Gaussian distribution (GGD) widely used in image processing applications. The proposed threshold is simple and closed-form, and it is adaptive to each subband because it depends on data-driven estimates of the parameters. Experimental results show that the proposed method, called BayesShrink, is typically within 5% of the MSE of the best soft-thresholding benchmark with the image assumed known. It also outperforms SureShrink (Donoho and Johnstone 1994, 1995; Donoho 1995) most of the time. The second part of the paper attempts to further validate claims that lossy compression can be used for denoising. The BayesShrink threshold can aid in the parameter selection of a coder designed with the intention of denoising, and thus achieving simultaneous denoising and compression. Specifically, the zero-zone in the quantization step of compression is analogous to the threshold value in the thresholding function. The remaining coder design parameters are chosen based on a criterion derived from Rissanen's minimum description length (MDL) principle. Experiments show that this compression method does indeed remove noise significantly, especially for large noise power. However, it introduces quantization noise and should be used only if bitrate were an additional concern to denoising.",Dadwathfoimdeanco,2836.0,46.0,225.0
1331,Denoising,102.0,denoising diffusion implicit models,3.0,56.0,4.0,201.0,1.0,2.8,113.3,35,https://arxiv.org/pdf/2010.02502,"Filtering real-world color images is challenging due to the complexity of noise that can not be formulated as a certain distribution. However, the rapid development of camera lens pos- es greater demands on image denoising in terms of both efficiency and effectiveness. Currently, the most widely accepted framework employs the combination of transform domain techniques and nonlocal similarity characteristics of natural images. Based on this framework, many competitive methods model the correlation of R, G, B channels with pre-defined or adaptively learned transforms. In this chapter, a brief review of related methods and publicly available datasets is presented, moreover, a new dataset that includes more natural outdoor scenes is introduced. Extensive experiments are performed and discussion on visual effect enhancement is included.",Ddediimmo,47.0,46.0,13.0
1332,Denoising,109.0,image denoising by sparse 3-d transform-domain collaborative filtering,3.0,201.0,1.0,7.0,5.0,2.8,115.2,36,http://web.eecs.utk.edu/~hqi/ece692/references/noise-BM3D-tip07.pdf,"We propose a novel image denoising strategy based on an enhanced sparse representation in transform domain. The enhancement of the sparsity is achieved by grouping similar 2D image fragments (e.g., blocks) into 3D data arrays which we call ""groups."" Collaborative Altering is a special procedure developed to deal with these 3D groups. We realize it using the three successive steps: 3D transformation of a group, shrinkage of the transform spectrum, and inverse 3D transformation. The result is a 3D estimate that consists of the jointly filtered grouped image blocks. By attenuating the noise, the collaborative filtering reveals even the finest details shared by grouped blocks and, at the same time, it preserves the essential unique features of each individual block. The filtered blocks are then returned to their original positions. Because these blocks are overlapping, for each pixel, we obtain many different estimates which need to be combined. Aggregation is a particular averaging procedure which is exploited to take advantage of this redundancy. A significant improvement is obtained by a specially developed collaborative Wiener filtering. An algorithm based on this novel denoising strategy and its efficient implementation are presented in full detail; an extension to color-image denoising is also developed. The experimental results demonstrate that this computationally scalable algorithm achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality.",Dimdebysp3-trcofi,5810.0,30.0,1042.0
1333,Denoising,110.0,weighted nuclear norm minimization with application to image denoising,3.0,201.0,1.0,26.0,5.0,2.8,121.2,37,https://openaccess.thecvf.com/content_cvpr_2014/papers/Gu_Weighted_Nuclear_Norm_2014_CVPR_paper.pdf,"As a convex relaxation of the low rank matrix factorization problem, the nuclear norm minimization has been attracting significant research interest in recent years. The standard nuclear norm minimization regularizes each singular value equally to pursue the convexity of the objective function. However, this greatly restricts its capability and flexibility in dealing with many practical problems (e.g., denoising), where the singular values have clear physical meanings and should be treated differently. In this paper we study the weighted nuclear norm minimization (WNNM) problem, where the singular values are assigned different weights. The solutions of the WNNM problem are analyzed under different weighting conditions. We then apply the proposed WNNM algorithm to image denoising by exploiting the image nonlocal self-similarity. Experimental results clearly show that the proposed WNNM algorithm outperforms many state-of-the-art denoising algorithms such as BM3D in terms of both quantitative measure and visual perception quality.",Dwenunomiwiaptoimde,1163.0,33.0,216.0
1334,Denoising,94.0,enhanced cnn for image denoising,4.0,201.0,1.0,59.0,4.0,2.8,126.3,38,https://onlinelibrary.wiley.com/doi/pdf/10.1049/trit.2018.1054,"Owing to flexible architectures of deep convolutional neural networks (CNNs), CNNs are successfully used for image denoising. However, they suffer from the following drawbacks: (i) deep network architecture is very difficult to train. (ii) Deeper networks face the challenge of performance saturation. In this study, the authors propose a novel method called enhanced convolutional neural denoising network (ECNDNet). Specifically, they use residual learning and batch normalisation techniques to address the problem of training difficulties and accelerate the convergence of the network. In addition, dilated convolutions are used in the proposed network to enlarge the context information and reduce the computational cost. Extensive experiments demonstrate that the ECNDNet outperforms the state-of-the-art methods for image denoising.",Dencnfoimde,41.0,49.0,0.0
1335,Denoising,78.0,a review on ct image noise and its denoising,4.0,201.0,1.0,79.0,4.0,2.8,127.5,39,http://arxiv.org/pdf/2104.02326v1,"Abstract CT imaging is widely used in medical science over the last decades. The process of CT image reconstruction depends on many physical measurements such as radiation dose, software/hardware. Due to statistical uncertainty in all physical measurements in Computed Tomography, the inevitable noise is introduced in CT images. Therefore, edge-preserving denoising methods are required to enhance the quality of CT images. However, there is a tradeoff between noise reduction and the preservation of actual medical relevant contents. Reducing the noise without losing the important features of the image such as edges, corners and other sharp structures, is a challenging task. Nevertheless, various techniques have been presented to suppress the noise from the CT scanned images. Each technique has their own assumptions, merits and limitations. This paper contains a survey of some significant work in the area of CT image denoising. Often, researchers face difficulty to understand the noise in CT images and also to select an appropriate denoising method that is specific to their purpose. Hence, a brief introduction about CT imaging, the characteristics of noise in CT images and the popular methods of CT image denoising are presented here. The merits and drawbacks of CT image denoising methods are also discussed.",Dareonctimnoanitde,97.0,139.0,3.0
1336,Denoising,62.0,deep learning for image denoising: a survey,4.0,201.0,1.0,98.0,4.0,2.8,128.4,40,https://arxiv.org/pdf/1810.05052,"Since the proposal of big data analysis and Graphic Processing Unit (GPU), the deep learning technique has received a great deal of attention and has been widely applied in the field of imaging processing. In this paper, we have an aim to completely review and summarize the deep learning technologies for image denoising in recent years. Moreover, we systematically analyze the conventional machine learning methods for image denoising. Finally, we point out some research directions for the deep learning technologies in image denoising.",Ddelefoimdeasu,47.0,59.0,0.0
1337,Denoising,72.0,image denoising and inpainting with deep neural networks,4.0,201.0,1.0,90.0,4.0,2.8,129.0,41,http://papers.nips.cc/paper/4686-image-denoisingand-inpainting-with-deep-neural-networks.pdf,"We present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (DA). We propose an alternative training scheme that successfully adapts DA, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting. Our method's performance in the image denoising task is comparable to that of KSVD which is a widely used sparse coding technique. More importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before. Specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random. Moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori. Experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting. We also show that our new training scheme for DA is more effective and can improve the performance of unsupervised feature learning.",Dimdeaninwidenene,1092.0,26.0,39.0
1338,Denoising,9.0,global image denoising,5.0,201.0,1.0,156.0,3.0,2.8,129.9,42,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.647.749&rep=rep1&type=pdf,"Most existing state-of-the-art image denoising algorithms are based on exploiting similarity between a relatively modest number of patches. These patch-based methods are strictly dependent on patch matching, and their performance is hamstrung by the ability to reliably find sufficiently similar patches. As the number of patches grows, a point of diminishing returns is reached where the performance improvement due to more patches is offset by the lower likelihood of finding sufficiently close matches. The net effect is that while patch-based methods, such as BM3D, are excellent overall, they are ultimately limited in how well they can do on (larger) images with increasing complexity. In this paper, we address these shortcomings by developing a paradigm for truly global filtering where each pixel is estimated from all pixels in the image. Our objectives in this paper are two-fold. First, we give a statistical analysis of our proposed global filter, based on a spectral decomposition of its corresponding operator, and we study the effect of truncation of this spectral decomposition. Second, we derive an approximation to the spectral (principal) components using the Nyström extension. Using these, we demonstrate that this global filter can be implemented efficiently by sampling a fairly small percentage of the pixels in the image. Experiments illustrate that our strategy can effectively globalize any existing denoising filters to estimate each pixel using all pixels in the image, hence improving upon the best patch-based methods.",Dglimde,178.0,37.0,14.0
1339,Denoising,151.0,a wavenet for speech denoising,3.0,201.0,1.0,38.0,5.0,2.8,137.1,43,https://arxiv.org/pdf/1706.07162,"Most speech processing techniques use magnitude spectrograms as front-end and are therefore by default discarding part of the signal: the phase. In order to overcome this limitation’ we propose an end-to-end learning method for speech denoising based on Wavenet. The proposed model adaptation retains Wavenet's powerful acoustic modeling capabilities, while significantly reducing its time-complexity by eliminating its autoregressive nature. Specifically, the model makes use of non-causal, dilated convolutions and predicts target fields instead of a single target sample. The discriminative adaptation of the model we propose, learns in a supervised fashion via minimizing a regression loss. These modifications make the model highly parallelizable during both training and inference. Both quantitative and qualitative evaluations indicate that the proposed method is preferred over Wiener filtering, a common method based on processing the magnitude spectrogram.",Dawafospde,241.0,41.0,25.0
1340,Denoising,196.0,image denoising using deep cnn with batch renormalization,3.0,201.0,1.0,6.0,5.0,2.8,141.0,44,https://www.researchgate.net/profile/Chunwei-Tian/publication/335646598_Image_denoising_using_deep_CNN_with_batch_renormalization/links/5da9ab954585155e27f65c17/Image-denoising-using-deep-CNN-with-batch-renormalization.pdf,"Deep convolutional neural networks (CNNs) have attracted great attention in the field of image denoising. However, there are two drawbacks: (1) it is very difficult to train a deeper CNN for denoising tasks, and (2) most of deeper CNNs suffer from performance saturation. In this paper, we report the design of a novel network called a batch-renormalization denoising network (BRDNet). Specifically, we combine two networks to increase the width of the network, and thus obtain more features. Because batch renormalization is fused into BRDNet, we can address the internal covariate shift and small mini-batch problems. Residual learning is also adopted in a holistic way to facilitate the network training. Dilated convolutions are exploited to extract more information for denoising tasks. Extensive experimental results show that BRDNet outperforms state-of-the-art image-denoising methods. The code of BRDNet is accessible at http://www.yongxu.org/lunwen.html.",Dimdeusdecnwibare,105.0,74.0,5.0
1341,Denoising,165.0,image blind denoising with generative adversarial network based noise modeling,3.0,201.0,1.0,39.0,5.0,2.8,141.6,45,https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Image_Blind_Denoising_CVPR_2018_paper.pdf,"In this paper, we consider a typical image blind denoising problem, which is to remove unknown noise from noisy images. As we all know, discriminative learning based methods, such as DnCNN, can achieve state-of-the-art denoising results, but they are not applicable to this problem due to the lack of paired training data. To tackle the barrier, we propose a novel two-step framework. First, a Generative Adversarial Network (GAN) is trained to estimate the noise distribution over the input noisy images and to generate noise samples. Second, the noise patches sampled from the first step are utilized to construct a paired training dataset, which is used, in turn, to train a deep Convolutional Neural Network (CNN) for denoising. Extensive experiments have been done to demonstrate the superiority of our approach in image blind denoising.",Dimbldewigeadnebanomo,237.0,38.0,13.0
1342,Denoising,14.0,an analysis and implementation of the bm3d image denoising method,5.0,201.0,1.0,198.0,3.0,2.8,144.0,46,http://www.ipol.im/pub/art/2012/l-bm3d/article_lr.pdf,BM3D is a recent denoising method based on the fact that an image has a locally sparse representation in transform domain. This sparsity is enhanced by grouping similar 2D image patches into 3D groups. In this paper we propose an open-source implementation of the method. We discuss the choice of all parameter methods and confirm their actual optimality. The description of the method is rewritten with a new notation. We hope this new notation is more transparent than in the original paper. A final index gives nonetheless the correspondence between the new notation and the original notation.,Danananimofthbmimdeme,270.0,19.0,22.0
1343,Denoising,30.0,the curvelet transform for image denoising,5.0,201.0,1.0,195.0,3.0,2.8,147.9,47,https://statweb.stanford.edu/~candes/publications/downloads/CurveDenoise.pdf,"We describe approximate digital implementations of two new mathematical transforms, namely, the ridgelet transform and the curvelet transform. Our implementations offer exact reconstruction, stability against perturbations, ease of implementation, and low computational complexity. A central tool is Fourier-domain computation of an approximate digital Radon transform. We introduce a very simple interpolation in the Fourier space which takes Cartesian samples and yields samples on a rectopolar grid, which is a pseudo-polar sampling set based on a concentric squares geometry. Despite the crudeness of our interpolation, the visual performance is surprisingly good. Our ridgelet transform applies to the Radon transform a special overcomplete wavelet pyramid whose wavelets have compact support in the frequency domain. Our curvelet transform uses our ridgelet transform as a component step, and implements curvelet subbands using a filter bank of a; trous wavelet filters. Our philosophy throughout is that transforms should be overcomplete, rather than critically sampled. We apply these digital transforms to the denoising of some standard images embedded in white noise. In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with ""state of the art"" techniques based on wavelets, including thresholding of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods. Moreover, the curvelet reconstructions exhibit higher perceptual quality than wavelet-based reconstructions, offering visually sharper images and, in particular, higher quality recovery of edges and of faint linear and curvilinear features. Existing theory for curvelet and ridgelet transforms suggests that these new approaches can outperform wavelet methods in certain image reconstruction problems. The empirical results reported here are in encouraging agreement.",Dthcutrfoimde,1610.0,46.0,58.0
1344,Denoising,70.0,self-guided network for fast image denoising,4.0,183.0,3.0,201.0,1.0,2.7,154.5,48,http://openaccess.thecvf.com/content_ICCV_2019/papers/Gu_Self-Guided_Network_for_Fast_Image_Denoising_ICCV_2019_paper.pdf,"Recently, Self-supervised learning methods able to perform image denoising without ground truth labels have been proposed. These methods create low-quality images by adding random or Gaussian noise to images and then train a model for denoising. Ideally, it would be beneficial if one can generate high-quality CT images with only a few training samples via self-supervision. However, the performance of CT denoising is generally limited due to the complexity of CT noise. To address this problem, we propose a novel self-supervised learning-based CT denoising method. In particular, we train pre-train CT denoising and noise models that can predict CT noise from Low-dose CT (LDCT) using available LDCT and Normal-dose CT (NDCT) pairs. For a given test LDCT, we generate Pseudo-LDCT and NDCT pairs using the pre-trained denoising and noise models and then update the parameters of the denoising model using these pairs to remove noise in the test LDCT. To make realistic Pseudo LDCT, we train multiple noise models from individual images and generate the noise using the ensemble of noise models. We evaluate our method on the 2016 AAPM Low-Dose CT Grand Challenge dataset. The proposed ensemble noise model can generate realistic CT noise, and thus our method significantly improves the denoising performance existing denoising models trained by supervised- and self-supervised learning.",Dsenefofaimde,54.0,49.0,12.0
1345,Denoising,401.0,a poisson-gaussian denoising dataset with real fluorescence microscopy images,1.0,154.0,3.0,42.0,4.0,2.7,194.5,49,http://arxiv.org/pdf/1812.10366v2,"Fluorescence microscopy has enabled a dramatic development in modern biology. Due to its inherently weak signal, fluorescence microscopy is not only much noisier than photography, but also presented with Poisson-Gaussian noise where Poisson noise, or shot noise, is the dominating noise source. To get clean fluorescence microscopy images, it is highly desirable to have effective denoising algorithms and datasets that are specifically designed to denoise fluorescence microscopy images. While such algorithms exist, no such datasets are available. In this paper, we fill this gap by constructing a dataset - the Fluorescence Microscopy Denoising (FMD) dataset - that is dedicated to Poisson-Gaussian denoising. The dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. We use image averaging to effectively obtain ground truth images and 60,000 noisy images with different noise levels. We use this dataset to benchmark 10 representative denoising algorithms and find that deep learning methods have the best performance. To our knowledge, this is the first real microscopy image dataset for Poisson-Gaussian denoising purposes and it could be an important tool for high-quality, real-time denoising applications in biomedical research.",Dapodedawireflmiim,56.0,37.0,15.0
1346,Denoising,401.0,energy-based processes for exchangeable data,1.0,3.0,5.0,201.0,1.0,2.6,181.8,50,http://arxiv.org/pdf/1512.01769v1,Purpose. To obtain the interference immunity of the data exchange by spread spectrum signals with variable entropy of the telemetric information data exchange with autonomous mobile robots.   Methodology. The results have been obtained by the theoretical investigations and have been confirmed by the modeling experiments.   Findings. The interference immunity in form of dependence of bit error probability on normalized signal/noise ratio of the data exchange by spread spectrum signals with variable entropy has been obtained.It has been proved that the interference immunity factor (needed normalized signal/noise ratio) is at least 2 dB better under condition of equal time complexity as compared with correlation processing methods of orthogonal signals.   Originality. For the first time the interference immunity in form of dependence of bit error probability on normalized signal/noise ratio of the data exchange by spread spectrum signals with variable entropy has been obtained.   Practical value. The obtained results prove the feasibility of using variable entropy spread spectrum signals data exchange method in the distributed telemetric information processing systems in specific circumstances.,Denprfoexda,4.0,76.0,0.0
1347,Denoising,401.0,large-scale graph representation learning with very deep gnns and self-supervision,1.0,6.0,5.0,201.0,1.0,2.6,183.0,51,http://arxiv.org/pdf/2106.05831v1,"Algorithm audits have increased in recent years due to a growing need to independently assess the performance of automatically curated services that process, filter and rank the large and dynamic amount of information available on the internet. Among several methodologies to perform such audits, virtual agents stand out because they offer the possibility of performing systematic experiments simulating human behaviour without the associated costs of recruiting participants. Motivated by the importance of research transparency and replicability of results, this paper focuses on the challenges of such an approach, and it provides methodological details, recommendations, lessons learned and limitations that researchers should take into consideration when setting up experiments with virtual agents. We demonstrate the successful performance of our research infrastructure in multiple data collections with diverse experimental designs, and point to different changes and strategies that improved the quality of the method. We conclude that virtual agents are a promising venue for monitoring the performance of algorithms during longer periods of time, and we hope that this paper serves as a base to widen the research in this direction.",Dlagrrelewivedegnanse,2.0,65.0,1.0
1348,Denoising,401.0,gaussian gated linear networks,1.0,7.0,5.0,201.0,1.0,2.6,183.4,52,http://arxiv.org/pdf/2006.05964v2,"We propose the Gaussian Gated Linear Network (G-GLN), an extension to the recently proposed GLN family of deep neural networks. Instead of using backpropagation to learn features, GLNs have a distributed and local credit assignment mechanism based on optimizing a convex objective. This gives rise to many desirable properties including universality, data-efficient online learning, trivial interpretability and robustness to catastrophic forgetting. We extend the GLN framework from classification to multiple regression and density modelling by generalizing geometric mixing to a product of Gaussian densities. The G-GLN achieves competitive or state-of-the-art performance on several univariate and multivariate regression benchmarks, and we demonstrate its applicability to practical tasks including online contextual bandits and density estimation via denoising.",Dgagaline,3.0,58.0,0.0
1349,Denoising,401.0,deep image prior,1.0,8.0,5.0,201.0,1.0,2.6,183.8,53,http://arxiv.org/pdf/1712.05016v2,"The recent literature on deep learning offers new tools to learn a rich probability distribution over high dimensional data such as images or sounds. In this work we investigate the possibility of learning the prior distribution over neural network parameters using such tools. Our resulting variational Bayes algorithm generalizes well to new tasks, even when very few training examples are provided. Furthermore, this learned prior allows the model to extrapolate correctly far from a given task's training data on a meta-dataset of periodic signals.",Ddeimpr,1080.0,34.0,215.0
1350,Denoising,401.0,learning to see in the dark,1.0,11.0,5.0,201.0,1.0,2.6,185.0,54,http://arxiv.org/pdf/2005.05353v1,We investigate the possibility of applying machine learning techniques to images of strongly lensed galaxies to detect a low mass cut-off in the spectrum of dark matter sub-halos within the lens system. We generate lensed images of systems containing substructure in seven different categories corresponding to lower mass cut-offs ranging from $10^9M_\odot$ down to $10^6M_\odot$. We use convolutional neural networks to perform a multi-classification sorting of these images and see that the algorithm is able to correctly identify the lower mass cut-off within an order of magnitude to better than 93% accuracy.,Dletoseinthda,422.0,44.0,76.0
1351,Denoising,401.0,handling background noise in neural speech generation,1.0,12.0,5.0,201.0,1.0,2.6,185.4,55,http://arxiv.org/pdf/2102.11906v1,"Recent advances in neural-network based generative modeling of speech has shown great potential for speech coding. However, the performance of such models drops when the input is not clean speech, e.g., in the presence of background noise, preventing its use in practical applications. In this paper we examine the reason and discuss methods to overcome this issue. Placing a denoising preprocessing stage when extracting features and target clean speech during training is shown to be the best performing strategy.",Dhabanoinnespge,0.0,26.0,0.0
1352,Denoising,401.0,liquid warping gan with attention: a unified framework for human image synthesis,1.0,13.0,5.0,201.0,1.0,2.6,185.8,56,http://arxiv.org/pdf/2011.09055v2,"We tackle human image synthesis, including human motion imitation, appearance transfer, and novel view synthesis, within a unified framework. It means that the model, once being trained, can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints to estimate the human body structure. However, they only express the position information with no abilities to characterize the personalized shape of the person and model the limb rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape. It can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block (AttLWB) that propagates the source information in both image and feature spaces to the synthesized reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method can support a more flexible warping from multiple sources. To further improve the generalization ability of the unseen source images, a one/few-shot adversarial learning is applied. In detail, it firstly trains a model in an extensive training set. Then, it finetunes the model by one/few-shot unseen image(s) in a self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024) results. Also, we build a new dataset, namely iPER dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our methods in terms of preserving face identity, shape consistency, and clothes details. All codes and dataset are available on https://impersonator.org/work/impersonator-plus-plus.html.",Dliwagawiataunfrfohuimsy,1.0,69.0,0.0
1353,Denoising,401.0,"liquid warping gan: a unified framework for human motion imitation, appearance transfer and novel view synthesis",1.0,14.0,5.0,201.0,1.0,2.6,186.2,57,http://arxiv.org/pdf/1909.12224v3,"We tackle the human motion imitation, appearance transfer, and novel view synthesis within a unified framework, which means that the model once being trained can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only expresses the position information with no abilities to characterize the personalized shape of the individual person and model the limbs rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape, which can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose a Liquid Warping GAN with Liquid Warping Block (LWB) that propagates the source information in both image and feature spaces, and synthesizes an image with respect to the reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method is able to support a more flexible warping from multiple sources. In addition, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our method in several aspects, such as robustness in occlusion case and preserving face identity, shape consistency and clothes details. All codes and datasets are available on https://svip-lab.github.io/project/impersonator.html",Dliwagaaunfrfohumoimaptrannovisy,92.0,54.0,21.0
1354,Denoising,401.0,real-time single-channel dereverberation and separation with time-domainaudio separation network,1.0,15.0,5.0,201.0,1.0,2.6,186.6,58,http://arxiv.org/pdf/2105.09188v1,"Existing image-to-image translation (I2IT) methods are either constrained to low-resolution images or long inference time due to their heavy computational burden on the convolution of high-resolution feature maps. In this paper, we focus on speeding-up the high-resolution photorealistic I2IT tasks based on closed-form Laplacian pyramid decomposition and reconstruction. Specifically, we reveal that the attribute transformations, such as illumination and color manipulation, relate more to the low-frequency component, while the content details can be adaptively refined on high-frequency components. We consequently propose a Laplacian Pyramid Translation Network (LPTN) to simultaneously perform these two tasks, where we design a lightweight network for translating the low-frequency component with reduced resolution and a progressive masking strategy to efficiently refine the high-frequency ones. Our model avoids most of the heavy computation consumed by processing high-resolution feature maps and faithfully preserves the image details. Extensive experimental results on various tasks demonstrate that the proposed method can translate 4K images in real-time using one normal GPU while achieving comparable transformation performance against existing methods. Datasets and codes are available: https://github.com/csjliang/LPTN.",Dresideansewitisene,30.0,23.0,2.0
1355,Denoising,401.0,exploring contextual word-level style relevance for unsupervised style transfer,1.0,18.0,5.0,201.0,1.0,2.6,187.8,59,http://arxiv.org/pdf/2005.02049v1,"Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style,they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.",Dexcowostrefounsttr,13.0,27.0,2.0
1356,Denoising,401.0,when awgn-based denoiser meets real noises,1.0,22.0,5.0,201.0,1.0,2.6,189.4,60,http://arxiv.org/pdf/1904.03485v2,"Discriminative learning-based image denoisers have achieved promising performance on synthetic noises such as Additive White Gaussian Noise (AWGN). The synthetic noises adopted in most previous work are pixel-independent, but real noises are mostly spatially/channel-correlated and spatially/channel-variant. This domain gap yields unsatisfied performance on images with real noises if the model is only trained with AWGN. In this paper, we propose a novel approach to boost the performance of a real image denoiser which is trained only with synthetic pixel-independent noise data dominated by AWGN. First, we train a deep model that consists of a noise estimator and a denoiser with mixed AWGN and Random Value Impulse Noise (RVIN). We then investigate Pixel-shuffle Down-sampling (PD) strategy to adapt the trained model to real noises. Extensive experiments demonstrate the effectiveness and generalization of the proposed approach. Notably, our method achieves state-of-the-art performance on real sRGB images in the DND benchmark among models trained with synthetic noises. Codes are available at https://github.com/yzhouas/PD-Denoising-pytorch.",Dwhawdemereno,25.0,72.0,1.0
1357,Denoising,401.0,image restoration using convolutional auto-encoders with symmetric skip connections,1.0,24.0,5.0,201.0,1.0,2.6,190.2,61,http://arxiv.org/pdf/1606.08921v3,"Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results.",Dimreuscoauwisyskco,250.0,66.0,23.0
1358,Denoising,401.0,multi-stage progressive image restoration,1.0,26.0,5.0,201.0,1.0,2.6,191.0,62,http://arxiv.org/pdf/2101.11852v1,"The speckle phenomenon remains a major hurdle for the analysis of SAR images. The development of speckle reduction methods closely follows methodological progress in the field of image restoration. The advent of deep neural networks has offered new ways to tackle this longstanding problem. Deep learning for speckle reduction is a very active research topic and already shows restoration performances that exceed that of the previous generations of methods based on the concepts of patches, sparsity, wavelet transform or total variation minimization. The objective of this paper is to give an overview of the most recent works and point the main research directions and current challenges of deep learning for SAR image restoration.",Dmuprimre,34.0,111.0,8.0
1359,Denoising,401.0,domain generalization for object recognition with multi-task autoencoders,1.0,27.0,5.0,201.0,1.0,2.6,191.4,63,http://arxiv.org/pdf/1508.07680v1,"The problem of domain generalization is to take knowledge acquired from a number of related domains where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition.   Our algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier.   We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.",Ddogefoobrewimuau,350.0,50.0,51.0
1360,Denoising,401.0,learning deep cnn denoiser prior for image restoration,1.0,29.0,5.0,201.0,1.0,2.6,192.2,64,http://arxiv.org/pdf/1704.03264v1,"Model-based optimization methods and discriminative learning methods have been the two dominant strategies for solving various inverse problems in low-level vision. Typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible for handling different inverse problems but are usually time-consuming with sophisticated priors for the purpose of good performance; in the meanwhile, discriminative learning methods have fast testing speed but their application range is greatly restricted by the specialized task. Recent works have revealed that, with the aid of variable splitting techniques, denoiser prior can be plugged in as a modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). Such an integration induces considerable advantage when the denoiser is obtained via discriminative learning. However, the study of integration with fast discriminative denoiser prior is still lacking. To this end, this paper aims to train a set of fast and effective CNN (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. Experimental results demonstrate that the learned set of denoisers not only achieve promising Gaussian denoising results but also can be used as prior to deliver good performance for various low-level vision applications.",Dledecndeprfoimre,914.0,79.0,154.0
1361,Denoising,401.0,interspeech 2021 deep noise suppression challenge,1.0,30.0,5.0,201.0,1.0,2.6,192.6,65,http://arxiv.org/pdf/2101.01902v3,"The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality. We recently organized a DNS challenge special session at INTERSPEECH and ICASSP 2020. We open-sourced training and test datasets for the wideband scenario. We also open-sourced a subjective evaluation framework based on ITU-T standard P.808, which was also used to evaluate participants of the challenge. Many researchers from academia and industry made significant contributions to push the field forward, yet even the best noise suppressor was far from achieving superior speech quality in challenging scenarios. In this version of the challenge organized at INTERSPEECH 2021, we are expanding both our training and test datasets to accommodate full band scenarios. The two tracks in this challenge will focus on real-time denoising for (i) wide band, and(ii) full band scenarios. We are also making available a reliable non-intrusive objective speech quality metric called DNSMOS for the participants to use during their development phase.",Din20denosuch,16.0,26.0,1.0
1362,Denoising,401.0,residual dense network for image restoration,1.0,31.0,5.0,201.0,1.0,2.6,193.0,66,http://arxiv.org/pdf/1906.12021v2,"Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.",Dredenefoimre,195.0,84.0,35.0
1363,Denoising,401.0,deep retinex decomposition for low-light enhancement,1.0,33.0,5.0,201.0,1.0,2.6,193.8,67,http://arxiv.org/pdf/1808.04560v1,"Retinex model is an effective tool for low-light image enhancement. It assumes that observed images can be decomposed into the reflectance and illumination. Most existing Retinex-based methods have carefully designed hand-crafted constraints and parameters for this highly ill-posed decomposition, which may be limited by model capacity when applied in various scenes. In this paper, we collect a LOw-Light dataset (LOL) containing low/normal-light image pairs and propose a deep Retinex-Net learned on this dataset, including a Decom-Net for decomposition and an Enhance-Net for illumination adjustment. In the training process for Decom-Net, there is no ground truth of decomposed reflectance and illumination. The network is learned with only key constraints including the consistent reflectance shared by paired low/normal-light images, and the smoothness of illumination. Based on the decomposition, subsequent lightness enhancement is conducted on illumination by an enhancement network called Enhance-Net, and for joint denoising there is a denoising operation on reflectance. The Retinex-Net is end-to-end trainable, so that the learned decomposition is by nature good for lightness adjustment. Extensive experiments demonstrate that our method not only achieves visually pleasing quality for low-light enhancement but also provides a good representation of image decomposition.",Dderedefoloen,266.0,30.0,98.0
1364,Denoising,401.0,learning enriched features for real image restoration and enhancement,1.0,34.0,5.0,201.0,1.0,2.6,194.2,68,http://arxiv.org/pdf/2003.06792v2,"With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography, medical imaging, and remote sensing. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatially precise but contextually less robust results are achieved, while in the latter case, semantically reliable but spatially less accurate outputs are generated. In this paper, we present a novel architecture with the collective goals of maintaining spatially-precise high-resolution representations through the entire network and receiving strong contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing several key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) spatial and channel attention mechanisms for capturing contextual information, and (d) attention based multi-scale feature aggregation. In a nutshell, our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on five real image benchmark datasets demonstrate that our method, named as MIRNet, achieves state-of-the-art results for a variety of image processing tasks, including image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNet.",Dleenfeforeimreanen,45.0,138.0,12.0
1365,Denoising,401.0,video enhancement with task-oriented flow,1.0,36.0,5.0,201.0,1.0,2.6,195.0,69,http://arxiv.org/pdf/2009.04642v1,"With the prosperity of digital video industry, video frame interpolation has arisen continuous attention in computer vision community and become a new upsurge in industry. Many learning-based methods have been proposed and achieved progressive results. Among them, a recent algorithm named quadratic video interpolation (QVI) achieves appealing performance. It exploits higher-order motion information (e.g. acceleration) and successfully models the estimation of interpolated flow. However, its produced intermediate frames still contain some unsatisfactory ghosting, artifacts and inaccurate motion, especially when large and complex motion occurs. In this work, we further improve the performance of QVI from three facets and propose an enhanced quadratic video interpolation (EQVI) model. In particular, we adopt a rectified quadratic flow prediction (RQFP) formulation with least squares method to estimate the motion more accurately. Complementary with image pixel-level blending, we introduce a residual contextual synthesis network (RCSN) to employ contextual information in high-dimensional feature space, which could help the model handle more complicated scenes and motion patterns. Moreover, to further boost the performance, we devise a novel multi-scale fusion network (MS-Fusion) which can be regarded as a learnable augmentation process. The proposed EQVI model won the first place in the AIM2020 Video Temporal Super-Resolution Challenge.",Dvienwitafl,280.0,78.0,76.0
1366,Denoising,401.0,index network,1.0,37.0,5.0,201.0,1.0,2.6,195.4,70,http://arxiv.org/pdf/1908.09895v2,"We show that existing upsampling operators can be unified using the notion of the index function. This notion is inspired by an observation in the decoding process of deep image matting where indices-guided unpooling can often recover boundary details considerably better than other upsampling operators such as bilinear interpolation. By viewing the indices as a function of the feature map, we introduce the concept of ""learning to index"", and present a novel index-guided encoder-decoder framework where indices are self-learned adaptively from data and are used to guide the downsampling and upsampling stages, without extra training supervision. At the core of this framework is a new learnable module, termed Index Network (IndexNet), which dynamically generates indices conditioned on the feature map itself. IndexNet can be used as a plug-in applying to almost all off-the-shelf convolutional networks that have coupled downsampling and upsampling stages, giving the networks the ability to dynamically capture variations of local patterns. In particular, we instantiate and investigate five families of IndexNet and demonstrate their effectiveness on four dense prediction tasks, including image denoising, image matting, semantic segmentation, and monocular depth estimation. Code and models have been made available at: https://tinyurl.com/IndexNetV1",Dinne,1.0,30.0,0.0
1367,Denoising,401.0,rethinking data augmentation for image super-resolution: a comprehensive analysis and a new strategy,1.0,39.0,5.0,201.0,1.0,2.6,196.2,71,http://arxiv.org/pdf/2101.09060v2,"Despite being very powerful in standard learning settings, deep learning models can be extremely brittle when deployed in scenarios different from those on which they were trained. Domain generalization methods investigate this problem and data augmentation strategies have shown to be helpful tools to increase data variability, supporting model robustness across domains. In our work we focus on style transfer data augmentation and we present how it can be implemented with a simple and inexpensive strategy to improve generalization. Moreover, we analyze the behavior of current state of the art domain generalization methods when integrated with this augmentation solution: our thorough experimental evaluation shows that their original effect almost always disappears with respect to the augmented baseline. This issue open new scenarios for domain generalization research, highlighting the need of novel methods properly able to take advantage of the introduced data variability.",Dredaaufoimsuacoanananest,32.0,45.0,5.0
1369,Denoising,44.0,a survey on the magnetic resonance image denoising methods,4.0,201.0,1.0,153.0,3.0,2.5,139.5,72,https://www.researchgate.net/file.PostFileLoader.html?id=564f7513614325c3a58b4592&assetKey=AS%3A297948605894664%401448047891441,"Abstract Over the past several years, although the resolution, signal-to-noise ratio and acquisition speed of magnetic resonance imaging (MRI) technology have been increased, MR images are still affected by artifacts and noise. A tradeoff between noise reduction and the preservation of actual detail features has to be made in the way that enhances the diagnostically relevant image content. Therefore, noise reduction is still a difficult task. A variety of techniques have been presented in the literature on denoising MR images and each technique has its own assumptions, advantages and limitations. The purpose of this paper is to present a survey of the published literature in dealing with denoising methods in MR images. After a brief introduction about magnetic resonance imaging and the characteristics of noise in MRI, the popular approaches are classified into different groups and an overview of various methods is provided. The denoising method's advantages and limitations are also discussed.",Dasuonthmareimdeme,185.0,104.0,9.0
1370,Denoising,121.0,deep burst denoising,3.0,201.0,1.0,92.0,4.0,2.5,144.3,73,http://openaccess.thecvf.com/content_ECCV_2018/papers/Clement_Godard_Deep_Burst_Denoising_ECCV_2018_paper.pdf,"Noise is an inherent issue of low-light image capture, one which is exacerbated on mobile devices due to their narrow apertures and small sensors. One strategy for mitigating noise in a low-light situation is to increase the shutter time of the camera, thus allowing each photosite to integrate more light and decrease noise variance. However, there are two downsides of long exposures: (a) bright regions can exceed the sensor range, and (b) camera and scene motion will result in blurred images. Another way of gathering more light is to capture multiple short (thus noisy) frames in a ""burst"" and intelligently integrate the content, thus avoiding the above downsides. In this paper, we use the burst-capture strategy and implement the intelligent integration via a recurrent fully convolutional deep neural net (CNN). We build our novel, multiframe architecture to be a simple addition to any single frame denoising model, and design to handle an arbitrary number of noisy input frames. We show that it achieves state of the art denoising results on our burst dataset, improving on the best published multi-frame techniques, such as VBM4D and FlexISP. Finally, we explore other applications of image enhancement by integrating content from multiple frames and demonstrate that our DNN architecture generalizes well to image super-resolution.",Ddebude,61.0,54.0,4.0
1371,Denoising,138.0,non-local color image denoising with convolutional neural networks,3.0,201.0,1.0,83.0,4.0,2.5,146.70000000000002,74,https://openaccess.thecvf.com/content_cvpr_2017/papers/Lefkimmiatis_Non-Local_Color_Image_CVPR_2017_paper.pdf,"We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.",Dnocoimdewiconene,215.0,47.0,21.0
1372,Denoising,160.0,blind denoising autoencoder,3.0,201.0,1.0,69.0,4.0,2.5,149.1,75,https://arxiv.org/pdf/1912.07358,"The term “blind denoising” refers to the fact that the basis used for denoising is learned from the noisy sample itself during denoising. Dictionary learning- and transform learning-based formulations for blind denoising are well known. But there has been no autoencoder-based solution for the said blind denoising approach. So far, autoencoder-based denoising formulations have learned the model on a separate training data and have used the learned model to denoise test samples. Such a methodology fails when the test image (to denoise) is not of the same kind as the models learned with. This will be the first work, where we learn the autoencoder from the noisy sample while denoising. Experimental results show that our proposed method performs better than dictionary learning (K-singular value decomposition), transform learning, sparse stacked denoising autoencoder, and the gold standard BM3D algorithm.",Dbldeau,29.0,36.0,0.0
1373,Denoising,186.0,collaborative denoising auto-encoders for top-n recommender systems,3.0,201.0,1.0,44.0,4.0,2.5,149.39999999999998,76,https://www.datascienceassn.org/sites/default/files/Collaborative%20Denoising%20Auto-Encoders%20for%20Top-N%20Recommender%20Systems.pdf,"Most real-world recommender services measure their performance based on the top-N results shown to the end users. Thus, advances in top-N recommendation have far-ranging consequences in practical applications. In this paper, we present a novel method, called Collaborative Denoising Auto-Encoder (CDAE), for top-N recommendation that utilizes the idea of Denoising Auto-Encoders. We demonstrate that the proposed model is a generalization of several well-known collaborative filtering models but with more flexible components. Thorough experiments are conducted to understand the performance of CDAE under various component settings. Furthermore, experimental results on several public datasets demonstrate that CDAE consistently outperforms state-of-the-art top-N recommendation methods on a variety of common evaluation metrics.",Dcodeaufotoresy,610.0,32.0,73.0
1374,Denoising,157.0,deep joint demosaicking and denoising,3.0,201.0,1.0,80.0,4.0,2.5,151.5,77,http://arxiv.org/pdf/1803.05215v4,"Demosaicking and denoising are the key first stages of the digital imaging pipeline but they are also a severely ill-posed problem that infers three color values per pixel from a single noisy measurement. Earlier methods rely on hand-crafted filters or priors and still exhibit disturbing visual artifacts in hard cases such as moiré or thin edges. We introduce a new data-driven approach for these challenges: we train a deep neural network on a large corpus of images instead of using hand-tuned filters. While deep learning has shown great success, its naive application using existing training datasets does not give satisfactory results for our problem because these datasets lack hard cases. To create a better training set, we present metrics to identify difficult patches and techniques for mining community photographs for such patches. Our experiments show that this network and training procedure outperform state-of-the-art both on noisy and noise-free data. Furthermore, our algorithm is an order of magnitude faster than the previous best performing techniques.",Ddejodeande,273.0,81.0,47.0
1375,Denoising,175.0,denoising the denoisers: an independent evaluation of microbiome sequence error-correction approaches,3.0,201.0,1.0,66.0,4.0,2.5,152.70000000000002,78,http://arxiv.org/pdf/1908.00111v2,"High-depth sequencing of universal marker genes such as the 16S rRNA gene is a common strategy to profile microbial communities. Traditionally, sequence reads are clustered into operational taxonomic units (OTUs) at a defined identity threshold to avoid sequencing errors generating spurious taxonomic units. However, there have been numerous bioinformatic packages recently released that attempt to correct sequencing errors to determine real biological sequences at single nucleotide resolution by generating amplicon sequence variants (ASVs). As more researchers begin to use high resolution ASVs, there is a need for an in-depth and unbiased comparison of these novel “denoising” pipelines. In this study, we conduct a thorough comparison of three of the most widely-used denoising packages (DADA2, UNOISE3, and Deblur) as well as an open-reference 97% OTU clustering pipeline on mock, soil, and host-associated communities. We found from the mock community analyses that although they produced similar microbial compositions based on relative abundance, the approaches identified vastly different numbers of ASVs that significantly impact alpha diversity metrics. Our analysis on real datasets using recommended settings for each denoising pipeline also showed that the three packages were consistent in their per-sample compositions, resulting in only minor differences based on weighted UniFrac and Bray–Curtis dissimilarity. DADA2 tended to find more ASVs than the other two denoising pipelines when analyzing both the real soil data and two other host-associated datasets, suggesting that it could be better at finding rare organisms, but at the expense of possible false positives. The open-reference OTU clustering approach identified considerably more OTUs in comparison to the number of ASVs from the denoising pipelines in all datasets tested. The three denoising approaches were significantly different in their run times, with UNOISE3 running greater than 1,200 and 15 times faster than DADA2 and Deblur, respectively. Our findings indicate that, although all pipelines result in similar general community structure, the number of ASVs/OTUs and resulting alpha-diversity metrics varies considerably and should be considered when attempting to identify rare organisms from possible background noise.",Ddethdeaninevofmiseerap,137.0,45.0,7.0
1376,Denoising,164.0,marginalized denoising autoencoders for domain adaptation,3.0,201.0,1.0,81.0,4.0,2.5,153.9,79,https://arxiv.org/pdf/1206.4683,"Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters--in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB™, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.",Dmadeaufodoad,674.0,33.0,129.0
1377,Denoising,183.0,denoising of diffusion mri using random matrix theory,3.0,201.0,1.0,62.0,4.0,2.5,153.9,80,http://arxiv.org/pdf/1004.1356v1,"We introduce and evaluate a post-processing technique for fast denoising of diffusion-weighted MR images. By exploiting the intrinsic redundancy in diffusion MRI using universal properties of the eigenspectrum of random covariance matrices, we remove noise-only principal components, thereby enabling signal-to-noise ratio enhancements. This yields parameter maps of improved quality for visual, quantitative, and statistical interpretation. By studying statistics of residuals, we demonstrate that the technique suppresses local signal fluctuations that solely originate from thermal noise rather than from other sources such as anatomical detail. Furthermore, we achieve improved precision in the estimation of diffusion parameters and fiber orientations in the human brain without compromising the accuracy and spatial resolution.",Ddeofdimrusramath,594.0,72.0,18.0
1378,Denoising,74.0,boosting of image denoising algorithms,4.0,201.0,1.0,192.0,3.0,2.5,160.2,81,https://arxiv.org/pdf/1502.06220,"In this paper we propose a generic recursive algorithm for improving image denoising methods. Given the initial denoised image, we suggest repeating the following “SOS” procedure: (i) Strengthen the signal by adding the previous denoised image to the degraded input image, (ii) Operate the denoising method on the strengthened image, and (iii) Subtract the previous denoised image from the restored signal-strengthened outcome. The convergence of this process is studied for the K-SVD image denoising and related algorithms. Still in the context of K-SVD image denoising, we introduce an interesting interpretation of the SOS algorithm as a technique for closing the gap between the local patch-modeling and the global restoration task, thereby leading to improved performance. In a quest for the theoretical origin of the SOS algorithm, we provide a graph-based interpretation of our method, where the SOS recursive update effectively minimizes a penalty function that aims to denoise the image, while being regularized...",Dboofimdeal,81.0,57.0,9.0
1379,Denoising,88.0,patch-based near-optimal image denoising,4.0,201.0,1.0,178.0,3.0,2.5,160.20000000000002,82,http://www.kresttechnology.com/krest-academic-projects/krest-mtech-projects/ECE/dspmt/%5B28%5D.pdf,"In this paper, we propose a denoising method motivated by our previous analysis of the performance bounds for image denoising. Insights from that study are used here to derive a high-performance practical denoising algorithm. We propose a patch-based Wiener filter that exploits patch redundancy for image denoising. Our framework uses both geometrically and photometrically similar patches to estimate the different filter parameters. We describe how these parameters can be accurately estimated directly from the input noisy image. Our denoising approach, designed for near-optimal performance (in the mean-squared error sense), has a sound statistical foundation that is analyzed in detail. The performance of our approach is experimentally verified on a variety of images and noise levels. The results presented here demonstrate that our proposed method is on par or exceeding the current state of the art, both visually and quantitatively.",Dpaneimde,302.0,58.0,30.0
1380,Denoising,96.0,adaptive image denoising by targeted databases,4.0,201.0,1.0,171.0,3.0,2.5,160.5,83,https://arxiv.org/pdf/1407.5055,"We propose a data-dependent denoising procedure to restore noisy images. Different from existing denoising algorithms which search for patches from either the noisy image or a generic database, the new algorithm finds patches from a database that contains relevant patches. We formulate the denoising problem as an optimal filter design problem and make two contributions. First, we determine the basis function of the denoising filter by solving a group sparsity minimization problem. The optimization formulation generalizes existing denoising algorithms and offers systematic analysis of the performance. Improvement methods are proposed to enhance the patch search process. Second, we determine the spectral coefficients of the denoising filter by considering a localized Bayesian prior. The localized prior leverages the similarity of the targeted database, alleviates the intensive Bayesian computation, and links the new method to the classical linear minimum mean squared error estimation. We demonstrate applications of the proposed method in a variety of scenarios, including text images, multiview images, and face images. Experimental results show the superiority of the new algorithm over existing methods.",Dadimdebytada,84.0,90.0,8.0
1381,Denoising,1.0,is denoising dead?,5.0,201.0,1.0,201.0,1.0,2.2,141.0,84,https://www.researchgate.net/profile/Peyman-Milanfar-2/publication/40026356_Is_Denoising_Dead/links/564bd93608ae4ae893b80b97/Is-Denoising-Dead.pdf,"In recent years, the supervised learning strategy for real noisy image denoising has been emerging and has achieved promising results. In contrast, realistic noise removal for raw noisy videos is rarely studied due to the lack of noisy-clean pairs for dynamic scenes. Clean video frames for dynamic scenes cannot be captured with a long-exposure shutter or averaging multi-shots as was done for static images. In this paper, we solve this problem by creating motions for controllable objects, such as toys, and capturing each static moment for multiple times to generate clean video frames. In this way, we construct a dataset with 55 groups of noisy-clean videos with ISO values ranging from 1600 to 25600. To our knowledge, this is the first dynamic video dataset with noisy-clean pairs. Correspondingly, we propose a raw video denoising network (RViDeNet) by exploring the temporal, spatial, and channel correlations of video frames. Since the raw video has Bayer patterns, we pack it into four sub-sequences, i.e RGBG sequences, which are denoised by the proposed RViDeNet separately and finally fused into a clean video. In addition, our network not only outputs a raw denoising result, but also the sRGB result by going through an image signal processing (ISP) module, which enables users to generate the sRGB result with their favourite ISPs. Experimental results demonstrate that our method outperforms state-of-the-art video and raw image denoising algorithms on both indoor and outdoor videos.",Disdede,414.0,61.0,15.0
1382,Denoising,3.0,on image denoising methods,5.0,201.0,1.0,201.0,1.0,2.2,141.60000000000002,85,https://www.ece.lsu.edu/gunturk/EE7700/OnImageDenoisingMethods.pdf,"In order to improve speckle noise denoising of block matching 3d filtering (BM3D) method, an image frequency-domain multi-layer fusion enhancement method (MLFE-BM3D) based on nonsubsampled contourlet transform (NSCT) has been proposed. The method designs a NSCT hard threshold denoising enhancement to preprocess the image, then uses fusion enhancement in NSCT domain to fuse the preliminary estimation results of images before and after the NSCT hard threshold denoising, finally, BM3D denoising is carried out with the fused image to obtain the final denoising result. Experiments on natural images and medical ultrasound images show that MLFE-BM3D method can achieve better visual effects than BM3D method, the peak signal to noise ratio (PSNR) of the denoised image is increased by 0.5dB. The MLFE-BM3D method can improve the denoising effect of speckle noise in the texture region, and still maintain a good denoising effect in the smooth region of the image.",Donimdeme,0.0,11.0,0.0
1383,Denoising,4.0,survey of image denoising techniques,5.0,201.0,1.0,201.0,1.0,2.2,141.9,86,https://www.researchgate.net/profile/Mukesh-Motwani/publication/228790062_Survey_of_image_denoising_techniques/links/5655816308ae4988a7b0b43f/Survey-of-image-denoising-techniques.pdf,"We present a comprehensive analysis of the performance of noise-reduction (``denoising'') algorithms to determine whether they provide advantages in source detection on extragalactic survey images. The methods under analysis are Perona-Malik filtering, Bilateral filter, Total Variation denoising, Structure-texture image decomposition, Non-local means, Wavelets, and Block-matching. We tested the algorithms on simulated images of extragalactic fields with resolution and depth typical of the Hubble, Spitzer, and Euclid Space Telescopes, and of ground-based instruments. After choosing their best internal parameters configuration, we assess their performance as a function of resolution, background level, and image type, also testing their ability to preserve the objects fluxes and shapes. We analyze in terms of completeness and purity the catalogs extracted after applying denoising algorithms on a simulated Euclid Wide Survey VIS image, on real H160 (HST) and K-band (HAWK-I) observations of the CANDELS GOODS-South field. Denoising algorithms often outperform the standard approach of filtering with the Point Spread Function (PSF) of the image. Applying Structure-Texture image decomposition, Perona-Malik filtering, the Total Variation method by Chambolle, and Bilateral filtering on the Euclid-VIS image, we obtain catalogs that are both more pure and complete by 0.2 magnitudes than those based on the standard approach. The same result is achieved with the Structure-Texture image decomposition algorithm applied on the H160 image. The advantage of denoising techniques with respect to PSF filtering increases at increasing depth. Moreover, these techniques better preserve the shape of the detected objects with respect to PSF smoothing. Denoising algorithms provide significant improvements in the detection of faint objects and enhance the scientific return of current and future extragalactic surveys.",Dsuofimdete,231.0,45.0,6.0
1384,Denoising,5.0,optimal wavelet denoising for phonocardiograms,5.0,201.0,1.0,201.0,1.0,2.2,142.2,87,https://www.academia.edu/download/54976313/MEJ_messer2001.pdf,"The optimal wavelet basis is used to develop quantitative, experimentally applicable criteria for self-organization. The choice of the optimal wavelet is based on the model of self-organization in the wavelet tree. The framework of the model is founded on the wavelet-domain hidden Markov model and the optimal wavelet basis criterion for self-organization which assumes inherent increase in statistical complexity, the information content necessary for maximally accurate prediction of the system's dynamics. At the same time the method, presented here for the one-dimensional data of any type, performs superior denoising and may be easily generalized to higher dimensions.",Dopwadefoph,186.0,17.0,5.0
1385,Denoising,6.0,robust wavelet denoising,5.0,201.0,1.0,201.0,1.0,2.2,142.5,88,http://unige.ch/math/folks/sardy/Papers/robustIEEE.pdf,"Sound recordings are used in various ecological studies, including acoustic wildlife monitoring. Such surveys require automatic detection of target sound events. However, current detectors, especially those relying on band-limited energy, are severely impacted by wind. The rapid dynamics of this noise invalidate standard noise estimators, and no satisfactory method for dealing with it exists in bioacoustics, where simple training and generalization between conditions are important. We propose to estimate the transient noise level by fitting short-term spectrum models to a wavelet packet representation. This estimator is then combined with log-spectral subtraction to stabilize the background level. The resulting adjusted wavelet series can be analysed by standard energy detectors. We use real monitoring data to tune this workflow, and test it on two acoustic surveys of birds. Additionally, we show how the estimator can be incorporated in a denoising method to restore sound. The proposed noise-robust workflow greatly reduced the number of false alarms in the surveys, compared to unadjusted energy detection. As a result, the survey efficiency (precision of the estimated call density) improved for both species. Denoising was also more effective when using the short-term estimate, whereas standard wavelet shrinkage with a constant noise estimate struggled to remove the effects of wind. In contrast to existing methods, the proposed estimator can adjust for transient broadband noises without requiring additional hardware or extensive tuning to each species. It improved the detection workflow based on very little training data, making it particularly attractive for detection of rare species.",Drowade,0.0,14.0,0.0
1386,Denoising,7.0,multiwavelets denoising using neighboring coefficients,5.0,201.0,1.0,201.0,1.0,2.2,142.8,89,https://www.researchgate.net/profile/Td-Bui/publication/3342873_Multiwavelets_denoising_using_neighboring_coefficients/links/55a506c708ae00cf99c93e9e/Multiwavelets-denoising-using-neighboring-coefficients.pdf,The multiresolution analysis of Alpert is considered. Explicit formulas for the entries in the matrix coefficients of the refinement equation are given in terms of hypergeometric functions. These entries are shown to solve generalized eigenvalue equations as well as partial difference equations. The matrix coefficients in the wavelet equation are also considered and conditions are given to obtain a unique solution.,Dmudeusneco,190.0,17.0,10.0
1387,Denoising,10.0,image denoising with complex ridgelets,5.0,201.0,1.0,201.0,1.0,2.2,143.7,90,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.3562&rep=rep1&type=pdf,"Fully supervised deep-learning based denoisers are currently the most performing image denoising solutions. However, they require clean reference images. When the target noise is complex, e.g. composed of an unknown mixture of primary noises with unknown intensity, fully supervised solutions are limited by the difficulty to build a suited training set for the problem. This paper proposes a gradual denoising strategy that iteratively detects the dominating noise in an image, and removes it using a tailored denoiser. The method is shown to keep up with state of the art blind denoisers on mixture noises. Moreover, noise analysis is demonstrated to guide denoisers efficiently not only on noise type, but also on noise intensity. The method provides an insight on the nature of the encountered noise, and it makes it possible to extend an existing denoiser with new noise nature. This feature makes the method adaptive to varied denoising cases.",Dimdewicori,142.0,26.0,2.0
1388,Denoising,12.0,manifold denoising,5.0,201.0,1.0,201.0,1.0,2.2,144.3,91,https://www.ml.uni-saarland.de/code/ManifoldDenoising/nips2006-denoising-final.pdf,"3D dynamic point clouds provide a natural discrete representation of real-world objects or scenes in motion, with a wide range of applications in immersive telepresence, autonomous driving, surveillance, \etc. Nevertheless, dynamic point clouds are often perturbed by noise due to hardware, software or other causes. While a plethora of methods have been proposed for static point cloud denoising, few efforts are made for the denoising of dynamic point clouds, which is quite challenging due to the irregular sampling patterns both spatially and temporally. In this paper, we represent dynamic point clouds naturally on spatial-temporal graphs, and exploit the temporal consistency with respect to the underlying surface (manifold). In particular, we define a manifold-to-manifold distance and its discrete counterpart on graphs to measure the variation-based intrinsic distance between surface patches in the temporal domain, provided that graph operators are discrete counterparts of functionals on Riemannian manifolds. Then, we construct the spatial-temporal graph connectivity between corresponding surface patches based on the temporal distance and between points in adjacent patches in the spatial domain. Leveraging the initial graph representation, we formulate dynamic point cloud denoising as the joint optimization of the desired point cloud and underlying graph representation, regularized by both spatial smoothness and temporal consistency. We reformulate the optimization and present an efficient algorithm. Experimental results show that the proposed method significantly outperforms independent denoising of each frame from state-of-the-art static point cloud denoising approaches, on both Gaussian noise and simulated LiDAR noise.",Dmade,45.0,54.0,5.0
1389,Denoising,105.0,an efficient svd-based method for image denoising,3.0,201.0,1.0,110.0,3.0,2.2,144.9,92,https://ieeexplore.ieee.org/iel7/76/4358651/07067415.pdf,"Nonlocal self-similarity of images has attracted considerable interest in the field of image processing and has led to several state-of-the-art image denoising algorithms, such as block matching and 3-D, principal component analysis with local pixel grouping, patch-based locally optimal wiener, and spatially adaptive iterative singular-value thresholding. In this paper, we propose a computationally simple denoising algorithm using the nonlocal self-similarity and the low-rank approximation (LRA). The proposed method consists of three basic steps. First, our method classifies similar image patches by the block-matching technique to form the similar patch groups, which results in the similar patch groups to be low rank. Next, each group of similar patches is factorized by singular value decomposition (SVD) and estimated by taking only a few largest singular values and corresponding singular vectors. Finally, an initial denoised image is generated by aggregating all processed patches. For low-rank matrices, SVD can provide the optimal energy compaction in the least square sense. The proposed method exploits the optimal energy compaction property of SVD to lead an LRA of similar patch groups. Unlike other SVD-based methods, the LRA in SVD domain avoids learning the local basis for representing image patches, which usually is computationally expensive. The experimental results demonstrate that the proposed method can effectively reduce noise and be competitive with the current state-of-the-art denoising algorithms in terms of both quantitative metrics and subjective visual quality.",Danefsvmefoimde,170.0,49.0,11.0
1390,Denoising,17.0,combining the power of internal and external denoising,5.0,201.0,1.0,201.0,1.0,2.2,145.8,93,https://core.ac.uk/download/pdf/189662801.pdf,"Given a set of image denoisers, each having a different denoising capability, is there a provably optimal way of combining these denoisers to produce an overall better result? An answer to this question is fundamental to designing an ensemble of weak estimators for complex scenes. In this paper, we present an optimal combination scheme by leveraging deep neural networks and convex optimization. The proposed framework, called the Consensus Neural Network (CsNet), introduces three new concepts in image denoising: (1) A provably optimal procedure to combine the denoised outputs via convex optimization; (2) A deep neural network to estimate the mean squared error (MSE) of denoised images without needing the ground truths; (3) An image boosting procedure using a deep neural network to improve contrast and to recover lost details of the combined images. Experimental results show that CsNet can consistently improve denoising performance for both deterministic and neural network denoisers.",Dcothpoofinanexde,93.0,12.0,7.0
1391,Denoising,18.0,noise2void-learning denoising from single noisy images,5.0,201.0,1.0,201.0,1.0,2.2,146.10000000000002,94,https://openaccess.thecvf.com/content_CVPR_2019/papers/Krull_Noise2Void_-_Learning_Denoising_From_Single_Noisy_Images_CVPR_2019_paper.pdf,"In the last several years deep learning based approaches have come to dominate many areas of computer vision, and image denoising is no exception. Neural networks can learn by example to map noisy images to clean images. However, access to noisy/clean or even noisy/noisy image pairs isn't always readily available in the desired domain. Recent approaches have allowed for the denoising of single noisy images without access to any training data aside from that very image. But since they require both training and inference to be carried out on each individual input image, these methods require significant computation time. As such, they are difficult to integrate into automated microscopy pipelines where denoising large datasets is essential but needs to be carried out in a timely manner. Here we present Noise2Fast, a fast single image blind denoiser. Our method is tailored for speed by training on a four-image dataset produced using a unique form of downsampling we refer to as ""checkerboard downsampling"". Noise2Fast is faster than all tested approaches and is more accurate than all except Self2Self, which takes well over 100 times longer to denoise an image. This allows for a combination of speed and flexibility that was not previously attainable using any other method.",Dnodefrsinoim,280.0,29.0,61.0
1392,Denoising,19.0,universal denoising networks: a novel cnn architecture for image denoising,5.0,201.0,1.0,201.0,1.0,2.2,146.4,95,https://openaccess.thecvf.com/content_cvpr_2018/papers/Lefkimmiatis_Universal_Denoising_Networks_CVPR_2018_paper.pdf,"Image denoising is an essential part of many image processing and computer vision tasks due to inevitable noise corruption during image acquisition. Traditionally, many researchers have investigated image priors for the denoising, within the Bayesian perspective based on image properties and statistics. Recently, deep convolutional neural networks (CNNs) have shown great success in image denoising by incorporating large-scale synthetic datasets. However, they both have pros and cons. While the deep CNNs are powerful for removing the noise with known statistics, they tend to lack flexibility and practicality for the blind and real-world noise. Moreover, they cannot easily employ explicit priors. On the other hand, traditional non-learning methods can involve explicit image priors, but they require considerable computation time and cannot exploit large-scale external datasets. In this paper, we present a CNN-based method that leverages the advantages of both methods based on the Bayesian perspective. Concretely, we divide the blind image denoising problem into sub-problems and conquer each inference problem separately. As the CNN is a powerful tool for inference, our method is rooted in CNNs and propose a novel design of network for efficient inference. With our proposed method, we can successfully remove blind and real-world noise, with a moderate number of parameters of universal CNN.",Dundeneanocnarfoimde,162.0,46.0,14.0
1393,Denoising,21.0,bilateral mesh denoising,5.0,201.0,1.0,201.0,1.0,2.2,147.0,96,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.3578&rep=rep1&type=pdf,"Denoising diffusion probabilistic models (DDPMs) have emerged as competitive generative models yet brought challenges to efficient sampling. In this paper, we propose novel bilateral denoising diffusion models (BDDMs), which take significantly fewer steps to generate high-quality samples. From a bilateral modeling objective, BDDMs parameterize the forward and reverse processes with a score network and a scheduling network, respectively. We show that a new lower bound tighter than the standard evidence lower bound can be derived as a surrogate objective for training the two networks. In particular, BDDMs are efficient, simple-to-train, and capable of further improving any pre-trained DDPM by optimizing the inference noise schedules. Our experiments demonstrated that BDDMs can generate high-fidelity samples with as few as 3 sampling steps and produce comparable or even higher quality samples than DDPMs using 1000 steps with only 16 sampling steps (a 62x speedup).",Dbimede,716.0,27.0,83.0
1394,Denoising,22.0,the sure-let approach to image denoising,5.0,201.0,1.0,201.0,1.0,2.2,147.3,97,https://infoscience.epfl.ch/record/130342/files/blu0702.pdf,"Removal of noise from an image is an extensively studied problem in image processing. Indeed, the recent advent of sophisticated and highly effective denoising algorithms lead some to believe that existing methods are touching the ceiling in terms of noise removal performance. Can we leverage this impressive achievement to treat other tasks in image processing? Recent work has answered this question positively, in the form of the Plug-and-Play Prior ($P^3$) method, showing that any inverse problem can be handled by sequentially applying image denoising steps. This relies heavily on the ADMM optimization technique in order to obtain this chained denoising interpretation.   Is this the only way in which tasks in image processing can exploit the image denoising engine? In this paper we provide an alternative, more powerful and more flexible framework for achieving the same goal. As opposed to the $P^3$ method, we offer Regularization by Denoising (RED): using the denoising engine in defining the regularization of the inverse problem. We propose an explicit image-adaptive Laplacian-based regularization functional, making the overall objective functional clearer and better defined. With a complete flexibility to choose the iterative optimization procedure for minimizing the above functional, RED is capable of incorporating any image denoising algorithm, treat general inverse problems very effectively, and is guaranteed to converge to the globally optimal result. We test this approach and demonstrate state-of-the-art results in the image deblurring and super-resolution problems.",Dthsuaptoimde,361.0,187.0,35.0
1395,Denoising,23.0,deep learning for denoising,5.0,201.0,1.0,201.0,1.0,2.2,147.60000000000002,98,https://arxiv.org/pdf/1810.11614,"Compared with traditional seismic noise attenuation algorithms that depend on signal models and their corresponding prior assumptions, removing noise with a deep neural network is trained based on a large training set, where the inputs are the raw datasets and the corresponding outputs are the desired clean data. After the completion of training, the deep learning method achieves adaptive denoising with no requirements of (i) accurate modelings of the signal and noise, or (ii) optimal parameters tuning. We call this intelligent denoising. We use a convolutional neural network as the basic tool for deep learning. In random and linear noise attenuation, the training set is generated with artificially added noise. In the multiple attenuation step, the training set is generated with acoustic wave equation. Stochastic gradient descent is used to solve the optimal parameters for the convolutional neural network. The runtime of deep learning on a graphics processing unit for denoising has the same order as the $f-x$ deconvolution method. Synthetic and field results show the potential applications of deep learning in automatic attenuation of random noise (with unknown variance), linear noise, and multiples.",Ddelefode,26.0,69.0,0.0
1396,Denoising,25.0,mdl denoising,5.0,201.0,1.0,201.0,1.0,2.2,148.2,99,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.8086&rep=rep1&type=pdf,"We refine and extend an earlier MDL denoising criterion for wavelet-based denoising. We start by showing that the denoising problem can be reformulated as a clustering problem, where the goal is to obtain separate clusters for informative and non-informative wavelet coefficients, respectively. This suggests two refinements, adding a code-length for the model index, and extending the model in order to account for subband-dependent coefficient distributions. A third refinement is derivation of soft thresholding inspired by predictive universal coding with weighted mixtures. We propose a practical method incorporating all three refinements, which is shown to achieve good performance and robustness in denoising both artificial and natural signals.",Dmdde,44.0,75.0,5.0
1397,Denoising,27.0,adaptive principal components and image denoising,5.0,201.0,1.0,201.0,1.0,2.2,148.8,100,http://dmmd.net/main_wp/wp-content/uploads/2012/02/denoiseImages.pdf,"Deep convolutional neural networks (CNNs) for image denoising are usually trained on large datasets. These models achieve the current state of the art, but they have difficulties generalizing when applied to data that deviate from the training distribution. Recent work has shown that it is possible to train denoisers on a single noisy image. These models adapt to the features of the test image, but their performance is limited by the small amount of information used to train them. Here we propose ""GainTuning"", in which CNN models pre-trained on large datasets are adaptively and selectively adjusted for individual test images. To avoid overfitting, GainTuning optimizes a single multiplicative scaling parameter (the ""Gain"") of each channel in the convolutional layers of the CNN. We show that GainTuning improves state-of-the-art CNNs on standard image-denoising benchmarks, boosting their denoising performance on nearly every image in a held-out test set. These adaptive improvements are even more substantial for test images differing systematically from the training data, either in noise level or image type. We illustrate the potential of adaptive denoising in a scientific application, in which a CNN is trained on synthetic data, and tested on real transmission-electron-microscope images. In contrast to the existing methodology, GainTuning is able to faithfully reconstruct the structure of catalytic nanoparticles from these data at extremely low signal-to-noise ratios.",Dadprcoanimde,266.0,13.0,17.0
1763,Depth Estimation,4.0,digging into self-supervised monocular depth estimation,5.0,9.0,5.0,4.0,5.0,5.0,6.0,1,http://openaccess.thecvf.com/content_ICCV_2019/papers/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.pdf,"Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.",Ddiinsemodees,505.0,86.0,156.0
1764,Depth Estimation,7.0,unsupervised monocular depth estimation with left-right consistency,5.0,12.0,5.0,2.0,5.0,5.0,7.5,2,http://openaccess.thecvf.com/content_cvpr_2017/papers/Godard_Unsupervised_Monocular_Depth_CVPR_2017_paper.pdf,"Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Ex-ploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth.",Dunmodeeswileco,1568.0,70.0,334.0
1765,Depth Estimation,1.0,consistent video depth estimation,5.0,19.0,5.0,6.0,5.0,5.0,9.7,3,https://dl.acm.org/doi/pdf/10.1145/3386569.3392377,"We present an algorithm for reconstructing dense, geometrically consistent depth for all pixels in a monocular video. We leverage a conventional structure-from-motion reconstruction to establish geometric constraints on pixels in the video. Unlike the ad-hoc priors in classical reconstruction, we use a learning-based prior, i.e., a convolutional neural network trained for single-image depth estimation. At test time, we fine-tune this network to satisfy the geometric constraints of a particular input video, while retaining its ability to synthesize plausible depth details in parts of the video that are less constrained. We show through quantitative validation that our method achieves higher accuracy and a higher degree of geometric consistency than previous monocular reconstruction methods. Visually, our results appear more stable. Our algorithm is able to handle challenging hand-held captured input videos with a moderate degree of dynamic motion. The improved quality of the reconstruction enables several applications, such as scene reconstruction and advanced video-based visual effects.",Dcovidees,64.0,114.0,6.0
1766,Depth Estimation,20.0,3d packing for self-supervised monocular depth estimation,5.0,28.0,5.0,3.0,5.0,5.0,18.1,4,https://openaccess.thecvf.com/content_CVPR_2020/papers/Guizilini_3D_Packing_for_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.pdf,"Although cameras are ubiquitous, robotic platforms typically rely on active sensors like LiDAR for direct 3D perception. In this work, we propose a novel self-supervised monocular depth estimation method combining geometry with a new deep network, PackNet, learned only from unlabeled monocular videos. Our architecture leverages novel symmetrical packing and unpacking blocks to jointly learn to compress and decompress detail-preserving representations using 3D convolutions. Although self-supervised, our method outperforms other self, semi, and fully supervised methods on the KITTI benchmark. The 3D inductive bias in PackNet enables it to scale with input resolution and number of parameters without overfitting, generalizing better on out-of-domain data such as the NuScenes dataset. Furthermore, it does not require large-scale supervised pretraining on ImageNet and can run in real-time. Finally, we release DDAD (Dense Depth for Automated Driving), a new urban driving dataset with more challenging and accurate depth evaluation, thanks to longer-range and denser ground-truth depth generated from high-density LiDARs mounted on a fleet of self-driving cars operating world-wide.",D3dpafosemodees,109.0,60.0,26.0
1767,Depth Estimation,22.0,learning single camera depth estimation using dual-pixels,5.0,5.0,5.0,38.0,5.0,5.0,20.0,5,http://openaccess.thecvf.com/content_ICCV_2019/papers/Garg_Learning_Single_Camera_Depth_Estimation_Using_Dual-Pixels_ICCV_2019_paper.pdf,"Deep learning techniques have enabled rapid progress in monocular depth estimation, but their quality is limited by the ill-posed nature of the problem and the scarcity of high quality datasets. We estimate depth from a single cam-era by leveraging the dual-pixel auto-focus hardware that is increasingly common on modern camera sensors. Classic stereo algorithms and prior learning-based depth estimation techniques underperform when applied on this dual-pixel data, the former due to too-strong assumptions about RGB image matching, and the latter due to not leveraging the understanding of optics of dual-pixel image formation. To allow learning based methods to work well on dual-pixel imagery, we identify an inherent ambiguity in the depth estimated from dual-pixel cues, and develop an approach to estimate depth up to this ambiguity. Using our approach, existing monocular depth estimation techniques can be effectively applied to dual-pixel data, and much smaller models can be constructed that still infer high quality depth. To demonstrate this, we capture a large dataset of in-the-wild 5-viewpoint RGB images paired with corresponding dual-pixel data, and show how view supervision with this data can be used to learn depth up to the unknown ambiguities. On our new task, our model is 30% more accurate than any prior work on learning-based monocular or stereoscopic depth estimation.",Dlesicadeesusdu,40.0,65.0,3.0
1768,Depth Estimation,3.0,fastdepth: fast monocular depth estimation on embedded systems,5.0,30.0,5.0,26.0,5.0,5.0,20.7,6,https://arxiv.org/pdf/1903.03273,"Depth sensing is a critical function for robotic tasks such as localization, mapping and obstacle detection. There has been a significant and growing interest in depth estimation from a single RGB image, due to the relatively low cost and size of monocular cameras. However, state-of-the-art single-view depth estimation algorithms are based on fairly complex deep neural networks that are too slow for real-time inference on an embedded platform, for instance, mounted on a micro aerial vehicle. In this paper, we address the problem of fast depth estimation on embedded systems. We propose an efficient and lightweight encoder-decoder network architecture and apply network pruning to further reduce computational complexity and latency. In particular, we focus on the design of a low-latency decoder. Our methodology demonstrates that it is possible to achieve similar accuracy as prior work on depth estimation, but at inference speeds that are an order of magnitude faster. Our proposed network, FastDepth, runs at 178 fps on an NVIDIA Jetson TX2 GPU and at 27 fps when using only the TX2 CPU, with active power consumption under 10 W. FastDepth achieves close to state-of-the-art accuracy on the NYU Depth v2 dataset. To the best of the authors’ knowledge, this paper demonstrates real-time monocular depth estimation using a deep neural network with the lowest latency and highest throughput on an embedded platform that can be carried by a micro aerial vehicle.",Dfafamodeesonemsy,100.0,39.0,23.0
1769,Depth Estimation,60.0,towards robust monocular depth estimation: mixing datasets for zero-shot cross-dataset transfer,4.0,18.0,5.0,1.0,5.0,4.7,25.5,7,https://arxiv.org/pdf/1907.01341,"The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible. In particular, we propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks. Armed with these tools, we experiment with six diverse training datasets, including a new, massive data source: 3D films. To demonstrate the generalization power of our approach we use zero-shot cross-dataset transfer, i.e. we evaluate on datasets that were not seen during training. The experiments confirm that mixing data from complementary sources greatly improves monocular depth estimation. Our approach clearly outperforms competing methods across diverse datasets, setting a new state of the art for monocular depth estimation.",Dtoromodeesmidafozecrtr,156.0,70.0,43.0
1770,Depth Estimation,6.0,deep ordinal regression network for monocular depth estimation,5.0,49.0,4.0,11.0,5.0,4.6,24.700000000000003,8,https://openaccess.thecvf.com/content_cvpr_2018/papers/Fu_Deep_Ordinal_Regression_CVPR_2018_paper.pdf,"Monocular depth estimation, which plays a crucial role in understanding 3D scene geometry, is an ill-posed problem. Recent methods have gained significant improvement by exploring image-level information and hierarchical features from deep convolutional neural networks (DCNNs). These methods model depth estimation as a regression problem and train the regression networks by minimizing mean squared error, which suffers from slow convergence and unsatisfactory local solutions. Besides, existing depth estimation networks employ repeated spatial pooling operations, resulting in undesirable low-resolution feature maps. To obtain high-resolution depth maps, skip-connections or multilayer deconvolution networks are required, which complicates network training and consumes much more computations. To eliminate or at least largely reduce these problems, we introduce a spacing-increasing discretization (SID) strategy to discretize depth and recast depth network learning as an ordinal regression problem. By training the network using an ordinary regression loss, our method achieves much higher accuracy and faster convergence in synch. Furthermore, we adopt a multi-scale network structure which avoids unnecessary spatial pooling and captures multi-scale information in parallel. The proposed deep ordinal regression network (DORN) achieves state-of-the-art results on three challenging benchmarks, i.e., KITTI [16], Make3D [49], and NYU Depth v2 [41], and outperforms existing methods by a large margin.",Ddeorrenefomodees,670.0,71.0,137.0
1771,Depth Estimation,17.0,from big to small: multi-scale local planar guidance for monocular depth estimation,5.0,50.0,4.0,22.0,5.0,4.6,31.700000000000003,9,https://arxiv.org/pdf/1907.10326,"Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoder-decoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction. In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method.",Dfrbitosmmuloplgufomodees,140.0,67.0,38.0
1772,Depth Estimation,35.0,anytime stereo image depth estimation on mobile devices,5.0,56.0,4.0,35.0,5.0,4.6,43.400000000000006,10,https://arxiv.org/pdf/1810.11408,"Many applications of stereo depth estimation in robotics require the generation of accurate disparity maps in real time under significant computational constraints. Current state-of-the-art algorithms force a choice between either generating accurate mappings at a slow pace, or quickly generating inaccurate ones, and additionally these methods typically require far too many parameters to be usable on power- or memory-constrained devices. Motivated by these shortcomings, we propose a novel approach for disparity prediction in the anytime setting. In contrast to prior work, our end-to-end learned approach can trade off computation and accuracy at inference time. Depth estimation is performed in stages, during which the model can be queried at any time to output its current best estimate. Our final model can process $1242 \times 375$ resolution images within a range of 10-35 FPS on an NVIDIA Jetson TX2 module with only marginal increases in error – using two orders of magnitude fewer parameters than the most competitive baseline. The source code is available at https://github.com/mileyan/AnyNet.",Danstimdeesonmode,60.0,55.0,13.0
1773,Depth Estimation,55.0,on the importance of stereo for accurate depth estimation: an efficient semi-supervised deep neural network approach,4.0,24.0,5.0,77.0,4.0,4.4,49.2,11,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w14/Smolyanskiy_On_the_Importance_CVPR_2018_paper.pdf,"We revisit the problem of visual depth estimation in the context of autonomous vehicles. Despite the progress on monocular depth estimation in recent years, we show that the gap between monocular and stereo depth accuracy remains large-a particularly relevant result due to the prevalent reliance upon monocular cameras by vehicles that are expected to be self-driving. We argue that the challenges of removing this gap are significant, owing to fundamental limitations of monocular vision. As a result, we focus our efforts on depth estimation by stereo. We propose a novel semi-supervised learning approach to training a deep stereo neural network, along with a novel architecture containing a machine-learned argmax layer and a custom runtime that enables a smaller version of our stereo DNN to run on an embedded GPU. Competitive results are shown on the KITTI 2015 stereo dataset. We also evaluate the recent progress of stereo algorithms by measuring the impact upon accuracy of various design criteria.1",Donthimofstfoacdeesanefsedeneneap,55.0,34.0,3.0
1774,Depth Estimation,8.0,depth estimation via affinity learned with convolutional spatial propagation network,5.0,48.0,4.0,50.0,4.0,4.3,36.6,12,http://openaccess.thecvf.com/content_ECCV_2018/papers/Xinjing_Cheng_Depth_Estimation_via_ECCV_2018_paper.pdf,"Depth estimation from a single image is a fundamental problem in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for depth prediction. Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We apply the designed CSPN to two depth estimation tasks given a single image: (1) Refine the depth output from existing state-of-the-art (SOTA) methods; (2) Convert sparse depth samples to a dense depth map by embedding the depth samples within the propagation procedure. The second task is inspired by the availability of LiDAR that provides sparse but accurate depth measurements. We experimented the proposed CSPN over the popular NYU v2 [1] and KITTI [2] datasets, where we show that our proposed approach improves not only quality (e.g., 30% more reduction in depth error), but also speed (e.g., 2 to 5\(\times \) faster) of depth maps than previous SOTA methods. The codes of CSPN are available at: https://github.com/XinJCheng/CSPN.",Ddeesviaflewicospprne,138.0,51.0,31.0
1775,Depth Estimation,9.0,unsupervised cnn for single view depth estimation: geometry to the rescue,5.0,81.0,4.0,47.0,4.0,4.3,49.2,13,http://arxiv.org/pdf/1603.04992v2,"A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground-truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photometric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset gives comparable performance to that of the state-of-the-art supervised methods for single view depth estimation.",Duncnfosivideesgetothre,908.0,37.0,83.0
1776,Depth Estimation,26.0,mvdepthnet: real-time multiview depth estimation neural network,5.0,76.0,4.0,79.0,4.0,4.3,61.900000000000006,14,https://arxiv.org/pdf/1807.08563,"Although deep neural networks have been widely applied to computer vision problems, extending them into multiview depth estimation is non-trivial. In this paper, we present MVDepthNet, a convolutional network to solve the depth estimation problem given several image-pose pairs from a localized monocular camera in neighbor viewpoints. Multiview observations are encoded in a cost volume and then combined with the reference image to estimate the depth map using an encoder-decoder network. By encoding the information from multiview observations into the cost volume, our method achieves real-time performance and the flexibility of traditional methods that can be applied regardless of the camera intrinsic parameters and the number of images. Geometric data augmentation is used to train MVDepthNet. We further apply MVDepthNet in a monocular dense mapping system that continuously estimates depth maps using a single localized moving camera. Experiments show that our method can generate depth maps efficiently and precisely.",Dmvremudeesnene,46.0,35.0,12.0
1777,Depth Estimation,18.0,towards real-time unsupervised monocular depth estimation on cpu,5.0,95.0,4.0,67.0,4.0,4.3,63.5,15,https://arxiv.org/pdf/1806.11430,"Unsupervised depth estimation from a single image is a very attractive technique with several implications in robotic, autonomous navigation, augmented reality and so on. This topic represents a very challenging task and the advent of deep learning enabled to tackle this problem with excellent results. However, these architectures are extremely deep and complex. Thus, real-time performance can be achieved only by leveraging power-hungry GPUs that do not allow to infer depth maps in application fields characterized by low-power constraints. To tackle this issue, in this paper we propose a novel architecture capable to quickly infer an accurate depth map on a CPU, even of an embedded system, using a pyramid of features extracted from a single input image. Similarly to state-of-the-art, we train our network in an unsupervised manner casting depth estimation as an image reconstruction problem. Extensive experimental results on the KITTI dataset show that compared to the top performing approach our network has similar accuracy but a much lower complexity (about 6% of parameters) enabling to infer a depth map for a KITTI image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard CPU. Moreover, by trading accuracy for efficiency, our network allows to infer maps at about 2 Hz and 40 Hz respectively, still being more accurate than most state-of-the-art slower methods. To the best of our knowledge, it is the first method enabling such performance on CPUs paving the way for effective deployment of unsupervised monocular depth estimation even on embedded systems.",Dtoreunmodeesoncp,84.0,33.0,11.0
1778,Depth Estimation,16.0,on the uncertainty of self-supervised monocular depth estimation,5.0,104.0,3.0,9.0,5.0,4.2,49.1,16,https://openaccess.thecvf.com/content_CVPR_2020/papers/Poggi_On_the_Uncertainty_of_Self-Supervised_Monocular_Depth_Estimation_CVPR_2020_paper.pdf,"Self-supervised paradigms for monocular depth estimation are very appealing since they do not require ground truth annotations at all. Despite the astonishing results yielded by such methodologies, learning to reason about the uncertainty of the estimated depth maps is of paramount importance for practical applications, yet uncharted in the literature. Purposely, we explore for the first time how to estimate the uncertainty for this task and how this affects depth accuracy, proposing a novel peculiar technique specifically designed for self-supervised approaches. On the standard KITTI dataset, we exhaustively assess the performance of each method with different self-supervised paradigms. Such evaluation highlights that our proposal i) always improves depth accuracy significantly and ii) yields state-of-the-art results concerning uncertainty estimation when training on sequences and competitive results uniquely deploying stereo pairs.",Donthunofsemodees,45.0,89.0,8.0
1779,Depth Estimation,38.0,self-supervised monocular depth estimation: solving the dynamic object problem by semantic guidance,5.0,124.0,3.0,7.0,5.0,4.2,63.1,17,https://arxiv.org/pdf/2007.06936,"Self-supervised monocular depth estimation presents a powerful method to obtain 3D scene information from single camera images, which is trainable on arbitrary image sequences without requiring depth labels, e.g., from a LiDAR sensor. In this work we present a new self-supervised semantically-guided depth estimation (SGDepth) method to deal with moving dynamic-class (DC) objects, such as moving cars and pedestrians, which violate the static-world assumptions typically made during training of such models. Specifically, we propose (i) mutually beneficial cross-domain training of (supervised) semantic segmentation and self-supervised depth estimation with task-specific network heads, (ii) a semantic masking scheme providing guidance to prevent moving DC objects from contaminating the photometric loss, and (iii) a detection method for frames with non-moving DC objects, from which the depth of DC objects can be learned. We demonstrate the performance of our method on several benchmarks, in particular on the Eigen split, where we exceed all baselines without test-time refinement.",Dsemodeessothdyobprbysegu,56.0,74.0,5.0
1780,Depth Estimation,10.0,learning monocular depth estimation infusing traditional stereo knowledge,5.0,141.0,3.0,30.0,5.0,4.2,68.4,18,http://openaccess.thecvf.com/content_CVPR_2019/papers/Tosi_Learning_Monocular_Depth_Estimation_Infusing_Traditional_Stereo_Knowledge_CVPR_2019_paper.pdf,"Depth estimation from a single image represents a fascinating, yet challenging problem with countless applications. Recent works proved that this task could be learned without direct supervision from ground truth labels leveraging image synthesis on sequences or stereo pairs. Focusing on this second case, in this paper we leverage stereo matching in order to improve monocular depth estimation. To this aim we propose monoResMatch, a novel deep architecture designed to infer depth from a single input image by synthesizing features from a different point of view, horizontally aligned with the input image, performing stereo matching between the two cues. In contrast to previous works sharing this rationale, our network is the first trained end-to-end from scratch. Moreover, we show how obtaining proxy ground truth annotation through traditional stereo algorithms, such as Semi-Global Matching, enables more accurate monocular depth estimation still countering the need for expensive depth labels by keeping a self-supervised approach. Exhaustive experimental results prove how the synergy between i) the proposed monoResMatch architecture and ii) proxy-supervision attains state-of-the-art for self-supervised monocular depth estimation. The code is publicly available at https://github.com/fabiotosi92/monoResMatch-Tensorflow.",Dlemodeesintrstkn,82.0,64.0,12.0
1781,Depth Estimation,54.0,real-time joint semantic segmentation and depth estimation using asymmetric annotations,4.0,101.0,3.0,31.0,5.0,3.9,65.9,19,https://arxiv.org/pdf/1809.04766,"Deployment of deep learning models in robotics as sensory information extractors can be a daunting task to handle, even using generic GPU cards. Here, we address three of its most prominent hurdles, namely, i) the adaptation of a single model to perform multiple tasks at once (in this work, we consider depth estimation and semantic segmentation crucial for acquiring geometric and semantic understanding of the scene), while ii) doing it in real-time, and iii) using asymmetric datasets with uneven numbers of annotations per each modality. To overcome the first two issues, we adapt a recently proposed real-time semantic segmentation network, making changes to further reduce the number of floating point operations. To approach the third issue, we embrace a simple solution based on hard knowledge distillation under the assumption of having access to a powerful ‘teacher’ network. We showcase how our system can be easily extended to handle more tasks, and more datasets, all at once, performing depth estimation and segmentation both indoors and outdoors with a single model. Quantitatively, we achieve results equivalent to (or better than) current state-of-the-art approaches with one forward pass costing just 13ms and 6.5 GFLOPs on 640×480 inputs. This efficiency allows us to directly incorporate the raw predictions of our network into the SemanticFusion framework [1] for dense 3D semantic reconstruction of the scene.33The models are available here: https://github.com/drsleep/ multi–task–refinenet",Drejoseseandeesusasan,69.0,54.0,6.0
1782,Depth Estimation,49.0,monocular depth estimation using multi-scale continuous crfs as sequential deep networks,4.0,102.0,3.0,36.0,5.0,3.9,66.3,20,https://arxiv.org/pdf/1803.00891,"Depth cues have been proved very useful in various computer vision and robotic tasks. This paper addresses the problem of monocular depth estimation from a single still image. Inspired by the effectiveness of recent works on multi-scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods using concatenation or weighted average schemes, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through an extensive experimental evaluation, we demonstrate the effectiveness of the proposed approach and establish new state of the art results for the monocular depth estimation task on three publicly available datasets, i.e., NYUD-V2, Make3D and KITTI.",Dmodeesusmucocrassedene,57.0,55.0,5.0
1783,Depth Estimation,28.0,visualization of convolutional neural networks for monocular depth estimation,5.0,134.0,3.0,45.0,4.0,3.9,75.5,21,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hu_Visualization_of_Convolutional_Neural_Networks_for_Monocular_Depth_Estimation_ICCV_2019_paper.pdf,"Recently, convolutional neural networks (CNNs) have shown great success on the task of monocular depth estimation. A fundamental yet unanswered question is: how CNNs can infer depth from a single image. Toward answering this question, we consider visualization of inference of a CNN by identifying relevant pixels of an input image to depth estimation. We formulate it as an optimization problem of identifying the smallest number of image pixels from which the CNN can estimate a depth map with the minimum difference from the estimate from the entire image. To cope with a difficulty with optimization through a deep CNN, we propose to use another network to predict those relevant image pixels in a forward computation. In our experiments, we first show the effectiveness of this approach, and then apply it to different depth estimation networks on indoor and outdoor scene datasets. The results provide several findings that help exploration of the above question.",Dviofconenefomodees,29.0,51.0,3.0
1784,Depth Estimation,51.0,unos: unified unsupervised optical-flow and stereo-depth estimation by watching videos,4.0,132.0,3.0,32.0,5.0,3.9,77.7,22,http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_UnOS_Unified_Unsupervised_Optical-Flow_and_Stereo-Depth_Estimation_by_Watching_Videos_CVPR_2019_paper.pdf,"In this paper, we propose UnOS, an unified system for unsupervised optical flow and stereo depth estimation using convolutional neural network (CNN) by taking advantages of their inherent geometrical consistency based on the rigid-scene assumption. UnOS significantly outperforms other state-of-the-art (SOTA) unsupervised approaches that treated the two tasks independently. Specifically, given two consecutive stereo image pairs from a video, UnOS estimates per-pixel stereo depth images, camera ego-motion and optical flow with three parallel CNNs. Based on these quantities, UnOS computes rigid optical flow and compares it against the optical flow estimated from the FlowNet, yielding pixels satisfying the rigid-scene assumption. Then, we encourage geometrical consistency between the two estimated flows within rigid regions, from which we derive a rigid-aware direct visual odometry (RDVO) module. We also propose rigid and occlusion-aware flow-consistency losses for the learning of UnOS. We evaluated our results on the popular KITTI dataset over 4 related tasks, \ie stereo depth, optical flow, visual odometry and motion segmentation.",Dunununopanstesbywavi,72.0,65.0,14.0
1785,Depth Estimation,25.0,geometry meets semantics for semi-supervised monocular depth estimation,5.0,154.0,3.0,71.0,4.0,3.9,90.4,23,https://arxiv.org/pdf/1810.04093,"Depth estimation from a single image represents a very exciting challenge in computer vision. While other image-based depth sensing techniques leverage on the geometry between different viewpoints (e.g., stereo or structure from motion), the lack of these cues within a single image renders ill-posed the monocular depth estimation task. For inference, state-of-the-art encoder-decoder architectures for monocular depth estimation rely on effective feature representations learned at training time. For unsupervised training of these models, geometry has been effectively exploited by suitable images warping losses computed from views acquired by a stereo rig or a moving camera. In this paper, we make a further step forward showing that learning semantic information from images enables to improve effectively monocular depth estimation as well. In particular, by leveraging on semantically labeled images together with unsupervised signals gained by geometry through an image warping loss, we propose a deep learning approach aimed at joint semantic segmentation and depth estimation. Our overall learning framework is semi-supervised, as we deploy groundtruth data only in the semantic domain. At training time, our network learns a common feature representation for both tasks and a novel cross-task loss function is proposed. The experimental findings show how, jointly tackling depth prediction and semantic segmentation, allows to improve depth estimation accuracy. In particular, on the KITTI dataset our network outperforms state-of-the-art methods for monocular depth estimation.",Dgemesefosemodees,69.0,49.0,6.0
1786,Depth Estimation,21.0,omnidepth: dense depth estimation for indoors spherical panoramas,5.0,165.0,3.0,72.0,4.0,3.9,93.9,24,https://openaccess.thecvf.com/content_ECCV_2018/papers/NIKOLAOS_ZIOULIS_OmniDepth_Dense_Depth_ECCV_2018_paper.pdf,"Recent work on depth estimation up to now has only focused on projective images ignoring \({360}^{\circ }\) content which is now increasingly and more easily produced. We show that monocular depth estimation models trained on traditional images produce sub-optimal results on omnidirectional images, showcasing the need for training directly on \({360}^{\circ }\) datasets, which however, are hard to acquire. In this work, we circumvent the challenges associated with acquiring high quality \({360}^{\circ }\) datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to \({360}^{\circ }\) via rendering. This dataset, which is considerably larger than similar projective datasets, is publicly offered to the community to enable future research in this direction. We use this dataset to learn in an end-to-end fashion the task of depth estimation from \({360}^{\circ }\) images. We show promising results in our synthesized data as well as in unseen realistic images.",Domdedeesfoinsppa,63.0,73.0,16.0
1787,Depth Estimation,401.0,pseudo-lidar from visual depth estimation: bridging the gap in 3d object detection for autonomous driving,1.0,29.0,5.0,5.0,5.0,3.8,133.4,25,http://arxiv.org/pdf/1812.07179v6,"3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies --- a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations --- essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance --- raising the detection accuracy of objects within the 30m range from the previous state-of-the-art of 22% to an unprecedented 74%. At the time of submission our algorithm holds the highest entry on the KITTI 3D object detection leaderboard for stereo-image-based approaches.",Dpsfrvideesbrthgain3dobdefoaudr,309.0,41.0,45.0
1788,Depth Estimation,401.0,high quality monocular depth estimation via transfer learning,1.0,20.0,5.0,24.0,5.0,3.8,135.5,26,http://arxiv.org/pdf/1812.11941v2,"Accurate depth estimation from images is a fundamental task in many applications including scene understanding and reconstruction. Existing solutions for depth estimation often produce blurry approximations of low resolution. This paper presents a convolutional neural network for computing a high-resolution depth map given a single RGB image with the help of transfer learning. Following a standard encoder-decoder architecture, we leverage features extracted using high performing pre-trained networks when initializing our encoder along with augmentation and training strategies that lead to more accurate results. We show how, even for a very simple decoder, our method is able to achieve detailed high-resolution depth maps. Our network, with fewer parameters and training iterations, outperforms state-of-the-art on two datasets and also produces qualitatively better results that capture object boundaries more faithfully. Code and corresponding pre-trained weights are made publicly available.",Dhiqumodeesvitrle,168.0,44.0,42.0
1789,Depth Estimation,50.0,unsupervised adversarial depth estimation using cycled generative networks,4.0,140.0,3.0,68.0,4.0,3.6000000000000005,91.4,27,https://arxiv.org/pdf/1807.10915,"While recent deep monocular depth estimation approaches based on supervised regression have achieved remarkable performance, costly ground truth annotations are required during training. To cope with this issue, in this paper we present a novel unsupervised deep learning approach for predicting depth maps and show that the depth estimation task can be effectively tackled within an adversarial learning framework. Specifically, we propose a deep generative network that learns to predict the correspondence field (i.e. the disparity map) between two image views in a calibrated stereo camera setting. The proposed architecture consists of two generative sub-networks jointly trained with adversarial learning for reconstructing the disparity map and organized in a cycle such as to provide mutual constraints and supervision to each other. Extensive experiments on the publicly available datasets KITTI and Cityscapes demonstrate the effectiveness of the proposed model and competitive results with state of the art methods. The code is available at https://github.com/andrea-pilzer/unsup-stereo-depthGAN",Dunaddeesuscygene,89.0,31.0,1.0
1790,Depth Estimation,401.0,revisiting single image depth estimation: toward higher resolution maps with accurate object boundaries,1.0,68.0,4.0,21.0,5.0,3.4000000000000004,153.8,28,http://arxiv.org/pdf/1803.08673v2,"This paper considers the problem of single image depth estimation. The employment of convolutional neural networks (CNNs) has recently brought about significant advancements in the research of this problem. However, most existing methods suffer from loss of spatial resolution in the estimated depth maps; a typical symptom is distorted and blurry reconstruction of object boundaries. In this paper, toward more accurate estimation with a focus on depth maps with higher spatial resolution, we propose two improvements to existing approaches. One is about the strategy of fusing features extracted at different scales, for which we propose an improved network architecture consisting of four modules: an encoder, decoder, multi-scale feature fusion module, and refinement module. The other is about loss functions for measuring inference errors used in training. We show that three loss terms, which measure errors in depth, gradients and surface normals, respectively, contribute to improvement of accuracy in an complementary fashion. Experimental results show that these two improvements enable to attain higher accuracy than the current state-of-the-arts, which is given by finer resolution reconstruction, for example, with small objects and object boundaries.",Dresiimdeestohiremawiacobbo,138.0,72.0,29.0
1791,Depth Estimation,401.0,unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction,1.0,67.0,4.0,29.0,5.0,3.4000000000000004,155.79999999999998,29,http://arxiv.org/pdf/1803.03893v3,"Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, real-world scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/Depth-VO-Feat.",Dunleofmodeesanviodwidefere,334.0,53.0,38.0
1792,Depth Estimation,5.0,monocular depth estimation based on deep learning: an overview,5.0,201.0,1.0,12.0,5.0,3.4,85.5,30,https://arxiv.org/pdf/2003.06620,"Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semi-supervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.",Dmodeesbaondeleanov,30.0,130.0,3.0
1793,Depth Estimation,12.0,sdc-depth: semantic divide-and-conquer network for monocular depth estimation,5.0,201.0,1.0,19.0,5.0,3.4,89.7,31,https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_SDC-Depth_Semantic_Divide-and-Conquer_Network_for_Monocular_Depth_Estimation_CVPR_2020_paper.pdf,"Monocular depth estimation is an ill-posed problem, and as such critically relies on scene priors and semantics. Due to its complexity, we propose a deep neural network model based on a semantic divide-and-conquer approach. Our model decomposes a scene into semantic segments, such as object instances and background stuff classes, and then predicts a scale and shift invariant depth map for each semantic segment in a canonical space. Semantic segments of the same category share the same depth decoder, so the global depth prediction task is decomposed into a series of category-specific ones, which are simpler to learn and easier to generalize to new scene types. Finally, our model stitches each local depth segment by predicting its scale and shift based on the global context of the image. The model is trained end-to-end using a multi-task loss for panoptic segmentation and depth prediction, and is therefore able to leverage large-scale panoptic segmentation datasets to boost its semantic understanding. We validate the effectiveness of our approach and show state-of-the-art performance on three benchmark datasets.",Dsdsedinefomodees,28.0,45.0,0.0
1794,Depth Estimation,24.0,"superdepth: self-supervised, super-resolved monocular depth estimation",5.0,201.0,1.0,27.0,5.0,3.4,95.7,32,https://arxiv.org/pdf/1810.01849,"Recent techniques in self-supervised monocular depth estimation are approaching the performance of supervised methods, but operate in low resolution only. We show that high resolution is key towards high-fidelity self-supervised monocular depth prediction. Inspired by recent deep learning methods for Single-Image Super-Resolution, we propose a subpixel convolutional layer extension for depth super-resolution that accurately synthesizes high-resolution disparities from their corresponding low-resolution convolutional features. In addition, we introduce a differentiable flip-augmentation layer that accurately fuses predictions from the image and its horizontally flipped version, reducing the effect of left and right shadow regions generated in the disparity map due to occlusions. Both contributions provide significant performance gains over the state-of-the-art in self-supervised depth and pose estimation on the public KITTI benchmark. A video of our approach can be found at https://youtu.be/jKNgBeBMx0I.",Dsusesumodees,97.0,42.0,5.0
1795,Depth Estimation,31.0,towards scene understanding: unsupervised monocular depth estimation with semantic-aware representation,5.0,201.0,1.0,25.0,5.0,3.4,97.2,33,http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Towards_Scene_Understanding_Unsupervised_Monocular_Depth_Estimation_With_Semantic-Aware_Representation_CVPR_2019_paper.pdf,"Monocular depth estimation is a challenging task in scene understanding, with the goal to acquire the geometric properties of 3D space from 2D images. Due to the lack of RGB-depth image pairs, unsupervised learning methods aim at deriving depth information with alternative supervision such as stereo pairs. However, most existing works fail to model the geometric structure of objects, which generally results from considering pixel-level objective functions during training. In this paper, we propose SceneNet to overcome this limitation with the aid of semantic understanding from segmentation. Moreover, our proposed model is able to perform region-aware depth estimation by enforcing semantics consistency between stereo pairs. In our experiments, we qualitatively and quantitatively verify the effectiveness and robustness of our model, which produces favorable results against the state-of-the-art approaches do.",Dtoscununmodeeswisere,92.0,28.0,15.0
1796,Depth Estimation,66.0,attention-based context aggregation network for monocular depth estimation,4.0,169.0,3.0,132.0,3.0,3.3000000000000003,127.0,34,https://arxiv.org/pdf/1901.10137,"Depth estimation is a traditional computer vision task, which plays a crucial role in understanding 3D scene geometry. Recently, deep-convolutional-neural-networks based methods have achieved promising results in the monocular depth estimation field. Specifically, the framework that combines the multi-scale features extracted by the dilated convolution based block (atrous spatial pyramid pooling, ASPP) has gained the significant improvement in the dense labeling task. However, the discretized and predefined dilation rates cannot capture the continuous context information that differs in diverse scenes and easily introduce the grid artifacts in depth estimation. In this paper, we propose an attention-based context aggregation network (ACAN) to tackle these difficulties. Based on the self-attention model, ACAN adaptively learns the task-specific similarities between pixels to model the context information. First, we recast the monocular depth estimation as a dense labeling multi-class classification problem. Then we propose a soft ordinal inference to transform the predicted probabilities to continuous depth values, which can reduce the discretization error (about 1% decrease in RMSE). Second, the proposed ACAN aggregates both the image-level and pixel-level context information for depth estimation, where the former expresses the statistical characteristic of the whole image and the latter extracts the long-range spatial dependencies for each pixel. Third, for further reducing the inconsistency between the RGB image and depth map, we construct an attention loss to minimize their information entropy. We evaluate on public monocular depth-estimation benchmark datasets (including NYU Depth V2, KITTI). The experiments demonstrate the superiority of our proposed ACAN and achieve the competitive results with the state of the arts.",Datcoagnefomodees,22.0,63.0,1.0
1797,Depth Estimation,401.0,semantically-guided representation learning for self-supervised monocular depth,1.0,27.0,5.0,190.0,3.0,3.2,188.1,35,http://arxiv.org/abs/1606.08315v1,"Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories.",Dserelefosemode,62.0,49.0,18.0
1798,Depth Estimation,2.0,monocular depth estimation: a survey,5.0,201.0,1.0,43.0,4.0,3.1,93.9,36,https://arxiv.org/pdf/1901.09402,"Monocular depth estimation is often described as an ill-posed and inherently ambiguous problem. Estimating depth from 2D images is a crucial step in scene reconstruction, 3Dobject recognition, segmentation, and detection. The problem can be framed as: given a single RGB image as input, predict a dense depth map for each pixel. This problem is worsened by the fact that most scenes have large texture and structural variations, object occlusions, and rich geometric detailing. All these factors contribute to difficulty in accurate depth estimation. In this paper, we review five papers that attempt to solve the depth estimation problem with various techniques including supervised, weakly-supervised, and unsupervised learning techniques. We then compare these papers and understand the improvements made over one another. Finally, we explore potential improvements that can aid to better solve this problem.",Dmodeesasu,45.0,23.0,2.0
1799,Depth Estimation,14.0,monocular depth estimation using relative depth maps,5.0,201.0,1.0,42.0,4.0,3.1,97.2,37,https://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Monocular_Depth_Estimation_Using_Relative_Depth_Maps_CVPR_2019_paper.pdf,"We propose a novel algorithm for monocular depth estimation using relative depth maps. First, using a convolutional neural network, we estimate relative depths between pairs of regions, as well as ordinary depths, at various scales. Second, we restore relative depth maps from selectively estimated data based on the rank-1 property of pairwise comparison matrices. Third, we decompose ordinary and relative depth maps into components and recombine them optimally to reconstruct a final depth map. Experimental results show that the proposed algorithm provides the state-of-art depth estimation performance.",Dmodeesusredema,45.0,70.0,1.0
1800,Depth Estimation,44.0,deep learning-based monocular depth estimation methods—a state-of-the-art review,4.0,201.0,1.0,23.0,5.0,3.1,100.5,38,https://www.mdpi.com/1424-8220/20/8/2272/pdf,"Monocular depth estimation from Red-Green-Blue (RGB) images is a well-studied ill-posed problem in computer vision which has been investigated intensively over the past decade using Deep Learning (DL) approaches. The recent approaches for monocular depth estimation mostly rely on Convolutional Neural Networks (CNN). Estimating depth from two-dimensional images plays an important role in various applications including scene reconstruction, 3D object-detection, robotics and autonomous driving. This survey provides a comprehensive overview of this research topic including the problem representation and a short description of traditional methods for depth estimation. Relevant datasets and 13 state-of-the-art deep learning-based approaches for monocular depth estimation are reviewed, evaluated and discussed. We conclude this paper with a perspective towards future research work requiring further investigation in monocular depth estimation challenges.",Ddelemodeesmestre,14.0,86.0,0.0
1801,Depth Estimation,62.0,self-supervised monocular trained depth estimation using self-attention and discrete disparity volume,4.0,201.0,1.0,8.0,5.0,3.1,101.4,39,https://openaccess.thecvf.com/content_CVPR_2020/papers/Johnston_Self-Supervised_Monocular_Trained_Depth_Estimation_Using_Self-Attention_and_Discrete_Disparity_CVPR_2020_paper.pdf,"Monocular depth estimation has become one of the most studied applications in computer vision, where the most accurate approaches are based on fully supervised learning models. However, the acquisition of accurate and large ground truth data sets to model these fully supervised methods is a major challenge for the further development of the area. Self-supervised methods trained with monocular videos constitute one the most promising approaches to mitigate the challenge mentioned above due to the wide-spread availability of training data. Consequently, they have been intensively studied, where the main ideas explored consist of different types of model architectures, loss functions, and occlusion masks to address non-rigid motion. In this paper, we propose two new ideas to improve self-supervised monocular trained depth estimation: 1) self-attention, and 2) discrete disparity prediction. Compared with the usual localised convolution operation, self-attention can explore a more general contextual information that allows the inference of similar disparity values at non-contiguous regions of the image. Discrete disparity prediction has been shown by fully supervised methods to provide a more robust and sharper depth estimation than the more common continuous disparity prediction, besides enabling the estimation of depth uncertainty. We show that the extension of the state-of-the-art self-supervised monocular trained depth estimator Monodepth2 with these two ideas allows us to design a model that produces the best results in the field in KITTI 2015 and Make3D, closing the gap with respect self-supervised stereo training and fully supervised approaches.",Dsemotrdeesusseandidivo,47.0,74.0,4.0
1802,Depth Estimation,30.0,deep optics for monocular depth estimation and 3d object detection,5.0,201.0,1.0,41.0,4.0,3.1,101.7,40,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.pdf,"Depth estimation and 3D object detection are critical for scene understanding but remain challenging to perform with a single image due to the loss of 3D information during image capture. Recent models using deep neural networks have improved monocular depth estimation performance, but there is still difficulty in predicting absolute depth and generalizing outside a standard dataset. Here we introduce the paradigm of deep optics, i.e. end-to-end design of optics and image processing, to the monocular depth estimation problem, using coded defocus blur as an additional depth cue to be decoded by a neural network. We evaluate several optical coding strategies along with an end-to-end optimization scheme for depth estimation on three datasets, including NYU Depth v2 and KITTI. We find an optimized freeform lens design yields the best results, but chromatic aberration from a singlet lens offers significantly improved performance as well. We build a physical prototype and validate that chromatic aberrations improve depth estimation on real-world results. In addition, we train object detection networks on the KITTI dataset and show that the lens optimized for depth estimation also results in improved 3D object detection performance.",Ddeopfomodeesan3dobde,60.0,55.0,1.0
1803,Depth Estimation,11.0,adadepth: unsupervised content congruent adaptation for depth estimation,5.0,201.0,1.0,61.0,4.0,3.1,102.0,41,http://openaccess.thecvf.com/content_cvpr_2018/papers/Kundu_AdaDepth_Unsupervised_content_cvpr_2018_paper.pdf,"Supervised deep learning methods have shown promising results for the task of monocular depth estimation; but acquiring ground truth is costly, and prone to noise as well as inaccuracies. While synthetic datasets have been used to circumvent above problems, the resultant models do not generalize well to natural scenes due to the inherent domain shift. Recent adversarial approaches for domain adaption have performed well in mitigating the differences between the source and target domains. But these methods are mostly limited to a classification setup and do not scale well for fully-convolutional architectures. In this work, we propose AdaDepth - an unsupervised domain adaptation strategy for the pixel-wise regression task of monocular depth estimation. The proposed approach is devoid of above limitations through a) adversarial learning and b) explicit imposition of content consistency on the adapted target representation. Our unsupervised approach performs competitively with other established approaches on depth estimation tasks and achieves state-of-the-art results in a semi-supervised setting.",Daduncocoadfodees,107.0,55.0,9.0
1804,Depth Estimation,58.0,revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers,4.0,62.0,4.0,201.0,1.0,3.1,102.5,42,https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Revisiting_Stereo_Depth_Estimation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_ICCV_2021_paper.pdf,"Stereo depth estimation relies on optimal correspondence matching between pixels on epipolar lines in the left and right images to infer depth. In this work, we revisit the problem from a sequence-to-sequence correspondence perspective to replace cost volume construction with dense pixel matching using position information and attention. This approach, named STereo TRansformer (STTR), has several advantages: It 1) relaxes the limitation of a fixed disparity range, 2) identifies occluded regions and provides confidence estimates, and 3) imposes uniqueness constraints during the matching process. We report promising results on both synthetic and real-world datasets and demonstrate that STTR generalizes across different domains, even without fine-tuning.",Drestdeesfrasepewitr,9.0,53.0,0.0
1805,Depth Estimation,15.0,look deeper into depth: monocular depth estimation with semantic booster and attention-driven loss,5.0,201.0,1.0,63.0,4.0,3.1,103.8,43,http://openaccess.thecvf.com/content_ECCV_2018/papers/Jianbo_Jiao_Look_Deeper_into_ECCV_2018_paper.pdf,"Monocular depth estimation benefits greatly from learning based techniques. By studying the training data, we observe that the per-pixel depth values in existing datasets typically exhibit a long-tailed distribution. However, most previous approaches treat all the regions in the training data equally regardless of the imbalanced depth distribution, which restricts the model performance particularly on distant depth regions. In this paper, we investigate the long tail property and delve deeper into the distant depth regions (i.e. the tail part) to propose an attention-driven loss for the network supervision. In addition, to better leverage the semantic information for monocular depth estimation, we propose a synergy network to automatically learn the information sharing strategies between the two tasks. With the proposed attention-driven loss and synergy network, the depth estimation and semantic labeling tasks can be mutually improved. Experiments on the challenging indoor dataset show that the proposed approach achieves state-of-the-art performance on both monocular depth estimation and semantic labeling tasks.",Dlodeindemodeeswiseboanatlo,101.0,60.0,9.0
1806,Depth Estimation,19.0,deep convolutional neural fields for depth estimation from a single image,5.0,201.0,1.0,66.0,4.0,3.1,105.9,44,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Deep_Convolutional_Neural_2015_CVPR_paper.pdf,"We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets.",Ddeconefifodeesfrasiim,676.0,24.0,53.0
1807,Depth Estimation,27.0,joint task-recursive learning for semantic segmentation and depth estimation,5.0,201.0,1.0,69.0,4.0,3.1,109.2,45,https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenyu_Zhang_Joint_Task-Recursive_Learning_ECCV_2018_paper.pdf,"In this paper, we propose a novel joint Task-Recursive Learning (TRL) framework for the closing-loop semantic segmentation and monocular depth estimation tasks. TRL can recursively refine the results of both tasks through serialized task-level interactions. In order to mutually-boost for each other, we encapsulate the interaction into a specific Task-Attentional Module (TAM) to adaptively enhance some counterpart patterns of both tasks. Further, to make the inference more credible, we propagate previous learning experiences on both tasks into the next network evolution by explicitly concatenating previous responses. The sequence of task-level interactions are finally evolved along a coarse-to-fine scale space such that the required details may be reconstructed progressively. Extensive experiments on NYU-Depth v2 and SUN RGB-D datasets demonstrate that our method achieves state-of-the-art results for monocular depth estimation and semantic segmentation.",Djotalefoseseandees,83.0,54.0,6.0
1808,Depth Estimation,13.0,learning monocular depth estimation with unsupervised trinocular assumptions,5.0,201.0,1.0,83.0,4.0,3.1,109.20000000000002,46,https://arxiv.org/pdf/1808.01606,"Obtaining accurate depth measurements out of a single image represents a fascinating solution to 3D sensing. CNNs led to considerable improvements in this field, and recent trends replaced the need for ground-truth labels with geometry-guided image reconstruction signals enabling unsupervised training. Currently, for this purpose, state-of-the-art techniques rely on images acquired with a binocular stereo rig to predict inverse depth (i.e., disparity) according to the aforementioned supervision principle. However, these methods suffer from well-known problems near occlusions, left image border, etc inherited from the stereo setup. Therefore, in this paper, we tackle these issues by moving to a trinocular domain for training. Assuming the central image as the reference, we train a CNN to infer disparity representations pairing such image with frames on its left and right side. This strategy allows obtaining depth maps not affected by typical stereo artifacts. Moreover, being trinocular datasets seldom available, we introduce a novel interleaved training procedure enabling to enforce the trinocular assumption outlined from current binocular datasets. Exhaustive experimental results on the KITTI dataset confirm that our proposal outperforms state-of-the-art methods for unsupervised monocular depth estimation trained on binocular stereo pairs as well as any known methods relying on other cues.",Dlemodeeswiuntras,65.0,47.0,5.0
1809,Depth Estimation,63.0,refine and distill: exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation,4.0,201.0,1.0,34.0,5.0,3.1,109.5,47,https://openaccess.thecvf.com/content_CVPR_2019/papers/Pilzer_Refine_and_Distill_Exploiting_Cycle-Inconsistency_and_Knowledge_Distillation_for_Unsupervised_CVPR_2019_paper.pdf,"Nowadays, the majority of state of the art monocular depth estimation techniques are based on supervised deep learning models. However, collecting RGB images with associated depth maps is a very time consuming procedure. Therefore, recent works have proposed deep architectures for addressing the monocular depth prediction task as a reconstruction problem, thus avoiding the need of collecting ground-truth depth. Following these works, we propose a novel self-supervised deep model for estimating depth maps. Our framework exploits two main strategies: refinement via cycle-inconsistency and distillation. Specifically, first a student network is trained to predict a disparity map such as to recover from a frame in a camera view the associated image in the opposite view. Then, a backward cycle network is applied to the generated image to re-synthesize back the input image, estimating the opposite disparity. A third network exploits the inconsistency between the original and the reconstructed input frame in order to output a refined depth map. Finally, knowledge distillation is exploited, such as to transfer information from the refinement network to the student. Our extensive experimental evaluation demonstrate the effectiveness of the proposed framework which outperforms state of the art unsupervised methods on the KITTI benchmark.",Dreandiexcyankndifounmodees,66.0,46.0,3.0
1810,Depth Estimation,32.0,single-image depth estimation based on fourier domain analysis,5.0,201.0,1.0,73.0,4.0,3.1,111.9,48,http://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_Single-Image_Depth_Estimation_CVPR_2018_paper.pdf,"We propose a deep learning algorithm for single-image depth estimation based on the Fourier frequency domain analysis. First, we develop a convolutional neural network structure and propose a new loss function, called depth-balanced Euclidean loss, to train the network reliably for a wide range of depths. Then, we generate multiple depth map candidates by cropping input images with various cropping ratios. In general, a cropped image with a small ratio yields depth details more faithfully, while that with a large ratio provides the overall depth distribution more reliably. To take advantage of these complementary properties, we combine the multiple candidates in the frequency domain. Experimental results demonstrate that proposed algorithm provides the state-of-art performance. Furthermore, through the frequency domain analysis, we validate the efficacy of the proposed algorithm in most frequency bands.",Dsideesbaonfodoan,72.0,43.0,2.0
1811,Depth Estimation,72.0,ganvo: unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks,4.0,201.0,1.0,40.0,5.0,3.1,114.0,49,https://arxiv.org/pdf/1809.05786,"In the last decade, supervised deep learning approaches have been extensively employed in visual odometry (VO) applications, which is not feasible in environments where labelled data is not abundant. On the other hand, unsupervised deep learning approaches for localization and mapping in unknown environments from unlabelled data have received comparatively less attention in VO research. In this study, we propose a generative unsupervised learning framework that predicts 6-DoF pose camera motion and monocular depth map of the scene from unlabelled RGB image sequences, using deep convolutional Generative Adversarial Networks (GANs). We create a supervisory signal by warping view sequences and assigning the re-projection minimization to the objective loss function that is adopted in multi-view pose estimation and single-view depth generation network. Detailed quantitative and qualitative evaluations of the proposed framework on the KITTI [1] and Cityscapes [2] datasets show that the proposed method outperforms both existing traditional and unsupervised deep VO methods providing better results for both pose estimation and depth recovery.",Dgaundemoviodandeeswigeadne,66.0,46.0,4.0
1812,Depth Estimation,23.0,on regression losses for deep depth estimation,5.0,201.0,1.0,92.0,4.0,3.1,114.9,50,https://hal.archives-ouvertes.fr/hal-01925321/file/DTIS18200.1541512021_preprint.pdf,"Depth estimation from a single monocular image has reached great performances thanks to recent works based on deep networks. However, as various choices of losses, architectures and experimental conditions are proposed in the literature, it is difficult to establish their respective influence on the performances. In this paper we propose an in-depth study of various losses and experimental conditions for depth regression, on NYUv2 dataset. From this study we propose a new network for depth estimation combining an encoder-decoder architecture with an adversarial loss. This network reaches top scores in the competitive evaluation of NUYv2 dataset while being simpler to train in a single phase.",Donrelofodedees,27.0,25.0,3.0
1813,Depth Estimation,401.0,geometry-aware symmetric domain adaptation for monocular depth estimation,1.0,137.0,3.0,33.0,5.0,3.0,185.0,51,http://arxiv.org/pdf/1904.01870v1,"Supervised depth estimation has achieved high accuracy due to the advanced deep network architectures. Since the groundtruth depth labels are hard to obtain, recent methods try to learn depth estimation networks in an unsupervised way by exploring unsupervised cues, which are effective but less reliable than true labels. An emerging way to resolve this dilemma is to transfer knowledge from synthetic images with ground truth depth via domain adaptation techniques. However, these approaches overlook specific geometric structure of the natural images in the target domain (i.e., real data), which is important for high-performing depth prediction. Motivated by the observation, we propose a geometry-aware symmetric domain adaptation framework (GASDA) to explore the labels in the synthetic data and epipolar geometry in the real data jointly. Moreover, by training two image style translators and depth estimators symmetrically in an end-to-end network, our model achieves better image style transfer and generates high-quality depth maps. The experimental results demonstrate the effectiveness of our proposed method and comparable performance against the state-of-the-art.",Dgesydoadfomodees,72.0,64.0,8.0
1814,Depth Estimation,401.0,dense depth estimation in monocular endoscopy with self-supervised learning methods,1.0,171.0,3.0,14.0,5.0,3.0,192.9,52,http://arxiv.org/pdf/1902.07766v2,"We present a self-supervised approach to training convolutional neural networks for dense depth estimation from monocular endoscopy data without a priori modeling of anatomy or shading. Our method only requires monocular endoscopic videos and a multi-view stereo method, e.g., structure from motion, to supervise learning in a sparse manner. Consequently, our method requires neither manual labeling nor patient computed tomography (CT) scan in the training and application phases. In a cross-patient experiment using CT scans as groundtruth, the proposed method achieved submillimeter mean residual error. In a comparison study to recent self-supervised depth estimation methods designed for natural video on in vivo sinus endoscopy data, we demonstrate that the proposed approach outperforms the previous methods by a large margin. The source code for this work is publicly available online at https://github.com/lppllppl920/EndoscopyDepthEstimation-Pytorch.",Ddedeesinmoenwiseleme,24.0,31.0,1.0
1815,Depth Estimation,401.0,single image depth estimation trained via depth from defocus cues,1.0,159.0,3.0,37.0,5.0,3.0,195.0,53,http://arxiv.org/pdf/2001.05036v1,"Estimating depth from a single RGB images is a fundamental task in computer vision, which is most directly solved using supervised deep learning. In the field of unsupervised learning of depth from a single RGB image, depth is not given explicitly. Existing work in the field receives either a stereo pair, a monocular video, or multiple views, and, using losses that are based on structure-from-motion, trains a depth estimation network. In this work, we rely, instead of different views, on depth from focus cues. Learning is based on a novel Point Spread Function convolutional layer, which applies location specific kernels that arise from the Circle-Of-Confusion in each image location. We evaluate our method on data derived from five common datasets for depth estimation and lightfield images, and present results that are on par with supervised methods on KITTI and Make3D datasets and outperform unsupervised learning approaches. Since the phenomenon of depth from defocus is not dataset specific, we hypothesize that learning based on it would overfit less to the specific content in each dataset. Our experiments show that this is indeed the case, and an estimator learned on one dataset using our method provides better results on other datasets, than the directly supervised methods.",Dsiimdeestrvidefrdecu,41.0,49.0,4.0
1816,Depth Estimation,47.0,robust semi-supervised monocular depth estimation with reprojected distances,4.0,201.0,1.0,54.0,4.0,2.8,110.7,54,http://proceedings.mlr.press/v100/guizilini20a/guizilini20a.pdf,"Dense depth estimation from a single image is a key problem in computer vision, with exciting applications in a multitude of robotic tasks. Initially viewed as a direct regression problem, requiring annotated labels as supervision at training time, in the past few years a substantial amount of work has been done in self-supervised depth training based on strong geometric cues, both from stereo cameras and more recently from monocular video sequences. In this paper we investigate how these two approaches (supervised & self-supervised) can be effectively combined, so that a depth model can learn to encode true scale from sparse supervision while achieving high fidelity local accuracy by leveraging geometric cues. To this end, we propose a novel supervised loss term that complements the widely used photometric loss, and show how it can be used to train robust semi-supervised monocular depth estimation models. Furthermore, we evaluate how much supervision is actually necessary to train accurate scale-aware monocular depth models, showing that with our proposed framework, very sparse LiDAR information, with as few as 4 beams (less than 100 valid depth values per image), is enough to achieve results competitive with the current state-of-the-art.",Drosemodeeswiredi,20.0,34.0,0.0
1817,Depth Estimation,48.0,deep monocular depth estimation via integration of global and local predictions,4.0,201.0,1.0,81.0,4.0,2.8,119.1,55,http://cvl.ewha.ac.kr/assets/journal/2018-TIP-Kim-Monodepth.pdf,"Recent works on machine learning have greatly advanced the accuracy of single image depth estimation. However, the resulting depth images are still over-smoothed and perceptually unsatisfying. This paper casts depth prediction from single image as a parametric learning problem. Specifically, we propose a deep variational model that effectively integrates heterogeneous predictions from two convolutional neural networks (CNNs), named global and local networks. They have contrasting network architecture and are designed to capture the depth information with complementary attributes. These intermediate outputs are then combined in the integration network based on the variational framework. By unrolling the optimization steps of Split Bregman iterations in the integration network, our model can be trained in an end-to-end manner. This enables one to simultaneously learn an efficient parameterization of the CNNs and hyper-parameter in the variational method. Finally, we offer a new data set of 0.22 million RGB-D images captured by Microsoft Kinect v2. Our model generates realistic and discontinuity-preserving depth prediction without involving any low-level segmentation or superpixels. Intensive experiments demonstrate the superiority of the proposed method in a range of RGB-D benchmarks, including both indoor and outdoor scenarios.",Ddemodeesviinofglanlopr,39.0,56.0,5.0
1818,Depth Estimation,69.0,monocular depth estimation with hierarchical fusion of dilated cnns and soft-weighted-sum inference,4.0,201.0,1.0,65.0,4.0,2.8,120.6,56,https://arxiv.org/pdf/1708.02287,"Monocular depth estimation is a challenging task in complex compositions depicting multiple objects of diverse scales. Albeit the recent great progress thanks to the deep convolutional neural networks (CNNs), the state-of-the-art monocular depth estimation methods still fall short to handle such real-world challenging scenarios. In this paper, we propose a deep end-to-end learning framework to tackle these challenges, which learns the direct mapping from a color image to the corresponding depth map. First, we represent monocular depth estimation as a multi-category dense labeling task by contrast to the regression based formulation. In this way, we could build upon the recent progress in dense labeling such as semantic segmentation. Second, we fuse different side-outputs from our front-end dilated convolutional neural network in a hierarchical way to exploit the multi-scale depth cues for depth estimation, which is critical to achieve scale-aware depth estimation. Third, we propose to utilize soft-weighted-sum inference instead of the hard-max inference, transforming the discretized depth score to continuous depth value. Thus, we reduce the influence of quantization error and improve the robustness of our method. Extensive experiments on the NYU Depth V2 and KITTI datasets show the superiority of our method compared with current state-of-the-art methods. Furthermore, experiments on the NYU V2 dataset reveal that our model is able to learn the probability distribution of depth.",Dmodeeswihifuofdicnansoin,91.0,49.0,6.0
1819,Depth Estimation,43.0,rethinking monocular depth estimation with adversarial training,4.0,201.0,1.0,94.0,4.0,2.8,121.5,57,https://arxiv.org/pdf/1808.07528,"Monocular depth estimation is an extensively studied computer vision problem with a vast variety of applications. Deep learning-based methods have demonstrated promise for both supervised and unsupervised depth estimation from monocular images. Most existing approaches treat depth estimation as a regression problem with a local pixel-wise loss function. In this work, we innovate beyond existing approaches by using adversarial training to learn a context-aware, non-local loss function. Such an approach penalizes the joint configuration of predicted depth values at the patch-level instead of the pixel-level, which allows networks to incorporate more global information. In this framework, the generator learns a mapping between RGB images and its corresponding depth map, while the discriminator learns to distinguish depth map and RGB pairs from ground truth. This conditional GAN depth estimation framework is stabilized using spectral normalization to prevent mode collapse when learning from diverse datasets. We test this approach using a diverse set of generators that include U-Net and joint CNN-CRF. We benchmark this approach on the NYUv2, Make3D and KITTI datasets, and observe that adversarial training reduces relative error by several fold, achieving state-of-the-art performance.",Dremodeeswiadtr,27.0,64.0,1.0
1820,Depth Estimation,65.0,depth estimation from a single image using deep learned phase coded mask,4.0,201.0,1.0,76.0,4.0,2.8,122.7,58,http://arxiv.org/pdf/2104.04641v1,"Depth estimation from a single image is a well-known challenge in computer vision. With the advent of deep learning, several approaches for monocular depth estimation have been proposed, all of which have inherent limitations due to the scarce depth cues that exist in a single image. Moreover, these methods are very demanding computationally, which makes them inadequate for systems with limited processing power. In this paper, a phase-coded aperture camera for depth estimation is proposed. The camera is equipped with an optical phase mask that provides unambiguous depth-related color characteristics for the captured image. These are used for estimating the scene depth map using a fully convolutional neural network. The phase-coded aperture structure is learned jointly with the network weights using backpropagation. The strong depth cues (encoded in the image by the phase mask, designed together with the network weights) allow a much simpler neural network architecture for faster and more accurate depth estimation. Performance achieved on simulated images as well as on a real optical setup is superior to the state-of-the-art monocular depth estimation methods (both with respect to the depth accuracy and required processing power), and is competitive with more complex and expensive depth estimation methods such as light-field cameras.",Ddeesfrasiimusdelephcoma,58.0,31.0,2.0
1821,Depth Estimation,59.0,deep hierarchical guidance and regularization learning for end-to-end depth estimation,4.0,201.0,1.0,84.0,4.0,2.8,123.3,59,http://arxiv.org/pdf/1807.07987v2,"Abstract In this work, we propose a novel deep Hierarchical Guidance and Regularization (HGR) learning framework for end-to-end monocular depth estimation, which well integrates a hierarchical depth guidance network and a hierarchical regularization learning method for fine-grained depth prediction. The two properties in our proposed HGR framework can be summarized as: (1) the hierarchical depth guidance network automatically learns hierarchical depth representations by supervision guidance and multiple side conv-operations from the basic CNN, leveraging the learned hierarchical depth representations to progressively guide the upsampling and prediction process of upper deconv-layers; (2) the hierarchical regularization learning method integrates various-level information of depth maps, optimizing the network to predict depth maps with similar structure to ground truth. Comprehensive evaluations over three public benchmark datasets (including NYU Depth V2, KITTI and Make3D datasets) well demonstrate the state-of-the-art performance of our proposed depth estimation framework.",Ddehiguanrelefoendees,36.0,48.0,2.0
1822,Depth Estimation,68.0,robust light field depth estimation using occlusion-noise aware data costs,4.0,201.0,1.0,75.0,4.0,2.8,123.3,60,http://arxiv.org/pdf/2104.05971v1,"Depth estimation is essential in many light field applications. Numerous algorithms have been developed using a range of light field properties. However, conventional data costs fail when handling noisy scenes in which occlusion is present. To address this problem, we introduce a light field depth estimation method that is more robust against occlusion and less sensitive to noise. Two novel data costs are proposed, which are measured using the angular patch and refocus image, respectively. The constrained angular entropy cost (CAE) reduces the effects of the dominant occluder and noise in the angular patch, resulting in a low cost. The constrained adaptive defocus cost (CAD) provides a low cost in the occlusion region, while also maintaining robustness against noise. Integrating the two data costs is shown to significantly improve the occlusion and noise invariant capability. Cost volume filtering and graph cut optimization are applied to improve the accuracy of the depth map. Our experimental results confirm the robustness of the proposed method and demonstrate its ability to produce high-quality depth maps from a range of scenes. The proposed method outperforms other state-of-the-art light field depth estimation methods in both qualitative and quantitative evaluations.",Drolifideesusocawdaco,60.0,35.0,7.0
1823,Depth Estimation,40.0,depth estimation with occlusion modeling using light-field cameras,5.0,201.0,1.0,110.0,3.0,2.8,125.4,61,http://arxiv.org/pdf/1908.11112v1,"Light-field cameras have become widely available in both consumer and industrial applications. However, most previous approaches do not model occlusions explicitly, and therefore fail to capture sharp object boundaries. A common assumption is that for a Lambertian scene, a pixel will exhibit photo-consistency, which means all viewpoints converge to a single point when focused to its depth. However, in the presence of occlusions this assumption fails to hold, making most current approaches unreliable precisely where accurate depth information is most important - at depth discontinuities. In this paper, an occlusion-aware depth estimation algorithm is developed; the method also enables identification of occlusion edges, which may be useful in other applications. It can be shown that although photo-consistency is not preserved for pixels at occlusions, it still holds in approximately half the viewpoints. Moreover, the line separating the two view regions (occluded object versus occluder) has the same orientation as that of the occlusion edge in the spatial domain. By ensuring photo-consistency in only the occluded view region, depth estimation can be improved. Occlusion predictions can also be computed and used for regularization. Experimental results show that our method outperforms current state-of-the-art light-field depth estimation algorithms, especially near occlusion boundaries.",Ddeeswiocmouslica,119.0,33.0,15.0
1824,Depth Estimation,64.0,collaborative deconvolutional neural networks for joint depth estimation and semantic segmentation,4.0,201.0,1.0,99.0,4.0,2.8,129.3,62,http://arxiv.org/pdf/2105.02569v2,"Semantic segmentation and single-view depth estimation are two fundamental problems in computer vision. They exploit the semantic and geometric properties of images, respectively, and are thus complementary in scene understanding. In this paper, we propose a collaborative deconvolutional neural network (C-DCNN) to jointly model these two problems for mutual promotion. The C-DCNN consists of two DCNNs, of which each is for one task. The DCNNs provide a finer resolution reconstruction method and are pretrained with hierarchical supervision. The feature maps from these two DCNNs are integrated via a pointwise bilinear layer, which fuses the semantic and depth information and produces higher order features. Then, the integrated features are fed into two sibling classification layers to simultaneously learn for semantic segmentation and depth estimation. In this way, we combine the semantic and depth features in a unified deep network and jointly train them to benefit each other. Specifically, during network training, we process depth estimation as a classification problem where a soft mapping strategy is proposed to map the continuous depth values into discrete probability distributions and the cross entropy loss is used. Besides, a fully connected conditional random field is also used as postprocessing to further improve the performance of semantic segmentation, where the proximity relations of pixels on position, intensity, and depth are jointly considered. We evaluate our approach on two challenging benchmarks: NYU Depth V2 and SUN RGB-D. It is demonstrated that our approach effectively utilizes these two kinds of information and achieves state-of-the-art results on both the semantic segmentation and depth estimation tasks.",Dcodenenefojodeesansese,23.0,43.0,1.0
1825,Depth Estimation,75.0,occlusion-aware depth estimation for light field using multi-orientation epis,4.0,201.0,1.0,88.0,4.0,2.8,129.3,63,http://arxiv.org/pdf/2007.04538v2,"Abstract Epipolar plane images (EPIs) contain special linear structures that reflect the disparity of a 3D point and are widely used in light field depth estimation. However, previous EPI-based approaches only utilize horizontal and vertical EPIs to estimate local disparities and ignore diagonal directions. In order to make full use of the regular grid light field images, we develop a strategy to extract epipolar plane images in all available directions. Based on the multi-orientation EPIs, a specific EPI in which the point is not occluded is found and used to calculate robust depth estimation. We also design a novel framework to estimate the depth information which combines the local depth with edge orientation. The multi-orientation EPIs and optimal orientation selection are proved to be effective in detecting and excluding occlusions. Experimental results show that the proposed method outperforms state-of-the-art depth estimation methods, especially near occlusion boundaries.",Docdeesfolifiusmuep,33.0,37.0,2.0
1826,Depth Estimation,94.0,accurate light field depth estimation with superpixel regularization over partially occluded regions,4.0,201.0,1.0,74.0,4.0,2.8,130.8,64,https://arxiv.org/pdf/1708.01964,"Depth estimation is a fundamental problem for light field photography applications. Numerous methods have been proposed in recent years, which either focus on crafting cost terms for more robust matching, or on analyzing the geometry of scene structures embedded in the epipolar-plane images. Significant improvements have been made in terms of overall depth estimation error; however, current state-of-the-art methods still show limitations in handling intricate occluding structures and complex scenes with multiple occlusions. To address these challenging issues, we propose a very effective depth estimation framework which focuses on regularizing the initial label confidence map and edge strength weights. Specifically, we first detect partially occluded boundary regions (POBR) via superpixel-based regularization. Series of shrinkage/reinforcement operations are then applied on the label confidence map and edge strength weights over the POBR. We show that after weight manipulations, even a low-complexity weighted least squares model can produce much better depth estimation than the state-of-the-art methods in terms of average disparity error rate, occlusion boundary precision-recall rate, and the preservation of intricate visual features.",Daclifideeswisureovpaocre,59.0,43.0,3.0
1827,Depth Estimation,33.0,light field scale-depth space transform for dense depth estimation,5.0,201.0,1.0,151.0,3.0,2.8,135.60000000000002,65,https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W13/papers/Tosic_Light_Field_Scale-Depth_2014_CVPR_paper.pdf,"Recent development of hand-held plenoptic cameras has brought light field acquisition into many practical and low-cost imaging applications. We address a crucial challenge in light field data processing: dense depth estimation of 3D scenes captured by camera arrays or plenoptic cameras. We first propose a method for construction of light field scale-depth spaces, by convolving a given light field with a special kernel adapted to the light field structure. We detect local extrema in such scale-depth spaces, which indicate the regions of constant depth, and convert them to dense depth maps after solving occlusion conflicts in a consistent way across all views. Due to the multi-scale characterization of objects in proposed representations, our method provides depth estimates for both uniform and textured regions, where uniform regions with large spatial extent are captured at coarser scales and textured regions are found at finer scales. Experimental results on the HCI (Heidelberg Collaboratory for Image Processing) light field benchmark show that our method gives state of the art depth accuracy. We also show results on plenoptic images from the RAYTRIX camera and our plenoptic camera prototype.",Dlifiscsptrfodedees,57.0,25.0,2.0
1828,Depth Estimation,97.0,trust your model: light field depth estimation with inline occlusion handling,4.0,201.0,1.0,89.0,4.0,2.8,136.2,66,https://openaccess.thecvf.com/content_cvpr_2018/papers/Schilling_Trust_Your_Model_CVPR_2018_paper.pdf,"We address the problem of depth estimation from light-field images. Our main contribution is a new way to handle occlusions which improves general accuracy and quality of object borders. In contrast to all prior work we work with a model which directly incorporates both depth and occlusion, using a local optimization scheme based on the PatchMatch algorithm. The key benefit of this joint approach is that we utilize all available data, and not erroneously discard valuable information in pre-processing steps. We see the benefit of our approach not only at improved object boundaries, but also at smooth surface reconstruction, where we outperform even methods which focus on good surface regularization. We have evaluated our method on a public light-field dataset, where we achieve state-of-the-art results in nine out of twelve error metrics, with a close tie for the remaining three.",Dtryomolifideeswiinocha,30.0,16.0,0.0
1829,Depth Estimation,39.0,single underwater image enhancement using depth estimation based on blurriness,5.0,201.0,1.0,150.0,3.0,2.8,137.10000000000002,67,http://code.ucsd.edu/pcosman/ICIP2015.pdf,"In this paper, we propose to use image blurriness to estimate the depth map for underwater image enhancement. It is based on the observation that objects farther from the camera are more blurry for underwater images. Adopting image blurriness with the image formation model (IFM), we can estimate the distance between scene points and the camera and thereby recover and enhance underwater images. Experimental results on enhancing such images in different lighting conditions demonstrate the proposed method performs better than other IFM-based enhancement methods.",Dsiunimenusdeesbaonbl,69.0,15.0,4.0
1830,Depth Estimation,36.0,absolute depth estimation from a single defocused image,5.0,201.0,1.0,182.0,3.0,2.8,145.8,68,https://www.researchgate.net/profile/Jingyu-Lin-7/publication/253335957_Absolute_Depth_Estimation_From_a_Single_Defocused_Image/links/55c54dcc08aeca747d618992/Absolute-Depth-Estimation-From-a-Single-Defocused-Image.pdf,"Shape from defocus (SFD) is one of the most popular techniques in monocular 3D vision. While most SFD approaches require two or more images of the same scene captured at a fixed view point, this paper presents an efficient approach to estimate absolute depth from a single defocused image. Instead of directly measuring defocus level of each pixel, we propose to design a sequence of aperture-shape filters to segment a defocused image by defocus level. A boundary-weighted belief propagation algorithm is employed to obtain a smooth depth map. We also give an estimation of depth error. Extensive experiments show that our approach outperforms the state-of-the-art single-image SFD approaches both in precision of the estimated absolute depth and running time.",Dabdeesfrasideim,60.0,29.0,2.0
1831,Depth Estimation,401.0,parse geometry from a line: monocular depth estimation with partial laser observation,1.0,59.0,4.0,108.0,3.0,2.8,176.3,69,http://arxiv.org/pdf/1611.02174v1,"Many standard robotic platforms are equipped with at least a fixed 2D laser range finder and a monocular camera. Although those platforms do not have sensors for 3D depth sensing capability, knowledge of depth is an essential part in many robotics activities. Therefore, recently, there is an increasing interest in depth estimation using monocular images. As this task is inherently ambiguous, the data-driven estimated depth might be unreliable in robotics applications. In this paper, we have attempted to improve the precision of monocular depth estimation by introducing 2D planar observation from the remaining laser range finder without extra cost. Specifically, we construct a dense reference map from the sparse laser range data, redefining the depth estimation task as estimating the distance between the real and the reference depth. To solve the problem, we construct a novel residual of residual neural network, and tightly combine the classification and regression losses for continuous depth estimation. Experimental results suggest that our method achieves considerable promotion compared to the state-of-the-art methods on both NYUD2 and KITTI, validating the effectiveness of our method on leveraging the additional sensory information. We further demonstrate the potential usage of our method in obstacle avoidance where our methodology provides comprehensive depth information compared to the solution using monocular camera or 2D laser range finder alone.",Dpagefralimodeeswipalaob,68.0,22.0,11.0
1832,Depth Estimation,401.0,fast robust monocular depth estimation for obstacle detection with fully convolutional networks,1.0,60.0,4.0,120.0,3.0,2.8,180.3,70,http://arxiv.org/pdf/1607.06349v1,"Obstacle Detection is a central problem for any robotic system, and critical for autonomous systems that travel at high speeds in unpredictable environment. This is often achieved through scene depth estimation, by various means. When fast motion is considered, the detection range must be longer enough to allow for safe avoidance and path planning. Current solutions often make assumption on the motion of the vehicle that limit their applicability, or work at very limited ranges due to intrinsic constraints. We propose a novel appearance-based Object Detection system that is able to detect obstacles at very long range and at a very high speed (~ 300Hz), without making assumptions on the type of motion. We achieve these results using a Deep Neural Network approach trained on real and synthetic images and trading some depth accuracy for fast, robust and consistent operation. We show how photo-realistic synthetic images are able to solve the problem of training set dimension and variety typical of machine learning approaches, and how our system is robust to massive blurring of test images.",Dfaromodeesfoobdewifucone,72.0,36.0,1.0
1833,Depth Estimation,401.0,learning depth with convolutional spatial propagation network,1.0,47.0,4.0,184.0,3.0,2.8,194.3,71,http://arxiv.org/pdf/1808.00150v1,"In this paper, we propose the convolutional spatial propagation network (CSPN) and demonstrate its effectiveness for various depth estimation tasks. CSPN is a simple and efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operations, in which the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). Compare to the previous state-of-the-art (SOTA) linear propagation model, i.e., spatial propagation networks (SPN), CSPN is <inline-formula><tex-math notation=""LaTeX"">$\mathbf 2$</tex-math><alternatives><mml:math><mml:mn mathvariant=""bold"">2</mml:mn></mml:math><inline-graphic xlink:href=""cheng-ieq1-2947374.gif""/></alternatives></inline-formula> to <inline-formula><tex-math notation=""LaTeX"">$\mathbf 5\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=""bold"">5</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=""cheng-ieq2-2947374.gif""/></alternatives></inline-formula> faster in practice. We concatenate CSPN and its variants to SOTA depth estimation networks, which significantly improve the depth accuracy. Specifically, we apply CSPN to two depth estimation problems: depth completion and stereo matching, in which we design modules which adapts the original 2D CSPN to embed sparse depth samples during the propagation, operate with 3D convolution and be synergistic with spatial pyramid pooling. In our experiments, we show that all these modules contribute to the final performance. For the task of depth completion, our method reduce the depth error over 30 percent in the NYU v2 and KITTI datasets. For the task of stereo matching, our method currently ranks <inline-formula><tex-math notation=""LaTeX"">$\mathbf 1st$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=""bold"">1</mml:mn><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""cheng-ieq3-2947374.gif""/></alternatives></inline-formula> on both the KITTI Stereo 2012 and 2015 benchmarks.",Dledewicospprne,95.0,111.0,12.0
1834,Depth Estimation,401.0,"competitive collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation",1.0,52.0,4.0,199.0,3.0,2.8,200.8,72,http://arxiv.org/pdf/1805.09806v3,"We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.",Dcocojounleofdecamoopflanmose,221.0,46.0,31.0
1836,Depth Estimation,80.0,"student becoming the master: knowledge amalgamation for joint scene parsing, depth estimation, and more",4.0,185.0,3.0,201.0,1.0,2.7,158.3,73,https://openaccess.thecvf.com/content_CVPR_2019/papers/Ye_Student_Becoming_the_Master_Knowledge_Amalgamation_for_Joint_Scene_Parsing_CVPR_2019_paper.pdf,"In this paper, we investigate a novel deep-model reusing task. Our goal is to train a lightweight and versatile student model, without human-labelled annotations, that amalgamates the knowledge and masters the expertise of two pretrained teacher models working on heterogeneous problems, one on scene parsing and the other on depth estimation. To this end, we propose an innovative training strategy that learns the parameters of the student intertwined with the teachers, achieved by 'projecting' its amalgamated features onto each teacher's domain and computing the loss. We also introduce two options to generalize the proposed training strategy to handle three or more tasks simultaneously. The proposed scheme yields very encouraging results. As demonstrated on several benchmarks, the trained student model achieves results even superior to those of the teachers in their own expertise domains and on par with the state-of-the-art fully supervised models relying on human-labelled annotations.",Dstbethmaknamfojoscpadeesanmo,35.0,52.0,0.0
1837,Depth Estimation,401.0,multi-scale continuous crfs as sequential deep networks for monocular depth estimation,1.0,103.0,3.0,56.0,4.0,2.7,178.3,74,http://arxiv.org/pdf/1803.00891v1,"This paper addresses the problem of depth estimation from a single still image. Inspired by recent works on multi-scale convolutional neural networks (CNN), we propose a deep model which fuses complementary information derived from multiple CNN side outputs. Different from previous methods, the integration is obtained by means of continuous Conditional Random Fields (CRFs). In particular, we propose two different variations, one based on a cascade of multiple CRFs, the other on a unified graphical model. By designing a novel CNN implementation of mean-field updates for continuous CRFs, we show that both proposed models can be regarded as sequential deep networks and that training can be performed end-to-end. Through extensive experimental evaluation we demonstrate the effectiveness of the proposed approach and establish new state of the art results on publicly available datasets.",Dmucocrassedenefomodees,280.0,37.0,23.0
1838,Depth Estimation,401.0,real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer,1.0,126.0,3.0,59.0,4.0,2.7,188.4,75,http://arxiv.org/pdf/1907.06882v2,"Monocular depth estimation using learning-based approaches has become promising in recent years. However, most monocular depth estimators either need to rely on large quantities of ground truth depth data, which is extremely expensive and difficult to obtain, or predict disparity as an intermediary step using a secondary supervisory signal leading to blurring and other artefacts. Training a depth estimation model using pixel-perfect synthetic data can resolve most of these issues but introduces the problem of domain bias. This is the inability to apply a model trained on synthetic data to real-world scenarios. With advances in image style transfer and its connections with domain adaptation (Maximum Mean Discrepancy), we take advantage of style transfer and adversarial training to predict pixel perfect depth from a single real-world color image based on training over a large corpus of synthetic environment data. Experimental results indicate the efficacy of our approach compared to contemporary state-of-the-art techniques.",Dremodeesussydawidoadviimsttr,168.0,92.0,13.0
1839,Depth Estimation,401.0,structured attention guided convolutional neural fields for monocular depth estimation,1.0,147.0,3.0,60.0,4.0,2.7,197.1,76,http://arxiv.org/pdf/1803.11029v1,"Recent works have shown the benefit of integrating Conditional Random Fields (CRFs) models into deep architectures for improving pixel-level prediction tasks. Following this line of research, in this paper we introduce a novel approach for monocular depth estimation. Similarly to previous works, our method employs a continuous CRF to fuse multi-scale information derived from different layers of a front-end Convolutional Neural Network (CNN). Differently from past works, our approach benefits from a structured attention model which automatically regulates the amount of information transferred between corresponding features at different scales. Importantly, the proposed attention model is seamlessly integrated into the CRF, allowing end-to-end training of the entire architecture. Our extensive experimental evaluation demonstrates the effectiveness of the proposed method which is competitive with previous methods on the KITTI benchmark and outperforms the state of the art on the NYU Depth V2 dataset.",Dstatguconefifomodees,163.0,41.0,10.0
1840,Depth Estimation,401.0,structure-aware residual pyramid network for monocular depth estimation,1.0,168.0,3.0,46.0,4.0,2.7,201.3,77,http://arxiv.org/pdf/1907.06023v1,"Monocular depth estimation is an essential task for scene understanding. The underlying structure of objects and stuff in a complex scene is critical to recovering accurate and visually-pleasing depth maps. Global structure conveys scene layouts, while local structure reflects shape details. Recently developed approaches based on convolutional neural networks (CNNs) significantly improve the performance of depth estimation. However, few of them take into account multi-scale structures in complex scenes. In this paper, we propose a Structure-Aware Residual Pyramid Network (SARPN) to exploit multi-scale structures for accurate depth prediction. We propose a Residual Pyramid Decoder (RPD) which expresses global scene structure in upper levels to represent layouts, and local structure in lower levels to present shape details. At each level, we propose Residual Refinement Modules (RRM) that predict residual maps to progressively add finer structures on the coarser structure predicted at the upper level. In order to fully exploit multi-scale image features, an Adaptive Dense Feature Fusion (ADFF) module, which adaptively fuses effective features from all scales for inferring structures of each scale, is introduced. Experiment results on the challenging NYU-Depth v2 dataset demonstrate that our proposed approach achieves state-of-the-art performance in both qualitative and quantitative evaluation. The code is available at https://github.com/Xt-Chen/SARPN.",Dstrepynefomodees,23.0,31.0,3.0
1841,Depth Estimation,401.0,unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints,1.0,1.0,5.0,201.0,1.0,2.6,181.0,78,http://arxiv.org/pdf/1802.05522v2,"We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the scene, enforcing consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures.   We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists.   We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself.",Dunleofdeanegfrmovius3dgeco,416.0,40.0,52.0
1842,Depth Estimation,401.0,depth prediction without the sensors: leveraging structure for unsupervised learning from monocular videos,1.0,2.0,5.0,201.0,1.0,2.6,181.4,79,http://arxiv.org/pdf/2105.11610v1,"We propose a monocular depth estimator SC-Depth, which requires only unlabelled videos for training and enables the scale-consistent prediction at inference time. Our contributions include: (i) we propose a geometry consistency loss, which penalizes the inconsistency of predicted depths between adjacent views; (ii) we propose a self-discovered mask to automatically localize moving objects that violate the underlying static scene assumption and cause noisy signals during training; (iii) we demonstrate the efficacy of each component with a detailed ablation study and show high-quality depth estimation results in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of scale-consistent prediction, we show that our monocular-trained deep networks are readily integrated into the ORB-SLAM2 system for more robust and accurate tracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in KITTI, and it generalizes well to the KAIST dataset without additional training. Finally, we provide several demos for qualitative evaluation.",Ddeprwithselestfounlefrmovi,190.0,28.0,28.0
1843,Depth Estimation,401.0,unsupervised monocular depth learning in dynamic scenes,1.0,3.0,5.0,201.0,1.0,2.6,181.8,80,http://arxiv.org/pdf/2010.16404v2,"We present a method for jointly training the estimation of depth, ego-motion, and a dense 3D translation field of objects relative to the scene, with monocular photometric consistency being the sole source of supervision. We show that this apparently heavily underdetermined problem can be regularized by imposing the following prior knowledge about 3D translation fields: they are sparse, since most of the scene is static, and they tend to be constant for rigid moving objects. We show that this regularization alone is sufficient to train monocular depth prediction models that exceed the accuracy achieved in prior work for dynamic scenes, including methods that require semantic input. Code is at https://github.com/google-research/google-research/tree/master/depth_and_motion_learning .",Dunmodeleindysc,18.0,48.0,4.0
1844,Depth Estimation,401.0,depth from videos in the wild: unsupervised monocular depth learning from unknown cameras,1.0,4.0,5.0,201.0,1.0,2.6,182.2,81,http://arxiv.org/pdf/1904.04998v1,"We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos.",Ddefrviinthwiunmodelefrunca,144.0,55.0,20.0
1845,Depth Estimation,401.0,a general and adaptive robust loss function,1.0,6.0,5.0,201.0,1.0,2.6,183.0,82,http://arxiv.org/pdf/1701.03077v10,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.",Dageanadrolofu,160.0,44.0,13.0
1846,Depth Estimation,401.0,learning to navigate in complex environments,1.0,7.0,5.0,201.0,1.0,2.6,183.4,83,http://arxiv.org/pdf/2004.00116v4,"Existing autonomous robot navigation systems allow robots to move from one point to another in a collision-free manner. However, when facing new environments, these systems generally require re-tuning by expert roboticists with a good understanding of the inner workings of the navigation system. In contrast, even users who are unversed in the details of robot navigation algorithms can generate desirable navigation behavior in new environments via teleoperation. In this paper, we introduce APPLD, Adaptive Planner Parameter Learning from Demonstration, that allows existing navigation systems to be successfully applied to new complex environments, given only a human teleoperated demonstration of desirable navigation. APPLD is verified on two robots running different navigation systems in different environments. Experimental results show that APPLD can outperform navigation systems with the default and expert-tuned parameters, and even the human demonstrator themselves.",Dletonaincoen,594.0,35.0,34.0
1847,Depth Estimation,401.0,two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge,1.0,8.0,5.0,201.0,1.0,2.6,183.8,84,http://arxiv.org/abs/1609.09545v1,"This paper describes our submission to the 1st 3D Face Alignment in the Wild (3DFAW) Challenge. Our method builds upon the idea of convolutional part heatmap regression [1], extending it for 3D face alignment. Our method decomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z (depth) estimation. At the first stage, our method estimates the X,Y coordinates of the facial landmarks by producing a set of 2D heatmaps, one for each landmark, using convolutional part heatmap regression. Then, these heatmaps, alongside the input RGB image, are used as input to a very deep subnetwork trained via residual learning for regressing the Z coordinate. Our method ranked 1st in the 3DFAW Challenge, surpassing the second best result by more than 22%.",Dtwcopaherefoth1s3dfaalinthwi(3ch,60.0,24.0,3.0
1848,Depth Estimation,401.0,efficient attention: attention with linear complexities,1.0,11.0,5.0,201.0,1.0,2.6,185.0,85,http://arxiv.org/abs/math/0506577v3,"It is known since 1954 that every 3-manifold bounds a 4-manifold. Thus, for instance, every 3-manifold has a surgery diagram. There are several proofs of this fact, including constructive proofs, but there has been little attention to the complexity of the 4-manifold produced. Given a 3-manifold M of complexity n, we show how to construct a 4-manifold bounded by M of complexity O(n^2). Here we measure ``complexity'' of a piecewise-linear manifold by the minimum number of n-simplices in a triangulation. It is an open question whether this quadratic bound can be replaced by a linear bound.   The proof goes through the notion of ""shadow complexity"" of a 3-manifold M. A shadow of M is a well-behaved 2-dimensional spine of a 4-manifold bounded by M. We prove that, for a manifold M satisfying the Geometrization Conjecture with Gromov norm G and shadow complexity S, c_1 G <= S <= c_2 G^2 for suitable constants c_1, c_2. In particular, the manifolds with shadow complexity 0 are the graph manifolds.",Defatatwilico,47.0,49.0,5.0
1849,Depth Estimation,401.0,probabilistic and geometric depth: detecting objects in perspective,1.0,13.0,5.0,201.0,1.0,2.6,185.8,86,http://arxiv.org/pdf/cs/0208005v1,"The problem of searching for a model-based scene interpretation is analyzed within a probabilistic framework. Object models are formulated as generative models for range data of the scene. A new statistical criterion, the truncated object probability, is introduced to infer an optimal sequence of object hypotheses to be evaluated for their match to the data. The truncated probability is partly determined by prior knowledge of the objects and partly learned from data. Some experiments on sequence quality and object segmentation and recognition from stereo data are presented. The article recovers classic concepts from object recognition (grouping, geometric hashing, alignment) from the probabilistic perspective and adds insight into the optimal ordering of object hypotheses for evaluation. Moreover, it introduces point-relation densities, a key component of the truncated probability, as statistical models of local surface shape.",Dprangededeobinpe,4.0,63.0,0.0
1850,Depth Estimation,401.0,unsupervised learning of depth and ego-motion from video,1.0,14.0,5.0,201.0,1.0,2.6,186.2,87,https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Unsupervised_Learning_of_CVPR_2017_paper.pdf,"Although depth extraction with passive sensors has seen remarkable improvement with deep learning, these approaches may fail to obtain correct depth if they are exposed to environments not observed during training. Online adaptation, where the neural network trains while deployed, with unsupervised learning provides a convenient solution. However, online adaptation causes a neural network to forget the past. Thus, past training is wasted and the network is not able to provide good results if it observes past scenes. This work deals with practical online-adaptation where the input is online and temporally-correlated, and training is completely unsupervised. Regularization and replay-based methods without task boundaries are proposed to avoid catastrophic forgetting while adapting to online data. Experiments are performed on different datasets with both structure-from-motion and stereo. Results of forgetting as well as adaptation are provided, which are superior to recent methods. The proposed approach is more inline with the artificial general intelligence paradigm as the neural network learns the scene where it is deployed without any supervision (target labels and tasks) and without forgetting about the past. Code is available at github.com/umarKarim/cou_stereo and github.com/umarKarim/cou_sfm.",Dunleofdeanegfrvi,1310.0,62.0,309.0
1851,Depth Estimation,401.0,3d ken burns effect from a single image,1.0,16.0,5.0,201.0,1.0,2.6,187.0,88,http://arxiv.org/pdf/1909.05483v1,"The Ken Burns effect allows animating still images with a virtual camera scan and zoom. Adding parallax, which results in the 3D Ken Burns effect, enables significantly more compelling results. Creating such effects manually is time-consuming and demands sophisticated editing skills. Existing automatic methods, however, require multiple input images from varying viewpoints. In this paper, we introduce a framework that synthesizes the 3D Ken Burns effect from a single image, supporting both a fully automatic mode and an interactive mode with the user controlling the camera. Our framework first leverages a depth prediction pipeline, which estimates scene depth that is suitable for view synthesis tasks. To address the limitations of existing depth estimation methods such as geometric distortions, semantic distortions, and inaccurate depth boundaries, we develop a semantic-aware neural network for depth prediction, couple its estimate with a segmentation-based depth adjustment process, and employ a refinement neural network that facilitates accurate depth predictions at object boundaries. According to this depth estimate, our framework then maps the input image to a point cloud and synthesizes the resulting video frames by rendering the point cloud from the corresponding camera positions. To address disocclusions while maintaining geometrically and temporally coherent synthesis results, we utilize context-aware color- and depth-inpainting to fill in the missing information in the extreme views of the camera path, thus extending the scene geometry of the point cloud. Experiments with a wide variety of image content show that our method enables realistic synthesis results. Our study demonstrates that our system allows users to achieve better results while requiring little effort compared to existing solutions for the 3D Ken Burns effect creation.",D3dkebueffrasiim,63.0,134.0,4.0
1852,Depth Estimation,401.0,vision transformers for dense prediction,1.0,17.0,5.0,201.0,1.0,2.6,187.4,89,http://arxiv.org/pdf/2103.13413v1,"We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",Dvitrfodepr,31.0,61.0,9.0
1853,Depth Estimation,401.0,pyramid stereo matching network,1.0,21.0,5.0,201.0,1.0,2.6,189.0,90,http://openaccess.thecvf.com/content_cvpr_2018/papers/Chang_Pyramid_Stereo_Matching_CVPR_2018_paper.pdf,"Recent work has shown that depth estimation from a stereo pair of images can be formulated as a supervised learning task to be resolved with convolutional neural networks (CNNs). However, current architectures rely on patch-based Siamese networks, lacking the means to exploit context information for finding correspondence in illposed regions. To tackle this problem, we propose PSMNet, a pyramid stereo matching network consisting of two main modules: spatial pyramid pooling and 3D CNN. The spatial pyramid pooling module takes advantage of the capacity of global context information by aggregating context in different scales and locations to form a cost volume. The 3D CNN learns to regularize cost volume using stacked multiple hourglass networks in conjunction with intermediate supervision. The proposed approach was evaluated on several benchmark datasets. Our method ranked first in the KITTI 2012 and 2015 leaderboards before March 18, 2018. The codes of PSMNet are available at: https://github.com/JiaRenChang/PSMNet.",Dpystmane,597.0,33.0,197.0
1854,Depth Estimation,401.0,deeper depth prediction with fully convolutional residual networks,1.0,22.0,5.0,201.0,1.0,2.6,189.4,91,https://arxiv.org/pdf/1606.00373.pdf),"This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available.",Ddedeprwifucorene,1077.0,53.0,194.0
1855,Depth Estimation,401.0,sparse auxiliary networks for unified monocular depth prediction and completion,1.0,25.0,5.0,201.0,1.0,2.6,190.6,92,http://arxiv.org/pdf/2103.16690v1,"Estimating scene geometry from data obtained with cost-effective sensors is key for robots and self-driving cars. In this paper, we study the problem of predicting dense depth from a single RGB image (monodepth) with optional sparse measurements from low-cost active depth sensors. We introduce Sparse Auxiliary Networks (SANs), a new module enabling monodepth networks to perform both the tasks of depth prediction and completion, depending on whether only RGB images or also sparse point clouds are available at inference time. First, we decouple the image and depth map encoding stages using sparse convolutions to process only the valid depth map pixels. Second, we inject this information, when available, into the skip connections of the depth prediction network, augmenting its features. Through extensive experimental analysis on one indoor (NYUv2) and two outdoor (KITTI and DDAD) benchmarks, we demonstrate that our proposed SAN architecture is able to simultaneously learn both tasks, while achieving a new state of the art in depth prediction by a significant margin.",Dspaunefounmodepranco,0.0,62.0,0.0
1856,Depth Estimation,401.0,neural ray surfaces for self-supervised learning of depth and ego-motion,1.0,26.0,5.0,201.0,1.0,2.6,191.0,93,http://arxiv.org/pdf/2104.04532v1,"In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we showcase our method and compare to existing works on classical RGB-D fusion and learned representations.",Dnerasufoseleofdeaneg,5.0,64.0,0.0
1857,Depth Estimation,401.0,boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging,1.0,31.0,5.0,201.0,1.0,2.6,193.0,94,http://arxiv.org/pdf/2105.14021v1,"Neural networks have shown great abilities in estimating depth from a single image. However, the inferred depth maps are well below one-megapixel resolution and often lack fine-grained details, which limits their practicality. Our method builds on our analysis on how the input resolution and the scene structure affects depth estimation performance. We demonstrate that there is a trade-off between a consistent scene structure and the high-frequency details, and merge low- and high-resolution estimations to take advantage of this duality using a simple depth merging network. We present a double estimation method that improves the whole-image depth estimation and a patch selection method that adds local details to the final result. We demonstrate that by merging estimations at different resolutions with changing context, we can generate multi-megapixel depth maps with a high level of detail using a pre-trained model.",Dbomodeesmotohivicomume,2.0,68.0,0.0
1858,Depth Estimation,401.0,geo-supervised visual depth prediction,1.0,32.0,5.0,201.0,1.0,2.6,193.4,95,http://arxiv.org/pdf/1810.01011v1,"Reliable feature correspondence between frames is a critical step in visual odometry (VO) and visual simultaneous localization and mapping (V-SLAM) algorithms. In comparison with existing VO and V-SLAM algorithms, semi-direct visual odometry (SVO) has two main advantages that lead to state-of-the-art frame rate camera motion estimation: direct pixel correspondence and efficient implementation of probabilistic mapping method. This paper improves the SVO mapping by initializing the mean and the variance of the depth at a feature location according to the depth prediction from a single-image depth prediction network. By significantly reducing the depth uncertainty of the initialized map point (i.e., small variance centred about the depth prediction), the benefits are twofold: reliable feature correspondence between views and fast convergence to the true depth in order to create new map points. We evaluate our method with two outdoor datasets: KITTI dataset and Oxford Robotcar dataset. The experimental results indicate that the improved SVO mapping results in increased robustness and camera tracking accuracy.",Dgevidepr,34.0,56.0,2.0
1860,Depth Estimation,401.0,unsupervised scale-consistent depth and ego-motion learning from monocular video,1.0,35.0,5.0,201.0,1.0,2.6,194.6,96,http://arxiv.org/pdf/1812.11671v1,"At present, deep learning has been applied more and more in monocular image depth estimation and has shown promising results. The current more ideal method for monocular depth estimation is the supervised learning based on ground truth depth, but this method requires an abundance of expensive ground truth depth as the supervised labels. Therefore, researchers began to work on unsupervised depth estimation methods. Although the accuracy of unsupervised depth estimation method is still lower than that of supervised method, it is a promising research direction.   In this paper, Based on the experimental results that the stereo matching models outperforms monocular depth estimation models under the same unsupervised depth estimation model, we proposed an unsupervised monocular vision stereo matching method. In order to achieve the monocular stereo matching, we constructed two unsupervised deep convolution network models, one was to reconstruct the right view from the left view, and the other was to estimate the depth map using the reconstructed right view and the original left view. The two network models are piped together during the test phase. The output results of this method outperforms the current mainstream unsupervised depth estimation method in the challenging KITTI dataset.",Dunscdeaneglefrmovi,135.0,35.0,26.0
1861,Depth Estimation,401.0,structured knowledge distillation for dense prediction,1.0,37.0,5.0,201.0,1.0,2.6,195.4,97,http://arxiv.org/pdf/1903.04197v7,"In this work, we consider transferring the structure information from large networks to compact ones for dense prediction tasks in computer vision. Previous knowledge distillation strategies used for dense prediction tasks often directly borrow the distillation scheme for image classification and perform knowledge distillation for each pixel separately, leading to sub-optimal performance. Here we propose to distill structured knowledge from large networks to compact networks, taking into account the fact that dense prediction is a structured prediction problem. Specifically, we study two structured distillation schemes: i) pair-wise distillation that distills the pair-wise similarities by building a static graph; and ii) holistic distillation that uses adversarial training to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by experiments on three dense prediction tasks: semantic segmentation, depth estimation and object detection. Code is available at: https://git.io/StructKD",Dstkndifodepr,38.0,84.0,5.0
1863,Depth Estimation,401.0,multi-task learning as multi-objective optimization,1.0,40.0,5.0,201.0,1.0,2.6,196.6,98,http://arxiv.org/pdf/2011.02159v1,"Learned optimizers are algorithms that can themselves be trained to solve optimization problems. In contrast to baseline optimizers (such as momentum or Adam) that use simple update rules derived from theoretical principles, learned optimizers use flexible, high-dimensional, nonlinear parameterizations. Although this can lead to better performance in certain settings, their inner workings remain a mystery. How is a learned optimizer able to outperform a well tuned baseline? Has it learned a sophisticated combination of existing optimization techniques, or is it implementing completely new behavior? In this work, we address these questions by careful analysis and visualization of learned optimizers. We study learned optimizers trained from scratch on three disparate tasks, and discover that they have learned interpretable mechanisms, including: momentum, gradient clipping, learning rate schedules, and a new form of learning rate adaptation. Moreover, we show how the dynamics of learned optimizers enables these behaviors. Our results help elucidate the previously murky understanding of how learned optimizers work, and establish tools for interpreting future learned optimizers.",Dmuleasmuop,287.0,65.0,49.0
1864,Depth Estimation,110.0,fully convolutional networks for monocular retinal depth estimation and optic disc-cup segmentation,3.0,201.0,1.0,44.0,4.0,2.5,126.6,99,https://arxiv.org/pdf/1902.01040,"Glaucoma is a serious ocular disorder for which the screening and diagnosis are carried out by the examination of the optic nerve head (ONH). The color fundus image (CFI) is the most common modality used for ocular screening. In CFI, the central region which is the optic disc and the optic cup region within the disc are examined to determine one of the important cues for glaucoma diagnosis called the optic cup-to-disc ratio (CDR). CDR calculation requires accurate segmentation of optic disc and cup. Another important cue for glaucoma progression is the variation of depth in ONH region. In this paper, we first propose a deep learning framework to estimate depth from a single fundus image. For the case of monocular retinal depth estimation, we are also plagued by the labeled data insufficiency. To overcome this problem we adopt the technique of pretraining the deep network where, instead of using a denoising autoencoder, we propose a new pretraining scheme called pseudo-depth reconstruction, which serves as a proxy task for retinal depth estimation. Empirically, we show pseudo-depth reconstruction to be a better proxy task than denoising. Our results outperform the existing techniques for depth estimation on the INSPIRE dataset. To extend the use of depth map for optic disc and cup segmentation, we propose a novel fully convolutional guided network, where, along with the color fundus image the network uses the depth map as a guide. We propose a convolutional block called multimodal feature extraction block to extract and fuse the features of the color image and the guide image. We extensively evaluate the proposed segmentation scheme on three datasets- ORIGA, RIMONEr3, and DRISHTI-GS. The performance of the method is comparable and in many cases, outperforms the most recent state of the art.",Dfuconefomoredeesanopdise,29.0,35.0,4.0
1865,Depth Estimation,53.0,a taxonomy and evaluation of dense light field depth estimation algorithms,4.0,201.0,1.0,114.0,3.0,2.5,130.5,100,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w27/papers/Johannsen_A_Taxonomy_and_CVPR_2017_paper.pdf,"This paper presents the results of the depth estimation challenge for dense light fields, which took place at the second workshop on Light Fields for Computer Vision (LF4CV) in conjunction with CVPR 2017. The challenge consisted of submission to a recent benchmark [7], which allows a thorough performance analysis. While individual results are readily available on the benchmark web page http://www.lightfield-analysis.net, we take this opportunity to give a detailed overview of the current participants. Based on the algorithms submitted to our challenge, we develop a taxonomy of light field disparity estimation algorithms and give a report on the current state-ofthe- art. In addition, we include more comparative metrics, and discuss the relative strengths and weaknesses of the algorithms. Thus, we obtain a snapshot of where light field algorithm development stands at the moment and identify aspects with potential for further improvement.",Dataanevofdelifideesal,52.0,31.0,3.0
2149,Domain adaptation,38.0,unsupervised pixel-level domain adaptation with generative adversarial networks,5.0,4.0,5.0,57.0,4.0,4.7,30.1,1,http://openaccess.thecvf.com/content_cvpr_2017/papers/Bousmalis_Unsupervised_Pixel-Level_Domain_CVPR_2017_paper.pdf,"Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",Dunpidoadwigeadne,974.0,57.0,71.0
2150,Domain adaptation,14.0,cycada: cycle-consistent adversarial domain adaptation,5.0,51.0,4.0,3.0,5.0,4.6,25.5,2,http://proceedings.mlr.press/v80/hoffman18a/hoffman18a.pdf,"Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",Dcycyaddoad,1471.0,54.0,168.0
2151,Domain adaptation,7.0,unsupervised domain adaptation by backpropagation,5.0,58.0,4.0,2.0,5.0,4.6,25.90000000000001,3,http://proceedings.mlr.press/v37/ganin15.pdf,"Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). 
As the training progresses, the approach promotes the emergence of ""deep"" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. 
Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.",Dundoadbyba,2623.0,52.0,566.0
2152,Domain adaptation,15.0,adversarial discriminative domain adaptation,5.0,54.0,4.0,1.0,5.0,4.6,26.4,4,https://openaccess.thecvf.com/content_cvpr_2017/papers/Tzeng_Adversarial_Discriminative_Domain_CVPR_2017_paper.pdf,"Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.",Daddidoad,2359.0,39.0,354.0
2153,Domain adaptation,19.0,maximum classifier discrepancy for unsupervised domain adaptation,5.0,49.0,4.0,47.0,4.0,4.3,39.4,5,https://openaccess.thecvf.com/content_cvpr_2018/papers/Saito_Maximum_Classifier_Discrepancy_CVPR_2018_paper.pdf,"In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classifier networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classifier only tries to distinguish the features as a source or target and thus does not consider task-specific decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difficult because of each domain's characteristics. To solve these problems, we introduce a new approach that attempts to align distributions of source and target by utilizing the task-specific decision boundaries. We propose to maximize the discrepancy between two classifiers' outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy. Our method outperforms other methods on several datasets of image classification and semantic segmentation. The codes are available at https://github.com/mil-tokyo/MCD_DA",Dmacldifoundoad,724.0,46.0,146.0
2154,Domain adaptation,20.0,conditional adversarial domain adaptation,5.0,53.0,4.0,45.0,4.0,4.3,40.7,6,https://arxiv.org/pdf/1705.10667,"Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimodal distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. Experiments testify that the proposed approach exceeds the state-of-the-art results on five benchmark datasets.",Dcoaddoad,657.0,64.0,182.0
2155,Domain adaptation,36.0,deep coral: correlation alignment for deep domain adaptation,5.0,55.0,4.0,74.0,4.0,4.3,55.0,7,https://link.springer.com/content/pdf/10.1007/978-3-319-49409-8_35.pdf,"Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a ""frustratingly easy"" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.",Ddecocoalfodedoad,1044.0,23.0,124.0
2156,Domain adaptation,40.0,deep hashing network for unsupervised domain adaptation,5.0,72.0,4.0,68.0,4.0,4.3,61.2,8,http://openaccess.thecvf.com/content_cvpr_2017/papers/Venkateswara_Deep_Hashing_Network_CVPR_2017_paper.pdf,"In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.",Ddehanefoundoad,596.0,51.0,153.0
2157,Domain adaptation,106.0,unsupervised intra-domain adaptation for semantic segmentation through self-supervision,3.0,96.0,4.0,14.0,5.0,4.0,74.4,9,http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Unsupervised_Intra-Domain_Adaptation_for_Semantic_Segmentation_Through_Self-Supervision_CVPR_2020_paper.pdf,"Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model, from this adaptation, we separate target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard subdomain. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.",Duninadfosesethse,97.0,40.0,19.0
2158,Domain adaptation,48.0,do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation,4.0,104.0,3.0,4.0,5.0,3.9,57.2,10,http://proceedings.mlr.press/v119/liang20a/liang20a.pdf,"Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.",Ddowerenetoacthsodasohytrfoundoad,110.0,84.0,37.0
2159,Domain adaptation,88.0,adversarial domain adaptation with domain mixup,4.0,141.0,3.0,15.0,5.0,3.9,87.30000000000001,11,https://ojs.aaai.org/index.php/AAAI/article/view/6123/5979,"Recent works on domain adaptation reveal the effectiveness of adversarial learning on filling the discrepancy between source and target domains. However, two common limitations exist in current adversarial-learning-based methods. First, samples from two domains alone are not sufficient to ensure domain-invariance at most part of latent space. Second, the domain discriminator involved in these methods can only judge real or fake with the guidance of hard label, while it is more reasonable to use soft scores to evaluate the generated images or features, i.e., to fully utilize the inter-domain information. In this paper, we present adversarial domain adaptation with domain mixup (DM-ADA), which guarantees domain-invariance in a more continuous latent space and guides the domain discriminator in judging samples' difference relative to source and target domains. Domain mixup is jointly conducted on pixel and feature level to improve the robustness of models. Extensive experiments prove that the proposed approach can achieve superior performance on tasks with various degrees of domain shift and data complexity.",Daddoadwidomi,91.0,47.0,6.0
2160,Domain adaptation,196.0,optimal transport for multi-source domain adaptation under target shift,3.0,38.0,5.0,122.0,3.0,3.8,110.6,12,http://proceedings.mlr.press/v89/redko19a/redko19a.pdf,"In this paper, we propose to tackle the problem of reducing discrepancies between multiple domains referred to as multi-source domain adaptation and consider it under the target shift assumption: in all domains we aim to solve a classification problem with the same output classes, but with labels' proportions differing across them. This problem, generally ignored in the vast majority papers on domain adaptation papers, is nevertheless critical in real-world applications, and we theoretically show its impact on the adaptation success. To address this issue, we design a method based on optimal transport, a theory that has been successfully used to tackle adaptation problems in machine learning. Our method performs multi-source adaptation and target shift correction simultaneously by learning the class probabilities of the unlabeled target sample and the coupling allowing to align two (or more) probability distributions. Experiments on both synthetic and real-world data related to satellite image segmentation task show the superiority of the proposed method over the state-of-the-art.",Doptrfomudoaduntash,53.0,39.0,4.0
2161,Domain adaptation,401.0,a survey of unsupervised deep domain adaptation,1.0,24.0,5.0,8.0,5.0,3.8,132.3,13,http://arxiv.org/pdf/1812.02849v3,"Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.",Dasuofundedoad,146.0,511.0,5.0
2162,Domain adaptation,105.0,domain adaptation for structured output via discriminative patch representations,3.0,63.0,4.0,95.0,4.0,3.7,85.2,14,https://openaccess.thecvf.com/content_ICCV_2019/papers/Tsai_Domain_Adaptation_for_Structured_Output_via_Discriminative_Patch_Representations_ICCV_2019_paper.pdf,"Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn supervised models like convolutional neural networks. However, models trained on one data domain may not generalize well to other domains without annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. We propose to learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. With such representations as guidance, we use an adversarial learning scheme to push the feature representations of target patches in the clustered space closer to the distributions of source patches. In addition, we show that our framework is complementary to existing domain adaptation techniques and achieves consistent improvements on semantic segmentation. Extensive ablations and results are demonstrated on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios.",Ddoadfostouvidipare,120.0,62.0,18.0
2163,Domain adaptation,84.0,continuously indexed domain adaptation,4.0,196.0,3.0,49.0,4.0,3.6000000000000005,118.3,15,https://arxiv.org/pdf/2007.01807,"Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B). However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains. In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the state-of-the-art domain adaption methods on both synthetic and real-world medical datasets.",Dcoindoad,18.0,31.0,4.0
2164,Domain adaptation,75.0,domain adaptation for semantic segmentation with maximum squares loss,4.0,183.0,3.0,98.0,4.0,3.6000000000000005,125.1,16,http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Domain_Adaptation_for_Semantic_Segmentation_With_Maximum_Squares_Loss_ICCV_2019_paper.pdf,"Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.",Ddoadfosesewimasqlo,98.0,36.0,14.0
2165,Domain adaptation,11.0,unsupervised domain adaptation with residual transfer networks,5.0,136.0,3.0,115.0,3.0,3.6,92.2,17,https://arxiv.org/pdf/1602.04433,"The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.",Dundoadwiretrne,856.0,51.0,101.0
2166,Domain adaptation,162.0,a curriculum domain adaptation approach to the semantic segmentation of urban scenes,3.0,145.0,3.0,23.0,5.0,3.6,113.5,18,https://arxiv.org/pdf/1812.09953,"During the last half decade, convolutional neural networks (CNNs) have triumphed over semantic segmentation, which is one of the core tasks in many applications such as autonomous driving and augmented reality. However, to train CNNs requires a considerable amount of data, which is difficult to collect and laborious to annotate. Recent advances in computer graphics make it possible to train CNNs on photo-realistic synthetic imagery with computer-generated annotations. Despite this, the domain mismatch between real images and the synthetic data hinders the models’ performance. Hence, we propose a curriculum-style learning approach to minimizing the domain gap in urban scene semantic segmentation. The curriculum domain adaptation solves easy tasks first to infer necessary properties about the target domain; in particular, the first task is to learn global label distributions over images and local distributions over landmark superpixels. These are easy to estimate because images of urban scenes have strong idiosyncrasies (e.g., the size and spatial relations of buildings, streets, cars, etc.). We then train a segmentation network, while regularizing its predictions in the target domain to follow those inferred properties. In experiments, our method outperforms the baselines on two datasets and three backbone networks. We also report extensive ablation studies about our approach.",Dacudoadaptothseseofursc,53.0,115.0,3.0
2167,Domain adaptation,401.0,visual domain adaptation with manifold embedded distribution alignment,1.0,17.0,5.0,70.0,4.0,3.5,148.1,19,http://arxiv.org/pdf/1807.07258v2,"Visual domain adaptation aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Existing methods either attempt to align the cross-domain distributions, or perform manifold subspace learning. However, there are two significant challenges: (1) degenerated feature transformation, which means that distribution alignment is often performed in the original feature space, where feature distortions are hard to overcome. On the other hand, subspace learning is not sufficient to reduce the distribution divergence. (2) unevaluated distribution alignment, which means that existing distribution alignment methods only align the marginal and conditional distributions with equal importance, while they fail to evaluate the different importance of these two distributions in real applications. In this paper, we propose a Manifold Embedded Distribution Alignment (MEDA) approach to address these challenges. MEDA learns a domain-invariant classifier in Grassmann manifold with structural risk minimization, while performing dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distributions. To the best of our knowledge, MEDA is the first attempt to perform dynamic distribution alignment for manifold domain adaptation. Extensive experiments demonstrate that MEDA shows significant improvements in classification accuracy compared to state-of-the-art traditional and deep methods.",Dvidoadwimaemdial,206.0,49.0,45.0
2168,Domain adaptation,401.0,improve unsupervised domain adaptation with mixup training,1.0,70.0,4.0,40.0,5.0,3.4000000000000004,160.3,20,https://arxiv.org/pdf/2001.00677,"Unsupervised domain adaptation studies the problem of utilizing a relevant source domain with abundant labels to build predictive modeling for an unannotated target domain. Recent work observe that the popular adversarial approach of learning domain-invariant features is insufficient to achieve desirable target domain performance and thus introduce additional training constraints, e.g. cluster assumption. However, these approaches impose the constraints on source and target domains individually, ignoring the important interplay between them. In this work, we propose to enforce training constraints across domains using mixup formulation to directly address the generalization performance for target data. In order to tackle potentially huge domain discrepancy, we further propose a feature-level consistency regularizer to facilitate the inter-domain constraint. When adding intra-domain mixup and domain adversarial learning, our general framework significantly improves state-of-the-art performance on several important tasks from both image classification and human activity recognition.",Dimundoadwimitr,33.0,18.0,5.0
2169,Domain adaptation,401.0,taking a closer look at domain shift: category-level adversaries for semantics consistent domain adaptation,1.0,93.0,4.0,11.0,5.0,3.4000000000000004,160.8,21,http://arxiv.org/pdf/1809.09478v3,"We consider the problem of unsupervised domain adaptation in semantic segmentation. The key in this campaign consists in reducing the domain shift, i.e., enforcing the data distributions of the two domains to be similar. A popular strategy is to align the marginal distribution in the feature space through adversarial learning. However, this global alignment strategy does not consider the local category-level feature distribution. A possible consequence of the global movement is that some categories which are originally well aligned between the source and target may be incorrectly mapped. To address this problem, this paper introduces a category-level adversarial network, aiming to enforce local semantic consistency during the trend of global alignment. Our idea is to take a close look at the category-level data distribution and align each class with an adaptive adversarial loss. Specifically, we reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those poorly aligned. In this process, we decide how well a feature is category-level aligned between source and target by a co-training approach. In two domain adaptation tasks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, we validate that the proposed method matches the state of the art in segmentation accuracy.",Dtaaclloatdoshcaadfosecodoad,268.0,55.0,45.0
2170,Domain adaptation,24.0,universal source-free domain adaptation,5.0,201.0,1.0,20.0,5.0,3.4,93.6,22,http://openaccess.thecvf.com/content_CVPR_2020/papers/Kundu_Universal_Source-Free_Domain_Adaptation_CVPR_2020_paper.pdf,"There is a strong incentive to develop versatile learning techniques that can transfer the knowledge of class-separability from a labeled source domain to an unlabeled target domain in the presence of a domain-shift. Existing domain adaptation (DA) approaches are not equipped for practical DA scenarios as a result of their reliance on the knowledge of source-target label-set relationship (e.g. Closed-set, Open-set or Partial DA). Furthermore, almost all prior unsupervised DA works require coexistence of source and target samples even during deployment, making them unsuitable for real-time adaptation. Devoid of such impractical assumptions, we propose a novel two-stage learning process. 1) In the Procurement stage, we aim to equip the model for future source-free deployment, assuming no prior knowledge of the upcoming category-gap and domain-shift. To achieve this, we enhance the model’s ability to reject out-of-source distribution samples by leveraging the available source data, in a novel generative classifier framework. 2) In the Deployment stage, the goal is to design a unified adaptation algorithm capable of operating across a wide range of category-gaps, with no access to the previously seen source samples. To this end, in contrast to the usage of complex adversarial training regimes, we define a simple yet effective source-free adaptation objective by utilizing a novel instance-level weighting mechanism, named as Source Similarity Metric (SSM). A thorough evaluation shows the practical usability of the proposed learning framework with superior DA performance even over state-of-the-art source-dependent approaches.",Dunsodoad,63.0,60.0,7.0
2171,Domain adaptation,28.0,correlation alignment for unsupervised domain adaptation,5.0,81.0,4.0,201.0,1.0,3.4,101.1,23,https://arxiv.org/pdf/1612.01939,"In this chapter, we present CORrelation ALignment (CORAL), a simple yet effective method for unsupervised domain adaptation. CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. In contrast to subspace manifold methods, it aligns the original feature distributions of the source and target domains, rather than the bases of lower-dimensional subspaces. It is also much simpler than other distribution matching methods. CORAL performs remarkably well in extensive evaluations on standard benchmark datasets. We first describe a solution that applies a linear transformation to source features to align them with target features before classifier training. For linear classifiers, we propose to equivalently apply CORAL to the classifier weights, leading to added efficiency when the number of classifiers is small but the number and dimensionality of target examples are very high. The resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a large margin on standard domain adaptation benchmarks. Finally, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (DNNs). The resulting Deep CORAL approach works seamlessly with DNNs and achieves state-of-the-art performance on standard benchmark datasets. Our code is available at:~\url{https://github.com/VisionLearningGroup/CORAL}",Dcoalfoundoad,134.0,33.0,17.0
2172,Domain adaptation,53.0,drop to adapt: learning discriminative features for unsupervised domain adaptation,4.0,128.0,3.0,102.0,3.0,3.3000000000000003,97.7,24,https://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Drop_to_Adapt_Learning_Discriminative_Features_for_Unsupervised_Domain_Adaptation_ICCV_2019_paper.pdf,"Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at https://github.com/postBG/DTA.pytorch.",Ddrtoadledifefoundoad,86.0,58.0,11.0
2173,Domain adaptation,98.0,multi-source domain adaptation for semantic segmentation,4.0,140.0,3.0,126.0,3.0,3.3000000000000003,123.2,25,https://arxiv.org/pdf/1910.12181,"Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting, which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper, we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically, we design a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which can be trained in an end-to-end manner. First, we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second, we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches. Our source code is released at: https://github.com/Luodian/MADAN.",Dmudoadfosese,53.0,77.0,3.0
2174,Domain adaptation,127.0,synergistic image and feature adaptation: towards cross-modality domain adaptation for medical image segmentation,3.0,123.0,3.0,94.0,4.0,3.3,115.5,26,https://ojs.aaai.org/index.php/AAAI/article/download/3874/3752,"This paper presents a novel unsupervised domain adaptation framework, called Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of crossmodality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin.",Dsyimanfeadtocrdoadfomeimse,113.0,29.0,12.0
2175,Domain adaptation,171.0,a review of single-source deep unsupervised visual domain adaptation,3.0,23.0,5.0,201.0,1.0,3.2,120.8,27,https://arxiv.org/pdf/2009.00155,"Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly due to the presence of domain shift or dataset bias. Domain adaptation is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we review the latest single-source deep unsupervised domain adaptation methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different domain adaptation strategies and the descriptions of existing benchmark datasets. We then summarize and compare different categories of single-source unsupervised domain adaptation methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and self-supervision-based methods. Finally, we discuss future research directions with challenges and possible solutions.",Dareofsideunvidoad,32.0,240.0,3.0
2176,Domain adaptation,79.0,minimum class confusion for versatile domain adaptation,4.0,42.0,4.0,201.0,1.0,3.1,100.8,28,https://arxiv.org/pdf/1912.03699,"Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.",Dmiclcofovedoad,26.0,75.0,7.0
2177,Domain adaptation,52.0,progressive domain adaptation for object detection,4.0,201.0,1.0,19.0,5.0,3.1,101.7,29,https://openaccess.thecvf.com/content_WACV_2020/papers/Hsu_Progressive_Domain_Adaptation_for_Object_Detection_WACV_2020_paper.pdf,"Recent deep learning methods for object detection rely on a large amount of bounding box annotations. Collecting these annotations is laborious and costly, yet supervised models do not generalize well when testing on images from a different distribution. Domain adaptation provides a solution by adapting existing labels to the target testing data. However, a large gap between domains could make adaptation a challenging task, which leads to unstable training processes and sub-optimal results. In this paper, we propose to bridge the domain gap with an intermediate domain and progressively solve easier adaptation subtasks. This intermediate domain is constructed by translating the source images to mimic the ones in the target domain. To tackle the domain-shift problem, we adopt adversarial learning to align distributions at the feature level. In addition, a weighted task loss is applied to deal with unbalanced image quality in the intermediate domain. Experimental results show that our method performs favorably against the state-of-the-art method in terms of the performance on the target domain.",Dprdoadfoobde,71.0,42.0,4.0
2178,Domain adaptation,4.0,universal domain adaptation,5.0,201.0,1.0,73.0,4.0,3.1,103.5,30,http://openaccess.thecvf.com/content_CVPR_2019/papers/You_Universal_Domain_Adaptation_CVPR_2019_paper.pdf,"Domain adaptation aims to transfer knowledge in the presence of the domain gap. Existing domain adaptation methods rely on rich prior knowledge about the relationship between the label sets of source and target domains, which greatly limits their application in the wild. This paper introduces Universal Domain Adaptation (UDA) that requires no prior knowledge on the label sets. For a given source label set and a target label set, they may contain a common label set and hold a private label set respectively, bringing up an additional category gap. UDA requires a model to either (1) classify the target sample correctly if it is associated with a label in the common label set, or (2) mark it as ``unknown'' otherwise. More importantly, a UDA model should work stably against a wide spectrum of commonness (the proportion of the common label set over the complete label set) so that it can handle real-world problems with unknown target label sets. To solve the universal domain adaptation problem, we propose Universal Adaptation Network (UAN). It quantifies sample-level transferability to discover the common label set and the label sets private to each domain, thereby promoting the adaptation in the automatically discovered common label set and recognizing the ``unknown'' samples successfully. A thorough evaluation shows that UAN outperforms the state of the art closed set, partial and open set domain adaptation methods in the novel UDA setting.",Dundoad,114.0,51.0,27.0
2179,Domain adaptation,8.0,domain adaptation via transfer component analysis,5.0,201.0,1.0,71.0,4.0,3.1,104.1,31,https://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/download/294/962,"Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.",Ddoadvitrcoan,2313.0,66.0,415.0
2180,Domain adaptation,46.0,enhanced transport distance for unsupervised domain adaptation,4.0,201.0,1.0,35.0,5.0,3.1,104.7,32,http://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Enhanced_Transport_Distance_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf,"Unsupervised domain adaptation (UDA) is a representative problem in transfer learning, which aims to improve the classification performance on an unlabeled target domain by exploiting discriminant information from a labeled source domain. The optimal transport model has been used for UDA in the perspective of distribution matching. However, the transport distance cannot reflect the discriminant information from either domain knowledge or category prior. In this work, we propose an enhanced transport distance (ETD) for UDA. This method builds an attention-aware transport distance, which can be viewed as the prediction feedback of the iteratively learned classifier, to measure the domain discrepancy. Further, the Kantorovich potential variable is re-parameterized by deep neural networks to learn the distribution in the latent space. The entropy-based regularization is developed to explore the intrinsic structure of the target domain. The proposed method is optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the SOTA performance of ETD.",Dentrdifoundoad,39.0,37.0,6.0
2181,Domain adaptation,47.0,stochastic classifiers for unsupervised domain adaptation,4.0,201.0,1.0,37.0,5.0,3.1,105.6,33,https://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Stochastic_Classifiers_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.pdf,"A common strategy adopted by existing state-of-the-art unsupervised domain adaptation (UDA) methods is to employ two classifiers to identify the misaligned local regions between source and target domain. Following the ’wisdom of the crowd’ principle, one has to ask: why stop at two? Indeed, we find that using more classifiers leads to better performance, but also introduces more model parameters, therefore risking overfitting. In this paper, we introduce a novel method called STochastic clAssifieRs (STAR) for addressing this problem. Instead of representing one classifier as a weight vector, STAR models it as a Gaussian distribution with its variance representing the inter-classifier discrepancy. With STAR, we can now sample an arbitrary number of classifiers from the distribution, whilst keeping the model size the same as having two classifiers. Extensive experiments demonstrate that a variety of existing UDA methods can greatly benefit from STAR and achieve the state-of-the-art performance on both image classification and semantic segmentation tasks.",Dstclfoundoad,31.0,64.0,6.0
2182,Domain adaptation,54.0,understanding self-training for gradual domain adaptation,4.0,201.0,1.0,32.0,5.0,3.1,106.2,34,http://proceedings.mlr.press/v119/kumar20c/kumar20c.pdf,"Machine learning systems must adapt to data distributions that evolve over time, in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces. We consider gradual domain adaptation, where the goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. We prove the first non-vacuous upper bound on the error of self-training with gradual shifts, under settings where directly adapting to the target domain can result in unbounded error. The theoretical analysis leads to algorithmic insights, highlighting that regularization and label sharpening are essential even when we have infinite data, and suggesting that self-training works particularly well for shifts with small Wasserstein-infinity distance. Leveraging the gradual shift structure leads to higher accuracies on a rotating MNIST dataset and a realistic Portraits dataset.",Dunsefogrdoad,42.0,52.0,4.0
2183,Domain adaptation,57.0,universal domain adaptation through self supervision,4.0,201.0,1.0,30.0,5.0,3.1,106.5,35,https://arxiv.org/pdf/2002.07953,"Unsupervised domain adaptation methods traditionally assume that all source categories are present in the target domain. In practice, little may be known about the category overlap between the two domains. While some methods address target settings with either partial or open-set categories, they assume that the particular setting is known a priori. We propose a more universally applicable domain adaptation approach that can handle arbitrary category shift, called Domain Adaptative Neighborhood Clustering via Entropy optimization (DANCE). DANCE combines two novel ideas: First, as we cannot fully rely on source categories to learn features discriminative for the target, we propose a novel neighborhood clustering technique to learn the structure of the target domain in a self-supervised way. Second, we use entropy-based feature alignment and rejection to align target features with the source, or reject them as unknown categories based on their entropy. We show through extensive experiments that DANCE outperforms baselines across open-set, open-partial and partial domain adaptation settings.",Dundoadthsesu,42.0,54.0,6.0
2184,Domain adaptation,39.0,domain adaptation with neural embedding matching,5.0,201.0,1.0,52.0,4.0,3.1,107.7,36,http://arxiv.org/pdf/1708.00938v1,"Domain adaptation aims to exploit the supervision knowledge in a source domain for learning prediction models in a target domain. In this article, we propose a novel representation learning-based domain adaptation method, i.e., neural embedding matching (NEM) method, to transfer information from the source domain to the target domain where labeled data is scarce. The proposed approach induces an intermediate common representation space for both domains with a neural network model while matching the embedding of data from the two domains in this common representation space. The embedding matching is based on the fundamental assumptions that a cross-domain pair of instances will be close to each other in the embedding space if they belong to the same class category, and the local geometry property of the data can be maintained in the embedding space. The assumptions are encoded via objectives of metric learning and graph embedding techniques to regularize and learn the semisupervised neural embedding model. We also provide a generalization bound analysis for the proposed domain adaptation method. Meanwhile, a progressive learning strategy is proposed and it improves the generalization ability of the neural network gradually. Experiments are conducted on a number of benchmark data sets and the results demonstrate that the proposed method outperforms several state-of-the-art domain adaptation methods and the progressive learning strategy is promising.",Ddoadwineemma,51.0,59.0,2.0
2185,Domain adaptation,3.0,return of frustratingly easy domain adaptation,5.0,201.0,1.0,88.0,4.0,3.1,107.70000000000002,37,https://ojs.aaai.org/index.php/AAAI/article/download/10306/10165,"Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ""frustratingly easy"" to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.",Dreoffreadoad,860.0,42.0,166.0
2186,Domain adaptation,68.0,adversarial-learned loss for domain adaptation,4.0,201.0,1.0,39.0,5.0,3.1,112.5,38,https://ojs.aaai.org/index.php/AAAI/article/download/5757/5613,"Recently, remarkable progress has been made in learning transferable representation across domains. Previous works in domain adaptation are majorly based on two techniques: domain-adversarial learning and self-training. However, domain-adversarial learning only aligns feature distributions between domains but does not consider whether the target features are discriminative. On the other hand, self-training utilizes the model predictions to enhance the discrimination of target features, but it is unable to explicitly align domain distributions. In order to combine the strengths of these two methods, we propose a novel method called Adversarial-Learned Loss for Domain Adaptation (ALDA). We first analyze the pseudo-label method, a typical self-training method. Nevertheless, there is a gap between pseudo-labels and the ground truth, which can cause incorrect training. Thus we introduce the confusion matrix, which is learned through an adversarial manner in ALDA, to reduce the gap and align the feature distributions. Finally, a new loss function is auto-constructed from the learned confusion matrix, which serves as the loss for unlabeled target samples. Our ALDA outperforms state-of-the-art approaches in four standard domain adaptation datasets. Our code is available at this https URL.",Dadlofodoad,33.0,43.0,4.0
2187,Domain adaptation,16.0,transferrable prototypical networks for unsupervised domain adaptation,5.0,201.0,1.0,92.0,4.0,3.1,112.8,39,https://openaccess.thecvf.com/content_CVPR_2019/papers/Pan_Transferrable_Prototypical_Networks_for_Unsupervised_Domain_Adaptation_CVPR_2019_paper.pdf,"In this paper, we introduce a new idea for unsupervised domain adaptation via a remold of Prototypical Networks, which learn an embedding space and perform classification via a remold of the distances to the prototype of each class. Specifically, we present Transferrable Prototypical Networks (TPN) for adaptation such that the prototypes for each class in source and target domains are close in the embedding space and the score distributions predicted by prototypes separately on source and target data are similar. Technically, TPN initially matches each target example to the nearest prototype in the source domain and assigns an example a ``pseudo"" label. The prototype of each class could then be computed on source-only, target-only and source-target data, respectively. The optimization of TPN is end-to-end trained by jointly minimizing the distance across the prototypes on three types of data and KL-divergence of score distributions output by each pair of the prototypes. Extensive experiments are conducted on the transfers across MNIST, USPS and SVHN datasets, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain an accuracy of 80.4% of single model on VisDA 2017 dataset.",Dtrprnefoundoad,130.0,35.0,13.0
2188,Domain adaptation,34.0,optimal transport for domain adaptation,5.0,201.0,1.0,79.0,4.0,3.1,114.3,40,https://arxiv.org/pdf/1507.00504.pdf%3E%60__,"Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.",Doptrfodoad,541.0,72.0,56.0
2189,Domain adaptation,17.0,learning to transfer examples for partial domain adaptation,5.0,201.0,1.0,97.0,4.0,3.1,114.6,41,https://openaccess.thecvf.com/content_CVPR_2019/papers/Cao_Learning_to_Transfer_Examples_for_Partial_Domain_Adaptation_CVPR_2019_paper.pdf,"Domain adaptation is critical for learning in new and unseen environments. With domain adversarial training, deep networks can learn disentangled and transferable features that effectively diminish the dataset shift between the source and target domains for knowledge transfer. In the era of Big Data, large-scale labeled datasets are readily available, stimulating the interest in partial domain adaptation (PDA), which transfers a recognizer from a large labeled domain to a small unlabeled domain. It extends standard domain adaptation to the scenario where target labels are only a subset of source labels. Under the condition that target labels are unknown, the key challenges of PDA are how to transfer relevant examples in the shared classes to promote positive transfer and how to ignore irrelevant ones in the source domain to mitigate negative transfer. In this work, we propose a unified approach to PDA, Example Transfer Network (ETN), which jointly learns domain-invariant representations across domains and a progressive weighting scheme to quantify the transferability of source examples. A thorough evaluation on several benchmark datasets shows that ETN consistently achieves state-of-the-art results for various partial domain adaptation tasks.",Dletotrexfopadoad,100.0,49.0,25.0
2190,Domain adaptation,27.0,domain-symmetric networks for adversarial domain adaptation,5.0,201.0,1.0,91.0,4.0,3.1,115.8,42,https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Domain-Symmetric_Networks_for_Adversarial_Domain_Adaptation_CVPR_2019_paper.pdf,"Unsupervised domain adaptation aims to learn a model of classifier for unlabeled samples on the target domain, given training data of labeled samples on the source domain. Impressive progress is made recently by learning invariant features via domain-adversarial training of deep networks. In spite of the recent progress, domain adaptation is still limited in achieving the invariance of feature distributions at a finer category level. To this end, we propose in this paper a new domain adaptation method called Domain-Symmetric Networks (SymNets). The proposed SymNet is based on a symmetric design of source and target task classifiers, based on which we also construct an additional classifier that shares with them its layer neurons. To train the SymNet, we propose a novel adversarial learning objective whose key design is based on a two-level domain confusion scheme, where the category-level confusion loss improves over the domain-level one by driving the learning of intermediate network features to be invariant at the corresponding categories of the two domains. Both domain discrimination and domain confusion are implemented based on the constructed additional classifier. Since target samples are unlabeled, we also propose a scheme of cross-domain training to help learn the target classifier. Careful ablation studies show the efficacy of our proposed method. In particular, based on commonly used base networks, our SymNets achieve the new state of the art on three benchmark domain adaptation datasets.",Ddonefoaddoad,125.0,34.0,16.0
2191,Domain adaptation,26.0,transferable attention for domain adaptation,5.0,201.0,1.0,96.0,4.0,3.1,117.0,43,https://ojs.aaai.org/index.php/AAAI/article/view/4472/4350,"Recent work in domain adaptation bridges different domains by adversarially learning a domain-invariant representation that cannot be distinguished by a domain discriminator. Existing methods of adversarial domain adaptation mainly align the global images across the source and target domains. However, it is obvious that not all regions of an image are transferable, while forcefully aligning the untransferable regions may lead to negative transfer. Furthermore, some of the images are significantly dissimilar across domains, resulting in weak image-level transferability. To this end, we present Transferable Attention for Domain Adaptation (TADA), focusing our adaptation model on transferable regions or images. We implement two types of complementary transferable attention: transferable local attention generated by multiple region-level domain discriminators to highlight transferable regions, and transferable global attention generated by single image-level domain discriminator to highlight transferable images. Extensive experiments validate that our proposed models exceed state of the art results on standard domain adaptation datasets.",Dtratfodoad,120.0,48.0,14.0
2192,Domain adaptation,401.0,bridging theory and algorithm for domain adaptation,1.0,43.0,4.0,44.0,4.0,3.1,150.7,44,http://arxiv.org/pdf/1904.05801v2,"This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adaptation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.",Dbrthanalfodoad,175.0,46.0,38.0
2193,Domain adaptation,401.0,larger norm more transferable: an adaptive feature norm approach for unsupervised domain adaptation,1.0,45.0,4.0,46.0,4.0,3.1,152.10000000000002,45,http://arxiv.org/pdf/1811.07456v2,"Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task. In this paper, we empirically reveal that the erratic discrimination of the target domain mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we propose a novel parameter-free Adaptive Feature Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range of values can result in significant transfer gains, implying that those task-specific features with larger norms are more transferable. Our method successfully unifies the computation of both standard and partial domain adaptation with more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective approach will shed some light on the future research of transfer learning. Code is available at https://github.com/jihanyang/AFN.",Dlanomotranadfenoapfoundoad,165.0,58.0,30.0
2194,Domain adaptation,401.0,image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification,1.0,50.0,4.0,66.0,4.0,3.1,160.10000000000002,46,http://arxiv.org/pdf/1807.11334v1,"Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a ""learning via translation"" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation. Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.",Dimdoadwiprseandofopere,438.0,62.0,72.0
2195,Domain adaptation,401.0,self-ensembling for visual domain adaptation,1.0,52.0,4.0,65.0,4.0,3.1,160.6,47,http://arxiv.org/pdf/1710.06924v2,"This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et al., 2017) of temporal ensembling (Laine et al;, 2017), a technique that achieved state of the art results in the area of semi-supervised learning. We introduce a number of modifications to their approach for challenging domain adaptation scenarios and evaluate its effectiveness. Our approach achieves state of the art results in a variety of benchmarks, including our winning entry in the VISDA-2017 visual domain adaptation challenge. In small image benchmarks, our algorithm not only outperforms prior art, but can also achieve accuracy that is close to that of a classifier trained in a supervised fashion.",Dsefovidoad,254.0,44.0,41.0
2196,Domain adaptation,401.0,partial adversarial domain adaptation,1.0,46.0,4.0,81.0,4.0,3.1,163.0,48,http://arxiv.org/pdf/1808.04205v1,"Domain adversarial learning aligns the feature distributions across the source and target domains in a two-player minimax game. Existing domain adversarial networks generally assume identical label space across different domains. In the presence of big data, there is strong motivation of transferring deep models from existing big domains to unknown small domains. This paper introduces partial domain adaptation as a new domain adaptation scenario, which relaxes the fully shared label space assumption to that the source label space subsumes the target label space. Previous methods typically match the whole source domain to the target domain, which are vulnerable to negative transfer for the partial domain adaptation problem due to the large mismatch between label spaces. We present Partial Adversarial Domain Adaptation (PADA), which simultaneously alleviates negative transfer by down-weighing the data of outlier source classes for training both source classifier and domain adversary, and promotes positive transfer by matching the feature distributions in the shared label space. Experiments show that PADA exceeds state-of-the-art results for partial domain adaptation tasks on several datasets.",Dpaaddoad,185.0,32.0,55.0
2197,Domain adaptation,401.0,open set domain adaptation by backpropagation,1.0,47.0,4.0,83.0,4.0,3.1,164.0,49,http://arxiv.org/pdf/1804.10427v2,"Numerous algorithms have been proposed for transferring knowledge from a label-rich domain (source) to a label-scarce domain (target). Most of them are proposed for closed-set scenario, where the source and the target domain completely share the class of their samples. However, in practice, a target domain can contain samples of classes that are not shared by the source domain. We call such classes the “unknown class” and algorithms that work well in the open set situation are very practical. However, most existing distribution matching methods for domain adaptation do not work well in this setting because unknown target samples should not be aligned with the source. In this paper, we propose a method for an open set domain adaptation scenario, which utilizes adversarial training. This approach allows to extract features that separate unknown target from known target samples. During training, we assign two options to the feature generator: aligning target samples with source known ones or rejecting them as unknown target ones. Our method was extensively evaluated and outperformed other methods with a large margin in most settings.",Dopsedoadbyba,185.0,35.0,47.0
2198,Domain adaptation,401.0,wasserstein distance guided representation learning for domain adaptation,1.0,78.0,4.0,63.0,4.0,3.1,170.4,50,http://arxiv.org/pdf/1707.01217v4,"Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.",Dwadigurelefodoad,318.0,52.0,43.0
2199,Domain adaptation,401.0,a dirt-t approach to unsupervised domain adaptation,1.0,88.0,4.0,62.0,4.0,3.1,174.1,51,http://arxiv.org/pdf/1802.08735v2,"Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.",Dadiaptoundoad,327.0,36.0,52.0
2200,Domain adaptation,401.0,cross-domain weakly-supervised object detection through progressive domain adaptation,1.0,80.0,4.0,80.0,4.0,3.1,176.3,52,http://arxiv.org/pdf/1803.11365v1,"Can we detect common objects in a variety of image domains without instance-level annotations? In this paper, we present a framework for a novel task, cross-domain weakly supervised object detection, which addresses this question. For this paper, we have access to images with instance-level annotations in a source domain (e.g., natural image) and images with image-level annotations in a target domain (e.g., watercolor). In addition, the classes to be detected in the target domain are all or a subset of those in the source domain. Starting from a fully supervised object detector, which is pre-trained on the source domain, we propose a two-step progressive domain adaptation technique by fine-tuning the detector on two types of artificially and automatically generated samples. We test our methods on our newly collected datasets1 containing three image domains, and achieve an improvement of approximately 5 to 20 percentage points in terms of mean average precision (mAP) compared to the best-performing baselines.",Dcrweobdethprdoad,194.0,44.0,44.0
2201,Domain adaptation,136.0,geometry-aware symmetric domain adaptation for monocular depth estimation,3.0,156.0,3.0,111.0,3.0,3.0,136.5,53,http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Geometry-Aware_Symmetric_Domain_Adaptation_for_Monocular_Depth_Estimation_CVPR_2019_paper.pdf,"Supervised depth estimation has achieved high accuracy due to the advanced deep network architectures. Since the groundtruth depth labels are hard to obtain, recent methods try to learn depth estimation networks in an unsupervised way by exploring unsupervised cues, which are effective but less reliable than true labels. An emerging way to resolve this dilemma is to transfer knowledge from synthetic images with ground truth depth via domain adaptation techniques. However, these approaches overlook specific geometric structure of the natural images in the target domain (i.e., real data), which is important for high-performing depth prediction. Motivated by the observation, we propose a geometry-aware symmetric domain adaptation framework (GASDA) to explore the labels in the synthetic data and epipolar geometry in the real data jointly. Moreover, by training two image style translators and depth estimators symmetrically in an end-to-end network, our model achieves better image style transfer and generates high-quality depth maps. The experimental results demonstrate the effectiveness of our proposed method and comparable performance against the state-of-the-art.",Dgesydoadfomodees,72.0,64.0,8.0
2202,Domain adaptation,33.0,a review of domain adaptation without target labels,5.0,199.0,3.0,201.0,1.0,3.0,149.8,54,https://arxiv.org/pdf/1901.05335,"We propose associative domain adaptation, a novel technique for end-to-end domain adaptation with neural networks, the task of inferring class labels for an unlabeled target domain based on the statistical properties of a labeled source domain. Our training scheme follows the paradigm that in order to effectively derive class labels for the target domain, a network should produce statistically domain invariant embeddings, while minimizing the classification error on the labeled source domain. We accomplish this by reinforcing associations between source and target data directly in embedding space. Our method can easily be added to any existing classification network with no structural and almost no computational overhead. We demonstrate the effectiveness of our approach on various benchmarks and achieve state-of-the-art results across the board with a generic convolutional neural network architecture not specifically tuned to the respective tasks. Finally, we show that the proposed association loss produces embeddings that are more effective for domain adaptation compared to methods employing maximum mean discrepancy as a similarity measure in embedding space.",Dareofdoadwitala,102.0,240.0,3.0
2203,Domain adaptation,401.0,bidirectional learning for domain adaptation of semantic segmentation,1.0,114.0,3.0,10.0,5.0,3.0,168.9,55,http://arxiv.org/pdf/1904.10620v1,"Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other.Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL",Dbilefodoadofsese,209.0,44.0,51.0
2204,Domain adaptation,401.0,semi-supervised domain adaptation via minimax entropy,1.0,120.0,3.0,12.0,5.0,3.0,171.9,56,http://arxiv.org/pdf/1904.06487v5,"Contemporary domain adaptation methods are very effective at aligning feature distributions of source and target domains without any target supervision. However, we show that these techniques perform poorly when even a few labeled examples are available in the target domain. To address this semi-supervised domain adaptation (SSDA) setting, we propose a novel Minimax Entropy (MME) approach that adversarially optimizes an adaptive few-shot model. Our base model consists of a feature encoding network, followed by a classification layer that computes the features' similarity to estimated prototypes (representatives of each class). Adaptation is achieved by alternately maximizing the conditional entropy of unlabeled target data with respect to the classifier and minimizing it with respect to the feature encoder. We empirically demonstrate the superiority of our method over many baselines, including conventional feature alignment and few-shot methods, setting a new state of the art for SSDA. Our code is available at \url{http://cs-people.bu.edu/keisaito/research/MME.html}.",Dsedoadvimien,170.0,44.0,42.0
2205,Domain adaptation,401.0,domain adaptation for image dehazing,1.0,133.0,3.0,33.0,5.0,3.0,183.4,57,http://arxiv.org/pdf/2005.04668v1,"Image dehazing using learning-based methods has achieved state-of-the-art performance in recent years. However, most existing methods train a dehazing model on synthetic hazy images, which are less able to generalize well to real hazy images due to domain shift. To address this issue, we propose a domain adaptation paradigm, which consists of an image translation module and two image dehazing modules. Specifically, we first apply a bidirectional translation network to bridge the gap between the synthetic and real domains by translating images from one domain to another. And then, we use images before and after translation to train the proposed two image dehazing networks with a consistency constraint. In this phase, we incorporate the real hazy image into the dehazing training via exploiting the properties of the clear image (e.g., dark channel prior and image gradient smoothing) to further improve the domain adaptivity. By training image translation and dehazing network in an end-to-end manner, we can obtain better effects of both image translation and dehazing. Experimental results on both synthetic and real-world images demonstrate that our model performs favorably against the state-of-the-art dehazing algorithms.",Ddoadfoimde,42.0,42.0,9.0
2206,Domain adaptation,401.0,instance adaptive self-training for unsupervised domain adaptation,1.0,195.0,3.0,36.0,5.0,3.0,209.1,58,https://arxiv.org/pdf/2008.12197,"The divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such a problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing scalability and performance. In this paper, we propose an instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. Besides, we propose the region-guided regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. Our method is so concise and efficient that it is easy to be generalized to other unsupervised domain adaptation methods. Experiments on 'GTA5 to Cityscapes' and 'SYNTHIA to Cityscapes' demonstrate the superior performance of our approach compared with the state-of-the-art methods.",Dinadsefoundoad,35.0,40.0,9.0
2207,Domain adaptation,51.0,phase consistent ecological domain adaptation,4.0,201.0,1.0,48.0,4.0,2.8,110.1,59,https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Phase_Consistent_Ecological_Domain_Adaptation_CVPR_2020_paper.pdf,"We introduce two criteria to regularize the optimization involved in learning a classifier in a domain where no annotated data are available, leveraging annotated data in a different domain, a problem known as unsupervised domain adaptation. We focus on the task of semantic segmentation, where annotated synthetic data are aplenty, but annotating real data is laborious. The first criterion, inspired by visual psychophysics, is that the map between the two image domains be phase-preserving. This restricts the set of possible learned maps, while enabling enough flexibility to transfer semantic information. The second criterion aims to leverage ecological statistics, or regularities in the scene which are manifest in any image of it, regardless of the characteristics of the illuminant or the imaging sensor. It is implemented using a deep neural network that scores the likelihood of each possible segmentation given a single un-annotated image. Incorporating these two priors in a standard domain adaptation framework improves performance across the board in the most common unsupervised domain adaptation benchmarks for semantic segmentation.",Dphcoecdoad,25.0,49.0,3.0
2208,Domain adaptation,99.0,active adversarial domain adaptation,4.0,201.0,1.0,42.0,4.0,2.8,122.7,60,https://openaccess.thecvf.com/content_WACV_2020/papers/Su_Active_Adversarial_Domain_Adaptation_WACV_2020_paper.pdf,"We propose an active learning approach for transferring representations across domains. Our approach, active adversarial domain adaptation (AADA), explores a duality between two related problems: adversarial domain alignment and importance sampling for adapting models across domains. The former uses a domain discriminative model to align domains, while the latter utilizes the model to weigh samples to account for distribution shifts. Specifically, our importance weight promotes unlabeled samples with large uncertainty in classification and diversity compared to la-beled examples, thus serving as a sample selection scheme for active learning. We show that these two views can be unified in one framework for domain adaptation and transfer learning when the source domain has many labeled examples while the target domain does not. AADA provides significant improvements over fine-tuning based approaches and other sampling methods when the two domains are closely related. Results on challenging domain adaptation tasks such as object detection demonstrate that the advantage over baseline approaches is retained even after hundreds of examples being actively annotated.",Dacaddoad,30.0,80.0,7.0
2209,Domain adaptation,23.0,geodesic flow kernel for unsupervised domain adaptation,5.0,201.0,1.0,120.0,3.0,2.8,123.3,61,https://www.cs.utexas.edu/users/grauman/papers/subspace-cvpr2012.pdf,"In real-world applications of visual recognition, many factors - such as pose, illumination, or image quality - can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied. As such, the classifiers often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose a new kernel-based method that takes advantage of such structures. Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes in geometric and statistical properties from the source to the target domain. Our approach is computationally advantageous, automatically inferring important algorithmic parameters without requiring extensive cross-validation or labeled data from either domain. We also introduce a metric that reliably measures the adaptability between a pair of source and target domains. For a given target domain and several source domains, the metric can be used to automatically select the optimal source domain to adapt and avoid less desirable ones. Empirical studies on standard datasets demonstrate the advantages of our approach over competing methods.",Dgeflkefoundoad,1657.0,30.0,388.0
2210,Domain adaptation,89.0,contextual-relation consistent domain adaptation for semantic segmentation,4.0,201.0,1.0,54.0,4.0,2.8,123.3,62,https://arxiv.org/pdf/2007.02424,"Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 to Cityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods.",Dcocodoadfosese,38.0,71.0,2.0
2211,Domain adaptation,6.0,deep visual domain adaptation: a survey,5.0,201.0,1.0,157.0,3.0,2.8,129.3,63,https://arxiv.org/pdf/1802.03601,"Deep domain adaption has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaption methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaption, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaption scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaption approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.",Ddevidoadasu,682.0,155.0,14.0
2212,Domain adaptation,182.0,mutual mean-teaching: pseudo label refinery for unsupervised domain adaptation on person re-identification,3.0,201.0,1.0,5.0,5.0,2.8,136.5,64,https://arxiv.org/pdf/2001.01526,"Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.",Dmumepslarefoundoadonpere,107.0,57.0,28.0
2213,Domain adaptation,30.0,an introduction to domain adaptation and transfer learning,5.0,201.0,1.0,193.0,3.0,2.8,147.3,65,https://arxiv.org/pdf/1812.11806,"In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.",Danintodoadantrle,106.0,233.0,6.0
2214,Domain adaptation,35.0,domain adaptation for large-scale sentiment classification: a deep learning approach,5.0,201.0,1.0,199.0,3.0,2.8,150.6,66,https://openreview.net/pdf?id=BkZFVibd-S,"The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.",Ddoadfolasecladeleap,1540.0,29.0,120.0
2215,Domain adaptation,401.0,importance weighted adversarial nets for partial domain adaptation,1.0,48.0,4.0,134.0,3.0,2.8,179.7,67,http://arxiv.org/pdf/1803.09210v2,"This paper proposes an importance weighted adversarial nets-based method for unsupervised domain adaptation, specific for partial domain adaptation where the target domain has less number of classes compared to the source domain. Previous domain adaptation methods generally assume the identical label spaces, such that reducing the distribution divergence leads to feasible knowledge transfer. However, such an assumption is no longer valid in a more realistic scenario that requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. This paper extends the adversarial nets-based domain adaptation and proposes a novel adversarial nets-based partial domain adaptation method to identify the source samples that are potentially from the outlier classes and, at the same time, reduce the shift of shared classes between domains.",Dimweadnefopadoad,202.0,29.0,33.0
2216,Domain adaptation,401.0,temporal attentive alignment for large-scale video domain adaptation,1.0,98.0,4.0,129.0,3.0,2.8,198.2,68,http://arxiv.org/pdf/1905.10861v5,"Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over “Source only” from 73.9% to 81.8% on “HMDB --> UCF”, and 10.3% gain on “Kinetics --> Gameplay”). The code and data are released at http://github.com/cmhungsteve/TA3N.",Dteatalfolavidoad,50.0,61.0,17.0
2217,Domain adaptation,87.0,implicit class-conditioned domain alignment for unsupervised domain adaptation,4.0,189.0,3.0,201.0,1.0,2.7,162.0,69,http://proceedings.mlr.press/v119/jiang20d/jiang20d.pdf,"Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: UCF-HMDB_full. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at http://github.com/cmhungsteve/TA3N.",Dimcldoalfoundoad,25.0,63.0,4.0
2218,Domain adaptation,401.0,contrastive adaptation network for unsupervised domain adaptation,1.0,107.0,3.0,43.0,4.0,2.7,176.0,70,http://arxiv.org/pdf/2103.15566v1,"Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while manual annotations are only available in the source domain. Previous methods minimize the domain discrepancy neglecting the class information, which may lead to misalignment and poor generalization performance. To address this issue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a new metric which explicitly models the intra-class domain discrepancy and the inter-class domain discrepancy. We design an alternating update strategy for training CAN in an end-to-end manner. Experiments on two real-world benchmarks Office-31 and VisDA-2017 demonstrate that CAN performs favorably against the state-of-the-art methods and produces more discriminative features.",Dcoadnefoundoad,239.0,43.0,33.0
2219,Domain adaptation,401.0,self-similarity grouping: a simple unsupervised cross domain adaptation approach for person re-identification,1.0,119.0,3.0,51.0,4.0,2.7,183.2,71,http://arxiv.org/abs/1701.04366v1,"Domain adaptation in person re-identification (re-ID) has always been a challenging task. In this work, we explore how to harness the similar natural characteristics existing in the samples from the target domain for learning to conduct person re-ID in an unsupervised manner. Concretely, we propose a Self-similarity Grouping (SSG) approach, which exploits the potential similarity (from the global body to local parts) of unlabeled samples to build multiple clusters from different views automatically. These independent clusters are then assigned with labels, which serve as the pseudo identities to supervise the training process. We repeatedly and alternatively conduct such a grouping and training process until the model is stable. Despite the apparent simplify, our SSG outperforms the state-of-the-arts by more than 4.6% (DukeMTMC→Market1501) and 4.4% (Market1501→DukeMTMC) in mAP, respectively. Upon our SSG, we further introduce a clustering-guided semisupervised approach named SSG ++ to conduct the one-shot domain adaption in an open set setting (i.e. the number of independent identities from the target domain is unknown). Without spending much effort on labeling, our SSG ++ can further promote the mAP upon SSG by 10.7% and 6.9%, respectively. Our Code is available at: https://github.com/OasisYang/SSG .",Dsegrasiuncrdoadapfopere,160.0,56.0,28.0
2220,Domain adaptation,401.0,unsupervised domain adaptation for semantic segmentation via class-balanced self-training,1.0,127.0,3.0,61.0,4.0,2.7,189.4,72,http://arxiv.org/pdf/2103.16694v2,"Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world “wild tasks” where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as “domain gap”, and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training (ST) procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of ST, we also propose a novel class-balanced self-training (CBST) framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.",Dundoadfoseseviclse,456.0,54.0,60.0
2221,Domain adaptation,401.0,sliced wasserstein discrepancy for unsupervised domain adaptation,1.0,164.0,3.0,69.0,4.0,2.7,206.6,73,http://arxiv.org/pdf/1903.04064v1,"In this work, we connect two distinct concepts for unsupervised domain adaptation: feature distribution alignment between domains by utilizing the task-specific decision boundary and the Wasserstein metric. Our proposed sliced Wasserstein discrepancy (SWD) is designed to capture the natural notion of dissimilarity between the outputs of task-specific classifiers. It provides a geometrically meaningful guidance to detect target samples that are far from the support of the source and enables efficient distribution alignment in an end-to-end trainable fashion. In the experiments, we validate the effectiveness and genericness of our method on digit and sign recognition, image classification, semantic segmentation, and object detection.",Dslwadifoundoad,187.0,83.0,25.0
2222,Domain adaptation,401.0,domain-specific batch normalization for unsupervised domain adaptation,1.0,180.0,3.0,90.0,4.0,2.7,219.3,74,http://arxiv.org/pdf/1906.03950v1,"We propose a novel unsupervised domain adaptation framework based on domain-specific batch normalization in deep neural networks. We aim to adapt to both domains by specializing batch normalization layers in convolutional neural networks while allowing them to share all other model parameters, which is realized by a two-stage algorithm. In the first stage, we estimate pseudo-labels for the examples in the target domain using an external unsupervised domain adaptation algorithm---for example, MSTN or CPUA---integrating the proposed domain-specific batch normalization. The second stage learns the final models using a multi-task classification loss for the source and target domains. Note that the two domains have separate batch normalization layers in both stages. Our framework can be easily incorporated into the domain adaptation techniques based on deep neural networks with batch normalization layers. We also present that our approach can be extended to the problem with multiple source domains. The proposed algorithm is evaluated on multiple benchmark datasets and achieves the state-of-the-art accuracy in the standard setting and the multi-source domain adaption scenario.",Ddobanofoundoad,125.0,33.0,13.0
2223,Domain adaptation,401.0,unified deep supervised domain adaptation and generalization,1.0,193.0,3.0,82.0,4.0,2.7,222.1,75,http://arxiv.org/pdf/1709.10190v1,"This work provides a unified framework for addressing the problem of visual supervised domain adaptation and generalization with deep models. The main idea is to exploit the Siamese architecture to learn an embedding subspace that is discriminative, and where mapped visual domains are semantically aligned and yet maximally separated. The supervised setting becomes attractive especially when only few target data samples need to be labeled. In this scenario, alignment and separation of semantic probability distributions is difficult because of the lack of data. We found that by reverting to point-wise surrogates of distribution distances and similarities provides an effective solution. In addition, the approach has a high “speed” of adaptation, which requires an extremely low number of labeled target training samples, even one per category can be effective. The approach is extended to domain generalization. For both applications the experiments show very promising results.",Dundesudoadange,334.0,68.0,47.0
2224,Domain adaptation,401.0,visual representations for semantic target driven navigation,1.0,1.0,5.0,201.0,1.0,2.6,181.0,76,http://arxiv.org/pdf/1805.06066v3,"What is a good visual representation for autonomous agents? We address this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a complex environment to a target object, e.g. go to the refrigerator. Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues. We propose to using high level semantic and contextual features including segmentation and detection masks obtained by off-the-shelf state-of-the-art vision as observations and use deep network to learn the navigation policy. This choice allows using additional data, from orthogonal sources, to better train different parts of the model the representation extraction is trained on large standard vision datasets while the navigation component leverages large synthetic environments for training. This combination of real and synthetic is possible because equitable feature representations are available in both (e.g., segmentation and detection masks), which alleviates the need for domain adaptation. Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1]. Our approach gets successfully to the target in 54% of the cases in unexplored environments, compared to 46% for non-learning based approach, and 28% for the learning-based baseline.",Dvirefosetadrna,93.0,41.0,7.0
2225,Domain adaptation,401.0,deep residual learning for image recognition,1.0,2.0,5.0,201.0,1.0,2.6,181.4,77,http://arxiv.org/pdf/1612.05400v1,"Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks.   Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.",Dderelefoimre,79331.0,61.0,15629.0
2226,Domain adaptation,401.0,domain separation networks,1.0,3.0,5.0,201.0,1.0,2.6,181.8,78,http://arxiv.org/pdf/1803.03018v1,"The behavior of users in certain services could be a clue that can be used to infer their preferences and may be used to make recommendations for other services they have never used. However, the cross-domain relationships between items and user consumption patterns are not simple, especially when there are few or no common users and items across domains. To address this problem, we propose a content-based cross-domain recommendation method for cold-start users that does not require user- and item- overlap. We formulate recommendation as extreme multi-class classification where labels (items) corresponding to the users are predicted. With this formulation, the problem is reduced to a domain adaptation setting, in which a classifier trained in the source domain is adapted to the target domain. For this, we construct a neural network that combines an architecture for domain adaptation, Domain Separation Network, with a denoising autoencoder for item representation. We assess the performance of our approach in experiments on a pair of data sets collected from movie and news services of Yahoo! JAPAN and show that our approach outperforms several baseline methods including a cross-domain collaborative filtering method.",Ddosene,837.0,35.0,99.0
2227,Domain adaptation,401.0,generalized end-to-end loss for speaker verification,1.0,5.0,5.0,201.0,1.0,2.6,182.6,79,http://arxiv.org/pdf/2006.04326v1,"This paper introduces a semi-supervised contrastive learning framework and its application to text-independent speaker verification. The proposed framework employs generalized contrastive loss (GCL). GCL unifies losses from two different learning frameworks, supervised metric learning and unsupervised contrastive learning, and thus it naturally determines the loss for semi-supervised learning. In experiments, we applied the proposed framework to text-independent speaker verification on the VoxCeleb dataset. We demonstrate that GCL enables the learning of speaker embeddings in three manners, supervised learning, semi-supervised learning, and unsupervised learning, without any changes in the definition of the loss function.",Dgeenlofospve,346.0,18.0,41.0
2228,Domain adaptation,401.0,we need to talk about random splits,1.0,6.0,5.0,201.0,1.0,2.6,183.0,80,http://arxiv.org/pdf/2005.00636v3,"Gorman and Bedrick (2019) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.",Dwenetotaabrasp,17.0,49.0,2.0
2229,Domain adaptation,401.0,data valuation using reinforcement learning,1.0,7.0,5.0,201.0,1.0,2.6,183.4,81,http://arxiv.org/pdf/1909.11671v1,"Quantifying the value of data is a fundamental problem in machine learning. Data valuation has multiple important use cases: (1) building insights about the learning task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. To adaptively learn data values jointly with the target task predictor model, we propose a meta learning framework which we name Data Valuation using Reinforcement Learning (DVRL). We employ a data value estimator (modeled by a deep neural network) to learn how likely each datum is used in training of the predictor model. We train the data value estimator using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively.",Ddavausrele,28.0,34.0,4.0
2230,Domain adaptation,401.0,language models are few-shot learners,1.0,8.0,5.0,201.0,1.0,2.6,183.8,82,http://arxiv.org/pdf/2101.11889v1,"Deep neural networks are powerful statistical learners. However, their predictions do not come with an explanation of their process. To analyze these models, explanation methods are being developed. We present a novel explanation method, called OLM, for natural language processing classifiers. This method combines occlusion and language modeling, which are techniques central to explainability and NLP, respectively. OLM gives explanations that are theoretically sound and easy to understand.   We make several contributions to the theory of explanation methods. Axioms for explanation methods are an interesting theoretical concept to explore their basics and deduce methods. We introduce a new axiom, give its intuition and show it contradicts another existing axiom. Additionally, we point out theoretical difficulties of existing gradient-based and some occlusion-based explanation methods in natural language processing. We provide an extensive argument why evaluation of explanation methods is difficult. We compare OLM to other explanation methods and underline its uniqueness experimentally. Finally, we investigate corner cases of OLM and discuss its validity and possible improvements.",Dlamoarfele,2403.0,144.0,233.0
2231,Domain adaptation,401.0,unsupervised image-to-image translation networks,1.0,9.0,5.0,201.0,1.0,2.6,184.2,83,http://arxiv.org/pdf/1806.03698v1,"Unsupervised image-to-image translation is a recently proposed task of translating an image to a different style or domain given only unpaired image examples at training time. In this paper, we formulate a new task of unsupervised video-to-video translation, which poses its own unique challenges. Translating video implies learning not only the appearance of objects and scenes but also realistic motion and transitions between consecutive frames.We investigate the performance of per-frame video-to-video translation using existing image-to-image translation networks, and propose a spatio-temporal 3D translator as an alternative solution to this problem. We evaluate our 3D method on multiple synthetic datasets, such as moving colorized digits, as well as the realistic segmentation-to-video GTA dataset and a new CT-to-MRI volumetric images translation dataset. Our results show that frame-wise translation produces realistic results on a single frame level but underperforms significantly on the scale of the whole video compared to our three-dimensional translation approach, which is better able to learn the complex structure of video and motion and continuity of object appearance.",Dunimtrne,1596.0,42.0,235.0
2232,Domain adaptation,401.0,coupled generative adversarial networks,1.0,10.0,5.0,201.0,1.0,2.6,184.6,84,http://arxiv.org/pdf/1606.07536v2,"We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.",Dcogeadne,1125.0,35.0,108.0
2233,Domain adaptation,401.0,cross-domain activity recognition via substructural optimal transport,1.0,12.0,5.0,201.0,1.0,2.6,185.4,85,http://arxiv.org/abs/2102.03353v3,"It is expensive and time-consuming to collect sufficient labeled data for human activity recognition (HAR). Domain adaptation is a promising approach for cross-domain activity recognition. Existing methods mainly focus on adapting cross-domain representations via domain-level, class-level, or sample-level distribution matching. However, they might fail to capture the fine-grained locality information in activity data. The domain- and class-level matching are too coarse that may result in under-adaptation, while sample-level matching may be affected by the noise seriously and eventually cause over-adaptation. In this paper, we propose substructure-level matching for domain adaptation (SSDA) to better utilize the locality information of activity data for accurate and efficient knowledge transfer. Based on SSDA, we propose an optimal transport-based implementation, Substructural Optimal Transport (SOT), for cross-domain HAR. We obtain the substructures of activities via clustering methods and seeks the coupling of the weighted substructures between different domains. We conduct comprehensive experiments on four public activity recognition datasets (i.e. UCI-DSADS, UCI-HAR, USC-HAD, PAMAP2), which demonstrates that SOT significantly outperforms other state-of-the-art methods w.r.t classification accuracy (9%+ improvement). In addition, our mehtod is 5x faster than traditional OT-based DA methods with the same hyper-parameters.",Dcracrevisuoptr,0.0,72.0,0.0
2234,Domain adaptation,401.0,learning to match distributions for domain adaptation,1.0,13.0,5.0,201.0,1.0,2.6,185.8,86,http://arxiv.org/pdf/2007.10791v3,"When the training and test data are from different distributions, domain adaptation is needed to reduce dataset bias to improve the model's generalization ability. Since it is difficult to directly match the cross-domain joint distributions, existing methods tend to reduce the marginal or conditional distribution divergence using predefined distances such as MMD and adversarial-based discrepancies. However, it remains challenging to determine which method is suitable for a given application since they are built with certain priors or bias. Thus they may fail to uncover the underlying relationship between transferable features and joint distributions. This paper proposes Learning to Match (L2M) to automatically learn the cross-domain distribution matching without relying on hand-crafted priors on the matching loss. Instead, L2M reduces the inductive bias by using a meta-network to learn the distribution matching loss in a data-driven way. L2M is a general framework that unifies task-independent and human-designed matching features. We design a novel optimization algorithm for this challenging objective with self-supervised label propagation. Experiments on public datasets substantiate the superiority of L2M over SOTA methods. Moreover, we apply L2M to transfer from pneumonia to COVID-19 chest X-ray images with remarkable performance. L2M can also be extended in other distribution matching applications where we show in a trial experiment that L2M generates more realistic and sharper MNIST samples.",Dletomadifodoad,2.0,68.0,0.0
2235,Domain adaptation,401.0,transfer learning with dynamic distribution adaptation,1.0,14.0,5.0,201.0,1.0,2.6,186.2,87,http://arxiv.org/pdf/1909.08184v1,"The recent advances in deep transfer learning reveal that adversarial learning can be embedded into deep networks to learn more transferable features to reduce the distribution discrepancy between two domains. Existing adversarial domain adaptation methods either learn a single domain discriminator to align the global source and target distributions or pay attention to align subdomains based on multiple discriminators. However, in real applications, the marginal (global) and conditional (local) distributions between domains are often contributing differently to the adaptation. There is currently no method to dynamically and quantitatively evaluate the relative importance of these two distributions for adversarial learning. In this paper, we propose a novel Dynamic Adversarial Adaptation Network (DAAN) to dynamically learn domain-invariant representations while quantitatively evaluate the relative importance of global and local domain distributions. To the best of our knowledge, DAAN is the first attempt to perform dynamic adversarial distribution adaptation for deep adversarial learning. DAAN is extremely easy to implement and train in real applications. We theoretically analyze the effectiveness of DAAN, and it can also be explained in an attention strategy. Extensive experiments demonstrate that DAAN achieves better classification accuracy compared to state-of-the-art deep and adversarial methods. Results also imply the necessity and effectiveness of the dynamic distribution adaptation in adversarial transfer learning.",Dtrlewidydiad,36.0,107.0,3.0
2236,Domain adaptation,401.0,easy transfer learning by exploiting intra-domain structures,1.0,15.0,5.0,201.0,1.0,2.6,186.6,88,http://arxiv.org/pdf/2008.04899v1,"Visual imitation learning provides a framework for learning complex manipulation behaviors by leveraging human demonstrations. However, current interfaces for imitation such as kinesthetic teaching or teleoperation prohibitively restrict our ability to efficiently collect large-scale data in the wild. Obtaining such diverse demonstration data is paramount for the generalization of learned skills to novel scenarios. In this work, we present an alternate interface for imitation that simplifies the data collection process while allowing for easy transfer to robots. We use commercially available reacher-grabber assistive tools both as a data collection device and as the robot's end-effector. To extract action information from these visual demonstrations, we use off-the-shelf Structure from Motion (SfM) techniques in addition to training a finger detection network. We experimentally evaluate on two challenging tasks: non-prehensile pushing and prehensile stacking, with 1000 diverse demonstrations for each task. For both tasks, we use standard behavior cloning to learn executable policies from the previously collected offline demonstrations. To improve learning performance, we employ a variety of data augmentations and provide an extensive analysis of its effects. Finally, we demonstrate the utility of our interface by evaluating on real robotic scenarios with previously unseen objects and achieve a 87% success rate on pushing and a 62% success rate on stacking. Robot videos are available at https://dhiraj100892.github.io/Visual-Imitation-Made-Easy.",Deatrlebyexinst,48.0,36.0,6.0
2237,Domain adaptation,401.0,accelerating deep unsupervised domain adaptation with transfer channel pruning,1.0,16.0,5.0,201.0,1.0,2.6,187.0,89,http://arxiv.org/pdf/1904.02654v1,"Deep unsupervised domain adaptation (UDA) has recently received increasing attention from researchers. However, existing methods are computationally intensive due to the computation cost of Convolutional Neural Networks (CNN) adopted by most work. To date, there is no effective network compression method for accelerating these models. In this paper, we propose a unified Transfer Channel Pruning (TCP) approach for accelerating UDA models. TCP is capable of compressing the deep UDA model by pruning less important channels while simultaneously learning transferable features by reducing the cross-domain distribution divergence. Therefore, it reduces the impact of negative transfer and maintains competitive performance on the target task. To the best of our knowledge, TCP is the first approach that aims at accelerating deep UDA models. TCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two common backbone networks-VGG16 and ResNet50. Experimental results demonstrate that TCP achieves comparable or better classification accuracy than other comparison methods while significantly reducing the computational cost. To be more specific, in VGG16, we get even higher accuracy after pruning 26% floating point operations (FLOPs); in ResNet50, we also get higher accuracy on half of the tasks after pruning 12% FLOPs. We hope that TCP will open a new door for future research on accelerating transfer learning models.",Dacdeundoadwitrchpr,7.0,43.0,2.0
2238,Domain adaptation,401.0,augmented sbert: data augmentation method for improving bi-encoders for pairwise sentence scoring tasks,1.0,19.0,5.0,201.0,1.0,2.6,188.2,90,http://arxiv.org/pdf/2010.08240v2,"There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance.",Dausbdaaumefoimbifopasescta,28.0,54.0,3.0
2239,Domain adaptation,401.0,generalization through memorization: nearest neighbor language models,1.0,20.0,5.0,201.0,1.0,2.6,188.6,91,http://arxiv.org/pdf/1911.00172v2,"We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",Dgethmenenelamo,109.0,33.0,8.0
2240,Domain adaptation,401.0,sim2real for self-supervised monocular depth and segmentation,1.0,21.0,5.0,201.0,1.0,2.6,189.0,92,http://arxiv.org/pdf/2012.00238v1,"Image-based learning methods for autonomous vehicle perception tasks require large quantities of labelled, real data in order to properly train without overfitting, which can often be incredibly costly. While leveraging the power of simulated data can potentially aid in mitigating these costs, networks trained in the simulation domain usually fail to perform adequately when applied to images in the real domain. Recent advances in domain adaptation have indicated that a shared latent space assumption can help to bridge the gap between the simulation and real domains, allowing the transference of the predictive capabilities of a network from the simulation domain to the real domain. We demonstrate that a twin VAE-based architecture with a shared latent space and auxiliary decoders is able to bridge the sim2real gap without requiring any paired, ground-truth data in the real domain. Using only paired, ground-truth data in the simulation domain, this architecture has the potential to generate perception tasks such as depth and segmentation maps. We compare this method to networks trained in a supervised manner to indicate the merit of these results.",Dsifosemodeanse,0.0,40.0,0.0
2241,Domain adaptation,401.0,discriminative adversarial search for abstractive summarization,1.0,22.0,5.0,201.0,1.0,2.6,189.4,93,http://arxiv.org/pdf/2002.10375v2,"We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is only used to drive sequence generation at inference time.   We investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs.",Ddiadsefoabsu,9.0,38.0,0.0
2242,Domain adaptation,401.0,learning generalisable omni-scale representations for person re-identification,1.0,25.0,5.0,201.0,1.0,2.6,190.6,94,http://arxiv.org/pdf/2001.04193v2,"Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for FOUR different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.",Dlegeomrefopere,32.0,112.0,3.0
2243,Domain adaptation,401.0,two at once: enhancing learning and generalization capacities via ibn-net,1.0,26.0,5.0,201.0,1.0,2.6,191.0,95,http://arxiv.org/pdf/1810.09663v2,"We explore the role of interaction for the problem of reliable computation over two-way multicast networks. Specifically we consider a four-node network in which two nodes wish to compute a modulo-sum of two independent Bernoulli sources generated from the other two, and a similar task is done in the other direction. The main contribution of this work lies in the characterization of the computation capacity region for a deterministic model of the network via a novel transmission scheme. One consequence of this result is that, not only we can get an interaction gain over the one-way non-feedback computation capacities, but also we can sometimes get all the way to perfect-feedback computation capacities simultaneously in both directions. This result draws a parallel with the recent result developed in the context of two-way interference channels.",Dtwatonenleangecaviib,214.0,34.0,32.0
2244,Domain adaptation,401.0,the natural language decathlon: multitask learning as question answering,1.0,28.0,5.0,201.0,1.0,2.6,191.8,96,http://arxiv.org/pdf/1705.07830v3,"We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.",Dthnalademuleasquan,339.0,138.0,33.0
2245,Domain adaptation,401.0,unsupervised cross-domain image generation,1.0,30.0,5.0,201.0,1.0,2.6,192.6,97,http://arxiv.org/pdf/1801.03318v1,"Contrast and quality of ultrasound images are adversely affected by the excessive presence of speckle. However, being an inherent imaging property, speckle helps in tissue characterization and tracking. Thus, despeckling of the ultrasound images requires the reduction of speckle extent without any oversmoothing. In this letter, we aim to address the despeckling problem using an unsupervised deep adversarial approach. A despeckling residual neural network (DRNN) is trained with an adversarial loss imposed by a discriminator. The discriminator tries to differentiate between the despeckled images generated by the DRNN and the set of high-quality images. Further to prevent the developed DRNN from oversmoothing, a structural loss term is used along with the adversarial loss. Experimental evaluations show that the proposed DRNN is able to outperform the state-of-the-art despeckling approaches.",Duncrimge,678.0,31.0,36.0
2246,Domain adaptation,401.0,masked language model scoring,1.0,31.0,5.0,201.0,1.0,2.6,193.0,98,http://arxiv.org/abs/1910.14659v3,"Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL's unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can finetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",Dmalamosc,66.0,64.0,8.0
2247,Domain adaptation,401.0,multi-task deep neural networks for natural language understanding,1.0,32.0,5.0,201.0,1.0,2.6,193.4,99,http://arxiv.org/pdf/2005.06420v3,"In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.",Dmudenenefonalaun,588.0,39.0,129.0
2248,Domain adaptation,401.0,generative adversarial network in medical imaging: a review,1.0,33.0,5.0,201.0,1.0,2.6,193.8,100,http://arxiv.org/pdf/1703.06490v3,"Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.",Dgeadneinmeimare,495.0,357.0,9.0
2565,Face Recognition,11.0,facenet: a unified embedding for face recognition and clustering,5.0,3.0,5.0,1.0,5.0,5.0,4.8,1,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf,"Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves 95.12%. Our system cuts the error rate in comparison to the best published result [15] by 30% on both datasets.",Ffaaunemfofareancl,7199.0,30.0,1103.0
2566,Face Recognition,4.0,deep face recognition,5.0,18.0,5.0,2.0,5.0,5.0,9.0,2,http://cis.csuohio.edu/~sschung/CIS660/DeepFaceRecognition_parkhi15.pdf,"The goal of this paper is face recognition – from either a single photograph or from a set of faces tracked in a video. Recent progress in this area has been due to two factors: (i) end to end learning for the task using a convolutional neural network (CNN), and (ii) the availability of very large scale training datasets. We make two contributions: first, we show how a very large scale dataset (2.6M images, over 2.6K people) can be assembled by a combination of automation and human in the loop, and discuss the trade off between data purity and time; second, we traverse through the complexities of deep network training and face recognition to present methods and procedures to achieve comparable state of the art results on the standard LFW and YTF face benchmarks.",Fdefare,3755.0,33.0,635.0
2567,Face Recognition,10.0,sphereface: deep hypersphere embedding for face recognition,5.0,26.0,5.0,6.0,5.0,5.0,15.2,3,https://openaccess.thecvf.com/content_cvpr_2017/papers/Liu_SphereFace_Deep_Hypersphere_CVPR_2017_paper.pdf,"This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter m. We further derive specific m to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge 1 show the superiority of A-Softmax loss in FR tasks.",Fspdehyemfofare,1479.0,43.0,246.0
2568,Face Recognition,25.0,masked face recognition dataset and application,5.0,27.0,5.0,8.0,5.0,5.0,20.7,4,https://arxiv.org/pdf/2003.09093,"In order to effectively prevent the spread of COVID-19 virus, almost everyone wears a mask during coronavirus epidemic. This almost makes conventional facial recognition technology ineffective in many cases, such as community access control, face access control, facial attendance, facial security checks at train stations, etc. Therefore, it is very urgent to improve the recognition performance of the existing face recognition technology on the masked faces. Most current advanced face recognition approaches are designed based on deep learning, which depend on a large number of face samples. However, at present, there are no publicly available masked face recognition datasets. To this end, this work proposes three types of masked face datasets, including Masked Face Detection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD) and Simulated Masked Face Recognition Dataset (SMFRD). Among them, to the best of our knowledge, RMFRD is currently theworld's largest real-world masked face dataset. These datasets are freely available to industry and academia, based on which various applications on masked faces can be developed. The multi-granularity masked face recognition model we developed achieves 95% accuracy, exceeding the results reported by the industry. Our datasets are available at: this https URL.",Fmafaredaanap,118.0,7.0,15.0
2569,Face Recognition,43.0,arcface: additive angular margin loss for deep face recognition,4.0,8.0,5.0,3.0,5.0,4.7,17.0,5,https://openaccess.thecvf.com/content_CVPR_2019/papers/Deng_ArcFace_Additive_Angular_Margin_Loss_for_Deep_Face_Recognition_CVPR_2019_paper.pdf,"One of the main challenges in feature learning using Deep Convolutional Neural Networks (DCNNs) for large-scale face recognition is the design of appropriate loss functions that can enhance the discriminative power. Centre loss penalises the distance between deep features and their corresponding class centres in the Euclidean space to achieve intra-class compactness. SphereFace assumes that the linear transformation matrix in the last fully connected layer can be used as a representation of the class centres in the angular space and therefore penalises the angles between deep features and their corresponding weights in a multiplicative way. Recently, a popular line of research is to incorporate margins in well-established loss functions in order to maximise face class separability. In this paper, we propose an Additive Angular Margin Loss (ArcFace) to obtain highly discriminative features for face recognition. The proposed ArcFace has a clear geometric interpretation due to its exact correspondence to geodesic distance on a hypersphere. We present arguably the most extensive experimental evaluation against all recent state-of-the-art face recognition methods on ten face recognition benchmarks which includes a new large-scale image database with trillions of pairs and a large-scale video dataset. We show that ArcFace consistently outperforms the state of the art and can be easily implemented with negligible computational overhead. To facilitate future research, the code has been made available.",Faradanmalofodefare,1732.0,161.0,436.0
2570,Face Recognition,57.0,cosface: large margin cosine loss for deep face recognition,4.0,15.0,5.0,9.0,5.0,4.7,25.8,6,http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_CosFace_Large_Margin_CVPR_2018_paper.pdf,"Face recognition has made extraordinary progress owing to the advancement of deep convolutional neural networks (CNNs). The central task of face recognition, including face verification and identification, involves face feature discrimination. However, the traditional softmax loss of deep CNNs usually lacks the power of discrimination. To address this problem, recently several loss functions such as center loss, large margin softmax loss, and angular softmax loss have been proposed. All these improved losses share the same idea: maximizing inter-class variance and minimizing intra-class variance. In this paper, we propose a novel loss function, namely large margin cosine loss (LMCL), to realize this idea from a different perspective. More specifically, we reformulate the softmax loss as a cosine loss by L2 normalizing both features and weight vectors to remove radial variations, based on which a cosine margin term is introduced to further maximize the decision margin in the angular space. As a result, minimum intra-class variance and maximum inter-class variance are achieved by virtue of normalization and cosine decision margin maximization. We refer to our model trained with LMCL as CosFace. Extensive experimental evaluations are conducted on the most popular public-domain face recognition datasets such as MegaFace Challenge, Youtube Faces (YTF) and Labeled Face in the Wild (LFW). We achieve the state-of-the-art performance on these benchmarks, which confirms the effectiveness of our proposed approach.",Fcolamacolofodefare,981.0,48.0,179.0
2571,Face Recognition,32.0,the devil of face recognition is in the noise,5.0,42.0,4.0,36.0,5.0,4.6,37.2,7,https://openaccess.thecvf.com/content_ECCV_2018/papers/Liren_Chen_The_Devil_of_ECCV_2018_paper.pdf,"The growing scale of face recognition datasets empowers us to train strong convolutional networks for face recognition. While a variety of architectures and loss functions have been devised, we still have a limited understanding of the source and consequence of label noise inherent in existing datasets. We make the following contributions: (1) We contribute cleaned subsets of popular face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new large-scale noise-controlled IMDb-Face dataset. (2) With the original datasets and cleaned subsets, we profile and analyze label noise properties of MegaFace and MS-Celeb-1M. We show that a few orders more samples are needed to achieve the same accuracy yielded by a clean subset. (3) We study the association between different types of noise, i.e., label flips and outliers, with the accuracy of face recognition models. (4) We investigate ways to improve data cleanliness, including a comprehensive user study on the influence of data labeling strategies to annotation accuracy. The IMDb-Face dataset has been released on https://github.com/fwang91/IMDb-Face.",Fthdeoffareisinthno,106.0,27.0,20.0
2572,Face Recognition,136.0,learning from millions of 3d scans for large-scale 3d face recognition,3.0,135.0,3.0,39.0,5.0,3.6,106.5,8,http://openaccess.thecvf.com/content_cvpr_2018/papers/Gilani_Learning_From_Millions_CVPR_2018_paper.pdf,"Deep networks trained on millions of facial images are believed to be closely approaching human-level performance in face recognition. However, open world face recognition still remains a challenge. Although, 3D face recognition has an inherent edge over its 2D counterpart, it has not benefited from the recent developments in deep learning due to the unavailability of large training as well as large test datasets. Recognition accuracies have already saturated on existing 3D face datasets due to their small gallery sizes. Unlike 2D photographs, 3D facial scans cannot be sourced from the web causing a bottleneck in the development of deep 3D face recognition networks and datasets. In this backdrop, we propose a method for generating a large corpus of labeled 3D face identities and their multiple instances for training and a protocol for merging the most challenging existing 3D datasets for testing. We also propose the first deep CNN model designed specifically for 3D face recognition and trained on 3.1 Million 3D facial scans of 100K identities. Our test dataset comprises 1,853 identities with a single 3D scan in the gallery and another 31K scans as probes, which is several orders of magnitude larger than existing ones. Without fine tuning on this dataset, our network already outperforms state of the art face recognition by over 10%. We fine tune our network on the gallery set to perform end-to-end large scale 3D face recognition which further improves accuracy. Finally, we show the efficacy of our method for the open world face recognition problem.",Flefrmiof3dscfola3dfare,75.0,73.0,11.0
2573,Face Recognition,401.0,pose-robust face recognition via deep residual equivariant mapping,1.0,50.0,4.0,37.0,5.0,3.4000000000000004,151.4,9,http://arxiv.org/pdf/1803.00839v1,"Face recognition achieves exceptional success thanks to the emergence of deep learning. However, many contemporary face recognition models still perform relatively poor in processing profile faces compared to frontal faces. A key reason is that the number of frontal and profile training faces are highly imbalanced - there are extensively more frontal training samples compared to profile ones. In addition, it is intrinsically hard to learn a deep representation that is geometrically invariant to large pose variations. In this study, we hypothesize that there is an inherent mapping between frontal and profile faces, and consequently, their discrepancy in the deep representation space can be bridged by an equivariant mapping. To exploit this mapping, we formulate a novel Deep Residual EquivAriant Mapping (DREAM) block, which is capable of adaptively adding residuals to the input deep representation to transform a profile face representation to a canonical pose that simplifies recognition. The DREAM block consistently enhances the performance of profile face recognition for many strong deep networks, including ResNet models, without deliberately augmenting training data of profile faces. The block is easy to use, light-weight, and can be implemented with a negligible computational overhead1.",Fpofarevidereeqma,108.0,43.0,17.0
2574,Face Recognition,3.0,face recognition: a literature survey,5.0,201.0,1.0,11.0,5.0,3.4,84.60000000000001,10,https://cs.ucf.edu/~dcm/Teaching/COT4810-Spring2011/Literature/FaceRecognition.pdf,"As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.",Ffarealisu,6943.0,431.0,229.0
2575,Face Recognition,12.0,face description with local binary patterns: application to face recognition,5.0,201.0,1.0,10.0,5.0,3.4,87.0,11,http://arxiv.org/pdf/1901.11179v1,This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed,Ffadewilobipaaptofare,5334.0,45.0,606.0
2576,Face Recognition,5.0,the feret evaluation methodology for face-recognition algorithms,5.0,201.0,1.0,22.0,5.0,3.4,88.5,12,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.9852&rep=rep1&type=pdf,"Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. To date, 14,126 images from 1,199 individuals are included in the FERET database, which is divided into development and sequestered portions of the database. In September 1996, the FERET program administered the third in a series of FERET face-recognition tests. The primary objectives of the third test were to 1) assess the state of the art, 2) identify future areas of research, and 3) measure algorithm performance.",Fthfeevmefofaal,3473.0,10.0,436.0
2577,Face Recognition,24.0,robust face recognition via sparse representation,5.0,201.0,1.0,5.0,5.0,3.4,89.10000000000001,13,https://www.ideals.illinois.edu/bitstream/handle/2142/103886/08-2203.pdf?sequence=2&isAllowed=y,"We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by C1-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.",Frofarevispre,8873.0,92.0,1345.0
2578,Face Recognition,1.0,face recognition using laplacianfaces,5.0,201.0,1.0,29.0,5.0,3.4,89.4,14,https://pdfs.semanticscholar.org/3486/0ead8af5cdb2550f2767eaacf71d4a5d0c80.pdf,"We propose an appearance-based face recognition method called the Laplacianface approach. By using locality preserving projections (LPP), the face images are mapped into a face subspace for analysis. Different from principal component analysis (PCA) and linear discriminant analysis (LDA) which effectively see only the Euclidean structure of face space, LPP finds an embedding that preserves local information, and obtains a face subspace that best detects the essential face manifold structure. The Laplacianfaces are the optimal linear approximations to the eigenfunctions of the Laplace Beltrami operator on the face manifold. In this way, the unwanted variations resulting from changes in lighting, facial expression, and pose may be eliminated or reduced. Theoretical analysis shows that PCA, LDA, and LPP can be obtained from different graph models. We compare the proposed Laplacianface approach with Eigenface and Fisherface methods on three different face data sets. Experimental results suggest that the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition.",Ffareusla,3156.0,46.0,410.0
2579,Face Recognition,37.0,a discriminative feature learning approach for deep face recognition,5.0,201.0,1.0,4.0,5.0,3.4,92.7,15,http://arxiv.org/pdf/1702.04471v1,"Convolutional neural networks (CNNs) have been widely used in computer vision community, significantly improving the state-of-the-art. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new supervision signal, called center loss, for face recognition task. Specifically, the center loss simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers. More importantly, we prove that the proposed center loss function is trainable and easy to optimize in the CNNs. With the joint supervision of softmax loss and center loss, we can train a robust CNNs to obtain the deep features with the two key learning objectives, inter-class dispension and intra-class compactness as much as possible, which are very essential to face recognition. It is encouraging to see that our CNNs (with such joint supervision) achieve the state-of-the-art accuracy on several important face recognition benchmarks, Labeled Faces in the Wild (LFW), YouTube Faces (YTF), and MegaFace Challenge. Especially, our new approach achieves the best results on MegaFace (the largest public domain face benchmark) under the protocol of small training set (contains under 500000 images and under 20000 persons), significantly improving the previous results and setting new state-of-the-art for both face recognition and face verification tasks.",Fadifeleapfodefare,2222.0,47.0,295.0
2580,Face Recognition,109.0,masked face recognition challenge: the insightface track report,3.0,5.0,5.0,201.0,1.0,3.2,95.0,16,https://openaccess.thecvf.com/content/ICCV2021W/MFR/papers/Deng_Masked_Face_Recognition_Challenge_The_InsightFace_Track_Report_ICCVW_2021_paper.pdf,"Although deep learning has yielded impressive performance for face recognition, many studies have shown that different networks learn different feature maps: while some networks are more receptive to pose and illumination others appear to capture more local information. Thus, in this work, we propose a deep heterogeneous feature fusion network to exploit the complementary information present in features generated by different deep convolutional neural networks (DCNNs) for template-based face recognition, where a template refers to a set of still face images or video frames from different sources which introduces more blur, pose, illumination and other variations than traditional face datasets. The proposed approach efficiently fuses the discriminative information of different deep features by 1) jointly learning the non-linear high-dimensional projection of the deep features and 2) generating a more discriminative template representation which preserves the inherent geometry of the deep features in the feature space. Experimental results on the IARPA Janus Challenge Set 3 (Janus CS3) dataset demonstrate that the proposed method can effectively improve the recognition performance. In addition, we also present a series of covariate experiments on the face verification task for in-depth qualitative evaluations for the proposed approach.",Fmafarechthintrre,4.0,41.0,1.0
2581,Face Recognition,9.0,overview of the face recognition grand challenge,5.0,201.0,1.0,52.0,4.0,3.1,98.7,17,https://www.bkd.com/sites/default/files/2020-03/Challenges%20in%20ERP%20Implementations%20-%20Christina%20Phillips.pdf,"Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.",Fovofthfaregrch,2505.0,12.0,291.0
2582,Face Recognition,16.0,face recognition with local binary patterns,5.0,201.0,1.0,51.0,4.0,3.1,100.5,18,https://link.springer.com/content/pdf/10.1007/978-3-540-24670-1_36.pdf,"This paper is about providing efficient face recognition i.e. feature extraction and face matching system using local binary patterns (LBP) method. It is a texture based algorithm for face recognition which describes the texture and shape of digital images. The preprocessed or facial image is first divided into small blocks from which LBP histograms are formed and then concatenated into a single feature vector. This feature vector plays a vital role in efficient representation of the face and is used to measure similarities by calculating the distance between Images. This paper presents the principles of the method and implementation to perform face recognition. Experiments have been carried out on Yale data set; high recognition rates are obtained, especially compared to other face recognition methods. Also few extensions are investigated and implemented successfully to further improve the performance of the method.",Ffarewilobipa,844.0,21.0,145.0
2583,Face Recognition,62.0,labeled faces in the wild: a database forstudying face recognition in unconstrained environments,4.0,201.0,1.0,7.0,5.0,3.1,101.1,19,https://hal.inria.fr/inria-00321923/file/Huang_long_eccv2008-lfw.pdf,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",Flafainthwiadafofareinunen,4645.0,64.0,1151.0
2584,Face Recognition,14.0,face-space: a unifying concept in face recognition research,5.0,201.0,1.0,56.0,4.0,3.1,101.4,20,https://research.gold.ac.uk/id/eprint/10982/5/ValentineLewisHillsAAC.pdf,"The concept of a multidimensional psychological space, in which faces can be represented according to their perceived properties, is fundamental to the modern theorist in face processing. Yet the idea was not clearly expressed until 1991. The background that led to the development of face-space is explained, and its continuing influence on theories of face processing is discussed. Research that has explored the properties of the face-space and sought to understand caricature, including facial adaptation paradigms, is reviewed. Face-space as a theoretical framework for understanding the effect of ethnicity and the development of face recognition is evaluated. Finally, two applications of face-space in the forensic setting are discussed. From initially being presented as a model to explain distinctiveness, inversion, and the effect of ethnicity, face-space has become a central pillar in many aspects of face processing. It is currently being developed to help us understand adaptation effects with faces. While being in principle a simple concept, face-space has shaped, and continues to shape, our understanding of face perception.",Ffaauncoinfarere,114.0,187.0,6.0
2585,Face Recognition,64.0,a survey on deep learning based face recognition,4.0,201.0,1.0,18.0,5.0,3.1,105.0,21,http://arxiv.org/pdf/2006.11395v1,"Abstract Deep learning, in particular the deep convolutional neural networks, has received increasing interests in face recognition recently, and a number of deep learning methods have been proposed. This paper summarizes about 330 contributions in this area. It reviews major deep learning concepts pertinent to face image analysis and face recognition, and provides a concise overview of studies on specific face recognition problems, such as handling variations in pose, age, illumination, expression, and heterogeneous face matching. A summary of databases used for deep face recognition is given as well. Finally, some open challenges and directions are discussed for future research.",Fasuondelebafare,102.0,379.0,2.0
2586,Face Recognition,81.0,enhanced local texture feature sets for face recognition under difficult lighting conditions,4.0,201.0,1.0,16.0,5.0,3.1,109.5,22,https://hal.archives-ouvertes.fr/hal-00565029/file/TIP-05069-2009.R1-double.pdf,"Making recognition more reliable under uncontrolled lighting conditions is one of the most important challenges for practical face recognition systems. We tackle this by combining the strengths of robust illumination normalization, local texture-based face representations, distance transform based matching, kernel-based feature extraction and multiple feature fusion. Specifically, we make three main contributions: 1) we present a simple and efficient preprocessing chain that eliminates most of the effects of changing illumination while still preserving the essential appearance details that are needed for recognition; 2) we introduce local ternary patterns (LTP), a generalization of the local binary pattern (LBP) local texture descriptor that is more discriminant and less sensitive to noise in uniform regions, and we show that replacing comparisons based on local spatial histograms with a distance transform based similarity metric further improves the performance of LBP/LTP based face recognition; and 3) we further improve robustness by adding Kernel principal component analysis (PCA) feature extraction and incorporating rich local appearance cues from two complementary sources-Gabor wavelets and LBP-showing that the combination is considerably more accurate than either feature set alone. The resulting method provides state-of-the-art performance on three data sets that are widely used for testing recognition under difficult illumination conditions: Extended Yale-B, CAS-PEAL-R1, and Face Recognition Grand Challenge version 2 experiment 4 (FRGC-204). For example, on the challenging FRGC-204 data set it halves the error rate relative to previously published methods, achieving a face verification rate of 88.1% at 0.1% false accept rate. Further experiments show that our preprocessing method outperforms several existing preprocessors for a range of feature sets, data sets and lighting conditions.",Fenlotefesefofareundilico,2817.0,65.0,420.0
2587,Face Recognition,8.0,face recognition by independent component analysis,5.0,201.0,1.0,92.0,4.0,3.1,110.4,23,http://arxiv.org/pdf/1802.01237v1,"A number of current face recognition algorithms use face representations found by unsupervised statistical methods. Typically these methods find a set of basis images and represent faces as a linear combination of those images. Principal component analysis (PCA) is a popular example of such methods. The basis images found by PCA depend only on pairwise relationships between pixels in the image database. In a task such as face recognition, in which important information may be contained in the high-order relationships among pixels, it seems reasonable to expect that better basis images may be found by methods sensitive to these high-order statistics. Independent component analysis (ICA), a generalization of PCA, is one such method. We used a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons. ICA was performed on face images in the FERET database under two different architectures, one which treated the images as random variables and the pixels as outcomes, and a second which treated the pixels as random variables and the images as outcomes. The first architecture found spatially local basis images for the faces. The second architecture produced a factorial face code. Both ICA representations were superior to representations based on PCA for recognizing faces across days and changes in expression. A classifier that combined the two ICA representations gave the best performance.",Ffarebyincoan,2134.0,75.0,201.0
2588,Face Recognition,88.0,sparse representation or collaborative representation: which helps face recognition?,4.0,201.0,1.0,28.0,5.0,3.1,115.20000000000002,24,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.231.8008&rep=rep1&type=pdf,"As a recently proposed technique, sparse representation based classification (SRC) has been widely used for face recognition (FR). SRC first codes a testing sample as a sparse linear combination of all the training samples, and then classifies the testing sample by evaluating which class leads to the minimum representation error. While the importance of sparsity is much emphasized in SRC and many related works, the use of collaborative representation (CR) in SRC is ignored by most literature. However, is it really the l1-norm sparsity that improves the FR accuracy? This paper devotes to analyze the working mechanism of SRC, and indicates that it is the CR but not the l1-norm sparsity that makes SRC powerful for face classification. Consequently, we propose a very simple yet much more efficient face classification scheme, namely CR based classification with regularized least square (CRC_RLS). The extensive experiments clearly show that CRC_RLS has very competitive classification results, while it has significantly less complexity than SRC.",Fspreorcorewhhefare,1767.0,38.0,288.0
2589,Face Recognition,96.0,towards pose invariant face recognition in the wild,4.0,201.0,1.0,34.0,5.0,3.1,119.4,25,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Towards_Pose_Invariant_CVPR_2018_paper.pdf,"Pose variation is one key challenge in face recognition. As opposed to current techniques for pose invariant face recognition, which either directly extract pose invariant features for recognition, or first normalize profile face images to frontal pose before feature extraction, we argue that it is more desirable to perform both tasks jointly to allow them to benefit from each other. To this end, we propose a Pose Invariant Model (PIM) for face recognition in the wild, with three distinct novelties. First, PIM is a novel and unified deep architecture, containing a Face Frontalization sub-Net (FFN) and a Discriminative Learning sub-Net (DLN), which are jointly learned from end to end. Second, FFN is a well-designed dual-path Generative Adversarial Network (GAN) which simultaneously perceives global structures and local details, incorporated with an unsupervised cross-domain adversarial training and a ""learning to learn"" strategy for high-fidelity and identity-preserving frontal view synthesis. Third, DLN is a generic Convolutional Neural Network (CNN) for face recognition with our enforced cross-entropy optimization strategy for learning discriminative yet generalized feature representation. Qualitative and quantitative experiments on both controlled and in-the-wild benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts.",Ftopoinfareinthwi,121.0,41.0,24.0
2590,Face Recognition,58.0,face recognition in unconstrained videos with matched background similarity,4.0,201.0,1.0,43.0,4.0,2.8,110.70000000000002,26,https://www.cs.tau.ac.il/~wolf/papers/lvfw.pdf,"Recognizing faces in unconstrained videos is a task of mounting importance. While obviously related to face recognition in still images, it has its own unique characteristics and algorithmic requirements. Over the years several methods have been suggested for this problem, and a few benchmark data sets have been assembled to facilitate its study. However, there is a sizable gap between the actual application needs and the current state of the art. In this paper we make the following contributions. (a) We present a comprehensive database of labeled videos of faces in challenging, uncontrolled conditions (i.e., ‘in the wild’), the ‘YouTube Faces’ database, along with benchmark, pair-matching tests1. (b) We employ our benchmark to survey and compare the performance of a large variety of existing video face recognition techniques. Finally, (c) we describe a novel set-to-set similarity measure, the Matched Background Similarity (MBGS). This similarity is shown to considerably improve performance on the benchmark tests.",Ffareinunviwimabasi,1105.0,63.0,301.0
2591,Face Recognition,59.0,on the effects of image alterations on face recognition accuracy,4.0,201.0,1.0,58.0,4.0,2.8,115.5,27,https://www.researchgate.net/file.PostFileLoader.html?id=576921fab0366d7cf80f59c4&assetKey=AS%3A375374950223874%401466507770369#page=204,"Face recognition in controlled environments is nowadays considered rather reliable, and if face is acquired in proper conditions, a good accuracy level can be achieved by state-of-the-art systems. However, we show that, even under these desirable conditions, some intentional or unintentional face image alterations can significantly affect the recognition performance. In particular, in scenarios where the user template is created from printed photographs rather than from images acquired live during enrollment (e.g., identity documents ), digital image alterations can severely affect the recognition results. In this chapter, we analyze both the effects of such alterations on face recognition algorithms and the human capabilities to deal with altered images.",Fonthefofimalonfareac,78.0,28.0,2.0
2592,Face Recognition,7.0,linear regression for face recognition,5.0,201.0,1.0,112.0,3.0,2.8,116.1,28,https://eem.eskisehir.edu.tr/atalaybarkan/EEM%20405%20(K)/icerik/linear%20Regression.pdf,"In this paper, we present a novel approach of face identification by formulating the pattern recognition problem in terms of linear regression. Using a fundamental concept that patterns from a single-object class lie on a linear subspace, we develop a linear model representing a probe image as a linear combination of class-specific galleries. The inverse problem is solved using the least-squares method and the decision is ruled in favor of the class with the minimum reconstruction error. The proposed Linear Regression Classification (LRC) algorithm falls in the category of nearest subspace classification. The algorithm is extensively evaluated on several standard databases under a number of exemplary evaluation protocols reported in the face recognition literature. A comparative study with state-of-the-art algorithms clearly reflects the efficacy of the proposed approach. For the problem of contiguous occlusion, we propose a Modular LRC approach, introducing a novel Distance-based Evidence Fusion (DEF) algorithm. The proposed methodology achieves the best results ever reported for the challenging problem of scarf occlusion.",Flirefofare,883.0,35.0,124.0
2593,Face Recognition,15.0,a review of face recognition methods,5.0,201.0,1.0,139.0,3.0,2.8,126.6,29,https://www.researchgate.net/profile/Drmparisa-Beham/publication/274521637_A_Review_Of_Face_Recognition_Methods/links/5a5b3555a6fdcc3bfb606a23/A-Review-Of-Face-Recognition-Methods.pdf,"Face recognition has become more significant and relevant in recent years owing to it potential applications. Since the faces are highly dynamic and pose more issues and challenges to solve, researchers in the domain of pattern recognition, computer vision and artificial intelligence have proposed many solutions to reduce such difficulties so as to improve the robustness and recognition accuracy. As many approaches have been proposed, efforts are also put in to provide an extensive survey of the methods developed over the years. The objective of this paper is to provide a survey of face recognition papers that appeared in the literature over the past decade under all severe conditions that were not discussed in the previous survey and to categorize them into meaningful approaches, viz. appearance based, feature based and soft computing based. A comparative study of merits and demerits of these approaches have been presented.",Fareoffareme,73.0,78.0,0.0
2594,Face Recognition,97.0,heterogeneous face recognition with cnns,4.0,201.0,1.0,57.0,4.0,2.8,126.6,30,https://link.springer.com/content/pdf/10.1007/978-3-319-49409-8_40.pdf,"Heterogeneous face recognition aims to recognize faces across different sensor modalities. Typically, gallery images are normal visible spectrum images, and probe images are infrared images or sketches. Recently significant improvements in visible spectrum face recognition have been obtained by CNNs learned from very large training datasets. In this paper, we are interested in the question to what extent the features from a CNN pre-trained on visible spectrum face images can be used to perform heterogeneous face recognition. We explore different metric learning strategies to reduce the discrepancies between the different modalities. Experimental results show that we can use CNNs trained on visible spectrum images to obtain results that are on par or improve over the state-of-the-art for heterogeneous recognition with near-infrared images and sketches.",Fhefarewicn,75.0,27.0,10.0
2595,Face Recognition,156.0,efficient decision-based black-box adversarial attacks on face recognition,3.0,201.0,1.0,14.0,5.0,2.8,131.4,31,http://openaccess.thecvf.com/content_CVPR_2019/papers/Dong_Efficient_Decision-Based_Black-Box_Adversarial_Attacks_on_Face_Recognition_CVPR_2019_paper.pdf,"Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",Fefdebladatonfare,131.0,40.0,10.0
2596,Face Recognition,98.0,discriminative k-svd for dictionary learning in face recognition,4.0,201.0,1.0,75.0,4.0,2.8,132.3,32,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.719.734&rep=rep1&type=pdf,"In a sparse-representation-based face recognition scheme, the desired dictionary should have good representational power (i.e., being able to span the subspace of all faces) while supporting optimal discrimination of the classes (i.e., different human subjects). We propose a method to learn an over-complete dictionary that attempts to simultaneously achieve the above two goals. The proposed method, discriminative K-SVD (D-KSVD), is based on extending the K-SVD algorithm by incorporating the classification error into the objective function, thus allowing the performance of a linear classifier and the representational power of the dictionary being considered at the same time by the same optimization procedure. The D-KSVD algorithm finds the dictionary and solves for the classifier using a procedure derived from the K-SVD algorithm, which has proven efficiency and performance. This is in contrast to most existing work that relies on iteratively solving sub-problems with the hope of achieving the global optimal through iterative approximation. We evaluate the proposed method using two commonly-used face databases, the Extended YaleB database and the AR database, with detailed comparison to 3 alternative approaches, including the leading state-of-the-art in the literature. The experiments show that the proposed method outperforms these competing methods in most of the cases. Further, using Fisher criterion and dictionary incoherence, we also show that the learned dictionary and the corresponding classifier are indeed better-posed to support sparse-representation-based recognition.",Fdik-fodileinfare,1161.0,25.0,178.0
2597,Face Recognition,167.0,from few to many: illumination cone models for face recognition under variable lighting and pose,3.0,201.0,1.0,17.0,5.0,2.8,135.6,33,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.3126&rep=rep1&type=pdf,"We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render (or synthesize) images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose, the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone. Test results show that the method performs almost without error, except on the most extreme lighting directions.",Ffrfetomailcomofofareunvalianpo,4633.0,179.0,380.0
2598,Face Recognition,20.0,face recognition applications,5.0,201.0,1.0,177.0,3.0,2.8,139.5,34,https://www.academia.edu/download/60825693/Stan_Z._Li__Anil_K._Jain_-_Handbook_Of_Face_Recognition-Springer_200520191007-15406-11983xh.pdf#page=381,"As one of the most nonintrusive biometrics, face recognition technology is becoming ever closer to people’s daily lives. Evidence of this is that in 2000 the International Civil Aviation Organization endorsed facial recognition as the most suitable biometrics for air travel. To our knowledge, no review papers are available on the newly enlarged application scenarios since then. We hope this chapter will be an extension of the previous studies. We review many face recognition applications that have already used face recognition technologies. This set of applications is a much larger super-set of previously reviewed. We also review some other new scenarios that will potentially utilize face recognition technologies in the near future.",Ffareap,36.0,27.0,0.0
2599,Face Recognition,401.0,longitudinal study of child face recognition,1.0,2.0,5.0,201.0,1.0,2.6,181.4,35,http://arxiv.org/pdf/1711.03990v1,"We present a longitudinal study of face recognition performance on Children Longitudinal Face (CLF) dataset containing 3,682 face images of 919 subjects, in the age group [2, 18] years. Each subject has at least four face images acquired over a time span of up to six years. Face comparison scores are obtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source matcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTS-A and FaceNet matchers. To improve the performance of the open-source FaceNet matcher for child face recognition, we were able to fine-tune it on an independent training set of 3,294 face images of 1,119 children in the age group [3, 18] years. Multilevel statistical models are fit to genuine comparison scores from the CLF dataset to determine the decrease in face recognition accuracy over time. Additionally, we analyze both the verification and open-set identification accuracies in order to evaluate state-of-the-art face recognition technology for tracing and identifying children lost at a young age as victims of child trafficking or abduction.",Flostofchfare,29.0,27.0,2.0
2600,Face Recognition,401.0,sub-center arcface: boosting face recognition by large-scale noisy web faces,1.0,4.0,5.0,201.0,1.0,2.6,182.2,36,http://arxiv.org/pdf/2108.07960v1,"With the recent success of deep neural networks, remarkable progress has been achieved on face recognition. However, collecting large-scale real-world training data for face recognition has turned out to be challenging, especially due to the label noise and privacy issues. Meanwhile, existing face recognition datasets are usually collected from web images, lacking detailed annotations on attributes (e.g., pose and expression), so the influences of different attributes on face recognition have been poorly investigated. In this paper, we address the above-mentioned issues in face recognition using synthetic face images, i.e., SynFace. Specifically, we first explore the performance gap between recent state-of-the-art face recognition models trained with synthetic and real face images. We then analyze the underlying causes behind the performance gap, e.g., the poor intra-class variations and the domain gap between synthetic and real face images. Inspired by this, we devise the SynFace with identity mixup (IM) and domain mixup (DM) to mitigate the above performance gap, demonstrating the great potentials of synthetic data for face recognition. Furthermore, with the controllable face synthesis model, we can easily manage different factors of synthetic face generation, including pose, expression, illumination, the number of identities, and samples per identity. Therefore, we also perform a systematically empirical analysis on synthetic face images to provide some insights on how to effectively utilize synthetic data for face recognition.",Fsuarbofarebylanowefa,42.0,46.0,5.0
2601,Face Recognition,401.0,partial fc: training 10 million identities on a single machine,1.0,6.0,5.0,201.0,1.0,2.6,183.0,37,http://arxiv.org/pdf/2010.05222v2,"Face recognition has been an active and vital topic among computer vision community for a long time. Previous researches mainly focus on loss functions used for facial feature extraction network, among which the improvements of softmax-based loss functions greatly promote the performance of face recognition. However, the contradiction between the drastically increasing number of face identities and the shortage of GPU memories is gradually becoming irreconcilable. In this paper, we thoroughly analyze the optimization goal of softmax-based loss functions and the difficulty of training massive identities. We find that the importance of negative classes in softmax function in face representation learning is not as high as we previously thought. The experiment demonstrates no loss of accuracy when training with only 10\% randomly sampled classes for the softmax-based loss functions, compared with training with full classes using state-of-the-art models on mainstream benchmarks. We also implement a very efficient distributed sampling algorithm, taking into account model accuracy and training efficiency, which uses only eight NVIDIA RTX2080Ti to complete classification tasks with tens of millions of identities. The code of this paper has been made available https://github.com/deepinsight/insightface/tree/master/recognition/partial_fc.",Fpafctr10miidonasima,21.0,24.0,6.0
2602,Face Recognition,401.0,stacked dense u-nets with dual transformers for robust face alignment,1.0,7.0,5.0,201.0,1.0,2.6,183.4,38,http://arxiv.org/pdf/1812.01936v1,"Facial landmark localisation in images captured in-the-wild is an important and challenging problem. The current state-of-the-art revolves around certain kinds of Deep Convolutional Neural Networks (DCNNs) such as stacked U-Nets and Hourglass networks. In this work, we innovatively propose stacked dense U-Nets for this task. We design a novel scale aggregation network topology structure and a channel aggregation building block to improve the model's capacity without sacrificing the computational complexity and model size. With the assistance of deformable convolutions inside the stacked dense U-Nets and coherent loss for outside data transformation, our model obtains the ability to be spatially invariant to arbitrary input face images. Extensive experiments on many in-the-wild datasets, validate the robustness of the proposed method under extreme poses, exaggerated expressions and heavy occlusions. Finally, we show that accurate 3D face alignment can assist pose-invariant face recognition where we achieve a new state-of-the-art accuracy on CFP-FP.",Fstdeu-widutrforofaal,31.0,42.0,4.0
2603,Face Recognition,401.0,"towards fast, accurate and stable 3d dense face alignment",1.0,12.0,5.0,201.0,1.0,2.6,185.4,39,http://arxiv.org/pdf/2009.09960v2,"Existing methods of 3D dense face alignment mainly concentrate on accuracy, thus limiting the scope of their practical applications. In this paper, we propose a novel regression framework named 3DDFA-V2 which makes a balance among speed, accuracy and stability. Firstly, on the basis of a lightweight backbone, we propose a meta-joint optimization strategy to dynamically regress a small set of 3DMM parameters, which greatly enhances speed and accuracy simultaneously. To further improve the stability on videos, we present a virtual synthesis method to transform one still image to a short-video which incorporates in-plane and out-of-plane face moving. On the premise of high accuracy and stability, 3DDFA-V2 runs at over 50fps on a single CPU core and outperforms other state-of-the-art heavy models simultaneously. Experiments on several challenging datasets validate the efficiency of our method. Pre-trained models and code are available at https://github.com/cleardusk/3DDFA_V2.",Ftofaacanst3ddefaal,57.0,70.0,9.0
2604,Face Recognition,401.0,circle loss: a unified perspective of pair similarity optimization,1.0,13.0,5.0,201.0,1.0,2.6,185.8,40,http://arxiv.org/pdf/2002.10857v2,"This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity $s_p$ and minimize the between-class similarity $s_n$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed $s_n$ and $s_p$ into similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning approaches, i.e. learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.",Fciloaunpeofpasiop,174.0,42.0,20.0
2605,Face Recognition,401.0,cross-resolution face recognition via prior-aided face hallucination and residual knowledge distillation,1.0,17.0,5.0,201.0,1.0,2.6,187.4,41,http://arxiv.org/pdf/1905.10777v1,"Recent deep learning based face recognition methods have achieved great performance, but it still remains challenging to recognize very low-resolution query face like 28x28 pixels when CCTV camera is far from the captured subject. Such face with very low-resolution is totally out of detail information of the face identity compared to normal resolution in a gallery and hard to find corresponding faces therein. To this end, we propose a Resolution Invariant Model (RIM) for addressing such cross-resolution face recognition problems, with three distinct novelties. First, RIM is a novel and unified deep architecture, containing a Face Hallucination sub-Net (FHN) and a Heterogeneous Recognition sub-Net (HRN), which are jointly learned end to end. Second, FHN is a well-designed tri-path Generative Adversarial Network (GAN) which simultaneously perceives facial structure and geometry prior information, i.e. landmark heatmaps and parsing maps, incorporated with an unsupervised cross-domain adversarial training strategy to super-resolve very low-resolution query image to its 8x larger ones without requiring them to be well aligned. Third, HRN is a generic Convolutional Neural Network (CNN) for heterogeneous face recognition with our proposed residual knowledge distillation strategy for learning discriminative yet generalized feature representation. Quantitative and qualitative experiments on several benchmarks demonstrate the superiority of the proposed model over the state-of-the-arts. Codes and models will be released upon acceptance.",Fcrfareviprfahaanrekndi,10.0,32.0,1.0
2606,Face Recognition,401.0,"deeply learned face representations are sparse, selective, and robust",1.0,19.0,5.0,201.0,1.0,2.6,188.2,42,http://arxiv.org/pdf/1412.1265v1,"This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set.",Fdelefarearspseanro,750.0,51.0,56.0
2607,Face Recognition,401.0,deep learning face representation by joint identification-verification,1.0,21.0,5.0,201.0,1.0,2.6,189.0,43,http://arxiv.org/pdf/1603.01801v2,"In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multimodal architecture, we observe that such models aren't very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close' to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes.",Fdelefarebyjoid,1772.0,30.0,154.0
2608,Face Recognition,401.0,learning face representation from scratch,1.0,22.0,5.0,201.0,1.0,2.6,189.4,44,http://arxiv.org/pdf/1411.7923v1,"Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild.",Flefarefrsc,1227.0,30.0,273.0
2609,Face Recognition,401.0,a multiresolution 3d morphable face model and fitting framework,1.0,25.0,5.0,201.0,1.0,2.6,190.6,45,http://arxiv.org/pdf/1606.00474v1,"Face analysis techniques have become a crucial component of human-machine interaction in the fields of assistive and humanoid robotics. However, the variations in head-pose that arise naturally in these environments are still a great challenge. In this paper, we present a real-time capable 3D face modelling framework for 2D in-the-wild images that is applicable for robotics. The fitting of the 3D Morphable Model is based exclusively on automatically detected landmarks. After fitting, the face can be corrected in pose and transformed back to a frontal 2D representation that is more suitable for face recognition. We conduct face recognition experiments with non-frontal images from the MUCT database and uncontrolled, in the wild images from the PaSC database, the most challenging face recognition database to date, showing an improved performance. Finally, we present our SCITOS G5 robot system, which incorporates our framework as a means of image pre-processing for face analysis.",Famu3dmofamoanfifr,171.0,20.0,20.0
2610,Face Recognition,401.0,building computationally efficient and well-generalizing person re-identification models with metric learning,1.0,30.0,5.0,201.0,1.0,2.6,192.6,46,http://arxiv.org/abs/2003.07618v2,"This work considers the problem of domain shift in person re-identification.Being trained on one dataset, a re-identification model usually performs much worse on unseen data. Partially this gap is caused by the relatively small scale of person re-identification datasets (compared to face recognition ones, for instance), but it is also related to training objectives. We propose to use the metric learning objective, namely AM-Softmax loss, and some additional training practices to build well-generalizing, yet, computationally efficient models. We use recently proposed Omni-Scale Network (OSNet) architecture combined with several training tricks and architecture adjustments to obtain state-of-the art results in cross-domain generalization problem on a large-scale MSMT17 dataset in three setups: MSMT17-all->DukeMTMC, MSMT17-train->Market1501 and MSMT17-all->Market1501.",Fbucoefanweperemowimele,3.0,64.0,0.0
2611,Face Recognition,401.0,age and gender classification using convolutional neural networks,1.0,31.0,5.0,201.0,1.0,2.6,193.0,47,http://arxiv.org/pdf/1806.05742v1,"In this paper, we present a detailed analysis on extracting soft biometric traits, age and gender, from ear images. Although there have been a few previous work on gender classification using ear images, to the best of our knowledge, this study is the first work on age classification from ear images. In the study, we have utilized both geometric features and appearance-based features for ear representation. The utilized geometric features are based on eight anthropometric landmarks and consist of 14 distance measurements and two area calculations. The appearance-based methods employ deep convolutional neural networks for representation and classification. The well-known convolutional neural network models, namely, AlexNet, VGG-16, GoogLeNet, and SqueezeNet have been adopted for the study. They have been fine-tuned on a large-scale ear dataset that has been built from the profile and close-to-profile face images in the Multi-PIE face dataset. This way, we have performed a domain adaptation. The updated models have been fine-tuned once more time on the small-scale target ear dataset, which contains only around 270 ear images for training. According to the experimental results, appearance-based methods have been found to be superior to the methods based on geometric features. We have achieved 94\% accuracy for gender classification, whereas 52\% accuracy has been obtained for age classification. These results indicate that ear images provide useful cues for age and gender classification, however, further work is required for age estimation.",Fagangeclusconene,818.0,60.0,96.0
2612,Face Recognition,401.0,a dataset and benchmark for large-scale multi-modal face anti-spoofing,1.0,32.0,5.0,201.0,1.0,2.6,193.4,48,http://arxiv.org/pdf/2106.14948v1,"Face anti-spoofing (FAS) has lately attracted increasing attention due to its vital role in securing face recognition systems from presentation attacks (PAs). As more and more realistic PAs with novel types spring up, traditional FAS methods based on handcrafted features become unreliable due to their limited representation capacity. With the emergence of large-scale academic datasets in the recent decade, deep learning based FAS achieves remarkable performance and dominates this area. However, existing reviews in this field mainly focus on the handcrafted features, which are outdated and uninspiring for the progress of FAS community. In this paper, to stimulate future research, we present the first comprehensive review of recent advances in deep learning based FAS. It covers several novel and insightful components: 1) besides supervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also investigate recent methods with pixel-wise supervision (e.g., pseudo depth map); 2) in addition to traditional intra-dataset evaluation, we collect and analyze the latest methods specially designed for domain generalization and open-set FAS; and 3) besides commercial RGB camera, we summarize the deep learning applications under multi-modal (e.g., depth and infrared) or specialized (e.g., light field and flash) sensors. We conclude this survey by emphasizing current open issues and highlighting potential prospects.",Fadaanbefolamufaan,60.0,55.0,18.0
2613,Face Recognition,401.0,regressing robust and discriminative 3d morphable models with a very deep neural network,1.0,33.0,5.0,201.0,1.0,2.6,193.8,49,http://arxiv.org/pdf/1612.04904v1,"The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied ""in the wild"", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.",Freroandi3dmomowiavedenene,301.0,58.0,53.0
2614,Face Recognition,401.0,a light cnn for deep face representation with noisy labels,1.0,35.0,5.0,201.0,1.0,2.6,194.6,50,http://arxiv.org/pdf/1511.02683v4,"The volume of convolutional neural network (CNN) models proposed for face recognition has been continuously growing larger to better fit large amount of training data. When training data are obtained from internet, the labels are likely to be ambiguous and inaccurate. This paper presents a Light CNN framework to learn a compact embedding on the large-scale face data with massive noisy labels. First, we introduce a variation of maxout activation, called Max-Feature-Map (MFM), into each convolutional layer of CNN. Different from maxout activation that uses many feature maps to linearly approximate an arbitrary convex activation function, MFM does so via a competitive relationship. MFM can not only separate noisy and informative signals but also play the role of feature selection between two feature maps. Second, three networks are carefully designed to obtain better performance meanwhile reducing the number of parameters and computational costs. Lastly, a semantic bootstrapping method is proposed to make the prediction of the networks more consistent with noisy labels. Experimental results show that the proposed framework can utilize large-scale noisy data to learn a Light model that is efficient in computational costs and storage spaces. The learned single network with a 256-D representation achieves state-of-the-art results on various face benchmarks without fine-tuning. The code is released on https://github.com/AlfredXiangWu/LightCNN.",Falicnfodefarewinola,669.0,99.0,122.0
2615,Face Recognition,401.0,learning robust deep face representation,1.0,36.0,5.0,201.0,1.0,2.6,195.0,51,http://arxiv.org/pdf/1403.2802v1,"Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy ($85.8\%$ on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance ($97.3\%$) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.",Flerodefare,8.0,16.0,2.0
2616,Face Recognition,401.0,consistent instance false positive improves fairness in face recognition,1.0,37.0,5.0,201.0,1.0,2.6,195.4,52,http://arxiv.org/pdf/2106.05519v1,"Demographic bias is a significant challenge in practical face recognition systems. Existing methods heavily rely on accurate demographic annotations. However, such annotations are usually unavailable in real scenarios. Moreover, these methods are typically designed for a specific demographic group and are not general enough. In this paper, we propose a false positive rate penalty loss, which mitigates face recognition bias by increasing the consistency of instance False Positive Rate (FPR). Specifically, we first define the instance FPR as the ratio between the number of the non-target similarities above a unified threshold and the total number of the non-target similarities. The unified threshold is estimated for a given total FPR. Then, an additional penalty term, which is in proportion to the ratio of instance FPR overall FPR, is introduced into the denominator of the softmax-based loss. The larger the instance FPR, the larger the penalty. By such unequal penalties, the instance FPRs are supposed to be consistent. Compared with the previous debiasing methods, our method requires no demographic annotations. Thus, it can mitigate the bias among demographic groups divided by various attributes, and these attributes are not needed to be previously predefined during training. Extensive experimental results on popular benchmarks demonstrate the superiority of our method over state-of-the-art competitors. Code and trained models are available at https://github.com/Tencent/TFace.",Fcoinfapoimfainfare,1.0,23.0,0.0
2617,Face Recognition,401.0,learning to cluster faces on an affinity graph,1.0,38.0,5.0,201.0,1.0,2.6,195.8,53,http://arxiv.org/pdf/1904.02749v2,"Face recognition sees remarkable progress in recent years, and its performance has reached a very high level. Taking it to a next level requires substantially larger data, which would involve prohibitive annotation cost. Hence, exploiting unlabeled data becomes an appealing alternative. Recent works have shown that clustering unlabeled faces is a promising approach, often leading to notable performance gains. Yet, how to effectively cluster, especially on a large-scale (i.e. million-level or above) dataset, remains an open question. A key challenge lies in the complex variations of cluster patterns, which make it difficult for conventional clustering methods to meet the needed accuracy. This work explores a novel approach, namely, learning to cluster instead of relying on hand-crafted criteria. Specifically, we propose a framework based on graph convolutional network, which combines a detection and a segmentation module to pinpoint face clusters. Experiments show that our method yields significantly more accurate face clusters, which, as a result, also lead to further performance gain in face recognition.",Fletoclfaonanafgr,43.0,33.0,10.0
2618,Face Recognition,401.0,correlation congruence for knowledge distillation,1.0,40.0,5.0,201.0,1.0,2.6,196.6,54,http://arxiv.org/pdf/1904.01802v1,"Most teacher-student frameworks based on knowledge distillation (KD) depend on a strong congruent constraint on instance level. However, they usually ignore the correlation between multiple instances, which is also valuable for knowledge transfer. In this work, we propose a new framework named correlation congruence for knowledge distillation (CCKD), which transfers not only the instance-level information, but also the correlation between instances. Furthermore, a generalized kernel method based on Taylor series expansion is proposed to better capture the correlation between instances. Empirical experiments and ablation studies on image classification tasks (including CIFAR-100, ImageNet-1K) and metric learning tasks (including ReID and Face Recognition) show that the proposed CCKD substantially outperforms the original KD and achieves state-of-the-art accuracy compared with other SOTA KD-based methods. The CCKD can be easily deployed in the majority of the teacher-student framework such as KD and hint-based learning methods.",Fcocofokndi,115.0,42.0,13.0
2619,Face Recognition,50.0,acquiring linear subspaces for face recognition under variable lighting,4.0,201.0,1.0,108.0,3.0,2.5,127.8,55,http://vision.ucsd.edu/~j-ho1/resources/papers/9pltsIEEEDraft.pdf,"Previous work has demonstrated that the image variation of many objects (human faces in particular) under variable lighting can be effectively modeled by low-dimensional linear spaces, even when there are multiple light sources and shadowing. Basis images spanning this space are usually obtained in one of three ways: a large set of images of the object under different lighting conditions is acquired, and principal component analysis (PCA) is used to estimate a subspace. Alternatively, synthetic images are rendered from a 3D model (perhaps reconstructed from images) under point sources and, again, PCA is used to estimate a subspace. Finally, images rendered from a 3D model under diffuse lighting based on spherical harmonics are directly used as basis images. In this paper, we show how to arrange physical lighting so that the acquired images of each object can be directly used as the basis vectors of a low-dimensional linear space and that this subspace is close to those acquired by the other methods. More specifically, there exist configurations of k point light source directions, with k typically ranging from 5 to 9, such that, by taking k images of an object under these single sources, the resulting subspace is an effective representation for recognition under a wide range of lighting conditions. Since the subspace is generated directly from real images, potentially complex and/or brittle intermediate steps such as 3D reconstruction can be completely avoided; nor is it necessary to acquire large numbers of training images or to physically construct complex diffuse (harmonic) light fields. We validate the use of subspaces constructed in this fashion within the context of face recognition.",Faclisufofareunvali,2255.0,32.0,148.0
2620,Face Recognition,61.0,face recognition based on fitting a 3d morphable model,4.0,201.0,1.0,117.0,3.0,2.5,133.8,56,http://arxiv.org/pdf/1709.08398v2,"This paper presents a method for face recognition across variations in pose, ranging from frontal to profile views, and across a wide range of illuminations, including cast shadows and specular reflections. To account for these variations, the algorithm simulates the process of image formation in 3D space, using computer graphics, and it estimates 3D shape and texture of faces from single images. The estimate is achieved by fitting a statistical, morphable model of 3D faces to images. The model is learned from a set of textured 3D scans of heads. We describe the construction of the morphable model, an algorithm to fit the model to images, and a framework for face identification. In this framework, faces are represented by model parameters for 3D shape and texture. We present results obtained with 4,488 images from the publicly available CMU-PIE database and 1,940 images from the FERET database.",Ffarebaonfia3dmomo,2106.0,48.0,171.0
2621,Face Recognition,56.0,the hierarchical brain network for face recognition,4.0,201.0,1.0,131.0,3.0,2.5,136.5,57,http://arxiv.org/pdf/1502.01241v3,"Numerous functional magnetic resonance imaging (fMRI) studies have identified multiple cortical regions that are involved in face processing in the human brain. However, few studies have characterized the face-processing network as a functioning whole. In this study, we used fMRI to identify face-selective regions in the entire brain and then explore the hierarchical structure of the face-processing network by analyzing functional connectivity among these regions. We identified twenty-five regions mainly in the occipital, temporal and frontal cortex that showed a reliable response selective to faces (versus objects) across participants and across scan sessions. Furthermore, these regions were clustered into three relatively independent sub-networks in a face-recognition task on the basis of the strength of functional connectivity among them. The functionality of the sub-networks likely corresponds to the recognition of individual identity, retrieval of semantic knowledge and representation of emotional information. Interestingly, when the task was switched to object recognition from face recognition, the functional connectivity between the inferior occipital gyrus and the rest of the face-selective regions were significantly reduced, suggesting that this region may serve as an entry node in the face-processing network. In sum, our study provides empirical evidence for cognitive and neural models of face recognition and helps elucidate the neural mechanisms underlying face recognition at the network level.",Fthhibrnefofare,102.0,64.0,5.0
2622,Face Recognition,72.0,biometrics and face recognition techniques,4.0,201.0,1.0,136.0,3.0,2.5,142.8,58,http://winteknologi.com/img/product/pdf/ede8225c99f6e1883d4ae14c66fb20191117.pdf,"Biometrics is a growing technology, which has been widely used in forensics, secured access and prison security. A biometric system is fundamentally a pattern recognition system that recognizes a person by determining the authentication by using his different biological features i.e. Fingerprint, retina-scan, iris scan, hand geometry, and face recognition are leading physiological biometrics and behavioral characteristic are Voice recognition, keystroke-scan, and signature-scan. In this paper different biometrics techniques such as Iris scan, retina scan and face recognition techniques are discussed. Keyword: Biometric, Biometric techniques, Eigenface, Face recognition.",Fbianfarete,70.0,12.0,4.0
2623,Face Recognition,65.0,a 3d face model for pose and illumination invariant face recognition,4.0,201.0,1.0,159.0,3.0,2.5,147.6,59,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.715.1710&rep=rep1&type=pdf,"Generative 3D face models are a powerful tool in computer vision. They provide pose and illumination invariance by modeling the space of 3D faces and the imaging process. The power of these models comes at the cost of an expensive and tedious construction process, which has led the community to focus on more easily constructed but less powerful models. With this paper we publish a generative 3D shape and texture model, the Basel Face Model (BFM), and demonstrate its application to several face recognition task. We improve on previous models by offering higher shape and texture accuracy due to a better scanning device and less correspondence artifacts due to an improved registration algorithm. The same 3D face model can be fit to 2D or 3D images acquired under different situations and with different sensors using an analysis by synthesis method. The resulting model parameters separate pose, lighting, imaging and identity parameters, which facilitates invariant face recognition across sensors and data sets by comparing only the identity parameters. We hope that the availability of this registered face model will spur research in generative models. Together with the model we publish a set of detailed recognition and reconstruction results on standard databases to allow complete algorithm comparisons.",Fa3dfamofopoanilinfare,860.0,26.0,143.0
2624,Face Recognition,94.0,indian movie face database: a benchmark for face recognition under wide variations,4.0,201.0,1.0,134.0,3.0,2.5,148.8,60,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.461.2333&rep=rep1&type=pdf,"Recognizing human faces in the wild is emerging as a critically important, and technically challenging computer vision problem. With a few notable exceptions, most previous works in the last several decades have focused on recognizing faces captured in a laboratory setting. However, with the introduction of databases such as LFW and Pubfigs, face recognition community is gradually shifting its focus on much more challenging unconstrained settings. Since its introduction, LFW verification benchmark is getting a lot of attention with various researchers contributing towards state-of-the-results. To further boost the unconstrained face recognition research, we introduce a more challenging Indian Movie Face Database (IMFDB) that has much more variability compared to LFW and Pubfigs. The database consists of 34512 faces of 100 known actors collected from approximately 103 Indian movies. Unlike LFW and Pubfigs which used face detectors to automatically detect the faces from the web collection, faces in IMFDB are detected manually from all the movies. Manual selection of faces from movies resulted in high degree of variability (in scale, pose, expression, illumination, age, occlusion, makeup) which one could ever see in natural world. IMFDB is the first face database that provides a detailed annotation in terms of age, pose, gender, expression, amount of occlusion, for each face which may help other face related applications.",Finmofadaabefofareunwiva,70.0,14.0,6.0
2625,Face Recognition,54.0,a new lda-based face recognition system which can solve the small sample size problem,4.0,201.0,1.0,187.0,3.0,2.5,152.70000000000002,61,http://uwe-mortensen.de/LiFenChenetalNewLDA-basedFaceRecognSystSmallSampleSizeprobl2000.pdf,"A new LDA-based face recognition system is presented in this paper. Linear discriminant analysis (LDA) is one of the most popular linear projection techniques for feature extraction. The major drawback of applying LDA is that it may encounter the small sample size problem. In this paper, we propose a new LDA-based technique which can solve the small sample size problem. We also prove that the most expressive vectors derived in the null space of the within-class scatter matrix using principal component analysis (PCA) are equal to the optimal discriminant vectors derived in the original space using LDA. The experimental results show that the new LDA process improves the performance of a face recognition system signi""cantly. ( 2000 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.",Faneldfaresywhcasothsmsasipr,1434.0,35.0,140.0
2626,Face Recognition,2.0,handbook of face recognition,5.0,201.0,1.0,201.0,1.0,2.2,141.3,62,https://www.academia.edu/download/60825693/Stan_Z._Li__Anil_K._Jain_-_Handbook_Of_Face_Recognition-Springer_200520191007-15406-11983xh.pdf,"From massive face-recognition-based surveillance and machine-learning-based decision systems predicting crime recidivism rates, to the move towards automated health diagnostic systems, artificial intelligence (AI) is being used in scenarios that have serious consequences in people's lives. However, this rapid permeation of AI into society has not been accompanied by a thorough investigation of the sociopolitical issues that cause certain groups of people to be harmed rather than advantaged by it. For instance, recent studies have shown that commercial face recognition systems have much higher error rates for dark skinned women while having minimal errors on light skinned men. A 2016 ProPublica investigation uncovered that machine learning based tools that assess crime recidivism rates in the US are biased against African Americans. Other studies show that natural language processing tools trained on newspapers exhibit societal biases (e.g. finishing the analogy ""Man is to computer programmer as woman is to X"" by homemaker). At the same time, books such as Weapons of Math Destruction and Automated Inequality detail how people in lower socioeconomic classes in the US are subjected to more automated decision making tools than those who are in the upper class. Thus, these tools are most often used on people towards whom they exhibit the most bias. While many technical solutions have been proposed to alleviate bias in machine learning systems, we have to take a holistic and multifaceted approach. This includes standardization bodies determining what types of systems can be used in which scenarios, making sure that automated decision tools are created by people from diverse backgrounds, and understanding the historical and political factors that disadvantage certain groups who are subjected to these tools.",Fhaoffare,0.0,32.0,0.0
2627,Face Recognition,6.0,orthogonal laplacianfaces for face recognition,5.0,201.0,1.0,201.0,1.0,2.2,142.5,63,http://people.cs.uchicago.edu/~xiaofei/journal-4.pdf,"Orthogonal surfaces are nice mathematical objects which have interesting connections to various fields, e.g., integer programming, monomial ideals and order dimension. While orthogonal surfaces in one or two dimensions are rather trivial already the three dimensional case has a rich structure with connections to Schnyder woods, planar graphs and 3-polytopes.   Our objective is to detect more of the structure of orthogonal surfaces in four and higher dimensions. In particular we are driven by the question which non-generic orthogonal surfaces have a polytopal structure.   We study characteristic points and the cp-orders of orthogonal surfaces, i.e., the dominance orders on the characteristic points. In the generic case these orders are (almost) face lattices of polytopes. Examples show that in general cp-orders can lack key properties of face lattices. We investigate extra requirements which may help to have cp-orders which are face lattices.   Finally, we turn the focus and ask for the realizability of polytopes on orthogonal surfaces. There are criteria which prevent large classes of simplicial polytopes from being realizable. On the other hand we identify some families of polytopes which can be realized on orthogonal surfaces.",Forlafofare,731.0,29.0,85.0
2628,Face Recognition,13.0,"face recognition: challenges, achievements and future directions",5.0,201.0,1.0,201.0,1.0,2.2,144.60000000000002,64,https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-cvi.2014.0084,"Face detection is a crucial first step in many facial recognition and face analysis systems. Early approaches for face detection were mainly based on classifiers built on top of hand-crafted features extracted from local image regions, such as Haar Cascades and Histogram of Oriented Gradients. However, these approaches were not powerful enough to achieve a high accuracy on images of from uncontrolled environments. With the breakthrough work in image classification using deep neural networks in 2012, there has been a huge paradigm shift in face detection. Inspired by the rapid progress of deep learning in computer vision, many deep learning based frameworks have been proposed for face detection over the past few years, achieving significant improvements in accuracy. In this work, we provide a detailed overview of some of the most representative deep learning based face detection methods by grouping them into a few major categories, and present their core architectural designs and accuracies on popular benchmarks. We also describe some of the most popular face detection datasets. Finally, we discuss some current challenges in the field, and suggest potential future research directions.",Ffarechacanfudi,99.0,169.0,4.0
2629,Face Recognition,17.0,three approaches for face recognition,5.0,201.0,1.0,201.0,1.0,2.2,145.8,65,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.5362&rep=rep1&type=pdf,"Recently, a variety of approaches has been enriching the field of Remote Sensing (RS) image processing and analysis. Unfortunately, existing methods remain limited faced to the rich spatio-spectral content of today's large datasets. It would seem intriguing to resort to Deep Learning (DL) based approaches at this stage with regards to their ability to offer accurate semantic interpretation of the data. However, the specificity introduced by the coexistence of spectral and spatial content in the RS datasets widens the scope of the challenges presented to adapt DL methods to these contexts. Therefore, the aim of this paper is firstly to explore the performance of DL architectures for the RS hyperspectral dataset classification and secondly to introduce a new three-dimensional DL approach that enables a joint spectral and spatial information process. A set of three-dimensional schemes is proposed and evaluated. Experimental results based on well knownhyperspectral datasets demonstrate that the proposed method is able to achieve a better classification rate than state of the art methods with lower computational costs.",Fthapfofare,57.0,10.0,1.0
2630,Face Recognition,18.0,2d and 3d face recognition: a survey,5.0,201.0,1.0,201.0,1.0,2.2,146.10000000000002,66,https://www.researchgate.net/profile/Daniel-Riccio/publication/220646065_2D_and_3D_Face_Recognition_A_Survey/links/59f828bfaca272607e2da58f/2D-and-3D-Face-Recognition-A-Survey.pdf,"We present here a survey of recent results concerning the mathematical analysis of instabilities of the interface between two incompressible, non viscous, fluids of constant density and vorticity concentrated on the interface. This configuration includes the so-called Kelvin-Helmholtz (the two densities are equal), Rayleigh-Taylor (two different, nonzero, densities) and the water waves (one of the densities is zero) problems. After a brief review of results concerning strong and weak solutions of the Euler equation, we derive interface equations (such as the Birkhoff-Rott equation) that describe the motion of the interface. A linear analysis allows us to exhibit the main features of these equations (such as ellipticity properties); the consequences for the full, non linear, equations are then described. In particular, the solutions of the Kelvin-Helmholtz and Rayleigh-Taylor problems are necessarily analytic if they are above a certain threshold of regularity (a consequence is the illposedness of the initial value problem in a non analytic framework). We also say a few words on the phenomena that may occur below this regularity threshold. Finally, special attention is given to the water waves problem, which is much more stable than the Kelvin-Helmholtz and Rayleigh-Taylor configurations. Most of the results presented here are in 2d (the interface has dimension one), but we give a brief description of similarities and differences in the 3d case.",F2dan3dfareasu,907.0,83.0,36.0
2631,Face Recognition,19.0,a review paper on face recognition techniques,5.0,201.0,1.0,201.0,1.0,2.2,146.4,67,https://www.researchgate.net/profile/Vijay-Mankar/publication/270583005_A_Review_Paper_on_Face_Recognition_Techniques/links/54b4c5c10cf28ebe92e48372/A-Review-Paper-on-Face-Recognition-Techniques.pdf,"The limited capacity to recognize faces under occlusions is a long-standing problem that presents a unique challenge for face recognition systems and even for humans. The problem regarding occlusion is less covered by research when compared to other challenges such as pose variation, different expressions, etc. Nevertheless, occluded face recognition is imperative to exploit the full potential of face recognition for real-world applications. In this paper, we restrict the scope to occluded face recognition. First, we explore what the occlusion problem is and what inherent difficulties can arise. As a part of this review, we introduce face detection under occlusion, a preliminary step in face recognition. Second, we present how existing face recognition methods cope with the occlusion problem and classify them into three categories, which are 1) occlusion robust feature extraction approaches, 2) occlusion aware face recognition approaches, and 3) occlusion recovery based face recognition approaches. Furthermore, we analyze the motivations, innovations, pros and cons, and the performance of representative approaches for comparison. Finally, future challenges and method trends of occluded face recognition are thoroughly discussed.",Farepaonfarete,70.0,109.0,5.0
2632,Face Recognition,21.0,what are the routes to face recognition?,5.0,201.0,1.0,201.0,1.0,2.2,147.0,68,http://arxiv.org/pdf/2107.06217v1,"Being uncertain when facing the unknown is key to intelligent decision making. However, machine learning algorithms lack reliable estimates about their predictive uncertainty. This leads to wrong and overly-confident decisions when encountering classes unseen during training. Despite the importance of equipping classifiers with uncertainty estimates ready for the real world, prior work has focused on small datasets and little or no class discrepancy between training and testing data. To close this gap, we introduce UIMNET: a realistic, ImageNet-scale test-bed to evaluate predictive uncertainty estimates for deep image classifiers. Our benchmark provides implementations of eight state-of-the-art algorithms, six uncertainty measures, four in-domain metrics, three out-domain metrics, and a fully automated pipeline to train, calibrate, ensemble, select, and evaluate models. Our test-bed is open-source and all of our results are reproducible from a fixed commit in our repository. Adding new datasets, algorithms, measures, or metrics is a matter of a few lines of code-in so hoping that UIMNET becomes a stepping stone towards realistic, rigorous, and reproducible research in uncertainty estimation. Our results show that ensembles of ERM classifiers as well as single MIMO classifiers are the two best alternatives currently available to measure uncertainty about both in-domain and out-domain classes.",Fwharthrotofare,87.0,0.0,3.0
2633,Face Recognition,23.0,face recognition using lda-based algorithms,5.0,201.0,1.0,201.0,1.0,2.2,147.60000000000002,69,https://tspace.library.utoronto.ca/bitstream/1807/10085/1/Venetsanopoulos_11349_Plataniotis_8667_431.pdf,"Face authentication is now widely used, especially on mobile devices, rather than authentication using a personal identification number or an unlock pattern, due to its convenience. It has thus become a tempting target for attackers using a presentation attack. Traditional presentation attacks use facial images or videos of the victim. Previous work has proven the existence of master faces, i.e., faces that match multiple enrolled templates in face recognition systems, and their existence extends the ability of presentation attacks. In this paper, we perform an extensive study on latent variable evolution (LVE), a method commonly used to generate master faces. We run an LVE algorithm for various scenarios and with more than one database and/or face recognition system to study the properties of the master faces and to understand in which conditions strong master faces could be generated. Moreover, through analysis, we hypothesize that master faces come from some dense areas in the embedding spaces of the face recognition systems. Last but not least, simulated presentation attacks using generated master faces generally preserve the false-matching ability of their original digital forms, thus demonstrating that the existence of master faces poses an actual threat.",Ffareusldal,814.0,20.0,61.0
2634,Face Recognition,26.0,face recognition using neural network: a review,5.0,201.0,1.0,201.0,1.0,2.2,148.5,70,https://www.researchgate.net/profile/Manisha-Kasar/publication/301727666_Face_Recognition_Using_Neural_Network_A_Review/links/5ef18af5a6fdcc73be96ccc2/Face-Recognition-Using-Neural-Network-A-Review.pdf,"This paper proposes PolyProtect, a method for protecting the sensitive face embeddings that are used to represent people's faces in neural-network-based face verification systems. PolyProtect transforms a face embedding to a more secure template, using a mapping based on multivariate polynomials parameterised by user-specific coefficients and exponents. In this work, PolyProtect is evaluated on two open-source face verification systems in a mobile application context, under the toughest threat model that assumes a fully-informed attacker with complete knowledge of the system and all its parameters. Results indicate that PolyProtect can be tuned to achieve a satisfactory trade-off between the recognition accuracy of the PolyProtected face verification system and the irreversibility of the PolyProtected templates. Furthermore, PolyProtected templates are shown to be effectively unlinkable, especially if the user-specific parameters employed in the PolyProtect mapping are selected in a non-naive manner. The evaluation is conducted using practical methodologies with tangible results, to present realistic insight into the method's robustness as a face embedding protection scheme in practice. The code to fully reproduce this work is available at: https://gitlab.idiap.ch/bob/bob.paper.polyprotect_2021.",Ffareusneneare,61.0,20.0,2.0
2635,Face Recognition,27.0,the “parts and wholes” of face recognition: a review of the literature,5.0,201.0,1.0,201.0,1.0,2.2,148.8,71,http://arxiv.org/pdf/2110.09299v1,"This paper is a brief survey of the recent literature on 3D face reconstruction from a single image. Most articles have been choosen among 2016 and 2020, in order to provide the most up-to-date view of the single image 3D face reconstruction.",Fth“panwhoffareareofthli,72.0,96.0,4.0
2637,Face Recognition,29.0,review of face recognition techniques,5.0,201.0,1.0,201.0,1.0,2.2,149.4,72,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.742.1483&rep=rep1&type=pdf,"From face recognition systems installed in phones to self-driving cars, the field of AI is witnessing rapid transformations and is being integrated into our everyday lives at an incredible pace. Any major failure in these system's predictions could be devastating, leaking sensitive information or even costing lives (as in the case of self-driving cars). However, deep neural networks, which form the basis of such systems, are highly susceptible to a specific type of attack, called adversarial attacks. A hacker can, even with bare minimum computation, generate adversarial examples (images or data points that belong to another class, but consistently fool the model to get misclassified as genuine) and crumble the basis of such algorithms. In this paper, we compile and test numerous approaches to defend against such adversarial attacks. Out of the ones explored, we found two effective techniques, namely Dropout and Denoising Autoencoders, and show their success in preventing such attacks from fooling the model. We demonstrate that these techniques are also resistant to both higher noise levels as well as different kinds of adversarial attacks (although not tested against all). We also develop a framework for deciding the suitable defense technique to use against attacks, based on the nature of the application and resource constraints of the Deep Neural Network.",Freoffarete,31.0,52.0,0.0
2638,Face Recognition,30.0,a direct lda algorithm for high-dimensional data—with application to face recognition,5.0,201.0,1.0,201.0,1.0,2.2,149.7,73,http://www.uwe-mortensen.de/YuYangDirectLDAAlgorithmHighDimData2001%20(2).pdf,"In a world where security issues have been gaining growing importance, face recognition systems have attracted increasing attention in multiple application areas, ranging from forensics and surveillance to commerce and entertainment. To help understanding the landscape and abstraction levels relevant for face recognition systems, face recognition taxonomies allow a deeper dissection and comparison of the existing solutions. This paper proposes a new, more encompassing and richer multi-level face recognition taxonomy, facilitating the organization and categorization of available and emerging face recognition solutions; this taxonomy may also guide researchers in the development of more efficient face recognition solutions. The proposed multi-level taxonomy considers levels related to the face structure, feature support and feature extraction approach. Following the proposed taxonomy, a comprehensive survey of representative face recognition solutions is presented. The paper concludes with a discussion on current algorithmic and application related challenges which may define future research directions for face recognition.",Fadildalfohidaaptofare,1587.0,11.0,214.0
2639,Face Recognition,31.0,face recognition using neural networks,5.0,201.0,1.0,201.0,1.0,2.2,150.0,74,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.233.5636&rep=rep1&type=pdf,"Deep learning is one of the new and important branches in machine learning. Deep learning refers to a set of algorithms that solve various problems such as images and texts by using various machine learning algorithms in multi-layer neural networks. Deep learning can be classified as a neural network from the general category, but there are many changes in the concrete realization. At the core of deep learning is feature learning, which is designed to obtain hierarchical information through hierarchical networks, so as to solve the important problems that previously required artificial design features. Deep Learning is a framework that contains several important algorithms. For different applications (images, voice, text), you need to use different network models to achieve better results. With the development of deep learning and the introduction of deep convolutional neural networks, the accuracy and speed of face recognition have made great strides. However, as we said above, the results from different networks and models are very different. In this paper, facial features are extracted by merging and comparing multiple models, and then a deep neural network is constructed to train and construct the combined features. In this way, the advantages of multiple models can be combined to mention the recognition accuracy. After getting a model with high accuracy, we build a product model. This article compares the pure-client model with the server-client model, analyzes the pros and cons of the two models, and analyzes the various commercial products that are required for the server-client model.",Ffareusnene,158.0,13.0,8.0
2641,Face Recognition,34.0,three-dimensional face recognition: an eigensurface approach,5.0,201.0,1.0,201.0,1.0,2.2,150.9,75,https://eprints.whiterose.ac.uk/1526/01/austinj4.pdf,"We discuss a general Bayesian framework on modeling multidimensional function-valued processes by using a Gaussian process or a heavy-tailed process as a prior, enabling us to handle nonseparable and/or nonstationary covariance structure. The nonstationarity is introduced by a convolution-based approach through a varying anisotropy matrix, whose parameters vary along the input space and are estimated via a local empirical Bayesian method. For the varying matrix, we propose to use a spherical parametrization, leading to unconstrained and interpretable parameters. The unconstrained nature allows the parameters to be modeled as a nonparametric function of time, spatial location or other covariates. The interpretation of the parameters is based on closed-form expressions, providing valuable insights into nonseparable covariance structures. Furthermore, to extract important information in data with complex covariance structure, the Bayesian framework can decompose the function-valued processes using the eigenvalues and eigensurfaces calculated from the estimated covariance structure. The results are demonstrated by simulation studies and by an application to wind intensity data. Supplementary materials for this article are available online.",Fthfareaneiap,79.0,24.0,3.0
2642,Face Recognition,35.0,face recognition using intrinsicfaces,5.0,201.0,1.0,201.0,1.0,2.2,151.2,76,http://arxiv.org/pdf/2106.04112v2,"The common implementation of face recognition systems as a cascade of a detection stage and a recognition or verification stage can cause problems beyond failures of the detector. When the detector succeeds, it can detect faces that cannot be recognized, no matter how capable the recognition system. Recognizability, a latent variable, should therefore be factored into the design and implementation of face recognition systems. We propose a measure of recognizability of a face image that leverages a key empirical observation: an embedding of face images, implemented by a deep neural network trained using mostly recognizable identities, induces a partition of the hypersphere whereby unrecognizable identities cluster together. This occurs regardless of the phenomenon that causes a face to be unrecognizable, it be optical or motion blur, partial occlusion, spatial quantization, poor illumination. Therefore, we use the distance from such an ""unrecognizable identity"" as a measure of recognizability, and incorporate it in the design of the over-all system. We show that accounting for recognizability reduces error rate of single-image face recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification benchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based recognition on the IJB-C benchmark.",Ffareusin,30.0,33.0,3.0
2643,Face Recognition,36.0,face recognition by humans: nineteen results all computer vision researchers should know about,5.0,201.0,1.0,201.0,1.0,2.2,151.5,77,https://www.cl.cam.ac.uk/teaching/2006/CompVision/SinhaFaceRecogProcIEEE.pdf,"In this paper, we present experimental results obtained from retraining the last layer of the Inception v3 model in classifying images of human faces into one of five basic face shapes. The accuracy of the retrained Inception v3 model was compared with that of the following classification methods that uses facial landmark distance ratios and angles as features: linear discriminant analysis (LDA), support vector machines with linear kernel (SVM-LIN), support vector machines with radial basis function kernel (SVM-RBF), artificial neural networks or multilayer perceptron (MLP), and k-nearest neighbors (KNN). All classifiers were trained and tested using a total of 500 images of female celebrities with known face shapes collected from the Internet. Results show that training accuracy and overall accuracy ranges from 98.0% to 100% and from 84.4% to 84.8% for Inception v3 and from 50.6% to 73.0% and from 36.4% to 64.6% for the other classifiers depending on the training set size used. This result shows that the retrained Inception v3 model was able to fit the training data well and outperform the other classifiers without the need to handpick specific features to include in model training. Future work should consider expanding the labeled dataset, preferably one that can also be freely distributed to the research community, so that proper model cross-validation can be performed. As far as we know, this is the first in the literature to use convolutional neural networks in face-shape classification. The scripts are available at https://github.com/adonistio/inception-face-shape-classifier.",Ffarebyhunirealcovireshknab,702.0,217.0,36.0
2644,Face Recognition,38.0,face recognition based on the appearance of local regions,5.0,201.0,1.0,201.0,1.0,2.2,152.10000000000002,78,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.2818&rep=rep1&type=pdf,"In this paper, we propose an effective face completion algorithm using a deep generative model. Different from well-studied background completion, the face completion task is more challenging as it often requires to generate semantically new pixels for the missing key components (e.g., eyes and mouths) that contain large appearance variations. Unlike existing nonparametric algorithms that search for patches to synthesize, our algorithm directly generates contents for missing regions based on a neural network. The model is trained with a combination of a reconstruction loss, two adversarial losses and a semantic parsing loss, which ensures pixel faithfulness and local-global contents consistency. With extensive experimental results, we demonstrate qualitatively and quantitatively that our model is able to deal with a large area of missing pixels in arbitrary shapes and generate realistic face completion results.",Ffarebaonthapoflore,68.0,14.0,2.0
2645,Face Recognition,39.0,a novel svm+ nda model for classification with an application to face recognition,5.0,201.0,1.0,201.0,1.0,2.2,152.4,79,http://arxiv.org/pdf/2107.06209v1,"Discriminative features play an important role in image and object classification and also in other fields of research such as semi-supervised learning, fine-grained classification, out of distribution detection. Inspired by Linear Discriminant Analysis (LDA), we propose an optimization called Neural Discriminant Analysis (NDA) for Deep Convolutional Neural Networks (DCNNs). NDA transforms deep features to become more discriminative and, therefore, improves the performances in various tasks. Our proposed optimization has two primary goals for inter- and intra-class variances. The first one is to minimize variances within each individual class. The second goal is to maximize pairwise distances between features coming from different classes. We evaluate our NDA optimization in different research fields: general supervised classification, fine-grained classification, semi-supervised learning, and out of distribution detection. We achieve performance improvements in all the fields compared to baseline methods that do not use NDA. Besides, using NDA, we also surpass the state of the art on the four tasks on various testing datasets.",Fanosvndmofoclwianaptofare,79.0,32.0,0.0
2646,Face Recognition,40.0,face recognition: from traditional to deep learning methods,5.0,201.0,1.0,201.0,1.0,2.2,152.7,80,https://arxiv.org/pdf/1811.00116,"In the application of face recognition, eyeglasses could significantly degrade the recognition accuracy. A feasible method is to collect large-scale face images with eyeglasses for training deep learning methods. However, it is difficult to collect the images with and without glasses of the same identity, so that it is difficult to optimize the intra-variations caused by eyeglasses. In this paper, we propose to address this problem in a virtual synthesis manner. The high-fidelity face images with eyeglasses are synthesized based on 3D face model and 3D eyeglasses. Models based on deep learning methods are then trained on the synthesized eyeglass face dataset, achieving better performance than previous ones. Experiments on the real face database validate the effectiveness of our synthesized data for improving eyeglass face recognition performance.",Ffarefrtrtodeleme,48.0,163.0,3.0
2647,Face Recognition,120.0,low-resolution face recognition: a review,3.0,201.0,1.0,130.0,3.0,2.2,155.4,81,http://arxiv.org/pdf/2108.11821v1,"Low-resolution face recognition (LR FR) aims to recognize faces from small size or poor quality images with varying pose, illumination, expression, etc. It has received much attention with increasing demands for long distance surveillance applications, and extensive efforts have been made on LR FR research in recent years. However, many issues in LR FR are still unsolved, such as super-resolution (SR) for face recognition, resolution-robust features, unified feature spaces, and face detection at a distance, although many methods have been developed for that. This paper provides a comprehensive survey on these methods and discusses many related issues. First, it gives an overview on LR FR, including concept description, system architecture, and method categorization. Second, many representative methods are broadly reviewed and discussed. They are classified into two different categories, super-resolution for LR FR and resolution-robust feature representation for LR FR. Their strategies and advantages/disadvantages are elaborated. Some relevant issues such as databases and evaluations for LR FR are also presented. By generalizing their performances and limitations, promising trends and crucial issues for future research are finally discussed.",Flofareare,95.0,160.0,6.0
2648,Face Recognition,137.0,gabor feature based classification using the enhanced fisher linear discriminant model for face recognition,3.0,201.0,1.0,125.0,3.0,2.2,159.0,82,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.7675&rep=rep1&type=pdf,"This paper introduces a novel Gabor-Fisher (1936) classifier (GFC) for face recognition. The GFC method, which is robust to changes in illumination and facial expression, applies the enhanced Fisher linear discriminant model (EFM) to an augmented Gabor feature vector derived from the Gabor wavelet representation of face images. The novelty of this paper comes from 1) the derivation of an augmented Gabor feature vector, whose dimensionality is further reduced using the EFM by considering both data compression and recognition (generalization) performance; 2) the development of a Gabor-Fisher classifier for multi-class problems; and 3) extensive performance evaluation studies. In particular, we performed comparative studies of different similarity measures applied to various classifiers. We also performed comparative experimental studies of various face recognition schemes, including our novel GFC method, the Gabor wavelet method, the eigenfaces method, the Fisherfaces method, the EFM method, the combination of Gabor and the eigenfaces method, and the combination of Gabor and the Fisherfaces method. The feasibility of the new GFC method has been successfully tested on face recognition using 600 FERET frontal face images corresponding to 200 subjects, which were acquired under variable illumination and facial expressions. The novel GFC method achieves 100% accuracy on face recognition using only 62 features.",Fgafebaclusthenfilidimofofare,1943.0,42.0,150.0
2649,Face Recognition,150.0,local derivative pattern versus local binary pattern: face recognition with high-order local pattern descriptor,3.0,201.0,1.0,126.0,3.0,2.2,163.2,83,https://www.researchgate.net/profile/Yongsheng-Gao-4/publication/224611962_Local_Derivative_Pattern_Versus_Local_Binary_Pattern_Face_Recognition_With_High-Order_Local_Pattern_Descriptor/links/0fcfd510344e6d065c000000/Local-Derivative-Pattern-Versus-Local-Binary-Pattern-Face-Recognition-With-High-Order-Local-Pattern-Descriptor.pdf,"This paper proposes a novel high-order local pattern descriptor, local derivative pattern (LDP), for face recognition. LDP is a general framework to encode directional pattern features based on local derivative variations. The nth-order LDP is proposed to encode the (n-1)th -order local derivative direction variations, which can capture more detailed information than the first-order local pattern used in local binary pattern (LBP). Different from LBP encoding the relationship between the central point and its neighbors, the LDP templates extract high-order local information by encoding various distinctive spatial relationships contained in a given local region. Both gray-level images and Gabor feature images are used to evaluate the comparative performances of LDP and LBP. Extensive experimental results on FERET, CAS-PEAL, CMU-PIE, Extended Yale B, and FRGC databases show that the high-order LDP consistently performs much better than LBP for both face identification and face verification under various conditions.",Flodepavelobipafarewihilopade,943.0,34.0,99.0
2650,Face Recognition,128.0,distinguishing identical twins by face recognition,3.0,201.0,1.0,171.0,3.0,2.2,170.10000000000002,84,https://www3.nd.edu/~kwb/Phillips_EtAl_FG_2011.pdf,"The paper measures the ability of face recognition algorithms to distinguish between identical twin siblings. The experimental dataset consists of images taken of 126 pairs of identical twins (252 people) collected on the same day and 24 pairs of identical twins (48 people) with images collected one year apart. In terms of both the number of paris of twins and lapsed time between acquisitions, this is the most extensive investigation of face recognition performance on twins to date. Recognition experiments are conducted using three of the top submissions to the Multiple Biometric Evaluation (MBE) 2010 Still Face Track [1]. Performance results are reported for both same day and cross year matching. Performance results are broken out by lighting conditions (studio and outside); expression (neutral and smiling); gender and age. Confidence intervals were generated by a bootstrap method. This is the most detailed covariate analysis of face recognition of twins to date.",Fdiidtwbyfare,91.0,13.0,8.0
2651,Face Recognition,170.0,image sets alignment for video-based face recognition,3.0,201.0,1.0,145.0,3.0,2.2,174.9,85,http://www.jdl.ac.cn/doc/2011/20127217272619068_cvpr12-z.cui-print.pdf,"Video-based Face Recognition (VFR) can be converted to the matching of two image sets containing face images captured from each video. For this purpose, we propose to bridge the two sets with a reference image set that is well-defined and pre-structured to a number of local models offline. In other words, given two image sets, as long as each of them is aligned to the reference set, they are mutually aligned and well structured. Therefore, the similarity between them can be computed by comparing only the corresponded local models rather than considering all the pairs. To align an image set with the reference set, we further formulate the problem as a quadratic programming. It integrates three constrains to guarantee robust alignment, including appearance matching cost term exploiting principal angles, geometric structure consistency using affine invariant reconstruction weights, smoothness constraint preserving local neighborhood relationship. Extensive experimental evaluations are performed on three databases: Honda, MoBo and YouTube. Compared with competing methods, our approach can consistently achieve better results.",Fimsealfovifare,91.0,41.0,2.0
2652,Face Recognition,187.0,3d face recognition using local binary patterns,3.0,201.0,1.0,135.0,3.0,2.2,177.0,86,http://arxiv.org/abs/1708.07937v1,"It is well recognized that expressions can significantly change facial geometry that results in a severe problem for robust 3D face recognition. So it is crucial for many applications that how to extract expression-robust features to describe 3D faces. In this paper, we develop a novel 3D face recognition algorithm using Local Binary Pattern (LBP) under expression varieties, which is an extension of the LBP operator widely used in ordinary facial analysis. First, to depict the human face more accurately and reduce the effect of facial local distortion for face recognition, a special feature-based 3D face division scheme is proposed. Then, the LBP representation framework for 3D faces is described, and the facial depth and normal information are extracted and encoded by LBP, to reduce the expression effect. For each face region, the statistical histogram is utilized to summarize the facial details, and accordingly three matching strategies are presented to address the recognition task. Finally, the proposed 3D face recognition algorithm is tested on BJUT-3D and FRGC v2.0 databases, achieves promising results, and concludes that it is feasible and valid to apply the LBP representation framework on 3D face recognition.",F3dfareuslobipa,97.0,37.0,2.0
2653,Face Recognition,401.0,consensus-driven propagation in massive unlabeled data for face recognition,1.0,41.0,4.0,201.0,1.0,2.2,197.0,87,http://arxiv.org/pdf/1809.01407v2,"Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the ""committee"" and the ""mediator"", which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.",Fcoprinmaundafofare,36.0,39.0,8.0
2654,Face Recognition,401.0,dual-cross central difference network for face anti-spoofing,1.0,43.0,4.0,201.0,1.0,2.2,197.8,88,http://arxiv.org/pdf/1807.09968v1,"Many prior face anti-spoofing works develop discriminative models for recognizing the subtle differences between live and spoof faces. Those approaches often regard the image as an indivisible unit, and process it holistically, without explicit modeling of the spoofing process. In this work, motivated by the noise modeling and denoising algorithms, we identify a new problem of face de-spoofing, for the purpose of anti-spoofing: inversely decomposing a spoof face into a spoof noise and a live face, and then utilizing the spoof noise for classification. A CNN architecture with proper constraints and supervisions is proposed to overcome the problem of having no ground truth for the decomposition. We evaluate the proposed method on multiple face anti-spoofing databases. The results show promising improvements due to our spoof noise modeling. Moreover, the estimated spoof noise provides a visualization which helps to understand the added spoof noise by each spoof medium.",Fducedinefofaan,6.0,38.0,1.0
2656,Face Recognition,401.0,searching central difference convolutional networks for face anti-spoofing,1.0,45.0,4.0,201.0,1.0,2.2,198.6,89,http://arxiv.org/pdf/2003.04092v1,"Face anti-spoofing (FAS) plays a vital role in face recognition systems. Most state-of-the-art FAS methods 1) rely on stacked convolutions and expert-designed network, which is weak in describing detailed fine-grained information and easily being ineffective when the environment varies (e.g., different illumination), and 2) prefer to use long sequence as input to extract dynamic features, making them difficult to deploy into scenarios which need quick response. Here we propose a novel frame level FAS method based on Central Difference Convolution (CDC), which is able to capture intrinsic detailed patterns via aggregating both intensity and gradient information. A network built with CDC, called the Central Difference Convolutional Network (CDCN), is able to provide more robust modeling capacity than its counterpart built with vanilla convolution. Furthermore, over a specifically designed CDC search space, Neural Architecture Search (NAS) is utilized to discover a more powerful network structure (CDCN++), which can be assembled with Multiscale Attention Fusion Module (MAFM) for further boosting performance. Comprehensive experiments are performed on six benchmark datasets to show that 1) the proposed method not only achieves superior performance on intra-dataset testing (especially 0.2% ACER in Protocol-1 of OULU-NPU dataset), 2) it also generalizes well on cross-dataset testing (particularly 6.5% HTER from CASIA-MFSD to Replay-Attack datasets). The codes are available at \href{https://github.com/ZitongYu/CDCN}{https://github.com/ZitongYu/CDCN}.",Fsecediconefofaan,54.0,75.0,13.0
2657,Face Recognition,401.0,an improvement of data classification using random multimodel deep learning (rmdl),1.0,46.0,4.0,201.0,1.0,2.2,199.0,90,http://arxiv.org/abs/1808.08121v1,"The exponential growth in the number of complex datasets every year requires more enhancement in machine learning methods to provide robust and accurate data classification. Lately, deep learning approaches have achieved surpassing results in comparison to previous machine learning algorithms. However, finding the suitable structure for these models has been a challenge for researchers. This paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble, deep learning approach for classification. RMDL solves the problem of finding the best deep learning structure and architecture while simultaneously improving robustness and accuracy through ensembles of deep learning architectures. In short, RMDL trains multiple randomly generated models of Deep Neural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) in parallel and combines their results to produce better result of any of those models individually. In this paper, we describe RMDL model and compare the results for image and text classification as well as face recognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for image classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text classification. Lastly, we used ORL dataset to compare the model performance on face recognition task.",Fanimofdaclusramudele(r,19.0,68.0,0.0
2658,Face Recognition,401.0,can we still avoid automatic face detection?,1.0,51.0,4.0,201.0,1.0,2.2,201.0,91,http://arxiv.org/pdf/1602.04504v2,"After decades of study, automatic face detection and recognition systems are now accurate and widespread. Naturally, this means users who wish to avoid automatic recognition are becoming less able to do so. Where do we stand in this cat-and-mouse race? We currently live in a society where everyone carries a camera in their pocket. Many people willfully upload most or all of the pictures they take to social networks which invest heavily in automatic face recognition systems. In this setting, is it still possible for privacy-conscientious users to avoid automatic face detection and recognition? If so, how? Must evasion techniques be obvious to be effective, or are there still simple measures that users can use to protect themselves?   In this work, we find ways to evade face detection on Facebook, a representative example of a popular social network that uses automatic face detection to enhance their service. We challenge widely-held beliefs about evading face detection: do our old techniques such as blurring the face region or wearing ""privacy glasses"" still work? We show that in general, state-of-the-art detectors can often find faces even if the subject wears occluding clothing or even if the uploader damages the photo to prevent faces from being detected.",Fcawestavaufade,44.0,29.0,2.0
2659,Face Recognition,401.0,look across elapse: disentangled representation learning and photorealistic cross-age face synthesis for age-invariant face recognition,1.0,53.0,4.0,201.0,1.0,2.2,201.8,92,http://arxiv.org/pdf/1809.00338v2,"Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.",Floaceldireleanphcrfasyfoagfare,45.0,59.0,6.0
2660,Face Recognition,401.0,face quality estimation and its correlation to demographic and non-demographic bias in face recognition,1.0,54.0,4.0,201.0,1.0,2.2,202.2,93,http://arxiv.org/pdf/2004.01019v3,"Face quality assessment aims at estimating the utility of a face image for the purpose of recognition. It is a key factor to achieve high face recognition performances. Currently, the high performance of these face recognition systems come with the cost of a strong bias against demographic and non-demographic sub-groups. Recent work has shown that face quality assessment algorithms should adapt to the deployed face recognition system, in order to achieve highly accurate and robust quality estimations. However, this could lead to a bias transfer towards the face quality assessment leading to discriminatory effects e.g. during enrolment. In this work, we present an in-depth analysis of the correlation between bias in face recognition and face quality assessment. Experiments were conducted on two publicly available datasets captured under controlled and uncontrolled circumstances with two popular face embeddings. We evaluated four state-of-the-art solutions for face quality assessment towards biases to pose, ethnicity, and age. The experiments showed that the face quality assessment solutions assign significantly lower quality values towards subgroups affected by the recognition bias demonstrating that these approaches are biased as well. This raises ethical questions towards fairness and discrimination which future works have to address.",Ffaquesanitcotodeannobiinfare,12.0,61.0,0.0
2661,Face Recognition,401.0,masked face recognition for secure authentication,1.0,56.0,4.0,201.0,1.0,2.2,203.0,94,http://arxiv.org/pdf/2008.11104v1,"With the recent world-wide COVID-19 pandemic, using face masks have become an important part of our lives. People are encouraged to cover their faces when in public area to avoid the spread of infection. The use of these face masks has raised a serious question on the accuracy of the facial recognition system used for tracking school/office attendance and to unlock phones. Many organizations use facial recognition as a means of authentication and have already developed the necessary datasets in-house to be able to deploy such a system. Unfortunately, masked faces make it difficult to be detected and recognized, thereby threatening to make the in-house datasets invalid and making such facial recognition systems inoperable. This paper addresses a methodology to use the current facial datasets by augmenting it with tools that enable masked faces to be recognized with low false-positive rates and high overall accuracy, without requiring the user dataset to be recreated by taking new pictures for authentication. We present an open-source tool, MaskTheFace to mask faces effectively creating a large dataset of masked faces. The dataset generated with this tool is then used towards training an effective facial recognition system with target accuracy for masked faces. We report an increase of 38% in the true positive rate for the Facenet system. We also test the accuracy of re-trained system on a custom real-world dataset MFR2 and report similar accuracy.",Fmafarefoseau,22.0,17.0,6.0
2663,Face Recognition,401.0,knowledge distillation via route constrained optimization,1.0,59.0,4.0,201.0,1.0,2.2,204.2,95,http://arxiv.org/pdf/1904.09149v1,"Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a high lower bound of congruence loss. In this work, inspired by curriculum learning we consider the knowledge distillation from the perspective of curriculum learning by routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR100 and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace.",Fkndivirocoop,36.0,38.0,7.0
2664,Face Recognition,401.0,disentangled representation learning gan for pose-invariant face recognition,1.0,201.0,1.0,12.0,5.0,2.2,204.3,96,http://arxiv.org/pdf/1909.13135v1,"The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art.",Fdirelegafopofare,628.0,48.0,78.0
2665,Face Recognition,401.0,neural aggregation network for video face recognition,1.0,201.0,1.0,13.0,5.0,2.2,204.6,97,http://arxiv.org/pdf/1603.05474v4,"This paper presents a Neural Aggregation Network (NAN) for video face recognition. The network takes a face video or face image set of a person with a variable number of face images as its input, and produces a compact, fixed-dimension feature representation for recognition. The whole network is composed of two modules. The feature embedding module is a deep Convolutional Neural Network (CNN) which maps each face image to a feature vector. The aggregation module consists of two attention blocks which adaptively aggregate the feature vectors to form a single feature inside the convex hull spanned by them. Due to the attention mechanism, the aggregation is invariant to the image order. Our NAN is trained with a standard classification or verification loss without any extra supervision signal, and we found that it automatically learns to advocate high-quality face images while repelling low-quality ones such as blurred, occluded and improperly exposed faces. The experiments on IJB-A, YouTube Face, Celebrity-1000 video face recognition benchmarks show that it consistently outperforms naive aggregation methods and achieves the state-of-the-art accuracy.",Fneagnefovifare,282.0,71.0,54.0
2666,Face Recognition,401.0,probabilistic face embeddings,1.0,63.0,4.0,201.0,1.0,2.2,205.8,98,http://arxiv.org/pdf/1904.09658v4,"Embedding methods have achieved success in face recognition by comparing facial features in a latent semantic space. However, in a fully unconstrained face setting, the facial features learned by the embedding model could be ambiguous or may not even be present in the input face, leading to noisy representations. We propose Probabilistic Face Embeddings (PFEs), which represent each face image as a Gaussian distribution in the latent space. The mean of the distribution estimates the most likely feature values while the variance shows the uncertainty in the feature values. Probabilistic solutions can then be naturally derived for matching and fusing PFEs using the uncertainty information. Empirical evaluation on different baseline models, training datasets and benchmarks show that the proposed method can improve the face recognition performance of deterministic embeddings by converting them into PFEs. The uncertainties estimated by PFEs also serve as good indicators of the potential matching accuracy, which are important for a risk-controlled recognition system.",Fprfaem,79.0,53.0,22.0
2667,Face Recognition,401.0,perception for autonomous systems (paz),1.0,65.0,4.0,201.0,1.0,2.2,206.6,99,http://arxiv.org/pdf/2010.14541v1,"In this paper we introduce the Perception for Autonomous Systems (PAZ) software library. PAZ is a hierarchical perception library that allow users to manipulate multiple levels of abstraction in accordance to their requirements or skill level. More specifically, PAZ is divided into three hierarchical levels which we refer to as pipelines, processors, and backends. These abstractions allows users to compose functions in a hierarchical modular scheme that can be applied for preprocessing, data-augmentation, prediction and postprocessing of inputs and outputs of machine learning (ML) models. PAZ uses these abstractions to build reusable training and prediction pipelines for multiple robot perception tasks such as: 2D keypoint estimation, 2D object detection, 3D keypoint discovery, 6D pose estimation, emotion classification, face recognition, instance segmentation, and attention mechanisms.",Fpefoausy(p,0.0,28.0,0.0
2668,Face Recognition,401.0,face recognition using eigenfaces,1.0,201.0,1.0,20.0,5.0,2.2,206.7,100,http://arxiv.org/pdf/1705.02782v1,"An approach to the detection and identification of human faces is presented, and a working, near-real-time face recognition system which tracks a subject's head and then recognizes the person by comparing characteristics of the face to those of known individuals is described. This approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space ('face space') that best encodes the variation among known face images. The face space is defined by the 'eigenfaces', which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses. The framework provides the ability to learn to recognize new faces in an unsupervised manner.<<ETX>>",Ffareusei,3853.0,12.0,396.0
3035,Image Captioning,3.0,self-critical sequence training for image captioning,5.0,16.0,5.0,5.0,5.0,5.0,8.8,1,http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf,"Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a baseline to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.",Isesetrfoimca,856.0,30.0,191.0
3036,Image Captioning,1.0,show and tell: lessons learned from the 2015 mscoco image captioning challenge,5.0,2.0,5.0,28.0,5.0,5.0,9.5,2,https://ieeexplore.ieee.org/iel7/34/4359286/07505636.pdf,"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.",Ishantelelefrth20msimcach,576.0,57.0,76.0
3037,Image Captioning,8.0,attention on attention for image captioning,5.0,15.0,5.0,4.0,5.0,5.0,9.6,3,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf,"Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet.",Iatonatfoimca,203.0,50.0,54.0
3038,Image Captioning,33.0,bottom-up and top-down attention for image captioning and visual question answering,5.0,10.0,5.0,1.0,5.0,5.0,14.2,4,https://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf,"Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",Iboantoatfoimcaanviquan,1851.0,67.0,432.0
3039,Image Captioning,9.0,meshed-memory transformer for image captioning,5.0,36.0,5.0,3.0,5.0,5.0,18.0,5,http://openaccess.thecvf.com/content_CVPR_2020/papers/Cornia_Meshed-Memory_Transformer_for_Image_Captioning_CVPR_2020_paper.pdf,"Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M² - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M² Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the ""Karpathy"" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.",Imetrfoimca,129.0,49.0,26.0
3040,Image Captioning,26.0,unified vision-language pre-training for image captioning and vqa,5.0,39.0,5.0,2.0,5.0,5.0,24.000000000000004,6,https://ojs.aaai.org/index.php/AAAI/article/download/7005/6859,"This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at this https URL.",Iunviprfoimcaanvq,213.0,43.0,30.0
3041,Image Captioning,27.0,knowing when to look: adaptive attention via a visual sentinel for image captioning,5.0,37.0,5.0,6.0,5.0,5.0,24.7,7,http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf,"Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",Iknwhtoloadatviavisefoimca,864.0,39.0,111.0
3042,Image Captioning,23.0,"conceptual captions: a cleaned, hypernymed, image alt-text dataset for automatic image captioning",5.0,38.0,5.0,14.0,5.0,5.0,26.3,8,https://www.aclweb.org/anthology/P18-1238.pdf,"We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",Icocaaclhyimaldafoauimca,385.0,33.0,78.0
3043,Image Captioning,13.0,x-linear attention networks for image captioning,5.0,50.0,4.0,8.0,5.0,4.6,26.3,9,http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_X-Linear_Attention_Networks_for_Image_Captioning_CVPR_2020_paper.pdf,"Recent progress on fine-grained visual recognition and visual question answering has featured Bilinear Pooling, which effectively models the 2nd order interactions across multi-modal inputs. Nevertheless, there has not been evidence in support of building such interactions concurrently with attention mechanism for image captioning. In this paper, we introduce a unified attention block --- X-Linear attention block, that fully employs bilinear pooling to selectively capitalize on visual information or perform multi-modal reasoning. Technically, X-Linear attention block simultaneously exploits both the spatial and channel-wise bilinear attention distributions to capture the 2$^{nd}$ order interactions between the input single-modal or multi-modal features. Higher and even infinity order feature interactions are readily modeled through stacking multiple X-Linear attention blocks and equipping the block with Exponential Linear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we present X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates X-Linear attention block(s) into image encoder and sentence decoder of image captioning model to leverage higher order intra- and inter-modal interactions. The experiments on COCO benchmark demonstrate that our X-LAN obtains to-date the best published CIDEr performance of 132.0% on COCO Karpathy test split. When further endowing Transformer with X-Linear attention blocks, CIDEr is boosted up to 132.8%. Source code is available at https://github.com/Panda-Peter/image-captioning.",Ix-atnefoimca,86.0,46.0,13.0
3044,Image Captioning,40.0,sca-cnn: spatial and channel-wise attention in convolutional networks for image captioning,5.0,49.0,4.0,9.0,5.0,4.6,34.300000000000004,10,http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf,"Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism &#x2014; a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.",Iscspanchatinconefoimca,886.0,47.0,53.0
3045,Image Captioning,32.0,auto-encoding scene graphs for image captioning,5.0,53.0,4.0,13.0,5.0,4.6,34.7,11,http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf,"We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into the encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use the inductive bias to compose collocations and contextual inference in discourse. For example, when we see the relation ``person on bike'', it is natural to replace ``on'' with ``ride'' and infer ``person riding bike on a road'' even the ``road'' is not evident. Therefore, exploiting such bias as a language prior is expected to help the conventional encoder-decoder models less likely to overfit to the dataset bias and focus on reasoning. Specifically, we use the scene graph --- a directed graph (G) where an object node is connected by adjective nodes and relationship nodes --- to represent the complex structural layout of both image (I) and sentence (S). In the textual domain, we use SGAE to learn a dictionary (D) that helps to reconstruct sentences in the S -> G -> D -> S pipeline, where D encodes the desired language prior; in the vision-language domain, we use the shared D to guide the encoder-decoder in the I -> G -> D -> S pipeline. Thanks to the scene graph representation and shared dictionary, the inductive bias is transferred across domains in principle. We validate the effectiveness of SGAE on the challenging MS-COCO image captioning benchmark, \eg, our SGAE-based single-model achieves a new state-of-the-art 127.8 CIDEr-D on the Karpathy split, and a competitive 125.5 CIDEr-D (c40) on the official server even compared to other ensemble models. Code has been made available at: https://github.com/yangxuntu/SGAE.",Iauscgrfoimca,243.0,67.0,34.0
3046,Image Captioning,14.0,image captioning: transforming objects into words,5.0,63.0,4.0,38.0,5.0,4.6,40.8,12,https://arxiv.org/pdf/1906.05963,"Image captioning models typically follow an encoder-decoder architecture which uses abstract image feature vectors as input to the encoder. One of the most successful algorithms uses feature vectors extracted from the region proposals obtained from an object detector. In this work we introduce the Object Relation Transformer, that builds upon this approach by explicitly incorporating information about the spatial relationship between input detected objects through geometric attention. Quantitative and qualitative results demonstrate the importance of such geometric attention for image captioning, leading to improvements on all common captioning metrics on the MS-COCO dataset.",Iimcatrobinwo,88.0,32.0,18.0
3047,Image Captioning,7.0,unsupervised image captioning,5.0,51.0,4.0,41.0,4.0,4.3,34.800000000000004,13,https://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Unsupervised_Image_Captioning_CVPR_2019_paper.pdf,"Deep neural networks have achieved great successes on the image captioning task. However, most of the existing models depend heavily on paired image-sentence datasets, which are very expensive to acquire. In this paper, we make the first attempt to train an image captioning model in an unsupervised manner. Instead of relying on manually labeled image-sentence pairs, our proposed model merely requires an image set, a sentence corpus, and an existing visual concept detector. The sentence corpus is used to teach the captioning model how to generate plausible sentences. Meanwhile, the knowledge in the visual concept detector is distilled into the captioning model to guide the model to recognize the visual concepts in an image. In order to further encourage the generated captions to be semantically consistent with the image, the image and caption are projected into a common latent space so that they can reconstruct each other. Given that the existing sentence corpora are mainly designed for linguistic research and are thus with little reference to image contents, we crawl a large-scale image description corpus of two million natural sentences to facilitate the unsupervised image captioning scenario. Experimental results show that our proposed model is able to produce quite promising results without any caption annotations.",Iunimca,74.0,50.0,12.0
3048,Image Captioning,4.0,convolutional image captioning,5.0,67.0,4.0,90.0,4.0,4.3,55.0,14,http://openaccess.thecvf.com/content_cvpr_2018/papers/Aneja_Convolutional_Image_Captioning_CVPR_2018_paper.pdf,"Image captioning is an important task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. In recent years significant progress has been made in image captioning, using Recurrent Neural Networks powered by long-short-term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies, LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation [9, 34, 35]. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the LSTM baseline [16], while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches.",Icoimca,203.0,49.0,19.0
3049,Image Captioning,11.0,learning to evaluate image captioning,5.0,86.0,4.0,128.0,3.0,4.0,76.1,15,http://openaccess.thecvf.com/content_cvpr_2018/papers/Cui_Learning_to_Evaluate_CVPR_2018_paper.pdf,"Evaluation metrics for image captioning face two challenges. Firstly, commonly used metrics such as CIDEr, METEOR, ROUGE and BLEU often do not correlate well with human judgments. Secondly, each metric has well known blind spots to pathological caption constructions, and rule-based metrics lack provisions to repair such blind spots once identified. For example, the newly proposed SPICE correlates well with human judgments, but fails to capture the syntactic structure of a sentence. To address these two challenges, we propose a novel learning based discriminative evaluation metric that is directly trained to distinguish between human and machine-generated captions. In addition, we further propose a data augmentation scheme to explicitly incorporate pathological transformations as negative examples during training. The proposed metric is evaluated with three kinds of robustness tests and its correlation with human judgments. Extensive experiments show that the proposed data augmentation scheme not only makes our metric more robust toward several pathological transformations, but also improves its correlation with human judgments. Our metric outperforms other metrics on both caption level human correlation in Flickr 8k and system level human correlation in COCO. The proposed approach could be served as a learning based evaluation metric that is complementary to existing rule-based metrics.",Iletoevimca,69.0,34.0,8.0
3050,Image Captioning,37.0,improved image captioning via policy gradient optimization of spider,5.0,77.0,4.0,115.0,3.0,4.0,76.4,16,http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Improved_Image_Captioning_ICCV_2017_paper.pdf,"Current image captioning methods are usually trained via maximum likelihood estimation. However, the log-likelihood score of a caption does not correlate well with human assessments of quality. Standard syntactic evaluation metrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The newer SPICE and CIDEr metrics are better correlated, but have traditionally been hard to optimize for. In this paper, we show how to use a policy gradient (PG) method to directly optimize a linear combination of SPICE and CIDEr (a combination we call SPIDEr): the SPICE score ensures our captions are semantically faithful to the image, while CIDEr score ensures our captions are syntactically fluent. The PG method we propose improves on the prior MIXER approach, by using Monte Carlo rollouts instead of mixing MLE training with PG. We show empirically that our algorithm leads to easier optimization and improved results compared to MIXER. Finally, we show that using our PG method we can optimize any of the metrics, including the proposed SPIDEr metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize MLE or the COCO metrics.",Iimimcavipogropofsp,257.0,34.0,22.0
3051,Image Captioning,21.0,image captioning with deep bidirectional lstms,5.0,84.0,4.0,174.0,3.0,4.0,92.1,17,https://arxiv.org/pdf/1604.00790,"This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models ""translate"" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task",Iimcawidebils,173.0,47.0,13.0
3052,Image Captioning,172.0,transform and tell: entity-aware news image captioning,3.0,93.0,4.0,17.0,5.0,4.0,93.9,18,https://openaccess.thecvf.com/content_CVPR_2020/papers/Tran_Transform_and_Tell_Entity-Aware_News_Image_Captioning_CVPR_2020_paper.pdf,"We propose an end-to-end model which generates captions for images embedded in news articles. News images present two key challenges: they rely on real-world knowledge, especially about named entities; and they typically have linguistically rich captions that include uncommon words. We address the first challenge by associating words in the caption with faces and objects in the image, via a multi-modal, multi-head attention mechanism. We tackle the second challenge with a state-of-the-art transformer language model that uses byte-pair-encoding to generate captions as a sequence of word parts. On the GoodNews dataset, our model outperforms the previous state of the art by a factor of four in CIDEr score (13 to 54). This performance gain comes from a unique combination of language models, word representation, image embeddings, face embeddings, object embeddings, and improvements in neural network design. We also introduce the NYTimes800k dataset which is 70% larger than GoodNews, has higher article quality, and includes the locations of images within articles as an additional contextual cue.",Itranteenneimca,16.0,66.0,6.0
3053,Image Captioning,75.0,more grounded image captioning by distilling image-text matching model,4.0,112.0,3.0,12.0,5.0,3.9,70.9,19,https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_More_Grounded_Image_Captioning_by_Distilling_Image-Text_Matching_Model_CVPR_2020_paper.pdf,"Visual attention not only improves the performance of image captioners, but also serves as a visual interpretation to qualitatively measure the caption rationality and model transparency. Specifically, we expect that a captioner can fix its attentive gaze on the correct objects while generating the corresponding words. This ability is also known as grounded image captioning. However, the grounding accuracy of existing captioners is far from satisfactory.To improve the grounding accuracy while retaining the captioning quality, it is expensive to collect the word-region alignment as strong supervision.To this end, we propose a Part-of-Speech (POS) enhanced image-text matching model (SCAN): POS-SCAN, as the effective knowledge distillation for more grounded image captioning. The benefits are two-fold: 1) given a sentence and an image, POS-SCAN can ground the objects more accurately than SCAN; 2) POS-SCAN serves as a word-region alignment regularization for the captioner's visual attention module. By showing benchmark experimental results, we demonstrate that conventional image captioners equipped with POS-SCAN can significantly improve the grounding accuracy without strong supervision. Last but not the least, we explore the indispensable Self-Critical Sequence Training (SCST) in the context of grounded image captioning and show that the image-text matching score can serve as a reward for more grounded captioning.",Imogrimcabydiimmamo,23.0,68.0,4.0
3054,Image Captioning,95.0,comprehensive image captioning via scene graph decomposition,4.0,102.0,3.0,26.0,5.0,3.9,77.10000000000001,20,https://arxiv.org/pdf/2007.11731,"We address the challenging problem of image captioning by revisiting the representation of image scene graph. At the core of our method lies the decomposition of a scene graph into a set of sub-graphs, with each sub-graph capturing a semantic component of the input image. We design a deep model to select important sub-graphs, and to decode each selected sub-graph into a single target sentence. By using sub-graphs, our model is able to attend to different components of the image. Our method thus accounts for accurate, diverse, grounded and controllable captioning at the same time. We present extensive experiments to demonstrate the benefits of our comprehensive captioning model. Our method establishes new state-of-the-art results in caption diversity, grounding, and controllability, and compares favourably to latest methods in caption quality. Our project website can be found at this http URL.",Icoimcaviscgrde,13.0,73.0,2.0
3055,Image Captioning,81.0,image captioning through image transformer,4.0,141.0,3.0,25.0,5.0,3.9,88.2,21,https://openaccess.thecvf.com/content/ACCV2020/papers/He_Image_Captioning_through_Image_Transformer_ACCV_2020_paper.pdf,"Automatic captioning of images is a task that combines the challenges of image analysis and text generation. One important aspect in captioning is the notion of attention: How to decide what to describe and in which order. Inspired by the successes in text analysis and translation, previous work have proposed the \textit{transformer} architecture for image captioning. However, the structure between the \textit{semantic units} in images (usually the detected regions from object detection model) and sentences (each single word) is different. Limited work has been done to adapt the transformer's internal architecture to images. In this work, we introduce the \textbf{\textit{image transformer}}, which consists of a modified encoding transformer and an implicit decoding transformer, motivated by the relative spatial relationship between image regions. Our design widen the original transformer layer's inner architecture to adapt to the structure of images. With only regions feature as inputs, our model achieves new state-of-the-art performance on both MSCOCO offline and online testing benchmarks.",Iimcathimtr,13.0,40.0,2.0
3056,Image Captioning,110.0,aligning linguistic words and visual semantic units for image captioning,3.0,42.0,4.0,61.0,4.0,3.7,68.1,22,https://arxiv.org/pdf/1908.02127,"Image captioning attempts to generate a sentence composed of several linguistic words, which are used to describe objects, attributes, and interactions in an image, denoted as visual semantic units in this paper. Based on this view, we propose to explicitly model the object interactions in semantics and geometry based on Graph Convolutional Networks (GCNs), and fully exploit the alignment between linguistic words and visual semantic units for image captioning. Particularly, we construct a semantic graph and a geometry graph, where each node corresponds to a visual semantic unit, i.e., an object, an attribute, or a semantic (geometrical) interaction between two objects. Accordingly, the semantic (geometrical) context-aware embeddings for each unit are obtained through the corresponding GCN learning processers. At each time step, a context gated attention module takes as inputs the embeddings of the visual semantic units and hierarchically align the current word with these units by first deciding which type of visual semantic unit (object, attribute, or interaction) the current word is about, and then finding the most correlated visual semantic units under this type. Extensive experiments are conducted on the challenging MS-COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches.",Ialliwoanviseunfoimca,30.0,47.0,3.0
3057,Image Captioning,57.0,attend to you: personalized image captioning with context sequence memory networks,4.0,47.0,4.0,154.0,3.0,3.7,82.1,23,https://openaccess.thecvf.com/content_cvpr_2017/papers/Park_Attend_to_You_CVPR_2017_paper.pdf,"We address personalization issues of image captioning, which have not been discussed yet in previous research. For a query image, we aim to generate a descriptive sentence, accounting for prior knowledge such as the users active vocabularies in previous documents. As applications of personalized image captioning, we tackle two post automation tasks: hashtag prediction and post generation, on our newly collected Instagram dataset, consisting of 1.1M posts from 6.3K users. We propose a novel captioning model named Context Sequence Memory Network (CSMN). Its unique updates over previous memory network models include (i) exploiting memory as a repository for multiple types of context information, (ii) appending previously generated words into memory to capture long-term information without suffering from the vanishing gradient problem, and (iii) adopting CNN memory structure to jointly represent nearby ordered memory slots for better context understanding. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the effectiveness of the three novel features of CSMN and its performance enhancement for personalized image captioning over state-of-the-art captioning models.",Iattoyopeimcawicosemene,99.0,40.0,8.0
3058,Image Captioning,70.0,watch what you just said: image captioning with text-conditional attention,4.0,100.0,4.0,192.0,3.0,3.7,118.6,24,https://dl.acm.org/doi/pdf/10.1145/3126686.3126717,"Attention mechanisms have attracted considerable interest in image captioning due to their powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called text-conditional attention, which allows the caption generator to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics as well as in human evaluation, which supports the use of our text-conditional attention in image captioning.",Iwawhyojusaimcawiteat,54.0,43.0,3.0
3059,Image Captioning,55.0,length-controllable image captioning,4.0,127.0,3.0,60.0,4.0,3.6000000000000005,85.30000000000001,25,https://arxiv.org/pdf/2007.09580,"The last decade has witnessed remarkable progress in the image captioning task; however, most existing methods cannot control their captions, \emph{e.g.}, choosing to describe the image either roughly or in detail. In this paper, we propose to use a simple length level embedding to endow them with this ability. Moreover, due to their autoregressive nature, the computational complexity of existing models increases linearly as the length of the generated captions grows. Thus, we further devise a non-autoregressive image captioning approach that can generate captions in a length-irrelevant complexity. We verify the merit of the proposed length level embedding on three models: two state-of-the-art (SOTA) autoregressive models with different types of decoder, as well as our proposed non-autoregressive model, to show its generalization ability. In the experiments, our length-controllable image captioning models not only achieve SOTA performance on the challenging MS COCO dataset but also generate length-controllable and diverse image captions. Specifically, our non-autoregressive model outperforms the autoregressive baselines in terms of controllability and diversity, and also significantly improves the decoding efficiency for long captions. Our code and models are released at \textcolor{magenta}{\texttt{this https URL}}.",Ileimca,6.0,55.0,0.0
3060,Image Captioning,48.0,describing like humans: on diversity in image captioning,4.0,152.0,3.0,52.0,4.0,3.6000000000000005,90.8,26,http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Describing_Like_Humans_On_Diversity_in_Image_Captioning_CVPR_2019_paper.pdf,"Recently, the state-of-the-art models for image captioning have overtaken human performance based on the most popular metrics, such as BLEU, METEOR, ROUGE and CIDEr. Does this mean we have solved the task of image captioning The above metrics only measure the similarity of the generated caption to the human annotations, which reflects its accuracy. However, an image contains many concepts and multiple levels of detail, and thus there is a variety of captions that express different concepts and details that might be interesting for different humans. Therefore only evaluating accuracy is not sufficient for measuring the performance of captioning models --- the diversity of the generated captions should also be considered. In this paper, we proposed a new metric for measuring the diversity of image captions, which is derived from latent semantic analysis and kernelized to use CIDEr similarity. We conduct extensive experiments to re-evaluate recent captioning models in the context of both diversity and accuracy. We find that there is still a large gap between the model and human performance in terms of both accuracy and diversity, and the models that have optimized accuracy (CIDEr) have low diversity. We also show that balancing the cross-entropy loss and CIDEr reward in reinforcement learning during training can effectively control the tradeoff between diversity and accuracy of the generated captions.",Idelihuondiinimca,39.0,35.0,3.0
3061,Image Captioning,79.0,adaptively aligned image captioning via adaptive attention time,4.0,125.0,3.0,70.0,4.0,3.6000000000000005,94.7,27,http://papers.nips.cc/paper/9096-adaptively-aligned-image-captioning-via-adaptive-attention-time.pdf,"Recent neural models for image captioning usually employ an encoder-decoder framework with an attention mechanism. However, the attention mechanism in such a framework aligns one single (attended) image feature vector to one caption word, assuming one-to-one mapping from source image regions and target caption words, which is never possible. In this paper, we propose a novel attention model, namely Adaptive Attention Time (AAT), to align the source and the target adaptively for image captioning. AAT allows the framework to learn how many attention steps to take to output a caption word at each decoding step. With AAT, an image region can be mapped to an arbitrary number of caption words while a caption word can also attend to an arbitrary number of image regions. AAT is deterministic and differentiable, and doesn't introduce any noise to the parameter gradients. In this paper, we empirically show that AAT improves over state-of-the-art methods on the task of image captioning. Code is available at https://github.com/husthuaan/AAT.",Iadalimcaviadatti,22.0,36.0,3.0
3062,Image Captioning,49.0,a survey on biomedical image captioning,4.0,159.0,3.0,82.0,4.0,3.6000000000000005,102.9,28,https://www.aclweb.org/anthology/W19-1803.pdf,"Image captioning applied to biomedical images can assist and accelerate the diagnosis process followed by clinicians. This article is the first survey of biomedical image captioning, discussing datasets, evaluation measures, and state of the art methods. Additionally, we suggest two baselines, a weak and a stronger one; the latter outperforms all current state of the art systems on one of the datasets.",Iasuonbiimca,20.0,67.0,1.0
3063,Image Captioning,22.0,stack-captioning: coarse-to-fine learning for image captioning,5.0,146.0,3.0,122.0,3.0,3.6,101.6,29,https://ojs.aaai.org/index.php/AAAI/article/download/12266/12125,"The existing image captioning approaches typically train a one-stage sentence decoder, which is difficult to generate rich fine-grained descriptions. On the other hand, multi-stage image caption model is hard to train due to the vanishing gradient problem. In this paper, we propose a coarse-to-fine multi-stage prediction framework for image captioning, composed of multiple decoders each of which operates on the output of the previous stage, producing increasingly refined image descriptions. Our proposed learning approach addresses the difficulty of vanishing gradients during training by providing a learning objective function that enforces intermediate supervisions. Particularly, we optimize our model with a reinforcement learning approach which utilizes the output of each intermediate decoder's test-time inference algorithm as well as the output of its preceding decoder to normalize the rewards, which simultaneously solves the well-known exposure bias problem and the loss-evaluation mismatch problem. We extensively evaluate the proposed approach on MSCOCO and show that our approach can achieve the state-of-the-art performance.",Istcolefoimca,102.0,40.0,9.0
3064,Image Captioning,38.0,an empirical study of language cnn for image captioning,5.0,147.0,3.0,162.0,3.0,3.6,118.8,30,http://openaccess.thecvf.com/content_ICCV_2017/papers/Gu_An_Empirical_Study_ICCV_2017_paper.pdf,"Language models based on recurrent neural networks have dominated recent image caption generation tasks. In this paper, we introduce a language CNN model which is suitable for statistical language modeling tasks and shows competitive performance in image captioning. In contrast to previous models which predict next word based on one previous word and hidden state, our language CNN is fed with all the previous words and can model the long-range dependencies in history words, which are critical for image captioning. The effectiveness of our approach is validated on two datasets: Flickr30K and MS COCO. Our extensive experimental results show that our method outperforms the vanilla recurrent neural network based language models and is competitive with the state-of-the-art methods.",Ianemstoflacnfoimca,77.0,63.0,8.0
3065,Image Captioning,5.0,image captioning with semantic attention,5.0,201.0,1.0,24.0,5.0,3.4,89.10000000000001,31,https://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf,"Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",Iimcawiseat,1112.0,40.0,107.0
3066,Image Captioning,2.0,boosting image captioning with attributes,5.0,201.0,1.0,30.0,5.0,3.4,90.0,32,https://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf,"Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. Particularly, the learning of attributes is strengthened by integrating inter-attribute correlations into Multiple Instance Learning (MIL). To incorporate attributes into captioning, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework shows clear improvements when compared to state-of-the-art deep models. More remarkably, we obtain METEOR/CIDEr-D of 25.5%/100.2% on testing data of widely used and publicly available splits in [10] when extracting image representations by GoogleNet and achieve superior performance on COCO captioning Leaderboard.",Iboimcawiat,427.0,51.0,40.0
3067,Image Captioning,10.0,a comprehensive survey of deep learning for image captioning,5.0,201.0,1.0,29.0,5.0,3.4,92.1,33,https://arxiv.org/pdf/1810.04020,"Generating a description of an image is called image captioning. Image captioning requires recognizing the important objects, their attributes, and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep-learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey article, we aim to present a comprehensive review of existing deep-learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths, and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep-learning-based automatic image captioning.",Iacosuofdelefoimca,230.0,211.0,15.0
3068,Image Captioning,18.0,exploring visual relationship for image captioning,5.0,201.0,1.0,21.0,5.0,3.4,92.1,34,https://openaccess.thecvf.com/content_ECCV_2018/papers/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf,"It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.",Iexvirefoimca,378.0,40.0,45.0
3069,Image Captioning,28.0,entangled transformer for image captioning,5.0,201.0,1.0,40.0,5.0,3.4,100.8,35,http://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf,"In image captioning, the typical attention mechanisms are arduous to identify the equivalent visual signals especially when predicting highly abstract words. This phenomenon is known as the semantic gap between vision and language. This problem can be overcome by providing semantic attributes that are homologous to language. Thanks to the inherent recurrent nature and gated operating mechanism, Recurrent Neural Network (RNN) and its variants are the dominating architectures in image captioning. However, when designing elaborate attention mechanisms to integrate visual inputs and semantic attributes, RNN-like variants become unflexible due to their complexities. In this paper, we investigate a Transformer-based sequence modeling framework, built only with attention layers and feedforward layers. To bridge the semantic gap, we introduce EnTangled Attention (ETA) that enables the Transformer to exploit semantic and visual information simultaneously. Furthermore, Gated Bilateral Controller (GBC) is proposed to guide the interactions between the multimodal information. We name our model as ETA-Transformer. Remarkably, ETA-Transformer achieves state-of-the-art performance on the MSCOCO image captioning dataset. The ablation studies validate the improvements of our proposed modules.",Ientrfoimca,75.0,49.0,11.0
3070,Image Captioning,20.0,exploring nearest neighbor approaches for image captioning,5.0,88.0,4.0,201.0,1.0,3.4,101.5,36,https://arxiv.org/pdf/1505.04467,"With the arrival of digital maps, the ubiquity of maps has increased sharply and new map functionalities have become available such as changing the scale on the fly or displaying/hiding layers. Users can now interact with maps on multiple devices (e.g. smartphones, desktop computers, large-scale displays, head-mounted displays) using different means of interaction such as touch, voice or gestures. However, ensuring map functionalities and good user experience across these devices and modalities frequently entails dedicated development efforts for each combination. In this paper, we argue that introducing an abstract representation of what a map contains and affords can unlock new opportunities. For this purpose, we propose the concept of map plasticity, the capability of a map-based system to support different contexts of use while preserving usability and functionality. Based on this definition, we discuss core components and an example. We also propose a research agenda for realising map plasticity and its benefits.",Iexneneapfoimca,163.0,40.0,17.0
3071,Image Captioning,112.0,guided open vocabulary image captioning with constrained beam search,3.0,96.0,4.0,144.0,3.0,3.4,115.2,37,https://arxiv.org/pdf/1612.00576,"Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.",Iguopvoimcawicobese,107.0,54.0,21.0
3072,Image Captioning,65.0,context-aware visual policy network for sequence-level image captioning,4.0,121.0,3.0,130.0,3.0,3.3000000000000003,106.9,38,https://arxiv.org/pdf/1808.05864,"Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the ""exposure bias'' during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (eg, visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (\eg, ""man riding horse'') and comparisons (eg. ""smaller cat""). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at \urlhttps://github.com/daqingliu/CAVP",Icoviponefoseimca,65.0,57.0,4.0
3073,Image Captioning,44.0,object hallucination in image captioning,4.0,167.0,3.0,124.0,3.0,3.3000000000000003,117.2,39,https://arxiv.org/pdf/1809.02156,"Despite continuously improving performance, contemporary image captioning models are prone to “hallucinating” objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.",Iobhainimca,77.0,24.0,9.0
3074,Image Captioning,73.0,paying attention to descriptions generated by image captioning models,4.0,106.0,3.0,179.0,3.0,3.3000000000000003,118.0,40,http://openaccess.thecvf.com/content_ICCV_2017/papers/Tavakoli_Paying_Attention_to_ICCV_2017_paper.pdf,"To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliencyboosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization is, however, observed for the saliency-boosted model on unseen data.",Ipaattodegebyimcamo,48.0,75.0,9.0
3075,Image Captioning,53.0,a neural compositional paradigm for image captioning,4.0,193.0,3.0,153.0,3.0,3.3000000000000003,139.0,41,https://proceedings.neurips.cc/paper/7346-a-neural-compositional-paradigm-for-image-captioning.pdf,"Mainstream captioning models often follow a sequential structure to generate captions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.",Ianecopafoimca,26.0,40.0,4.0
3076,Image Captioning,118.0,context-aware visual policy network for fine-grained image captioning,3.0,120.0,3.0,45.0,4.0,3.3,96.9,42,https://arxiv.org/pdf/1906.02365,"With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., ""man riding horse"") and visual comparisons (e.g., ""small(er) cat""). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model-CAVP and its subsequent language policy network - can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.",Icoviponefofiimca,51.0,49.0,3.0
3077,Image Captioning,106.0,improving image captioning with conditional generative adversarial nets,3.0,190.0,3.0,55.0,4.0,3.3,124.3,43,https://ojs.aaai.org/index.php/AAAI/article/download/4823/4696,"In this paper, we propose a novel conditional-generativeadversarial-nets-based image captioning framework as an extension of traditional reinforcement-learning (RL)-based encoder-decoder architecture. To deal with the inconsistent evaluation problem among different objective language metrics, we are motivated to design some “discriminator” networks to automatically and progressively determine whether generated caption is human described or machine generated. Two kinds of discriminator architectures (CNN and RNNbased structures) are introduced since each has its own advantages. The proposed algorithm is generic so that it can enhance any existing RL-based image captioning framework and we show that the conventional RL training method is just a special case of our approach. Empirically, we show consistent improvements over all language evaluation metrics for different state-of-the-art image captioning models. In addition, the well-trained discriminators can also be viewed as objective image captioning evaluators.",Iimimcawicogeadne,41.0,57.0,4.0
3078,Image Captioning,138.0,compositional generalization in image captioning,3.0,163.0,3.0,94.0,4.0,3.3,134.79999999999998,44,https://arxiv.org/pdf/1909.04402,"Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image–sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.",Icogeinimca,17.0,72.0,1.0
3079,Image Captioning,401.0,image captioning,1.0,8.0,5.0,105.0,3.0,3.2,155.0,45,https://arxiv.org/pdf/1805.09137,"This paper discusses and demonstrates the outcomes from our experimentation on Image Captioning. Image captioning is a much more involved task than image recognition or classification, because of the additional challenge of recognizing the interdependence between the objects/concepts in the image and the creation of a succinct sentential narration. Experiments on several labeled datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. As a toy application, we apply image captioning to create video captions, and we advance a few hypotheses on the challenges we encountered.",Iimca,1851.0,67.0,432.0
3080,Image Captioning,43.0,multimodal transformer with multi-view visual representation for image captioning,4.0,201.0,1.0,7.0,5.0,3.1,95.4,46,https://arxiv.org/pdf/1905.07841,"Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN) based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model simultaneously captures intra- and inter-modal interactions in a unified attention block. Due to the in-depth modular composition of such attention blocks, the MT model can perform complex multimodal reasoning and output accurate captions. Moreover, to further improve the image captioning performance, multi-view visual features are seamlessly introduced into the MT model. We quantitatively and qualitatively evaluate our approach using the benchmark MSCOCO image captioning dataset and conduct extensive ablation studies to investigate the reasons behind its effectiveness. The experimental results show that our method significantly outperforms the previous state-of-the-art methods. With an ensemble of seven models, our solution ranks the 1st place on the real-time leaderboard of the MSCOCO image captioning challenge at the time of the writing of this paper.",Imutrwimuvirefoimca,122.0,62.0,3.0
3081,Image Captioning,42.0,textcaps: a dataset for image captioning with reading comprehension,4.0,201.0,1.0,11.0,5.0,3.1,96.3,47,https://arxiv.org/pdf/2003.12462,"Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.",Iteadafoimcawireco,28.0,41.0,9.0
3082,Image Captioning,41.0,memcap: memorizing style knowledge for image captioning,4.0,201.0,1.0,16.0,5.0,3.1,97.5,48,https://ojs.aaai.org/index.php/AAAI/article/download/6998/6852,"Generating stylized captions for images is a challenging task since it requires not only describing the content of the image accurately but also expressing the desired linguistic style appropriately. In this paper, we propose MemCap, a novel stylized image captioning method that explicitly encodes the knowledge about linguistic styles with memory mechanism. Rather than relying heavily on a language model to capture style factors in existing methods, our method resorts to memorizing stylized elements learned from training corpus. Particularly, we design a memory module that comprises a set of embedding vectors for encoding style-related phrases in training corpus. To acquire the style-related phrases, we develop a sentence decomposing algorithm that splits a stylized sentence into a style-related part that reflects the linguistic style and a content-related part that contains the visual content. When generating captions, our MemCap first extracts content-relevant style knowledge from the memory module via an attention mechanism and then incorporates the extracted knowledge into a language model. Extensive experiments on two stylized image captioning datasets (SentiCap and FlickrStyle10K) demonstrate the effectiveness of our method.",Imemestknfoimca,13.0,27.0,3.0
3083,Image Captioning,58.0,normalized and geometry-aware self-attention network for image captioning,4.0,201.0,1.0,10.0,5.0,3.1,100.8,49,http://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Normalized_and_Geometry-Aware_Self-Attention_Network_for_Image_Captioning_CVPR_2020_paper.pdf,"Self-attention (SA) network has shown profound value in image captioning. In this paper, we improve SA from two aspects to promote the performance of image captioning. First, we propose Normalized Self-Attention (NSA), a reparameterization of SA that brings the benefits of normalization inside SA. While normalization is previously only applied outside SA, we introduce a novel normalization method and demonstrate that it is both possible and beneficial to perform it on the hidden activations inside SA. Second, to compensate for the major limit of Transformer that it fails to model the geometry structure of the input objects, we propose a class of Geometry-aware Self-Attention (GSA) that extends SA to explicitly and efficiently consider the relative geometry relations between the objects in the image. To construct our image captioning model, we combine the two modules and apply it to the vanilla self-attention network. We extensively evaluate our proposals on MS-COCO image captioning dataset and superior results are achieved when comparing to state-of-the-art approaches. Further experiments on three challenging tasks, i.e. video captioning, machine translation, and visual question answering, show the generality of our methods.",Inoangesenefoimca,32.0,46.0,4.0
3084,Image Captioning,30.0,engaging image captioning via personality,5.0,201.0,1.0,42.0,4.0,3.1,102.0,50,https://openaccess.thecvf.com/content_CVPR_2019/papers/Shuster_Engaging_Image_Captioning_via_Personality_CVPR_2019_paper.pdf,"Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). While such tasks are useful to verify that a machine understands the content of an image, they are not engaging to humans as captions. With this in mind we define a new task, PERSONALITY-CAPTIONS, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 241,858 of such captions conditioned over 215 possible traits. We build models that combine existing work from (i) sentence representations [36] with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations [32] with ResNets trained on 3.5 billion social media images. We obtain state-of-the-art performance on Flickr30k and COCO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance.",Ienimcavipe,64.0,68.0,5.0
3085,Image Captioning,31.0,hierarchy parsing for image captioning,5.0,201.0,1.0,48.0,4.0,3.1,104.1,51,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yao_Hierarchy_Parsing_for_Image_Captioning_ICCV_2019_paper.pdf,"It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.",Ihipafoimca,62.0,38.0,7.0
3086,Image Captioning,39.0,look back and predict forward in image captioning,5.0,201.0,1.0,53.0,4.0,3.1,108.0,52,https://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.pdf,"Most existing attention-based methods on image captioning focus on the current word and visual information in one time step and generate the next word, without considering the visual and linguistic coherence. We propose Look Back (LB) method to embed visual information from the past and Predict Forward (PF) approach to look into future. LB method introduces attention value from the previous time step into the current attention generation to suit visual coherence of human. PF model predicts the next two words in one time step and jointly employs their probabilities for inference. Then the two approaches are combined together as LBPF to further integrate visual information from the past and linguistic information in the future to improve image captioning performance. All the three methods are applied on a classic base decoder, and show remarkable improvements on MSCOCO dataset with small increments on parameter counts. Our LBPF model achieves BLEU-4 / CIDEr / SPICE scores of 37.4 / 116.4 / 21.2 with cross-entropy loss and 38.3 / 127.6 / 22.0 with CIDEr optimization. Our three proposed methods can be easily applied on most attention-based encoder-decoder models for image captioning.",Ilobaanprfoinimca,39.0,32.0,3.0
3087,Image Captioning,77.0,does multimodality help human and machine for translation and image captioning?,4.0,66.0,4.0,201.0,1.0,3.1,109.8,53,https://arxiv.org/pdf/1605.09186,"This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.",Idomuhehuanmafotranimca,63.0,32.0,8.0
3088,Image Captioning,89.0,improving image captioning evaluation by considering inter references variance,4.0,201.0,1.0,33.0,5.0,3.1,117.0,54,https://www.aclweb.org/anthology/2020.acl-main.93.pdf,"Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance. In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation. The experimental results show that our metric achieves state-of-the-art human judgment correlation.",Iimimcaevbycoinreva,8.0,30.0,3.0
3089,Image Captioning,180.0,attacking visual language grounding with adversarial examples: a case study on neural image captioning,3.0,113.0,3.0,129.0,3.0,3.0,137.9,55,https://arxiv.org/pdf/1712.02051,"Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.",Iatvilagrwiadexacastonneimca,73.0,44.0,16.0
3090,Image Captioning,153.0,fluency-guided cross-lingual image captioning,3.0,156.0,3.0,198.0,3.0,3.0,167.70000000000002,56,https://arxiv.org/pdf/1708.04390,"Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual setting. Different from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of fluency in the translated sentences, we propose in this paper a fluency-guided learning framework. The framework comprises a module to automatically estimate the fluency of the sentences and another module to utilize the estimated fluency scores to effectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both fluency and relevance of the generated captions in Chinese, but without using any manually written sentences from the target language.",Iflcrimca,36.0,45.0,0.0
3091,Image Captioning,157.0,gated hierarchical attention for image captioning,3.0,192.0,3.0,169.0,3.0,3.0,174.6,57,https://arxiv.org/pdf/1810.12535,"Attention modules connecting encoder and decoders have been widely applied in the field of object recognition, image captioning, visual question answering and neural machine translation, and significantly improves the performance. In this paper, we propose a bottom-up gated hierarchical attention (GHA) mechanism for image captioning. Our proposed model employs a CNN as the decoder which is able to learn different concepts at different layers, and apparently, different concepts correspond to different areas of an image. Therefore, we develop the GHA in which low-level concepts are merged into high-level concepts and simultaneously low-level attended features pass to the top to make predictions. Our GHA significantly improves the performance of the model that only applies one level attention, for example, the CIDEr score increases from 0.923 to 0.999, which is comparable to the state-of-the-art models that employ attributes boosting and reinforcement learning (RL). We also conduct extensive experiments to analyze the CNN decoder and our proposed GHA, and we find that deeper decoders cannot obtain better performance, and when the convolutional decoder becomes deeper the model is likely to collapse during training.",Igahiatfoimca,14.0,50.0,0.0
3092,Image Captioning,166.0,face-cap: image captioning using facial expression analysis,3.0,194.0,3.0,172.0,3.0,3.0,179.0,58,https://arxiv.org/pdf/1807.02250,"Image captioning is the process of generating a natural language description of an image. Most current image captioning models, however, do not take into account the emotional aspect of an image, which is very relevant to activities and interpersonal relationships represented therein. Towards developing a model that can produce human-like captions incorporating these, we use facial expression features extracted from images including human faces, with the aim of improving the descriptive ability of the model. In this work, we present two variants of our Face-Cap model, which embed facial expression features in different ways, to generate image captions. Using all standard evaluation metrics, our Face-Cap models outperform a state-of-the-art baseline model for generating image captions when applied to an image caption dataset extracted from the standard Flickr 30 K dataset, consisting of around 11 K images containing faces. An analysis of the captions finds that, perhaps surprisingly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions. Code related to this paper is available at: https://github.com/omidmn/Face-Cap.",Ifaimcausfaexan,16.0,45.0,0.0
3093,Image Captioning,47.0,pointing novel objects in image captioning,4.0,201.0,1.0,62.0,4.0,2.8,113.1,59,https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Pointing_Novel_Objects_in_Image_Captioning_CVPR_2019_paper.pdf,"Image captioning has received significant attention with remarkable improvements in recent advances. Nevertheless, images in the wild encapsulate rich knowledge and cannot be sufficiently described with models built on image-caption pairs containing only in-domain objects. In this paper, we propose to address the problem by augmenting standard deep captioning architectures with object learners. Specifically, we present Long Short-Term Memory with Pointing (LSTM-P) --- a new architecture that facilitates vocabulary expansion and produces novel objects via pointing mechanism. Technically, object learners are initially pre-trained on available object recognition data. Pointing in LSTM-P then balances the probability between generating a word through LSTM and copying a word from the recognized objects at each time step in decoder stage. Furthermore, our captioning encourages global coverage of objects in the sentence. Extensive experiments are conducted on both held-out COCO image captioning and ImageNet datasets for describing novel objects, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, we obtain an average of 60.9% in F1 score on held-out COCO dataset.",Iponoobinimca,38.0,37.0,2.0
3094,Image Captioning,67.0,know more say less: image captioning based on scene graphs,4.0,201.0,1.0,44.0,4.0,2.8,113.7,60,http://vipl.ict.ac.cn/uploadfile/upload/2019112511482118.pdf,"Automatically describing the content of an image has been attracting considerable research attention in the multimedia field. To represent the content of an image, many approaches directly utilize convolutional neural networks (CNNs) to extract visual representations, which are fed into recurrent neural networks to generate natural language. Recently, some approaches have detected semantic concepts from images and then encoded them into high-level representations. Although substantial progress has been achieved, most of the previous methods treat entities in images individually, thus lacking structured information that provides important cues for image captioning. In this paper, we propose a framework based on scene graphs for image captioning. Scene graphs contain abundant structured information because they not only depict object entities in images but also present pairwise relationships. To leverage both visual features and semantic knowledge in structured scene graphs, we extract CNN features from the bounding box offsets of object entities for visual representations, and extract semantic relationship features from triples (e.g., man riding bike) for semantic representations. After obtaining these features, we introduce a hierarchical-attention-based module to learn discriminative features for word generation at each time step. The experimental results on benchmark datasets demonstrate the superiority of our method compared with several state-of-the-art methods.",Iknmosaleimcabaonscgr,59.0,79.0,4.0
3095,Image Captioning,45.0,self-critical n-step training for image captioning,4.0,201.0,1.0,73.0,4.0,2.8,115.8,61,https://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Self-Critical_N-Step_Training_for_Image_Captioning_CVPR_2019_paper.pdf,"Existing methods for image captioning are usually trained by cross entropy loss, which leads to exposure bias and the inconsistency between the optimizing function and evaluation metrics. Recently it has been shown that these two issues can be addressed by incorporating techniques from reinforcement learning, where one of the popular techniques is the advantage actor-critic algorithm that calculates per-token advantage by estimating state value with a parametrized estimator at the cost of introducing estimation bias. In this paper, we estimate state value without using a parametrized value estimator. With the properties of image captioning, namely, the deterministic state transition function and the sparse reward, state value is equivalent to its preceding state-action value, and we reformulate advantage function by simply replacing the former with the latter. Moreover, the reformulated advantage is extended to n-step, which can generally increase the absolute value of the mean of reformulated advantage while lowering variance. Then two kinds of rollout are adopted to estimate state-action value, which we call self-critical n-step training. Empirically we find that our method can obtain better performance compared to the state-of-the-art methods that use the sequence level advantage and parametrized estimator respectively on the widely used MSCOCO benchmark.",Isen-trfoimca,24.0,32.0,2.0
3096,Image Captioning,105.0,image captioning with end-to-end attribute detection and subsequent attributes prediction,3.0,201.0,1.0,18.0,5.0,2.8,117.3,62,https://www.researchgate.net/profile/Yiqing-Huang-8/publication/338926415_Image_Captioning_With_End-to-End_Attribute_Detection_and_Subsequent_Attributes_Prediction/links/5f8ea855a6fdccfd7b6e9f42/Image-Captioning-With-End-to-End-Attribute-Detection-and-Subsequent-Attributes-Prediction.pdf,"Semantic attention has been shown to be effective in improving the performance of image captioning. The core of semantic attention based methods is to drive the model to attend to semantically important words, or attributes. In previous works, the attribute detector and the captioning network are usually independent, leading to the insufficient usage of the semantic information. Also, all the detected attributes, no matter whether they are appropriate for the linguistic context at the current step, are attended to through the whole caption generation process. This may sometimes disrupt the captioning model to attend to incorrect visual concepts. To solve these problems, we introduce two end-to-end trainable modules to closely couple attribute detection with image captioning as well as prompt the effective uses of attributes by predicting appropriate attributes at each time step. The multimodal attribute detector (MAD) module improves the attribute detection accuracy by using not only the image features but also the word embedding of attributes already existing in most captioning models. MAD models the similarity between the semantics of attributes and the image object features to facilitate accurate detection. The subsequent attribute predictor (SAP) module dynamically predicts a concise attribute subset at each time step to mitigate the diversity of image attributes. Compared to previous attribute based methods, our approach enhances the explainability in how the attributes affect the generated words and achieves a state-of-the-art single model performance of 128.8 CIDEr-D on the MSCOCO dataset. Extensive experiments on the MSCOCO dataset show that our proposal actually improves the performances in both image captioning and attribute detection simultaneously. The codes are available at: https://github.com/RubickH/Image-Captioning-with-MAD-and-SAP.",Iimcawienatdeansuatpr,18.0,44.0,2.0
3097,Image Captioning,51.0,learning to collocate neural modules for image captioning,4.0,201.0,1.0,77.0,4.0,2.8,118.8,63,https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Learning_to_Collocate_Neural_Modules_for_Image_Captioning_ICCV_2019_paper.pdf,"We do not speak word by word from scratch; our brain quickly structures a pattern like \textsc{sth do sth at someplace} and then fill in the detailed description. To render existing encoder-decoder image captioners such human-like reasoning, we propose a novel framework: learning to Collocate Neural Modules (CNM), to generate the ``inner pattern'' connecting visual encoder and language decoder. Unlike the widely-used neural module networks in visual Q\&A, where the language (\ie, question) is fully observable, CNM for captioning is more challenging as the language is being generated and thus is partially observable. To this end, we make the following technical contributions for CNM training: 1) compact module design --- one for function words and three for visual content words (\eg, noun, adjective, and verb), 2) soft module fusion and multi-step module execution, robustifying the visual reasoning in partial observation, 3) a linguistic loss for module controller being faithful to part-of-speech collocations (\eg, adjective is before noun). Extensive experiments on the challenging MS-COCO image captioning benchmark validate the effectiveness of our CNM image captioner. In particular, CNM achieves a new state-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40 on the official server. CNM is also robust to few training samples, \eg, by training only one sentence per image, CNM can halve the performance loss compared to a strong baseline.",Iletoconemofoimca,28.0,72.0,0.0
3098,Image Captioning,62.0,hierarchical attention network for image captioning,4.0,201.0,1.0,66.0,4.0,2.8,118.8,64,https://ojs.aaai.org/index.php/AAAI/article/download/4924/4797,"Recently, attention mechanism has been successfully applied in image captioning, but the existing attention methods are only established on low-level spatial features or high-level text features, which limits richness of captions. In this paper, we propose a Hierarchical Attention Network (HAN) that enables attention to be calculated on pyramidal hierarchy of features synchronously. The pyramidal hierarchy consists of features on diverse semantic levels, which allows predicting different words according to different features. On the other hand, due to the different modalities of features, a Multivariate Residual Module (MRM) is proposed to learn the joint representations from features. The MRM is able to model projections and extract relevant relations among different features. Furthermore, we introduce a context gate to balance the contribution of different features. Compared with the existing methods, our approach applies hierarchical features and exploits several multimodal integration strategies, which can significantly improve the performance. The HAN is verified on benchmark MSCOCO dataset, and the experimental results indicate that our model outperforms the state-of-the-art methods, achieving a BLEU1 score of 80.9 and a CIDEr score of 121.7 in the Karpathy’s test split.",Ihiatnefoimca,32.0,30.0,2.0
3099,Image Captioning,74.0,mscap: multi-style image captioning with unpaired stylized text,4.0,201.0,1.0,54.0,4.0,2.8,118.8,65,https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_MSCap_Multi-Style_Image_Captioning_With_Unpaired_Stylized_Text_CVPR_2019_paper.pdf,"In this paper, we propose an adversarial learning network for the task of multi-style image captioning (MSCap) with a standard factual image caption dataset and a multi-stylized language corpus without paired images. How to learn a single model for multi-stylized image captioning with unpaired data is a challenging and necessary task, whereas rarely studied in previous works. The proposed framework mainly includes four contributive modules following a typical image encoder. First, a style dependent caption generator to output a sentence conditioned on an encoded image and a specified style. Second, a caption discriminator is presented to distinguish the input sentence to be real or not. The discriminator and the generator are trained in an adversarial manner to enable more natural and human-like captions. Third, a style classifier is employed to discriminate the specific style of the input sentence. Besides, a back-translation module is designed to enforce the generated stylized captions are visually grounded, with the intuition of the cycle consistency for factual caption and stylized caption. We enable an end-to-end optimization of the whole model with differentiable softmax approximation. At last, we conduct comprehensive experiments using a combined dataset containing four caption styles to demonstrate the outstanding performance of our proposed method.",Imsmuimcawiunstte,35.0,49.0,8.0
3100,Image Captioning,12.0,recurrent fusion network for image captioning,5.0,201.0,1.0,123.0,3.0,2.8,120.9,66,https://openaccess.thecvf.com/content_ECCV_2018/papers/Wenhao_Jiang_Recurrent_Fusion_Network_ECCV_2018_paper.pdf,"Recently, much advance has been made in image captioning, and an encoder-decoder framework has been adopted by all the state-of-the-art models. Under this framework, an input image is encoded by a convolutional neural network (CNN) and then translated into natural language with a recurrent neural network (RNN). The existing models counting on this framework employ only one kind of CNNs, e.g., ResNet or Inception-X, which describes the image contents from only one specific view point. Thus, the semantic meaning of the input image cannot be comprehensively understood, which restricts improving the performance. In this paper, to exploit the complementary information from multiple encoders, we propose a novel recurrent fusion network (RFNet) for the image captioning task. The fusion process in our model can exploit the interactions among the outputs of the image encoders and generate new compact and informative representations for the decoder. Experiments on the MSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets a new state-of-the-art for image captioning.",Irefunefoimca,103.0,60.0,9.0
3101,Image Captioning,90.0,"fast, diverse and accurate image captioning guided by part-of-speech",4.0,201.0,1.0,46.0,4.0,2.8,121.2,67,http://openaccess.thecvf.com/content_CVPR_2019/papers/Deshpande_Fast_Diverse_and_Accurate_Image_Captioning_Guided_by_Part-Of-Speech_CVPR_2019_paper.pdf,"Image captioning is an ambiguous problem, with many suitable captions for an image. To address ambiguity, beam search is the de facto method for sampling multiple captions. However, beam search is computationally expensive and known to produce generic captions. To address this concern, some variational auto-encoder (VAE) and generative adversarial net (GAN) based methods have been proposed. Though diverse, GAN and VAE are less accurate. In this paper, we first predict a meaningful summary of the image, then generate the caption based on that summary. We use part-of-speech as summaries, since our summary should drive caption generation. We achieve the trifecta: (1) High accuracy for the diverse captions as evaluated by standard captioning metrics and user studies; (2) Faster computation of diverse captions compared to beam search and diverse beam search; and (3) High diversity as evaluated by counting novel sentences, distinct n-grams and mutual overlap (i.e., mBleu-4) scores.",Ifadianacimcagubypa,59.0,36.0,5.0
3102,Image Captioning,122.0,learning visual relationship and context-aware attention for image captioning,3.0,201.0,1.0,15.0,5.0,2.8,121.5,68,http://cognn.com/papers/10%20PR%202020%20Junbo%20Learning%20visual%20relationship%20and%20context-aware%20attention%20for%20image.pdf,"Abstract Image captioning which automatically generates natural language descriptions for images has attracted lots of research attentions and there have been substantial progresses with attention based captioning methods. However, most attention-based image captioning methods focus on extracting visual information in regions of interest for sentence generation and usually ignore the relational reasoning among those regions of interest in an image. Moreover, these methods do not take into account previously attended regions which can be used to guide the subsequent attention selection. In this paper, we propose a novel method to implicitly model the relationship among regions of interest in an image with a graph neural network, as well as a novel context-aware attention mechanism to guide attention selection by fully memorizing previously attended visual content. Compared with the existing attention-based image captioning methods, ours can not only learn relation-aware visual representations for image captioning, but also consider historical context information on previous attention. We perform extensive experiments on two public benchmark datasets: MS COCO and Flickr30K, and the experimental results indicate that our proposed method is able to outperform various state-of-the-art methods in terms of the widely used evaluation metrics.",Ilevireancoatfoimca,34.0,60.0,2.0
3103,Image Captioning,54.0,reflective decoding network for image captioning,4.0,201.0,1.0,85.0,4.0,2.8,122.1,69,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf,"State-of-the-art image captioning methods mostly focus on improving visual features, less attention has been paid to utilizing the inherent properties of language to boost captioning performance. In this paper, we show that vocabulary coherence between words and syntactic paradigm of sentences are also important to generate high-quality image caption. Following the conventional encoder-decoder framework, we propose the Reflective Decoding Network (RDN) for image captioning, which enhances both the long-sequence dependency and position perception of words in a caption decoder. Our model learns to collaboratively attend on both visual and textual features and meanwhile perceive each word's relative position in the sentence to maximize the information delivered in the generated caption. We evaluate the effectiveness of our RDN on the COCO image captioning datasets and achieve superior performance over the previous methods. Further experiments reveal that our approach is particularly advantageous for hard cases with complex scenes to describe by captions.",Iredenefoimca,19.0,52.0,0.0
3104,Image Captioning,91.0,compare and reweight: distinctive image captioning using similar images sets,4.0,201.0,1.0,49.0,4.0,2.8,122.4,70,https://arxiv.org/pdf/2007.06877,"A wide range of image captioning models has been developed, achieving significant improvement based on popular metrics, such as BLEU, CIDEr, and SPICE. However, although the generated captions can accurately describe the image, they are generic for similar images and lack distinctiveness, i.e., cannot properly describe the uniqueness of each image. In this paper, we aim to improve the distinctiveness of image captions through training with sets of similar images. First, we propose a distinctiveness metric -- between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric shows that the human annotations of each image are not equivalent based on distinctiveness. Thus we propose several new training strategies to encourage the distinctiveness of the generated caption for each image, which are based on using CIDErBtw in a weighted loss function or as a reinforcement learning reward. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study.",Icoanrediimcaussiimse,6.0,47.0,1.0
3105,Image Captioning,56.0,a systematic literature review on image captioning,4.0,201.0,1.0,93.0,4.0,2.8,125.1,71,https://www.mdpi.com/2076-3417/9/10/2024/pdf,"Natural language problems have already been investigated for around five years. Recent progress in artificial intelligence (AI) has greatly improved the performance of models. However, the results are still not sufficiently satisfying. Machines cannot imitate human brains and the way they communicate, so it remains an ongoing task. Due to the increasing amount of information on this topic, it is very difficult to keep on track with the newest researches and results achieved in the image captioning field. In this study a comprehensive Systematic Literature Review (SLR) provides a brief overview of improvements in image captioning over the last four years. The main focus of the paper is to explain the most common techniques and the biggest challenges in image captioning and to summarize the results from the newest papers. Inconsistent comparison of results achieved in image captioning was noticed during this study and hence the awareness of incomplete data collection is raised in this paper. Therefore, it is very important to compare results of a newly created model produced with the newest information and not only with the state of the art methods. This SLR is a source of such information for researchers in order for them to be precisely correct on result comparison before publishing new achievements in the image caption generation field.",Iasylireonimca,15.0,79.0,2.0
3106,Image Captioning,64.0,informative image captioning with external sources of information,4.0,201.0,1.0,86.0,4.0,2.8,125.4,72,https://arxiv.org/pdf/1906.08876,"An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important “informativeness” dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",Iinimcawiexsoofin,21.0,26.0,0.0
3107,Image Captioning,84.0,more is better: precise and detailed image captioning using online positive recall and missing concepts mining,4.0,201.0,1.0,68.0,4.0,2.8,126.0,73,http://arxiv.org/pdf/1812.03283v2,"Recently, a great progress in automatic image captioning has been achieved by using semantic concepts detected from the image. However, we argue that existing concepts-to-caption framework, in which the concept detector is trained using the image-caption pairs to minimize the vocabulary discrepancy, suffers from the deficiency of insufficient concepts. The reasons are two-fold: 1) the extreme imbalance between the number of occurrence positive and negative samples of the concept and 2) the incomplete labeling in training captions caused by the biased annotation and usage of synonyms. In this paper, we propose a method, termed online positive recall and missing concepts mining, to overcome those problems. Our method adaptively re-weights the loss of different samples according to their predictions for online positive recall and uses a two-stage optimization strategy for missing concepts mining. In this way, more semantic concepts can be detected and a high accuracy will be expected. On the caption generation stage, we explore an element-wise selection process to automatically choose the most suitable concepts at each time step. Thus, our method can generate more precise and detailed caption to describe the image. We conduct extensive experiments on the MSCOCO image captioning data set and the MSCOCO online test server, which shows that our method achieves superior image captioning performance compared with other competitive methods.",Imoisbeprandeimcausonporeanmicomi,41.0,57.0,0.0
3108,Image Captioning,88.0,multitask learning for cross-domain image captioning,4.0,201.0,1.0,67.0,4.0,2.8,126.9,74,https://www.researchgate.net/profile/Kai-Lei/publication/327768110_Multitask_Learning_for_Cross-Domain_Image_Captioning/links/5c2848c1458515a4c700b829/Multitask-Learning-for-Cross-Domain-Image-Captioning.pdf,"Recent artificial intelligence research has witnessed great interest in automatically generating text descriptions of images, which are known as the <italic>image captioning</italic> task. Remarkable success has been achieved on domains where a large number of paired data in multimedia are available. Nevertheless, annotating sufficient data is labor-intensive and time-consuming, establishing significant barriers for adapting the image captioning systems to new domains. In this study, we introduc a novel Multitask Learning Algorithm for cross-Domain Image Captioning (MLADIC). MLADIC is a multitask system that simultaneously optimizes two coupled objectives via a dual learning mechanism: image captioning and text-to-image synthesis, with the hope that by leveraging the correlation of the two dual tasks, we are able to enhance the image captioning performance in the target domain. Concretely, the image captioning task is trained with an encoder–decoder model (i.e., CNN-LSTM) to generate textual descriptions of the input images. The image synthesis task employs the conditional generative adversarial network (C-GAN) to synthesize plausible images based on text descriptions. In C-GAN, a generative model <inline-formula><tex-math notation=""LaTeX"">$G$</tex-math></inline-formula> synthesizes plausible images given text descriptions, and a discriminative model <inline-formula><tex-math notation=""LaTeX"">$D$</tex-math></inline-formula> tries to distinguish the images in training data from the generated images by <inline-formula><tex-math notation=""LaTeX"">$G$</tex-math></inline-formula>. The adversarial process can eventually guide <inline-formula><tex-math notation=""LaTeX"">$G$</tex-math></inline-formula> to generate plausible and high-quality images. To bridge the gap between different domains, a two-step strategy is adopted in order to transfer knowledge from the source domains to the target domains. First, we pre-train the model to learn the alignment between the neural representations of images and that of text data with the sufficient labeled source domain data. Second, we fine-tune the learned model by leveraging the limited image–text pairs and unpaired data in the target domain. We conduct extensive experiments to evaluate the performance of MLADIC by using the MSCOCO as the source domain data, and using Flickr30k and Oxford-102 as the target domain data. The results demonstrate that MLADIC achieves substantially better performance than the strong competitors for the cross-domain image captioning task.",Imulefocrimca,33.0,57.0,2.0
3109,Image Captioning,140.0,multi-level policy and reward-based deep reinforcement learning framework for image captioning,3.0,201.0,1.0,19.0,5.0,2.8,128.1,75,http://arxiv.org/pdf/2004.02435v2,"Image captioning is one of the most challenging tasks in AI because it requires an understanding of both complex visuals and natural language. Because image captioning is essentially a sequential prediction task, recent advances in image captioning have used reinforcement learning (RL) to better explore the dynamics of word-by-word generation. However, the existing RL-based image captioning methods rely primarily on a single policy network and reward function—an approach that is not well matched to the multi-level (word and sentence) and multi-modal (vision and language) nature of the task. To solve this problem, we propose a novel multi-level policy and reward RL framework for image captioning that can be easily integrated with RNN-based captioning models, language metrics, or visual-semantic functions for optimization. Specifically, the proposed framework includes two modules: 1) a multi-level policy network that jointly updates the word- and sentence-level policies for word generation; and 2) a multi-level reward function that collaboratively leverages both a vision-language reward and a language-language reward to guide the policy. Furthermore, we propose a guidance term to bridge the policy and the reward for RL optimization. The extensive experiments on the MSCOCO and Flickr30k datasets and the analyses show that the proposed framework achieves competitive performances on a variety of evaluation metrics. In addition, we conduct ablation studies on multiple variants of the proposed framework and explore several representative image captioning models and metrics for the word-level policy network and the language-language reward function to evaluate the generalization ability of the proposed framework.",Imupoanrederelefrfoimca,26.0,60.0,0.0
3110,Image Captioning,19.0,re-evaluating automatic metrics for image captioning,5.0,201.0,1.0,140.0,3.0,2.8,128.10000000000002,76,https://arxiv.org/pdf/1612.07600,"The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.",Ireaumefoimca,123.0,47.0,16.0
3111,Image Captioning,72.0,image captioning and visual question answering based on attributes and external knowledge,4.0,201.0,1.0,91.0,4.0,2.8,129.3,77,https://arxiv.org/pdf/1603.02814,"Much of the recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to progress directly from image features to text. In this paper we first propose a method of incorporating high-level concepts into the successful CNN-RNN approach, and show that it achieves a significant improvement on the state-of-the-art in both image captioning and visual question answering. We further show that the same mechanism can be used to incorporate external knowledge, which is critically important for answering high level visual questions. Specifically, we design a visual question answering model that combines an internal representation of the content of an image with information extracted from a general knowledge base to answer a broad range of image-based questions. It particularly allows questions to be asked where the image alone does not contain the information required to select the appropriate answer. Our final model achieves the best reported results for both image captioning and visual question answering on several of the major benchmark datasets.",Iimcaanviquanbaonatanexkn,208.0,87.0,5.0
3112,Image Captioning,16.0,areas of attention for image captioning,5.0,201.0,1.0,150.0,3.0,2.8,130.2,78,https://openaccess.thecvf.com/content_ICCV_2017/papers/Pedersoli_Areas_of_Attention_ICCV_2017_paper.pdf,"We propose “Areas of Attention”, a novel attentionbased model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attentionbased approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.",Iarofatfoimca,139.0,53.0,5.0
3113,Image Captioning,78.0,human attention in image captioning: dataset and analysis,4.0,201.0,1.0,89.0,4.0,2.8,130.5,79,https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Human_Attention_in_Image_Captioning_Dataset_and_Analysis_ICCV_2019_paper.pdf,"In this work, we present a novel dataset consisting of eye movements and verbal descriptions recorded synchronously over images. Using this data, we study the differences in human attention during free-viewing and image captioning tasks. We look into the relationship between human atten- tion and language constructs during perception and sen- tence articulation. We also analyse attention deployment mechanisms in the top-down soft attention approach that is argued to mimic human attention in captioning tasks, and investigate whether visual saliency can help image caption- ing. Our study reveals that (1) human attention behaviour differs in free-viewing and image description tasks. Hu- mans tend to fixate on a greater variety of regions under the latter task, (2) there is a strong relationship between de- scribed objects and attended objects (97% of the described objects are being attended), (3) a convolutional neural net- work as feature encoder accounts for human-attended re- gions during image captioning to a great extent (around 78%), (4) soft-attention mechanism differs from human at- tention, both spatially and temporally, and there is low correlation between caption scores and attention consis- tency scores. These indicate a large gap between humans and machines in regards to top-down attention, and (5) by integrating the soft attention model with image saliency, we can significantly improve the model’s performance on Flickr30k and MSCOCO benchmarks. The dataset can be found at: https://github.com/SenHe/ Human-Attention-in-Image-Captioning.",Ihuatinimcadaanan,18.0,43.0,1.0
3114,Image Captioning,134.0,image captioning: a comprehensive survey,3.0,201.0,1.0,35.0,5.0,2.8,131.1,80,http://arxiv.org/pdf/1911.07685v1,"The primary purpose of image captioning is to generate a caption for an image. Image captioning needs to identify objects in image, actions, their relationship and some silent feature that may be missing in the image. After identification the next step is to generate a most relevant and brief description for the image that must be syntactically and semantically correct. It uses both computer vision concepts for identification of objects and natural language processing methods for description. It’s difficult for a machine to imitate human brain ability however researches in this field have shown a great achievement. Deep learning techniques are enough capable to handle such problems using CNN and LSTM. It can be used in many intelligent control systems and IOT based devices. In this survey paper, we are presenting different approaches of image captioning such as retrieval based, template based and deep learning based as well as different evaluation techniques.",Iimcaacosu,10.0,26.0,0.0
3115,Image Captioning,92.0,improving image captioning by leveraging knowledge graphs,4.0,201.0,1.0,80.0,4.0,2.8,132.0,81,https://arxiv.org/pdf/1901.08942,"We explore the use of a knowledge graphs, that capture general or commonsense knowledge, to augment the information extracted from images by the state-of-the-art methods for image captioning. We compare the performance of image captioning systems that as measured by CIDEr-D, a performance measure that is explicitly designed for evaluating image captioning systems, on several benchmark data sets such as MS COCO. The results of our experiments show that the variants of the state-of-the-art methods for image captioning that make use of the information extracted from knowledge graphs can substantially outperform those that rely solely on the information extracted from images.",Iimimcabylekngr,20.0,46.0,1.0
3116,Image Captioning,94.0,towards unsupervised image captioning with shared multimodal embeddings,4.0,201.0,1.0,78.0,4.0,2.8,132.0,82,http://openaccess.thecvf.com/content_ICCV_2019/papers/Laina_Towards_Unsupervised_Image_Captioning_With_Shared_Multimodal_Embeddings_ICCV_2019_paper.pdf,"Understanding images without explicit supervision has become an important problem in computer vision. In this paper, we address image captioning by generating language descriptions of scenes without learning from annotated pairs of images and their captions. The core component of our approach is a shared latent space that is structured by visual concepts. In this space, the two modalities should be indistinguishable. A language model is first trained to encode sentences into semantically structured embeddings. Image features that are translated into this embedding space can be decoded into descriptions through the same language model, similarly to sentence embeddings. This translation is learned from weakly paired images and text using a loss robust to noisy assignments and a conditional adversarial component. Our approach allows to exploit large text corpora outside the annotated distributions of image/caption data. Our experiments show that the proposed domain alignment learns a semantically meaningful representation which outperforms previous work.",Itounimcawishmuem,22.0,72.0,2.0
3117,Image Captioning,15.0,contrastive learning for image captioning,5.0,201.0,1.0,158.0,3.0,2.8,132.3,83,https://arxiv.org/pdf/1710.02534,"Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.",Icolefoimca,86.0,28.0,8.0
3118,Image Captioning,35.0,learning to guide decoding for image captioning,5.0,201.0,1.0,138.0,3.0,2.8,132.3,84,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/16666/16283,"Recently, much advance has been made in image captioning, and an encoder-decoder framework has achieved outstanding performance for this task. In this paper, we propose an extension of the encoder-decoder framework by adding a component called guiding network. The guiding network models the attribute properties of input images, and its output is leveraged to compose the input of the decoder at each time step. The guiding network can be plugged into the current encoder-decoder framework and trained in an end-to-end manner. Hence, the guiding vector can be adaptively learned according to the signal from the decoder, making itself to embed information from both image and language. Additionally, discriminative supervision can be employed to further improve the quality of guidance. The advantages of our proposed approach are verified by experiments carried out on the MS COCO dataset.",Iletogudefoimca,46.0,54.0,2.0
3119,Image Captioning,99.0,topic-oriented image captioning based on order-embedding,4.0,201.0,1.0,83.0,4.0,2.8,135.0,85,http://arxiv.org/pdf/1705.00823v1,"We present an image captioning framework that generates captions under a given topic. The topic candidates are extracted from the caption corpus. A given image’s topics are then selected from these candidates by a CNN-based multi-label classifier. The input to the caption generation model is an image-topic pair, and the output is a caption of the image. For this purpose, a cross-modal embedding method is learned for the images, topics, and captions. In the proposed framework, the topic, caption, and image are organized in a hierarchical structure, which is preserved in the embedding space by using the order-embedding method. The caption embedding is upper bounded by the corresponding image embedding and lower bounded by the topic embedding. The lower bound pushes the images and captions about the same topic closer together in the embedding space. A bidirectional caption-image retrieval task is conducted on the learned embedding space and achieves the state-of-the-art performance on the MS-COCO and Flickr30K datasets, demonstrating the effectiveness of the embedding method. To generate a caption for an image, an embedding vector is sampled from the region bounded by the embeddings of the image and the topic, then a language model decodes it to a sentence as the output. The lower bound set by the topic shrinks the output space of the language model, which may help the model to learn to match images and captions better. Experiments on the image captioning task on the MS-COCO and Flickr30K datasets validate the usefulness of this framework by showing that the different given topics can lead to different captions describing specific aspects of the given image and that the quality of generated captions is higher than the control model without a topic as input. In addition, the proposed method is competitive with many state-of-the-art methods in terms of standard evaluation metrics.",Itoimcabaonor,20.0,40.0,1.0
3120,Image Captioning,34.0,incorporating copying mechanism in image captioning for learning novel objects,5.0,201.0,1.0,151.0,3.0,2.8,135.9,86,http://openaccess.thecvf.com/content_cvpr_2017/papers/Yao_Incorporating_Copying_Mechanism_CVPR_2017_paper.pdf,"Image captioning often requires a large set of training image-sentence pairs. In practice, however, acquiring sufficient training pairs is always expensive, making the recent captioning models limited in their ability to describe objects outside of training corpora (i.e., novel objects). In this paper, we present Long Short-Term Memory with Copying Mechanism (LSTM-C) &#x2014; a new architecture that incorporates copying into the Convolutional Neural Networks (CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for describing novel objects in captions. Specifically, freely available object recognition datasets are leveraged to develop classifiers for novel objects. Our LSTM-C then nicely integrates the standard word-by-word sentence generation by a decoder RNN with copying mechanism which may instead select words from novel objects at proper places in the output sentence. Extensive experiments are conducted on both MSCOCO image captioning and ImageNet datasets, demonstrating the ability of our proposed LSTM-C architecture to describe novel objects. Furthermore, superior results are reported when compared to state-of-the-art deep models.",Iincomeinimcafolenoob,109.0,32.0,8.0
3121,Image Captioning,148.0,image captioning via semantic element embedding,3.0,201.0,1.0,37.0,5.0,2.8,135.9,87,http://arxiv.org/pdf/2012.07333v1,"Abstract Image caption approaches that use the global Convolutional Neural Network (CNN) features are not able to represent and describe all the important elements in complex scenes. In this paper, we propose to enrich the semantic representations of images and update the language model by proposing semantic element embedding. For the semantic element discovery, an object detection module is used to predict regions of the image, and a captioning model, Long Short-Term Memory (LSTM), is employed to generate local descriptions for these regions. The predicted descriptions and categories are used to generate the semantic feature, which not only contains detailed information but also shares a word space with descriptions, and thus bridges the modality gap between visual images and semantic captions. We further integrate the CNN feature with the semantic feature into the proposed Element Embedding LSTM (EE-LSTM) model to predict a language description. Experiments on MS COCO datasets demonstrate that the proposed approach outperforms conventional caption methods and is flexible to combine with baseline models to achieve superior performance.",Iimcaviseelem,12.0,42.0,0.0
3122,Image Captioning,158.0,incorporating external knowledge for image captioning using cnn and lstm,3.0,201.0,1.0,27.0,5.0,2.8,135.9,88,http://arxiv.org/pdf/1808.02861v1,Image captioning is a multidisciplinary artificial intelligence (AI) research task that has captures the interest of both image and natural language processing experts. Image captioning is a comple...,Iinexknfoimcauscnanls,14.0,46.0,0.0
3123,Image Captioning,98.0,comic: toward a compact image captioning model with attention,4.0,201.0,1.0,96.0,4.0,2.8,138.60000000000002,89,https://arxiv.org/pdf/1903.01072,"Recent works in image captioning have shown very promising raw performance. However, we realize that most of these encoder–decoder style networks with attention do not scale naturally to large vocabulary size, making them difficult to deploy on embedded systems with limited hardware resources. This is because the size of word and output embedding matrices grow proportionally with the size of vocabulary, adversely affecting the compactness of these networks. To address this limitation, this paper introduces a brand new idea in the domain of image captioning. That is, we tackle the problem of compactness of image captioning models which is hitherto unexplored. We showed that our proposed model, named COMIC for compact image captioning, achieves comparable results in five common evaluation metrics with state-of-the-art approaches on both MS-COCO and InstaPIC-1.1M datasets despite having an embedded vocabulary size that is 39×−99× smaller.",Icotoacoimcamowiat,14.0,52.0,2.0
3124,Image Captioning,167.0,multimodal attention with image text spatial relationship for ocr-based image captioning,3.0,201.0,1.0,36.0,5.0,2.8,141.3,90,http://arxiv.org/pdf/1904.09073v3,"OCR-based image captioning is the task of automatically describing images based on reading and understanding written text contained in images. Compared to conventional image captioning, this task is more challenging, especially when the image contains multiple text tokens and visual objects. The difficulties originate from how to make full use of the knowledge contained in the textual entities to facilitate sentence generation and how to predict a text token based on the limited information provided by the image. Such problems are not yet fully investigated in existing research. In this paper, we present a novel design - Multimodal Attention Captioner with OCR Spatial Relationship (dubbed as MMA-SR) architecture, which manages information from different modalities with a multimodal attention network and explores spatial relationships between text tokens for OCR-based image captioning. Specifically, the representations of text tokens and objects are fed into a three-layer LSTM captioner. Different attention scores for text tokens and objects are exploited through the multimodal attention network. Based on the attended features and the LSTM states, words are selected from the common vocabulary or from the image text by incorporating the learned spatial relationships between text tokens. Extensive experiments conducted on the TextCaps dataset verify the effectiveness of the proposed MMA-SR method. More remarkably, our MMA-SR increases CIDEr-D score from 93.7% to 98.0%.",Imuatwiimtesprefoocimca,10.0,58.0,0.0
3125,Image Captioning,179.0,"show, recall, and tell: image captioning with recall mechanism",3.0,201.0,1.0,39.0,5.0,2.8,145.79999999999998,91,https://ojs.aaai.org/index.php/AAAI/article/view/6898/6752,"Generating natural and accurate descriptions in image cap-tioning has always been a challenge. In this paper, we pro-pose a novel recall mechanism to imitate the way human con-duct captioning. There are three parts in our recall mecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS). Recall unit is a text-retrieval module designedto retrieve recalled words for images. SG and RWS are de-signed for the best use of recalled words. SG branch cangenerate a recalled context, which can guide the process ofgenerating caption. RWS branch is responsible for copyingrecalled words to the caption. Inspired by pointing mecha-nism in text summarization, we adopt a soft switch to balancethe generated-word probabilities between SG and RWS. Inthe CIDEr optimization step, we also introduce an individualrecalled-word reward (WR) to boost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr / SPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 / 22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the results of other state-of-the-artmethods.",Ishreanteimcawireme,11.0,32.0,2.0
3126,Image Captioning,24.0,language models for image captioning: the quirks and what works,5.0,201.0,1.0,200.0,3.0,2.8,147.60000000000002,92,https://arxiv.org/pdf/1505.01809,"Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-ofthe-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.",Ilamofoimcathquanwhwo,238.0,26.0,21.0
3127,Image Captioning,196.0,dual-level collaborative transformer for image captioning,3.0,79.0,4.0,201.0,1.0,2.8,150.7,93,https://arxiv.org/pdf/2101.06462,"Computing author intent from multimodal data like Instagram posts requires modeling a complex relationship between text and image. For example, a caption might evoke an ironic contrast with the image, so neither caption nor image is a mere transcript of the other. Instead they combine -- via what has been called meaning multiplication -- to create a new meaning that has a more complex relation to the literal meanings of text and image. Here we introduce a multimodal dataset of 1299 Instagram posts labeled for three orthogonal taxonomies: the authorial intent behind the image-caption pair, the contextual relationship between the literal meanings of the image and caption, and the semiotic relationship between the signified meanings of the image and caption. We build a baseline deep multimodal classifier to validate the taxonomy, showing that employing both text and image improves intent detection by 9.6% compared to using only the image modality, demonstrating the commonality of non-intersective meaning multiplication. The gain with multimodality is greatest when the image and caption diverge semiotically. Our dataset offers a new resource for the study of the rich meanings that result from pairing text and image.",Iducotrfoimca,8.0,31.0,0.0
3128,Image Captioning,401.0,"egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models",1.0,199.0,3.0,65.0,4.0,2.7,219.4,94,https://arxiv.org/pdf/2003.11743,"Image captioning models have been able to generate grammatically correct and human understandable sentences. However most of the captions convey limited information as the model used is trained on datasets that do not caption all possible objects existing in everyday life. Due to this lack of prior information most of the captions are biased to only a few objects present in the scene, hence limiting their usage in daily life. In this paper, we attempt to show the biased nature of the currently existing image captioning models and present a new image captioning dataset, Egoshots, consisting of 978 real life images with no captions. We further exploit the state of the art pre-trained image captioning and object recognition networks to annotate our images and show the limitations of existing works. Furthermore, in order to evaluate the quality of the generated captions, we propose a new image captioning metric, object based Semantic Fidelity (SF). Existing image captioning metrics can evaluate a caption only in the presence of their corresponding annotations; however, SF allows evaluating captions generated for images without annotations, making it highly useful for real life generated captions.",Ieganeglidaansefimetoevdiinimcamo,5.0,46.0,0.0
3129,Image Captioning,401.0,can active memory replace attention?,1.0,1.0,5.0,201.0,1.0,2.6,181.0,95,http://arxiv.org/pdf/1610.08613v2,"Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.   Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling.   So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.",Icaacmereat,44.0,35.0,5.0
3130,Image Captioning,401.0,a neural attention model for speech command recognition,1.0,3.0,5.0,201.0,1.0,2.6,181.8,96,http://arxiv.org/pdf/1808.08929v1,"This paper introduces a convolutional recurrent network with attention for speech command recognition. Attention models are powerful tools to improve performance on natural language, image captioning and speech tasks. The proposed model establishes a new state-of-the-art accuracy of 94.1% on Google Speech Commands dataset V1 and 94.5% on V2 (for the 20-commands recognition task), while still keeping a small footprint of only 202K trainable parameters. Results are compared with previous convolutional implementations on 5 different tasks (20 commands recognition (V1 and V2), 12 commands recognition (V1), 35 word recognition (V1) and left-right (V1)). We show detailed performance results and demonstrate that the proposed attention mechanism not only improves performance but also allows inspecting what regions of the audio were taken into consideration by the network when outputting a given category.",Ianeatmofospcore,52.0,26.0,9.0
3131,Image Captioning,401.0,one model to learn them all,1.0,5.0,5.0,201.0,1.0,2.6,182.6,97,http://arxiv.org/pdf/1707.03184v1,"Machine learning based system are increasingly being used for sensitive tasks such as security surveillance, guiding autonomous vehicle, taking investment decisions, detecting and blocking network intrusion and malware etc. However, recent research has shown that machine learning models are venerable to attacks by adversaries at all phases of machine learning (eg, training data collection, training, operation). All model classes of machine learning systems can be misled by providing carefully crafted inputs making them wrongly classify inputs. Maliciously created input samples can affect the learning process of a ML system by either slowing down the learning process, or affecting the performance of the learned mode, or causing the system make error(s) only in attacker's planned scenario. Because of these developments, understanding security of machine learning algorithms and systems is emerging as an important research area among computer security and machine learning researchers and practitioners. We present a survey of this emerging area in machine learning.",Ionmotolethal,222.0,30.0,8.0
3132,Image Captioning,401.0,show and tell: a neural image caption generator,1.0,9.0,5.0,201.0,1.0,2.6,184.2,98,http://arxiv.org/pdf/1903.06275v1,"Humans have an incredible ability to process and understand information from multiple sources such as images, video, text, and speech. Recent success of deep neural networks has enabled us to develop algorithms which give machines the ability to understand and interpret this information. There is a need to both broaden their applicability and develop methods which correlate visual information along with semantic content. We propose a unified model which jointly trains on images and captions, and learns to generate new captions given either an image or a caption query. We evaluate our model on three different tasks namely cross-modal retrieval, image captioning, and sentence paraphrasing. Our model gains insight into cross-modal vector embeddings, generalizes well on multiple tasks and is competitive to state of the art methods on retrieval.",Ishanteaneimcage,4184.0,36.0,509.0
3133,Image Captioning,401.0,diverse beam search: decoding diverse solutions from neural sequence models,1.0,11.0,5.0,201.0,1.0,2.6,185.0,99,http://arxiv.org/pdf/1610.02424v2,"Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates - resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory over- head as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.",Idibesededisofrnesemo,237.0,33.0,25.0
3135,Image Captioning,401.0,explicit sparse transformer: concentrated attention through explicit selection,1.0,14.0,5.0,201.0,1.0,2.6,186.2,100,http://arxiv.org/pdf/1912.11637v1,"Self-attention based Transformer has demonstrated the state-of-the-art performances in a number of natural language processing tasks. Self-attention is able to model long-term dependencies, but it may suffer from the extraction of irrelevant information in the context. To tackle the problem, we propose a novel model called \textbf{Explicit Sparse Transformer}. Explicit Sparse Transformer is able to improve the concentration of attention on the global context through an explicit selection of the most relevant segments. Extensive experimental results on a series of natural language processing and computer vision tasks, including neural machine translation, image captioning, and language modeling, all demonstrate the advantages of Explicit Sparse Transformer in model performance. We also show that our proposed sparse attention method achieves comparable or better results than the previous sparse attention method, but significantly reduces training and testing time. For example, the inference speed is twice that of sparsemax in Transformer model. Code will be available at \url{https://github.com/lancopku/Explicit-Sparse-Transformer}",Iexsptrcoatthexse,22.0,55.0,3.0
3398,Image Compression,3.0,end-to-end optimized image compression,5.0,2.0,5.0,6.0,5.0,5.0,3.5,1,https://arxiv.org/pdf/1611.01704,"We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.",Ienopimco,606.0,64.0,138.0
3399,Image Compression,10.0,variational image compression with a scale hyperprior,5.0,5.0,5.0,3.0,5.0,5.0,5.9,2,https://arxiv.org/pdf/1802.01436,"We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.",Ivaimcowiaschy,410.0,23.0,126.0
3400,Image Compression,11.0,full resolution image compression with recurrent neural networks,5.0,1.0,5.0,9.0,5.0,5.0,6.4,3,http://openaccess.thecvf.com/content_cvpr_2017/papers/Toderici_Full_Resolution_Image_CVPR_2017_paper.pdf,"This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study one-shot versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%&#x2013;8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.",Ifureimcowirenene,498.0,25.0,68.0
3401,Image Compression,23.0,lossy image compression with compressive autoencoders,5.0,22.0,5.0,8.0,5.0,5.0,18.1,4,https://arxiv.org/pdf/1703.00395,"We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.",Iloimcowicoau,516.0,41.0,77.0
3402,Image Compression,53.0,learned image compression with discretized gaussian mixture likelihoods and attention modules,4.0,26.0,5.0,1.0,5.0,4.7,26.6,5,http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_Learned_Image_Compression_With_Discretized_Gaussian_Mixture_Likelihoods_and_Attention_CVPR_2020_paper.pdf,"Image compression is a fundamental research field and many well-known compression standards have been developed for many decades. Recently, learned compression methods exhibit a fast development trend with promising results. However, there is still a performance gap between learned compression algorithms and reigning compression standards, especially in terms of widely used PSNR metric. In this paper, we explore the remaining redundancy of recent learned compression algorithms. We have found accurate entropy models for rate estimation largely affect the optimization of network parameters and thus affect the rate-distortion performance. Therefore, in this paper, we propose to use discretized Gaussian Mixture Likelihoods to parameterize the distributions of latent codes, which can achieve a more accurate and flexible entropy model. Besides, we take advantage of recent attention modules and incorporate them into network architecture to enhance the performance. Experimental results demonstrate our proposed method achieves a state-of-the-art performance compared to existing learned compression methods on both Kodak and high-resolution datasets. To our knowledge our approach is the first work to achieve comparable performance with latest compression standard Versatile Video Coding (VVC) regarding PSNR. More importantly, our approach generates more visually pleasant results when optimized by MS-SSIM.",Ileimcowidigamilianatmo,65.0,39.0,17.0
3403,Image Compression,66.0,deep convolutional autoencoder-based lossy image compression,4.0,88.0,4.0,19.0,5.0,4.3,60.7,6,https://arxiv.org/pdf/1804.09535,"Image compression has been investigated as a fundamental research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and is gradually being used in image compression. In this paper, we present a lossy image compression architecture, which utilizes the advantages of convolutional autoencoder (CAE) to achieve a high coding efficiency. First, we design a novel CAE architecture to replace the conventional transforms and train this CAE using a rate-distortion loss function. Second, to generate a more energy-compact representation, we utilize the principal components analysis (PCA) to rotate the feature maps produced by the CAE, and then apply the quantization and entropy coder to generate the codes. Experimental results demonstrate that our method outperforms traditional image coding algorithms, by achieving a 13.7% BD-rate decrement on the Kodak database images compared to JPEG2000. Besides, our method maintains a moderate complexity similar to JPEG2000.",Idecoauloimco,63.0,13.0,1.0
3404,Image Compression,33.0,high-fidelity generative image compression,5.0,4.0,5.0,201.0,1.0,3.8,71.8,7,https://arxiv.org/pdf/2006.09965,"For an image with multiple scene texts, different people may be interested in different text information. Current text-aware image captioning models are not able to generate distinctive captions according to various information needs. To explore how to generate personalized text-aware captions, we define a new challenging task, namely Question-controlled Text-aware Image Captioning (Qc-TextCap). With questions as control signals, this task requires models to understand questions, find related scene texts and describe them together with objects fluently in human language. Based on two existing text-aware captioning datasets, we automatically construct two datasets, ControlTextCaps and ControlVizWiz to support the task. We propose a novel Geometry and Question Aware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to fuse region-level object features and region-level scene text features with considering spatial relationships. Then, we design a Question-guided Encoder to select the most relevant visual features for each question. Finally, GQAM generates a personalized text-aware caption with a Multimodal Decoder. Our model achieves better captioning performance and question answering ability than carefully designed baselines on both two datasets. With questions as control signals, our model generates more informative and diverse captions than the state-of-the-art text-aware captioning model. Our code and datasets are publicly available at https://github.com/HAWLYQ/Qc-TextCap.",Ihigeimco,56.0,55.0,6.0
3405,Image Compression,401.0,generative adversarial networks for extreme learned image compression,1.0,7.0,5.0,2.0,5.0,3.8,123.7,8,http://arxiv.org/pdf/1804.02958v3,"We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.",Igeadnefoexleimco,237.0,59.0,28.0
3406,Image Compression,401.0,joint autoregressive and hierarchical priors for learned image compression,1.0,10.0,5.0,4.0,5.0,3.8,125.5,9,http://arxiv.org/pdf/1809.02736v1,"Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate–distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.",Ijoauanhiprfoleimco,286.0,38.0,70.0
3407,Image Compression,401.0,conditional probability models for deep image compression,1.0,19.0,5.0,5.0,5.0,3.8,129.39999999999998,10,http://arxiv.org/pdf/1801.04260v4,"Deep Neural Networks trained as image auto-encoders have recently emerged as a promising direction for advancing the state-of-the-art in image compression. The key challenge in learning such networks is twofold: To deal with quantization, and to control the trade-off between reconstruction error (distortion) and entropy (rate) of the latent image representation. In this paper, we focus on the latter challenge and propose a new technique to navigate the rate-distortion trade-off for an image compression auto-encoder. The main idea is to directly model the entropy of the latent representation by using a context model: A 3D-CNN which learns a conditional probability model of the latent distribution of the auto-encoder. During training, the auto-encoder makes use of the context model to estimate the entropy of its representation, and the context model is concurrently updated to learn the dependencies between the symbols in the latent representation. Our experiments show that this approach, when measured in MS-SSIM, yields a state-of-the-art image compression system based on a simple convolutional auto-encoder.",Icoprmofodeimco,240.0,27.0,60.0
3408,Image Compression,401.0,variable rate image compression with recurrent neural networks,1.0,23.0,5.0,17.0,5.0,3.8,134.6,11,http://arxiv.org/pdf/1511.06085v5,"A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32$\times$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more.",Ivaraimcowirenene,344.0,23.0,33.0
3409,Image Compression,100.0,efficient nonlinear transforms for lossy image compression,4.0,6.0,5.0,201.0,1.0,3.5,92.7,12,https://arxiv.org/pdf/1802.00847,"We assess the performance of two techniques in the context of nonlinear transform coding with artificial neural networks, Sadam and GDN. Both techniques have been successfully used in state-of-the-art image compression methods, but their performance has not been individually assessed to this point. Together, the techniques stabilize the training procedure of nonlinear image transforms and increase their capacity to approximate the (unknown) rate-distortion optimal transform functions. Besides comparing their performance to established alternatives, we detail the implementation of both methods and provide open-source code along with the paper.",Iefnotrfoloimco,40.0,18.0,8.0
3410,Image Compression,401.0,learning convolutional networks for content-weighted image compression,1.0,68.0,4.0,13.0,5.0,3.4000000000000004,151.4,13,http://arxiv.org/pdf/2103.08870v2,"Lossy image compression is generally formulated as a joint rate-distortion optimization problem to learn encoder, quantizer, and decoder. Due to the non-differentiable quantizer and discrete entropy estimation, it is very challenging to develop a convolutional network (CNN)-based image compression system. In this paper, motivated by that the local information content is spatially variant in an image, we suggest that: (i) the bit rate of the different parts of the image is adapted to local content, and (ii) the content-aware bit rate is allocated under the guidance of a content-weighted importance map. The sum of the importance map can thus serve as a continuous alternative of discrete entropy estimation to control compression rate. The binarizer is adopted to quantize the output of encoder and a proxy function is introduced for approximating binary operation in backward propagation to make it differentiable. The encoder, decoder, binarizer and importance map can be jointly optimized in an end-to-end manner. And a convolutional entropy encoder is further presented for lossless compression of importance map and binary codes. In low bit rate image compression, experiments show that our system significantly outperforms JPEG and JPEG 2000 by structural similarity (SSIM) index, and can produce the much better visual result with sharp edges, rich textures, and fewer artifacts.",Ileconefocoimco,226.0,36.0,27.0
3411,Image Compression,38.0,deep learning model for real-time image compression in internet of underwater things (iout),5.0,201.0,1.0,11.0,5.0,3.4,95.1,14,http://arxiv.org/abs/2012.06712v2,"Recently, the advancements of Internet-of-Things (IoT) have expanded its application in underwater environment which leads to the development of a new field of Internet of Underwater Things (IoUT). It offers a broader view of applications such as atmosphere observation, habitat monitoring of sea animals, defense and disaster prediction. Data transmission of images captured by the smart underwater objects is very challenging due to the nature of underwater environment and necessitates an efficient image transmission strategy for IoUT. In this paper, we model and implement a discrete wavelet transform (DWT) based deep learning model for image compression in IoUT. For achieving effective compression with better reconstruction image quality, convolution neural network (CNN) is used at the encoding as well as decoding side. We validate DWT–CNN model using extensive set of experimentations and depict that the presented deep learning model is superior to existing methods such as super-resolution convolutional neural networks (SRCNN), JPEG and JPEG2000 in terms of compression performance as well as reconstructed image quality. The DWT–CNN model attains an average peak signal-to-noise ratio (PSNR) of 53.961 with average space saving (SS) of 79.7038%.",Idelemoforeimcoininofunth(i,93.0,28.0,0.0
3412,Image Compression,101.0,semantic perceptual image compression using deep convolution networks,3.0,14.0,5.0,201.0,1.0,3.2,96.2,15,https://arxiv.org/pdf/1612.08712,"The Internet of Underwater Things (IoUT) is an emerging communication ecosystem developed for connecting underwater objects in maritime and underwater environments. The IoUT technology is intricately linked with intelligent boats and ships, smart shores and oceans, automatic marine transportations, positioning and navigation, underwater exploration, disaster prediction and prevention, as well as with intelligent monitoring and security. The IoUT has an influence at various scales ranging from a small scientific observatory, to a midsized harbor, and to covering global oceanic trade. The network architecture of IoUT is intrinsically heterogeneous and should be sufficiently resilient to operate in harsh environments. This creates major challenges in terms of underwater communications, whilst relying on limited energy resources. Additionally, the volume, velocity, and variety of data produced by sensors, hydrophones, and cameras in IoUT is enormous, giving rise to the concept of Big Marine Data (BMD), which has its own processing challenges. Hence, conventional data processing techniques will falter, and bespoke Machine Learning (ML) solutions have to be employed for automatically learning the specific BMD behavior and features facilitating knowledge extraction and decision support. The motivation of this paper is to comprehensively survey the IoUT, BMD, and their synthesis. It also aims for exploring the nexus of BMD with ML. We set out from underwater data collection and then discuss the family of IoUT data communication techniques with an emphasis on the state-of-the-art research challenges. We then review the suite of ML solutions suitable for BMD handling and analytics. We treat the subject deductively from an educational perspective, critically appraising the material surveyed.",Isepeimcousdecone,45.0,45.0,2.0
3413,Image Compression,401.0,gradient magnitude similarity deviation: a highly efficient perceptual image quality index,1.0,18.0,5.0,143.0,3.0,3.2,170.4,16,http://arxiv.org/pdf/1308.3052v2,"It is an important task to faithfully evaluate the perceptual quality of output images in many applications, such as image compression, image restoration, and multimedia streaming. A good image quality assessment (IQA) model should not only deliver high quality prediction accuracy, but also be computationally efficient. The efficiency of IQA metrics is becoming particularly important due to the increasing proliferation of high-volume visual data in high-speed networks. We present a new effective and efficient IQA model, called gradient magnitude similarity deviation (GMSD). The image gradients are sensitive to image distortions, while different local structures in a distorted image suffer different degrees of degradations. This motivates us to explore the use of global variation of gradient based local quality map for overall image quality prediction. We find that the pixel-wise gradient magnitude similarity (GMS) between the reference and distorted images combined with a novel pooling strategy-the standard deviation of the GMS map-can predict accurately perceptual image quality. The resulting GMSD algorithm is much faster than most state-of-the-art IQA methods, and delivers highly competitive prediction accuracy. MATLAB source code of GMSD can be downloaded at http://www4.comp.polyu.edu.hk/~cslzhang/IQA/GMSD/GMSD.htm.",Igrmasideahiefpeimquin,839.0,53.0,144.0
3414,Image Compression,5.0,a survey: various techniques of image compression,5.0,201.0,1.0,62.0,4.0,3.1,100.5,17,https://arxiv.org/pdf/1311.6877,"This paper addresses about various image compression techniques. On the basis of analyzing the various image compression techniques this paper presents a survey of existing research papers. In this paper we analyze different types of existing method of image compression. Compression of an image is significantly different then compression of binary raw data. To solve these use different types of techniques for image compression. Now there is question may be arise that how to image compress and which types of technique is used. For this purpose there are basically two types are method are introduced namely lossless and lossy image compression techniques. In present time some other techniques are added with basic method. In some area neural network genetic algorithms are used for image compression. 
Keywords-Image Compression; Lossless; Lossy; Redundancy; Benefits of Compression.",Iasuvateofimco,81.0,39.0,2.0
3415,Image Compression,60.0,variable rate deep image compression with a conditional autoencoder,4.0,201.0,1.0,12.0,5.0,3.1,102.0,18,https://openaccess.thecvf.com/content_ICCV_2019/papers/Choi_Variable_Rate_Deep_Image_Compression_With_a_Conditional_Autoencoder_ICCV_2019_paper.pdf,"In this paper, we propose a novel variable-rate learned image compression framework with a conditional autoencoder. Previous learning-based image compression methods mostly require training separate networks for different compression rates so they can yield compressed images of varying quality. In contrast, we train and deploy only one variable-rate image compression network implemented with a conditional autoencoder. We provide two rate control parameters, i.e., the Lagrange multiplier and the quantization bin size, which are given as conditioning variables to the network. Coarse rate adaptation to a target is performed by changing the Lagrange multiplier, while the rate can be further fine-tuned by adjusting the bin size used in quantizing the encoded representation. Our experimental results show that the proposed scheme provides a better rate-distortion trade-off than the traditional variable-rate image compression codecs such as JPEG2000 and BPG. Our model also shows comparable and sometimes better performance than the state-of-the-art learned image compression models that deploy multiple networks trained for varying rates.",Ivaradeimcowiacoau,61.0,32.0,13.0
3416,Image Compression,18.0,image compression: a survey,5.0,201.0,1.0,58.0,4.0,3.1,103.20000000000002,19,https://tarjomefa.com/wp-content/uploads/2017/09/7774-English-TarjomeFa.pdf,"Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction.",Iimcoasu,33.0,98.0,1.0
3417,Image Compression,54.0,an encryption-then-compression system for lossless image compression standards,4.0,201.0,1.0,29.0,5.0,3.1,105.3,20,https://www.jstage.jst.go.jp/article/transinf/E100.D/1/E100.D_2016MUL0002/_pdf,"In many multimedia applications, image encryption has to be conducted prior to image compression. This letter proposes an Encryption-then-Compression system using JPEG XR/JPEG-LS friendly perceptual encryption method, which enables to be conducted prior to the JPEG XR/JPEG-LS standard used as an international standard lossless compression method. The proposed encryption scheme can provides approximately the same compression performance as that of the lossless compression without any encryption. It is also shown that the proposed system consists of four block-based encryption steps, and provides a reasonably high level of security. Existing conventional encryption methods have not been designed for international lossless compression standards, but for the first time this letter focuses on applying the standards. key words: Encryption-then-Compression system, lossless compression, international standard, JPEG XR, JPEG-LS",Ianensyfoloimcost,49.0,12.0,0.0
3418,Image Compression,28.0,a tutorial on image compression for optical space imaging systems,5.0,201.0,1.0,55.0,4.0,3.1,105.3,21,https://core.ac.uk/download/pdf/76525707.pdf,"Public policies and private initiatives share the will to explore outer space and to monitor the Earth from space sensors. Recent years have seen an increased number of space missions, while the sensors on board aircrafts or spacecrafts have also significantly improved their acquisition capabilities. Given this huge volume of remote sensing data and the detailed characteristics of the acquired images, a data compression process is in order to allow as large a transmission rate as possible.",Iatuonimcofoopspimsy,71.0,71.0,7.0
3419,Image Compression,46.0,wavelet based volumetric medical image compression,4.0,201.0,1.0,39.0,5.0,3.1,105.9,22,http://arxiv.org/pdf/1002.2418v1,"The amount of image data generated each day in health care is ever increasing, especially in combination with the improved scanning resolutions and the importance of volumetric image data sets. Handling these images raises the requirement for efficient compression, archival and transmission techniques. Currently, JPEG 2000's core coding system, defined in Part 1, is the default choice for medical images as it is the DICOM-supported compression technique offering the best available performance for this type of data. Yet, JPEG 2000 provides many options that allow for further improving compression performance for which DICOM offers no guidelines. Moreover, over the last years, various studies seem to indicate that performance improvements in wavelet-based image coding are possible when employing directional transforms. In this paper, we thoroughly investigate techniques allowing for improving the performance of JPEG 2000 for volumetric medical image compression. For this purpose, we make use of a newly developed generic codec framework that supports JPEG 2000 with its volumetric extension (JP3D), various directional wavelet transforms as well as a generic intra-band prediction mode. A thorough objective investigation of the performance-complexity trade-offs offered by these techniques on medical data is carried out. Moreover, we provide a comparison of the presented techniques to H.265/MPEG-H HEVC, which is currently the most state-of-the-art video codec available. Additionally, we present results of a first time study on the subjective visual performance when using the aforementioned techniques. This enables us to provide a set of guidelines and settings on how to optimally compress medical volumetric images at an acceptable complexity level. HighlightsWe investigated how to optimally compress volumetric medical images with JP3D.We extend JP3D with directional wavelets and intra-band prediction.Volumetric wavelets and entropy-coding improve the compression performance.Compression gains for medical images with directional wavelets are often minimal.We recommend further adoption of JP3D for volumetric medical image compression.",Iwabavomeimco,102.0,65.0,2.0
3420,Image Compression,19.0,analysis of image compression algorithm using dct,5.0,201.0,1.0,66.0,4.0,3.1,105.9,23,https://www.academia.edu/download/28318445/CE21515521.pdf,"Image compression is the application of Data compression on digital images. The discrete cosine transform (DCT) is a technique for converting a signal into elementary frequency components. It is widely used in image compression. Here we develop some simple functions to compute the DCT and to compress images. An image compression algorithm was comprehended using Matlab code, and modified to perform better when implemented in hardware description language. The IMAP block and IMAQ block of MATLAB was used to analyse and study the results of Image Compression using DCT and varying co-efficients for compression were developed to show the resulting image and error image from the original images. Image Compression is studied using 2-D discrete Cosine Transform. The original image is transformed in 8-by-8 blocks and then inverse transformed in 8-by-8 blocks to create the reconstructed image. The inverse DCT would be performed using the subset of DCT coefficients. The error image (the difference between the original and reconstructed image) would be displayed. Error value for every image would be calculated over various values of DCT co-efficients as selected by the user and would be displayed in the end to detect the accuracy and compression in the resulting image and resulting performance parameter would be indicated in terms of MSE , i.e. Mean Square Error.",Ianofimcoalusdc,73.0,24.0,3.0
3421,Image Compression,2.0,a review of image compression and comparison of its algorithms,5.0,201.0,1.0,89.0,4.0,3.1,107.7,24,https://ceng460.cankaya.edu.tr/uploads/files/file/Review%20paper.pdf,"Image compression is now essential for applications such as transmission and storage in data bases. In this paper we review and discuss about the image compression, need of compression, its principles, and classes of compression and various algorithm of image compression. This paper attempts to give a recipe for selecting one of the popular image compression algorithms based on Wavelet, JPEG/DCT, VQ, and Fractal approaches. We review and discuss the advantages and disadvantages of these algorithms for compressing grayscale images, give an experimental comparison on 256×256 commonly used image of Lenna and one 400×400 fingerprint image.",Iareofimcoancoofital,119.0,22.0,4.0
3422,Image Compression,71.0,image compression,4.0,201.0,1.0,24.0,5.0,3.1,108.9,25,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.8013&rep=rep1&type=pdf,In this paper offers a simple and lossless compression method for compression of medical images. Method is based on wavelet decomposition of the medical images followed by the correlation analysis of coefficients. The correlation analyses are the basis of prediction equation for each sub band. Predictor variable selection is performed through coefficient graphic method to avoid multicollinearity problem and to achieve high prediction accuracy and compression rate. The method is applied on MRI and CT images. Results show that the proposed approach gives a high compression rate for MRI and CT images comparing with state of the art methods.,Iimco,65.0,39.0,17.0
3423,Image Compression,39.0,discrete anamorphic transform for image compression,5.0,201.0,1.0,56.0,4.0,3.1,108.9,26,https://ieeexplore.ieee.org/iel7/97/6799218/06804641.pdf,"To deal with the exponential increase of digital data, new compression technologies are needed for more efficient representation of information. We introduce a physics-based transform that enables image compression by increasing the spatial coherency. We also present the Stretched Modulation Distribution, a new density function that provides the recipe for the proposed image compression. Experimental results show pre-compression using our method can improve the performance of JPEG 2000 format.",Idiantrfoimco,38.0,24.0,1.0
3424,Image Compression,17.0,high performance scalable image compression with ebcot,5.0,201.0,1.0,79.0,4.0,3.1,109.2,27,https://www.academia.edu/download/28454989/high_performance_scalable_image_compression_with_ebcot..pdf,"A new image compression algorithm is proposed, based on independent embedded block coding with optimized truncation of the embedded bit-streams (EBCOT). The algorithm exhibits state-of-the-art compression performance while producing a bit-stream with a rich feature set, including resolution and SNR scalability together with a random access property. The algorithm has modest complexity and is extremely well suited to applications involving remote browsing of large compressed images. The algorithm lends itself to explicit optimization with respect to MSE as well as more realistic psychovisual metrics, capable of modeling the spatially varying visual masking phenomenon.",Ihipescimcowieb,1692.0,22.0,195.0
3425,Image Compression,26.0,the loco-i lossless image compression algorithm: principles and standardization into jpeg-ls,5.0,201.0,1.0,81.0,4.0,3.1,112.5,28,https://www.hpl.hp.com/research/papers/JPEG-LS-LOCO.pdf,"LOCO-I (LOw COmplexity LOssless COmpression for Images) is the algorithm at the core of the new ISO/ITU standard for lossless and near-lossless compression of continuous-tone images, JPEG-LS. It is conceived as a ""low complexity projection"" of the universal context modeling paradigm, matching its modeling unit to a simple coding unit. By combining simplicity with the compression potential of context models, the algorithm ""enjoys the best of both worlds."" It is based on a simple fixed context model, which approaches the capability of the more complex universal techniques for capturing high-order dependencies. The model is tuned for efficient performance in conjunction with an extended family of Golomb-type codes, which are adaptively chosen, and an embedded alphabet extension for coding of low-entropy image regions. LOCO-I attains compression ratios similar or superior to those obtained with state-of-the-art schemes based on arithmetic coding. Moreover, it is within a few percentage points of the best available compression ratios, at a much lower complexity level. We discuss the principles underlying the design of LOCO-I, and its standardization into JPEC-LS.",Ithloloimcoalpranstinjp,1582.0,131.0,239.0
3426,Image Compression,99.0,an image compression and encryption algorithm based on chaotic system and compressive sensing,4.0,201.0,1.0,14.0,5.0,3.1,114.3,29,http://arxiv.org/pdf/1709.01597v1,"Abstract For a linear image encryption system, it is vulnerable to the chosen-plaintext attack. To overcome the weakness and reduce the correlation among pixels of the encryption image, an effective image compression and encryption algorithm based on chaotic system and compressive sensing is proposed. The original image is first permuted by the Arnold transform to reduce the block effect in the compression process, and then the resulting image is compressed and re-encrypted by compressive sensing, simultaneously. Moreover, the bitwise XOR operation based on chaotic system is performed on the measurements to change the pixel values and a pixel scrambling method is employed to disturb the positions of pixels. Besides, the keys used in chaotic systems are related to the plaintext image. Simulation results verify the effectiveness and reliability of the proposed image compression and encryption algorithm with considerable compression and security performance.",Ianimcoanenalbaonchsyancose,79.0,36.0,2.0
3427,Image Compression,37.0,analysis of various image compression techniques,5.0,201.0,1.0,78.0,4.0,3.1,114.9,30,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.674.6761&rep=rep1&type=pdf,"With the rapid development of digital technology in consumer electronics, the demand to preserve raw image data for further editing or repeated compression is increasing. Image compression is minimizing the size in bytes of an image without degrading the quality of the image to an unacceptable level. There are several different ways in which images can be compressed. This paper analyzes various image compression techniques. In addition, specific methods are presented illustrating the application of such techniques to the real-world images. We have presented various steps involved in the general procedure for compressing images. We provided the basics of image coding with a discussion of vector quantization and one of the main technique of wavelet compression under vector quantization. This analysis of various compression techniques provides knowledge in identifying the advantageous features and helps in choosing correct method for compression.",Ianofvaimcote,49.0,6.0,0.0
3428,Image Compression,35.0,a low-complexity parametric transform for image compression,5.0,201.0,1.0,92.0,4.0,3.1,118.5,31,http://arxiv.org/pdf/1405.6147v1,"In this paper, a one-parameter eight-point orthogonal transform suitable for image compression is proposed. An algorithm for its fast computation is developed and an efficient structure for a simple implementation valid for all possible values of its independent parameter is proposed. It is shown that an appropriate selection of the values of the parameter results in a number of new multiplication-free transforms having a good compromise between the computational complexity and performance. Applying the proposed transform to image compression, we show that it outperforms the existing transforms having complexities similar to that of the proposed one.",Ialopatrfoimco,57.0,10.0,8.0
3429,Image Compression,401.0,five modulus method for image compression,1.0,93.0,4.0,77.0,4.0,3.1,180.6,32,http://arxiv.org/pdf/1211.4591v1,"Data is compressed by reducing its redundancy, but this also makes the data less reliable, more prone to errors. In this paper a novel approach of image compression based on a new method that has been created for image compression which is called Five Modulus Method (FMM). The new method consists of converting each pixel value in an 8-by-8 block into a multiple of 5 for each of the R, G and B arrays. After that, the new values could be divided by 5 to get new values which are 6-bit length for each pixel and it is less in storage space than the original value which is 8-bits. Also, a new protocol for compression of the new values as a stream of bits has been presented that gives the opportunity to store and transfer the new compressed image easily.",Ifimomefoimco,42.0,15.0,2.0
3430,Image Compression,14.0,a novel transform for image compression,5.0,201.0,1.0,109.0,3.0,2.8,117.3,33,http://arxiv.org/pdf/0903.2272v1,"In this paper, we propose an orthogonal multiplication-free transform of order that is an integral power of two by an appropriate extension of the well-known fourthorder integer discrete cosine transform. Moreover, we develop an efficient algorithm for its fast computation. It is shown that the computational and structural complexities of the algorithm are similar to that of the Hadamard transform. By applying the proposed transform to image compression, we show that it outperforms the existing transforms having complexities similar to that of the proposed one.",Ianotrfoimco,46.0,8.0,7.0
3431,Image Compression,55.0,"a flexible representation of quantum images for polynomial preparation, image compression, and processing operations",4.0,201.0,1.0,69.0,4.0,2.8,117.6,34,https://www.academia.edu/download/51232348/s11128-010-0177-y20170107-22172-fhuge8.pdf,"A Flexible Representation of Quantum Images (FRQI) is proposed to provide a representation for images on quantum computers in the form of a normalized state which captures information about colors and their corresponding positions in the images. A constructive polynomial preparation for the FRQI state from an initial state, an algorithm for quantum image compression (QIC), and processing operations for quantum images are combined to build the whole process for quantum image processing on FRQI. The simulation experiments on FRQI include storing, retrieving of images and a detection of a line in binary images by applying quantum Fourier transform as a processing operation. The compression ratios of QIC between groups of same color positions range from 68.75 to 90.63% on single digit images and 6.67–31.62% on the Lena image. The FRQI provides a foundation not only to express images but also to explore theoretical and practical aspects of image processing on quantum computers.",Iaflreofquimfopoprimcoanprop,390.0,18.0,37.0
3432,Image Compression,43.0,image compression using dct and wavelet transformations,4.0,201.0,1.0,87.0,4.0,2.8,119.4,35,https://www.academia.edu/download/34375174/5.pdf,"Image compression is a widely addressed researched area. Many compression standards are in place. But still here there is a scope for high compression with quality reconstruction. The JPEG standard makes use of Discrete Cosine Transform (DCT) for compression. The introduction of the wavelets gave a different dimensions to the compression. This paper aims at the analysis of compression using DCT and Wavelet transform by selecting proper threshold method, better result for PSNR have been obtained. Extensive experimentation has been carried out to arrive at the conclusion.",Iimcousdcanwatr,123.0,11.0,4.0
3433,Image Compression,105.0,a novel image compression–encryption hybrid algorithm based on the analysis sparse representation,3.0,201.0,1.0,27.0,5.0,2.8,120.0,36,https://185.236.37.180/article/preview/5449606.pdf,"Abstract Recent advances on the compressive sensing theory were invoked for image compression–encryption based on the synthesis sparse model. In this paper we concentrate on an alternative sparse representation model, i.e., the analysis sparse model, to propose a novel image compression–encryption hybrid algorithm. The analysis sparse representation of the original image is obtained with an overcomplete fixed dictionary that the order of the dictionary atoms is scrambled, and the sparse representation can be considered as an encrypted version of the image. Moreover, the sparse representation is compressed to reduce its dimension and re-encrypted by the compressive sensing simultaneously. To enhance the security of the algorithm, a pixel-scrambling method is employed to re-encrypt the measurements of the compressive sensing. Various simulation results verify that the proposed image compression–encryption hybrid algorithm could provide a considerable compression performance with a good security.",Ianoimcohyalbaonthanspre,64.0,31.0,2.0
3434,Image Compression,44.0,comparative analysis between dct & dwt techniques of image compression,4.0,201.0,1.0,91.0,4.0,2.8,120.9,37,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.474.5478&rep=rep1&type=pdf,"Image compression is a method through which we can reduce the storage space of images, videos which will helpful to increase storage and transmission process's performance. In image compression, we do not only concentrate on reducing size but also concentrate on doing it without losing quality and information of image. In this paper, two image compression techniques are simulated. The first technique is based on Discrete Cosine Transform (DCT) and the second one is based on Discrete Wavelet Transform (DWT). The results of simulation are shown and compared different quality parameters of its by applying on various images Keywords: DCT, DWT, Image compression, Image processing",Icoanbedc&dwteofimco,78.0,7.0,1.0
3435,Image Compression,4.0,fractal image compression: theory and application,5.0,201.0,1.0,136.0,3.0,2.8,122.4,38,http://arxiv.org/pdf/2002.00239v2,"From the contents: Recent theoretical results on fast encoding and decoding methods, various schemes for encoding images using fractal methods, and theoretical models for the encoding/decoding process.- Working C code for a fractal encoding/decoding scheme capable of encoding images in a few seconds, decoding at arbitrary resolution, and achieving high compression rations.- Experimental results from various schemes showing their capability and forming the basis for a sophisticated implementation.- A list of previously unresearched projects containing both new ideas and inhancements to the schemes discussed in the book.- A comparison of the fractal schemes in the book with JPEG, commercial fractal software, and wavelet methods.",Ifrimcothanap,1193.0,0.0,112.0
3436,Image Compression,12.0,optical image compression and encryption methods,5.0,201.0,1.0,128.0,3.0,2.8,122.4,39,http://arxiv.org/pdf/1105.5949v1,"Over the years extensive studies have been carried out to apply coherent optics methods in real-time communications and image transmission. This is especially true when a large amount of information needs to be processed, e.g., in high-resolution imaging. The recent progress in data-processing networks and communication systems has considerably increased the capacity of information exchange. However, the transmitted data can be intercepted by nonauthorized people. This explains why considerable effort is being devoted at the current time to data encryption and secure transmission. In addition, only a small part of the overall information is really useful for many applications. Consequently, applications can tolerate information compression that requires important processing when the transmission bit rate is taken into account. To enable efficient and secure information exchange, it is often necessary to reduce the amount of transmitted information. In this context, much work has been undertaken using the principle of coherent optics filtering for selecting relevant information and encrypting it. Compression and encryption operations are often carried out separately, although they are strongly related and can influence each other. Optical processing methodologies, based on filtering, are described that are applicable to transmission and/or data storage. Finally, the advantages and limitations of a set of optical compression and encryption methods are discussed.",Iopimcoanenme,418.0,161.0,5.0
3437,Image Compression,91.0,compression efficiency for combining different embedded image compression techniques with huffman encoding,4.0,201.0,1.0,65.0,4.0,2.8,127.2,40,http://ethesis.nitrkl.ac.in/4767/1/211EC3320.pdf,"This paper proposes a technique for image compression which uses the different embedded Wavelet based image coding in combination with Huffman-encoder for further compression. There are different types of algorithms available for lossy image compression out of which EZW, SPIHT and Modified SPIHT algorithms are the some of the important compression techniques. EZW algorithm is based on progressive encoding to compress an image into a bit stream with increasing accuracy. SPIHT is a very efficient image compression algorithm that is based on the idea of coding groups of wavelet coefficients as zero trees. Modified SPIHT algorithm and the preprocessing techniques provide significant quality (both subjectively and objectively) reconstruction at the decoder with little additional computational complexity as compared to the previous techniques. Simulation results show that these hybrid algorithms yield quite promising PSNR values at low bitrates.",Icoeffocodiemimcotewihuen,47.0,21.0,1.0
3438,Image Compression,9.0,an introduction to image compression,5.0,201.0,1.0,156.0,3.0,2.8,129.9,41,http://www.unioviedo.es/compnum/transversal_eng/intro_compression.pdf,"In recent years, the development and demand of multimedia product grows increasingly fast, contributing to insufficient bandwidth of network and storage of memory device. Therefore, the theory of data compression becomes more and more significant for reducing the data redundancy to save more hardware space and transmission bandwidth. In computer science and information theory, data compression or source coding is the process of encoding information using fewer bits or other information-bearing units than an unencoded representation. Compression is useful because it helps reduce the consumption of expensive resources such as hard disk space or transmission bandwidth. In this paper, we briefly introduce the fundamental theory of image compression in chapter 1, two typical standards JPEG and JPEG 2000 will be described in chapter 2. Finally, the newly proposed image compression algorithm – Shape Adaptive image Compression will be introduced in chapter 3.",Ianintoimco,46.0,6.0,1.0
3439,Image Compression,170.0,deep image compression via end-to-end learning,3.0,71.0,4.0,201.0,1.0,2.8,139.7,42,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w50/Liu_Deep_Image_Compression_CVPR_2018_paper.pdf,"We present an indirect imaging method that measures both amplitude and phase information from a transmissive target. Our method is based on an optical eigenmode decomposition of the light intensity and the first-order cross correlation between a target field and these eigenmodes. We demonstrate that such optical eigenmode imaging does not need any a priori knowledge of the imaging system and corresponds to a compressive full-field sampling leading to high image extraction efficiencies. Finally, we discuss the implications with respect to second-order correlation imaging.",Ideimcovienle,27.0,23.0,1.0
3440,Image Compression,36.0,hyperspectral image compression using jpeg2000 and principal component analysis,5.0,201.0,1.0,179.0,3.0,2.8,144.9,43,https://www.researchgate.net/profile/Qian-Du-3/publication/3449831_Hyperspectral_Image_Compression_Using_JPEG2000_and_Principal_Component_Analysis/links/540e00c00cf2df04e756c70e/Hyperspectral-Image-Compression-Using-JPEG2000-and-Principal-Component-Analysis.pdf,"Principal component analysis (PCA) is deployed in JPEG2000 to provide spectral decorrelation as well as spectral dimensionality reduction. The proposed scheme is evaluated in terms of rate-distortion performance as well as in terms of information preservation in an anomaly-detection task. Additionally, the proposed scheme is compared to the common approach of JPEG2000 coupled with a wavelet transform for spectral decorrelation. Experimental results reveal that, not only does the proposed PCA-based coder yield rate-distortion and information-preservation performance superior to that of the wavelet-based coder, the best PCA performance occurs when a reduced number of PCs are retained and coded. A linear model to estimate the optimal number of PCs to use in such dimensionality reduction is proposed",Ihyimcousjpanprcoan,375.0,10.0,20.0
3441,Image Compression,178.0,boosted dictionary learning for image compression,3.0,201.0,1.0,37.0,5.0,2.8,144.9,44,http://arxiv.org/pdf/1610.02483v2,"Sparse representations over redundant dictionaries have shown to produce high-quality results in various signal and image processing tasks. Recent advancements in learning-based dictionary design have made image compression using data-adaptive learned dictionaries a promising field. In this paper, we present a boosted dictionary learning framework to construct an ensemble of complementary specialized dictionaries for sparse image representation. Boosted dictionaries along with a competitive sparse coding form our ensemble model which can provide us with more efficient sparse representations. The constituent dictionaries of the ensemble are obtained using a coherence regularized dictionary learning model for which two novel dictionary optimization algorithms are proposed. These algorithms improve the generalization properties of the trained dictionary compared with several incoherent dictionary learning methods. Based on the proposed ensemble model, we then develop a new image compression algorithm using boosted multi-scale dictionaries learned in the wavelet domain. Our algorithm is evaluated for the compression of natural images. Experimental results demonstrate that the proposed algorithm has better rate-distortion performance as compared with several competing compression methods, including analytic and learned dictionary schemes.",Ibodilefoimco,33.0,62.0,2.0
3442,Image Compression,32.0,a variation on svd based image compression,5.0,201.0,1.0,188.0,3.0,2.8,146.4,45,http://dspace.library.iitb.ac.in/xmlui/bitstream/handle/10054/1338/5641.pdf?sequence=1&isAllowed=y,"We present a variation to the well studied SVD based image compression technique. Our variation can be viewed as a preprocessing step in which the input image is permuted as per a fixed, data independent permutation, after which it is fed to the standard SVD algorithm. Likewise, our decompression algorithm can be viewed as the standard SVD algorithm followed by a postprocessing step which applies the inverse permutation. On experimenting with standard images we show that our method performs substantially better than the standard method. Typically, for any given compression quality, our method needs about 30% fewer singular values and vectors to be retained. We also present a bit allocation scheme and show that our method also performs better than the more familiar discrete cosine transform (DCT). We show that the original SVD algorithm as well as our variation, can be viewed as instances of the Karhunen-Loeve transform (KLT). In fact, we observe that there is a whole family of variations possible by choosing different parameter values while applying the KLT. We present heuristic arguments to show that our variation is likely to yield the best compression of all these. We also present experimental evidence, which appears to justify our analysis.",Iavaonsvbaimco,55.0,11.0,0.0
3443,Image Compression,27.0,space-frequency quantization for image compression with directionlets,5.0,201.0,1.0,194.0,3.0,2.8,146.7,46,https://infoscience.epfl.ch/record/134260/files/Space-frequency%20quantization.pdf,"The standard separable 2-D wavelet transform (WT) has recently achieved a great success in image processing because it provides a sparse representation of smooth images. However, it fails to efficiently capture 1-D discontinuities, like edges or contours. These features, being elongated and characterized by geometrical regularity along different directions, intersect and generate many large magnitude wavelet coefficients. Since contours are very important elements in the visual perception of images, to provide a good visual quality of compressed images, it is fundamental to preserve good reconstruction of these directional features. In our previous work, we proposed a construction of critically sampled perfect reconstruction transforms with directional vanishing moments imposed in the corresponding basis functions along different directions, called directionlets. In this paper, we show how to design and implement a novel efficient space-frequency quantization (SFQ) compression algorithm using directionlets. Our new compression method outperforms the standard SFQ in a rate-distortion sense, both in terms of mean-square error and visual quality, especially in the low-rate compression regime. We also show that our compression method, does not increase the order of computational complexity as compared to the standard SFQ algorithm.",Ispqufoimcowidi,70.0,46.0,7.0
3444,Image Compression,401.0,an end-to-end compression framework based on convolutional neural networks,1.0,43.0,4.0,190.0,3.0,2.8,194.5,47,http://arxiv.org/pdf/1708.00838v1,"Traditional image coding standards (such as JPEG and JPEG2000) make the decoded image suffer from many blocking artifacts or noises since the use of big quantization steps. To overcome this problem, we proposed an end-to-end compression framework based on two CNNs, as shown in Figure 1, which produce a compact representation for encoding using a third party coding standard and reconstruct the decoded image, respectively. To make two CNNs effectively collaborate, we develop a unified end-to-end learning framework to simultaneously learn CrCNN and ReCNN such that the compact representation obtained by CrCNN preserves the structural information of the image, which facilitates to accurately reconstruct the decoded image using ReCNN and also makes the proposed compression framework compatible with existing image coding standards.",Ianencofrbaonconene,82.0,41.0,6.0
3445,Image Compression,401.0,lossless image compression through super-resolution,1.0,3.0,5.0,201.0,1.0,2.6,181.8,48,http://arxiv.org/pdf/2004.02872v1,"We introduce a simple and efficient lossless image compression algorithm. We store a low resolution version of an image as raw pixels, followed by several iterations of lossless super-resolution. For lossless super-resolution, we predict the probability of a high-resolution image, conditioned on the low-resolution input, and use entropy coding to compress this super-resolution operator. Super-Resolution based Compression (SReC) is able to achieve state-of-the-art compression rates with practical runtimes on large datasets. Code is available online at https://github.com/caoscott/SReC.",Iloimcothsu,12.0,54.0,0.0
3446,Image Compression,401.0,residual dense network for image restoration,1.0,8.0,5.0,201.0,1.0,2.6,183.8,49,http://arxiv.org/pdf/1906.12021v2,"Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.",Iredenefoimre,195.0,84.0,35.0
3447,Image Compression,401.0,"""zero-shot"" super-resolution using deep internal learning",1.0,11.0,5.0,201.0,1.0,2.6,185.0,50,http://arxiv.org/pdf/2106.02619v1,"Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of images in practice. We prove that when a distribution has a structure that we refer to as Forward Super-Resolution, then simply training generative adversarial networks using gradient descent ascent (GDA) can indeed learn this distribution efficiently, both in terms of sample and time complexities. We also provide concrete empirical evidence that not only our assumption ""forward super-resolution"" is very natural in practice, but also the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via GDA in theory) simulates the actual learning process of GANs in practice on real-world problems.","I""zsuusdeinle",340.0,21.0,74.0
3448,Image Compression,401.0,variational quantum singular value decomposition,1.0,12.0,5.0,201.0,1.0,2.6,185.4,51,http://arxiv.org/abs/2006.02336v3,"Singular value decomposition is central to many problems in engineering and scientific fields. Several quantum algorithms have been proposed to determine the singular values and their associated singular vectors of a given matrix. Although these algorithms are promising, the required quantum subroutines and resources are too costly on near-term quantum devices. In this work, we propose a variational quantum algorithm for singular value decomposition (VQSVD). By exploiting the variational principles for singular values and the Ky Fan Theorem, we design a novel loss function such that two quantum neural networks (or parameterized quantum circuits) could be trained to learn the singular vectors and output the corresponding singular values. Furthermore, we conduct numerical simulations of VQSVD for random matrices as well as its applications in image compression of handwritten digits. Finally, we discuss the applications of our algorithm in recommendation systems and polar decomposition. Our work explores new avenues for quantum information processing beyond the conventional protocols that only works for Hermitian data, and reveals the capability of matrix decomposition on near-term quantum devices.",Ivaqusivade,5.0,100.0,0.0
3449,Image Compression,401.0,practical full resolution learned lossless image compression,1.0,13.0,5.0,201.0,1.0,2.6,185.8,52,http://arxiv.org/pdf/1811.12817v3,"We propose the first practical learned lossless image compression system, L3C, and show that it outperforms the popular engineered codecs, PNG, WebP and JPEG 2000. At the core of our method is a fully parallelizable hierarchical probabilistic model for adaptive entropy coding which is optimized end-to-end for the compression task. In contrast to recent autoregressive discrete probabilistic models such as PixelCNN, our method i) models the image distribution jointly with learned auxiliary representations instead of exclusively modeling the image distribution in RGB space, and ii) only requires three forward-passes to predict all pixel probabilities instead of one for each pixel. As a result, L3C obtains over two orders of magnitude speedups when sampling compared to the fastest PixelCNN variant (Multiscale-PixelCNN). Furthermore, we find that learning the auxiliary representation is crucial and outperforms predefined auxiliary representations such as an RGB pyramid significantly.",Iprfureleloimco,69.0,54.0,12.0
3450,Image Compression,401.0,practical lossless compression with latent variables using bits back coding,1.0,15.0,5.0,201.0,1.0,2.6,186.6,53,http://arxiv.org/pdf/1901.04866v1,"Deep latent variable models have seen recent success in many data domains. Lossless compression is an application of these models which, despite having the potential to be highly useful, has yet to be implemented in a practical manner. We present `Bits Back with ANS' (BB-ANS), a scheme to perform lossless compression with latent variable models at a near optimal rate. We demonstrate this scheme by using it to compress the MNIST dataset with a variational auto-encoder model (VAE), achieving compression rates superior to standard methods with only a simple VAE. Given that the scheme is highly amenable to parallelization, we conclude that with a sufficiently high quality generative model this scheme could be used to achieve substantial improvements in compression rate with acceptable running time. We make our implementation available open source at https://github.com/bits-back/bits-back .",Iprlocowilavausbibaco,51.0,29.0,9.0
3451,Image Compression,401.0,comparison of image quality models for optimization of image processing systems,1.0,16.0,5.0,201.0,1.0,2.6,187.0,54,http://arxiv.org/abs/2005.01338v3,"The performance of objective image quality assessment (IQA) models has been evaluated primarily by comparing model predictions to human quality judgments. Perceptual datasets gathered for this purpose have provided useful benchmarks for improving IQA methods, but their heavy use creates a risk of overfitting. Here, we perform a large-scale comparison of IQA models in terms of their use as objectives for the optimization of image processing algorithms. Specifically, we use eleven full-reference IQA models to train deep neural networks for four low-level vision tasks: denoising, deblurring, super-resolution, and compression. Subjective testing on the optimized images allows us to rank the competing models in terms of their perceptual performance, elucidate their relative advantages and disadvantages in these tasks, and propose a set of desirable properties for incorporation into future IQA models.",Icoofimqumofoopofimprsy,12.0,117.0,0.0
3452,Image Compression,401.0,mean deviation similarity index: efficient and reliable full-reference image quality evaluator,1.0,17.0,5.0,201.0,1.0,2.6,187.4,55,http://arxiv.org/abs/1608.07433v4,"Applications of perceptual image quality assessment (IQA) in image and video processing, such as image acquisition, image compression, image restoration and multimedia communication, have led to the development of many IQA metrics. In this paper, a reliable full reference IQA model is proposed that utilize gradient similarity (GS), chromaticity similarity (CS), and deviation pooling (DP). By considering the shortcomings of the commonly used GS to model human visual system (HVS), a new GS is proposed through a fusion technique that is more likely to follow HVS. We propose an efficient and effective formulation to calculate the joint similarity map of two chromatic channels for the purpose of measuring color changes. In comparison with a commonly used formulation in the literature, the proposed CS map is shown to be more efficient and provide comparable or better quality predictions. Motivated by a recent work that utilizes the standard deviation pooling, a general formulation of the DP is presented in this paper and used to compute a final score from the proposed GS and CS maps. This proposed formulation of DP benefits from the Minkowski pooling and a proposed power pooling as well. The experimental results on six datasets of natural images, a synthetic dataset, and a digitally retouched dataset show that the proposed index provides comparable or better quality predictions than the most recent and competing state-of-the-art IQA metrics in the literature, it is reliable and has low complexity. The MATLAB source code of the proposed metric is available at https://www.mathworks.com/matlabcentral/fileexchange/59809.",Imedesiinefanrefuimquev,75.0,52.0,11.0
3453,Image Compression,401.0,a unified end-to-end framework for efficient deep image compression,1.0,20.0,5.0,201.0,1.0,2.6,188.6,56,http://arxiv.org/pdf/2002.03370v3,"Image compression is a widely used technique to reduce the spatial redundancy in images. Recently, learning based image compression has achieved significant progress by using the powerful representation ability from neural networks. However, the current state-of-the-art learning based image compression methods suffer from the huge computational cost, which limits their capacity for practical applications. In this paper, we propose a unified framework called Efficient Deep Image Compression (EDIC) based on three new technologies, including a channel attention module, a Gaussian mixture model and a decoder-side enhancement module. Specifically, we design an auto-encoder style network for learning based image compression. To improve the coding efficiency, we exploit the channel relationship between latent representations by using the channel attention module. Besides, the Gaussian mixture model is introduced for the entropy model and improves the accuracy for bitrate estimation. Furthermore, we introduce the decoder-side enhancement module to further improve image compression performance. Our EDIC method can also be readily incorporated with the Deep Video Compression (DVC) framework to further improve the video compression performance. Simultaneously, our EDIC method boosts the coding performance significantly while bringing slightly increased computational cost. More importantly, experimental results demonstrate that the proposed approach outperforms the current state-of-the-art image compression methods and is up to more than 150 times faster in terms of decoding speed when compared with Minnen's method. The proposed framework also successfully improves the performance of the recent deep video compression system DVC. Our code will be released at https://github.com/liujiaheng/compression.",Iaunenfrfoefdeimco,11.0,44.0,2.0
3454,Image Compression,401.0,towards flexible blind jpeg artifacts removal,1.0,21.0,5.0,201.0,1.0,2.6,189.0,57,http://arxiv.org/pdf/2109.14573v1,"Training a single deep blind model to handle different quality factors for JPEG image artifacts removal has been attracting considerable attention due to its convenience for practical usage. However, existing deep blind methods usually directly reconstruct the image without predicting the quality factor, thus lacking the flexibility to control the output as the non-blind methods. To remedy this problem, in this paper, we propose a flexible blind convolutional neural network, namely FBCNN, that can predict the adjustable quality factor to control the trade-off between artifacts removal and details preservation. Specifically, FBCNN decouples the quality factor from the JPEG image via a decoupler module and then embeds the predicted quality factor into the subsequent reconstructor module through a quality factor attention block for flexible control. Besides, we find existing methods are prone to fail on non-aligned double JPEG images even with only a one-pixel shift, and we thus propose a double JPEG degradation model to augment the training data. Extensive experiments on single JPEG images, more general double JPEG images, and real-world JPEG images demonstrate that our proposed FBCNN achieves favorable performance against state-of-the-art methods in terms of both quantitative metrics and visual quality.",Itoflbljparre,1.0,55.0,0.0
3455,Image Compression,401.0,learning for video compression with hierarchical quality and recurrent enhancement,1.0,24.0,5.0,201.0,1.0,2.6,190.2,58,http://arxiv.org/pdf/2003.01966v7,"In this paper, we propose a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. The frames in the first layer are compressed by an image compression method with the highest quality. Using these frames as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving bits for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art of deep video compression methods, and outperforms the ""Low-Delay P (LDP) very fast"" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at https://github.com/RenYang-home/HLVC.",Ilefovicowihiquanreen,42.0,46.0,7.0
3456,Image Compression,401.0,an end-to-end joint learning scheme of image compression and quality enhancement with improved entropy minimization,1.0,25.0,5.0,201.0,1.0,2.6,190.6,59,http://arxiv.org/pdf/1912.12817v2,"Recently, learned image compression methods have been actively studied. Among them, entropy-minimization based approaches have achieved superior results compared to conventional image codecs such as BPG and JPEG2000. However, the quality enhancement and rate-minimization are conflictively coupled in the process of image compression. That is, maintaining high image quality entails less compression and vice versa. However, by jointly training separate quality enhancement in conjunction with image compression, the coding efficiency can be improved. In this paper, we propose a novel joint learning scheme of image compression and quality enhancement, called JointIQ-Net, as well as entropy model improvement, thus achieving significantly improved coding efficiency against the previous methods. Our proposed JointIQ-Net combines an image compression sub-network and a quality enhancement sub-network in a cascade, both of which are end-to-end trained in a combined manner within the JointIQ-Net. Also the JointIQ-Net benefits from improved entropy-minimization that newly adopts a Gussian Mixture Model (GMM) and further exploits global context to estimate the probabilities of latent representations. In order to show the effectiveness of our proposed JointIQ-Net, extensive experiments have been performed, and showed that the JointIQ-Net achieves a remarkable performance improvement in coding efficiency in terms of both PSNR and MS-SSIM, compared to the previous learned image compression methods and the conventional codecs such as VVC Intra (VTM 7.1), BPG, and JPEG2000. To the best of our knowledge, this is the first end-to-end optimized image compression method that outperforms VTM 7.1 (Intra), the latest reference software of the VVC standard, in terms of the PSNR and MS-SSIM.",Ianenjolescofimcoanquenwiimenmi,15.0,22.0,0.0
3457,Image Compression,401.0,locally masked convolution for autoregressive models,1.0,28.0,5.0,201.0,1.0,2.6,191.8,60,http://arxiv.org/pdf/2106.02514v2,"Recently, AutoRegressive (AR) models for the whole image generation empowered by transformers have achieved comparable or even better performance to Generative Adversarial Networks (GANs). Unfortunately, directly applying such AR models to edit/change local image regions, may suffer from the problems of missing global information, slow inference speed, and information leakage of local guidance. To address these limitations, we propose a novel model -- image Local Autoregressive Transformer (iLAT), to better facilitate the locally guided image synthesis. Our iLAT learns the novel local discrete representations, by the newly proposed local autoregressive (LA) transformer of the attention mask and convolution mechanism. Thus iLAT can efficiently synthesize the local image regions by key guidance information. Our iLAT is evaluated on various locally guided image syntheses, such as pose-guided person image synthesis and face editing. Both the quantitative and qualitative results show the efficacy of our model.",Ilomacofoaumo,10.0,62.0,1.0
3458,Image Compression,401.0,enhanced invertible encoding for learned image compression,1.0,31.0,5.0,201.0,1.0,2.6,193.0,61,https://arxiv.org/pdf/2108.03690,"Although deep learning based image compression methods have achieved promising progress these days, the performance of these methods still cannot match the latest compression standard Versatile Video Coding (VVC). Most of the recent developments focus on designing a more accurate and flexible entropy model that can better parameterize the distributions of the latent features. However, few efforts are devoted to structuring a better transformation between the image space and the latent feature space. In this paper, instead of employing previous autoencoder style networks to build this transformation, we propose an enhanced Invertible Encoding Network with invertible neural networks (INNs) to largely mitigate the information loss problem for better compression. Experimental results on the Kodak, CLIC, and Tecnick datasets show that our method outperforms the existing learned image compression methods and compression standards, including VVC (VTM 12.1), especially for high-resolution images. Our source code is available at https://github.com/xyq7/InvCompress.",Ieninenfoleimco,1.0,90.0,0.0
3459,Image Compression,401.0,learning to structure an image with few colors,1.0,32.0,5.0,201.0,1.0,2.6,193.4,62,http://arxiv.org/pdf/2003.07848v2,"Color and structure are the two pillars that construct an image. Usually, the structure is well expressed through a rich spectrum of colors, allowing objects in an image to be recognized by neural networks. However, under extreme limitations of color space, the structure tends to vanish, and thus a neural network might fail to understand the image. Interested in exploring this interplay between color and structure, we study the scientific problem of identifying and preserving the most informative image structures while constraining the color space to just a few bits, such that the resulting image can be recognized with possibly high accuracy. To this end, we propose a color quantization network, ColorCNN, which learns to structure the images from the classification loss in an end-to-end manner. Given a color space size, ColorCNN quantizes colors in the original image by generating a color index map and an RGB color palette. Then, this color-quantized image is fed to a pre-trained task network to evaluate its performance. In our experiment, with only a 1-bit color space (i.e., two colors), the proposed network achieves 82.1% top-1 accuracy on the CIFAR10 dataset, outperforming traditional color quantization methods by a large margin. For applications, when encoded with PNG, the proposed color quantization shows superiority over other image compression methods in the extremely low bit-rate regime. The code is available at: https://github.com/hou-yz/color_distillation.",Iletostanimwifeco,3.0,54.0,0.0
3460,Image Compression,401.0,image segmentation using deep learning: a survey,1.0,33.0,5.0,201.0,1.0,2.6,193.8,63,http://arxiv.org/pdf/1911.07685v1,"Image co-segmentation is important for its advantage of alleviating the ill-pose nature of image segmentation through exploring the correlation between related images. Many automatic image co-segmentation algorithms have been developed in the last decade, which are investigated comprehensively in this paper. We firstly analyze visual/semantic cues for guiding image co-segmentation, including object cues and correlation cues. Then we describe the traditional methods in three categories of object elements based, object regions/contours based, common object model based. In the next part, deep learning based methods are reviewed. Furthermore, widely used test datasets and evaluation criteria are introduced and the reported performances of the surveyed algorithms are compared with each other. Finally, we discuss the current challenges and possible future directions and conclude the paper. Hopefully, this comprehensive investigation will be helpful for the development of image co-segmentation technique.",Iimseusdeleasu,282.0,207.0,10.0
3461,Image Compression,401.0,learning end-to-end lossy image compression: a benchmark,1.0,34.0,5.0,201.0,1.0,2.6,194.2,64,http://arxiv.org/pdf/2007.11797v1,"Pre-trained convolutional neural networks (CNNs) are powerful off-the-shelf feature generators and have been shown to perform very well on a variety of tasks. Unfortunately, the generated features are high dimensional and expensive to store: potentially hundreds of thousands of floats per example when processing videos. Traditional entropy based lossless compression methods are of little help as they do not yield desired level of compression, while general purpose lossy compression methods based on energy compaction (e.g. PCA followed by quantization and entropy coding) are sub-optimal, as they are not tuned to task specific objective. We propose a learned method that jointly optimizes for compressibility along with the task objective for learning the features. The plug-in nature of our method makes it straight-forward to integrate with any target objective and trade-off against compressibility. We present results on multiple benchmarks and demonstrate that our method produces features that are an order of magnitude more compressible, while having a regularization effect that leads to a consistent improvement in accuracy.",Ileenloimcoabe,14.0,63.0,1.0
3462,Image Compression,401.0,learning better lossless compression using lossy compression,1.0,35.0,5.0,201.0,1.0,2.6,194.6,65,http://arxiv.org/pdf/2003.10184v1,"We leverage the powerful lossy image compression algorithm BPG to build a lossless image compression system. Specifically, the original image is first decomposed into the lossy reconstruction obtained after compressing it with BPG and the corresponding residual. We then model the distribution of the residual with a convolutional neural network-based probabilistic model that is conditioned on the BPG reconstruction, and combine it with entropy coding to losslessly encode the residual. Finally, the image is stored using the concatenation of the bitstreams produced by BPG and the learned residual coder. The resulting compression system achieves state-of-the-art performance in learned lossless full-resolution image compression, outperforming previous learned approaches as well as PNG, WebP, and JPEG2000.",Ilebelocousloco,14.0,56.0,2.0
3463,Image Compression,401.0,deep image compression using decoder side information,1.0,37.0,5.0,201.0,1.0,2.6,195.4,66,http://arxiv.org/pdf/2001.04753v2,"We present a Deep Image Compression neural network that relies on side information, which is only available to the decoder. We base our algorithm on the assumption that the image available to the encoder and the image available to the decoder are correlated, and we let the network learn these correlations in the training phase.   Then, at run time, the encoder side encodes the input image without knowing anything about the decoder side image and sends it to the decoder. The decoder then uses the encoded input image and the side information image to reconstruct the original image.   This problem is known as Distributed Source Coding in Information Theory, and we discuss several use cases for this technology. We compare our algorithm to several image compression algorithms and show that adding decoder-only side information does indeed improve results. Our code is publicly available at https://github.com/ayziksha/DSIN.",Ideimcousdesiin,3.0,48.0,1.0
3464,Image Compression,401.0,neural image compression for gigapixel histopathology image analysis,1.0,38.0,5.0,201.0,1.0,2.6,195.8,67,http://arxiv.org/abs/1811.02840v2,"We propose Neural Image Compression (NIC), a two-step method to build convolutional neural networks for gigapixel image analysis solely using weak image-level labels. First, gigapixel images are compressed using a neural network trained in an unsupervised fashion, retaining high-level information while suppressing pixel-level noise. Second, a convolutional neural network (CNN) is trained on these compressed image representations to predict image-level labels, avoiding the need for fine-grained manual annotations. We compared several encoding strategies, namely reconstruction error minimization, contrastive training and adversarial feature learning, and evaluated NIC on a synthetic task and two public histopathology datasets. We found that NIC can exploit visual cues associated with image-level labels successfully, integrating both global and local visual information. Furthermore, we visualized the regions of the input gigapixel images where the CNN attended to, and confirmed that they overlapped with annotations from human experts.",Ineimcofogihiiman,49.0,54.0,3.0
3465,Image Compression,401.0,deep homography for efficient stereo image compression,1.0,40.0,5.0,201.0,1.0,2.6,196.6,68,http://arxiv.org/pdf/1606.03798v1,"We present a deep convolutional neural network for estimating the relative homography between a pair of images. Our feed-forward network has 10 layers, takes two stacked grayscale images as input, and produces an 8 degree of freedom homography which can be used to map the pixels from the first image to the second. We present two convolutional neural network architectures for HomographyNet: a regression network which directly estimates the real-valued homography parameters, and a classification network which produces a distribution over quantized homographies. We use a 4-point homography parameterization which maps the four corners from one image into the second image. Our networks are trained in an end-to-end fashion using warped MS-COCO images. Our approach works without the need for separate local feature detection and transformation estimation stages. Our deep models are compared to a traditional homography estimator based on ORB features and we highlight the scenarios where HomographyNet outperforms the traditional technique. We also describe a variety of applications powered by deep homography estimation, thus showcasing the flexibility of a deep learning approach.",Idehofoefstimco,2.0,52.0,1.0
3466,Image Compression,115.0,image compression algorithms in wireless multimedia sensor networks: a survey,3.0,201.0,1.0,41.0,4.0,2.5,127.2,69,http://arxiv.org/pdf/1311.6877v1,"Abstract Unlike classical wired networks and wireless sensor networks, WMSN differs from their predecessor’s scalar network basically in the following points; nature and size of data being transmitted, important memory resources, as well as, power consumed per each node for processing and transmission. The most effective solution to overcome those problems is image compression. As the image contains massive amount of redundancies resulting from high correlation between pixels, many compression algorithms have been developed. The main objective of this survey was to study and analyze relevant research directions and the most recent algorithms of image compression over WMSN. This survey characterizes the benefits and shortcomings of recent efforts of such algorithms. Moreover, it provides an open research issue for each compression method; and its potentials to WMSN. Reducing consumed power thus granting long life time is considered the main performance metric and will be the main target in the investigated solution.",Iimcoalinwimuseneasu,77.0,37.0,2.0
3467,Image Compression,106.0,new simple and efficient color space transformations for lossless image compression,3.0,201.0,1.0,53.0,4.0,2.5,128.1,70,https://www.researchgate.net/profile/Roman-Starosolski/publication/260806121_New_simple_and_efficient_color_space_transformations_for_lossless_image_compression/links/59e11542aca2724cbfdb6f3f/New-simple-and-efficient-color-space-transformations-for-lossless-image-compression.pdf,"Abstract We present simple color space transformations for lossless image compression and compare them with established transformations including RCT, YCoCg-R and with the optimal KLT for 3 sets of test images and for significantly different compression algorithms: JPEG-LS, JPEG2000 and JPEG XR. One of the transformations, RDgDb, which requires just 2 integer subtractions per image pixel, on average results in the best ratios for JPEG2000 and JPEG XR, while for a specific set or in case of JPEG-LS its compression ratios are either the best or within 0.1 bpp from the best. The overall best ratios were obtained with JPEG-LS and the modular-arithmetic variant of RDgDb (mRDgDb). Another transformation (LDgEb), based on analog transformations in human vision system, is with respect to complexity and average ratios better than RCT and YCoCg-R, although worse than RDgDb; for one of the sets it obtains the best ratios.",Inesianefcosptrfoloimco,51.0,20.0,2.0
3468,Image Compression,58.0,the jpeg 2000 still image compression standard,4.0,201.0,1.0,101.0,3.0,2.5,128.10000000000002,71,https://infoscience.epfl.ch/record/111813/files/223%20The%20JPEG%202000%20still%20image%20compression%20standard.pdf,"One of the aims of the standardization committee has been the development of Part I, which could be used on a royalty- and fee-free basis. This is important for the standard to become widely accepted. The standardization process, which is coordinated by the JTCI/SC29/WG1 of the ISO/IEC has already produced the international standard (IS) for Part I. In this article the structure of Part I of the JPFG 2000 standard is presented and performance comparisons with established standards are reported. This article is intended to serve as a tutorial for the JPEG 2000 standard. The main application areas and their requirements are given. The architecture of the standard follows with the description of the tiling, multicomponent transformations, wavelet transforms, quantization and entropy coding. Some of the most significant features of the standard are presented, such as region-of-interest coding, scalability, visual weighting, error resilience and file format aspects. Finally, some comparative results are reported and the future parts of the standard are discussed.",Ithjp20stimcost,1592.0,81.0,131.0
3469,Image Compression,126.0,high dynamic range image compression by optimizing tone mapped image quality index,3.0,201.0,1.0,46.0,4.0,2.5,132.0,72,https://ece.uwaterloo.ca/~k29ma/papers/15_TIP_TMQI-II.pdf,"Tone mapping operators (TMOs) aim to compress high dynamic range (HDR) images to low dynamic range (LDR) ones so as to visualize HDR images on standard displays. Most existing TMOs were demonstrated on specific examples without being thoroughly evaluated using well-designed and subject-validated image quality assessment models. A recently proposed tone mapped image quality index (TMQI) made one of the first attempts on objective quality assessment of tone mapped images. Here, we propose a substantially different approach to design TMO. Instead of using any predefined systematic computational structure for tone mapping (such as analytic image transformations and/or explicit contrast/edge enhancement), we directly navigate in the space of all images, searching for the image that optimizes an improved TMQI. In particular, we first improve the two building blocks in TMQI-structural fidelity and statistical naturalness components-leading to a TMQI-II metric. We then propose an iterative algorithm that alternatively improves the structural fidelity and statistical naturalness of the resulting image. Numerical and subjective experiments demonstrate that the proposed algorithm consistently produces better quality tone mapped images even when the initial images of the iteration are created by the most competitive TMOs. Meanwhile, these results also validate the superiority of TMQI-II over TMQI.",Ihidyraimcobyoptomaimquin,104.0,36.0,12.0
3470,Image Compression,104.0,image compression using block truncation coding,3.0,201.0,1.0,70.0,4.0,2.5,132.60000000000002,73,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.389.4785&rep=rep1&type=pdf,"The present work investigates image compression using block truncation coding. Two algorithms were selected namely, the original block truncation coding (BTC) and Absolute Moment block truncation coding (AMBTC) and a comparative study was performed. Both of two techniques rely on applying divided image into non overlapping blocks. They differ in the way of selecting the quantization level in order to remove redundancy. Objectives measures were used to evaluate the image quality such as: Peak Signal to Noise Ratio (PSNR), Weighted Peak Signal to Noise Ratio (WPSNR), Bit Rate (BR) and Structural Similarity Index (SSIM).The results have shown that the ATBTC algorithm outperforms the BTC. It has been show that the image compression using AMBTC provides better image quality than image compression using BTC at the same bit rate. Moreover, the AMBTC is quite faster compared to BTC Index Terms—BTC, AMBTC, WPSNR, SSIM.",Iimcousbltrco,440.0,14.0,35.0
3471,Image Compression,52.0,study on huber fractal image compression,4.0,201.0,1.0,132.0,3.0,2.5,135.6,74,http://arxiv.org/pdf/1404.0774v1,"In this paper, a new similarity measure for fractal image compression (FIC) is introduced. In the proposed Huber fractal image compression (HFIC), the linear Huber regression technique from robust statistics is embedded into the encoding procedure of the fractal image compression. When the original image is corrupted by noises, we argue that the fractal image compression scheme should be insensitive to those noises presented in the corrupted image. This leads to a new concept of robust fractal image compression. The proposed HFIC is one of our attempts toward the design of robust fractal image compression. The main disadvantage of HFIC is the high computational cost. To overcome this drawback, particle swarm optimization (PSO) technique is utilized to reduce the searching time. Simulation results show that the proposed HFIC is robust against outliers in the image. Also, the PSO method can effectively reduce the encoding time while retaining the quality of the retrieved image.",Istonhufrimco,67.0,27.0,3.0
3472,Image Compression,140.0,lenselet image compression scheme based on subaperture images streaming,3.0,201.0,1.0,45.0,4.0,2.5,135.9,75,http://arxiv.org/pdf/1909.05638v1,"Plenoptic cameras capture the light field in a scene with a single shot and produce lenselet images. From a lenselet image, light field can be reconstructed, with which we can render images with different viewpoints and focal length. Because of large volume data, high efficient image compression scheme for storage and transmission is urgent. Containing 4D light field information, lenselet images have much more redundant information than traditional 2D images. In this paper, we propose a subaperture images streaming scheme to compress lenselet images, in which rotation scan mapping is adopted to further improve compression efficiency. The experiment results show our approach can efficient compress the redundancy in lenselet images and outperform traditional image compression method.",Ileimcoscbaonsuimst,58.0,10.0,3.0
3473,Image Compression,87.0,color image compression based on wavelet packet best tree,4.0,201.0,1.0,112.0,3.0,2.5,140.1,76,https://arxiv.org/pdf/1004.3276,"In Image Compression, the researchers’ aim is to reduce the number of bits required to represent an image by removing the spatial and spectral redundancies. Recently discrete wavelet transform and wavelet packet has emerged as popular techniques for image compression. The wavelet transform is one of the major processing components of image compression. The result of the compression changes as per the basis and tap of the wavelet used. It is proposed that proper selection of mother wavelet on the basis of nature of images, improve the quality as well as compression ratio remarkably. We suggest the novel technique, which is based on wavelet packet best tree based on Threshold Entropy with enhanced run-length encoding. This method reduces the time complexity of wavelet packets decomposition as complete tree is not decomposed. Our algorithm selects the sub-bands, which include significant information based on threshold entropy. The enhanced run length encoding technique is suggested provides better results than RLE. The result when compared with JPEG-2000 proves to be better.",Icoimcobaonwapabetr,60.0,26.0,3.0
3474,Image Compression,45.0,an overview of image compression approaches,4.0,201.0,1.0,162.0,3.0,2.5,142.5,77,http://arxiv.org/pdf/1702.03049v1,"In this research, under the theory of data compression main subjects of compression (proportion of compression, repetitions caused by coding, fidelity criteria) components of image compression systems are analyzed. Thereafter lossy and lossless image compressions are analyzed.",Ianovofimcoap,30.0,11.0,1.0
3476,Image Compression,81.0,image compression using neural networks and haar wavelet,4.0,201.0,1.0,155.0,3.0,2.5,151.2,78,http://wseas.us/e-library/transactions/signal/2008/27-363.pdf,"Wavelet-based image compression provides substantial improvements in picture quality at higher compression ratios. Haar wavelet transform based compression is one of the methods that can be applied for compressing images. An ideal image compression system must yield good quality compressed images with good compression ratio, while maintaining minimal time cost. With Wavelet transform based compression, the quality of compressed images is usually high, and the choice of an ideal compression ratio is difficult to make as it varies depending on the content of the image. Therefore, it is of great advantage to have a system that can determine an optimum compression ratio upon presenting it with an image. We propose that neural networks can be trained to establish the non-linear relationship between the image intensity and its compression ratios in search for an optimum ratio. This paper suggests that a neural network could be trained to recognize an optimum ratio for Haar wavelet compression of an image upon presenting the image to the network. Two neural networks receiving different input image sizes are developed in this work and a comparison between their performances in finding optimum Haar-based compression is presented.",Iimcousneneanhawa,75.0,25.0,1.0
3477,Image Compression,89.0,sparse orthonormal transforms for image compression,4.0,201.0,1.0,150.0,3.0,2.5,152.10000000000002,79,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.5685&rep=rep1&type=pdf,"We propose a block-based transform optimization and associated image compression technique that exploits regularity along directional image singularities. Unlike established work, directionality comes about as a byproduct of the proposed optimization rather than a built in constraint. Our work classifies image blocks and uses transforms that are optimal for each class, thereby decomposing image information into classification and transform coefficient information. The transforms are optimized using a set of training images. Our algebraic framework allows straightforward extension to non-block transforms, allowing us to also design sparse lapped transforms that exploit geometric regularity. We use an EZW/SPIHT like entropy coder to encode the transform coefficients to show that our block and lapped designs have competitive rate-distortion performance. Our work can be seen as nonlinear approximation optimized transform coding of images subject to structural constraints on transform basis functions.",Isportrfoimco,68.0,16.0,9.0
3478,Image Compression,148.0,roi-based dicom image compression for telemedicine,3.0,201.0,1.0,94.0,4.0,2.5,153.0,80,https://www.ias.ac.in/article/fulltext/sadh/038/01/0123-0131,"Many classes of images contain spatial regions which are more important than other regions. Compression methods capable of delivering higher reconstruction quality for important parts are attractive in this situation. For medical images, only a small portion of the image might be diagnostically useful, but the cost of a wrong interpretation is high. Hence, Region Based Coding (RBC) technique is significant for medical image compression and transmission. Lossless compression schemes with secure transmission play a key role in telemedicine applications that help in accurate diagnosis and research. In this paper, we propose lossless scalable RBC for Digital Imaging and Communications in Medicine (DICOM) images based on Integer Wavelet Transform (IWT) and with distortion limiting compression technique for other regions in image. The main objective of this work is to reject the noisy background and reconstruct the image portions losslessly. The compressed image can be accessed and sent over telemedicine network using personal digital assistance (PDA) like mobile.",Irodiimcofote,50.0,16.0,1.0
3479,Image Compression,177.0,comparison of fractal coding methods for medical image compression,3.0,201.0,1.0,71.0,4.0,2.5,154.8,81,http://arxiv.org/abs/1707.03164v2,"In this study, the performance of fractal-based coding algorithms such as standard fractal coding, quasi-lossless fractal coding and improved quasi-lossless fractal coding are evaluated by investigating their ability to compress magnetic resonance images (MRIs) based on compression ratio, peak signal-to-noise ratio and encoding time. For this purpose, MRI head scan test sets of 512 × 512 pixels have been used. A novel quasi-lossless fractal coding scheme, which preserves important feature-rich portions of the medical image, such as domain blocks and generates the remaining part of the image from it, has been proposed using fractal transformations. One of the biggest tasks in fractal image compression is reduction of encoding computation time. A machine learning-based model is used for reducing the encoding time and also for improving the performance of the quasi-lossless fractal coding scheme. The results show a better performance of improved quasi-lossless fractal compression method. The quasi-lossless and improved quasi-lossless fractal coding algorithms are found to outperform standard fractal coding thereby proving the possibility of using fractal-based image compression algorithms for medical image compression. The proposed algorithm allows significant reduction of encoding time and also improvement in the compression ratio.",Icooffrcomefomeimco,35.0,14.0,0.0
3480,Image Compression,189.0,a novel fractal image compression scheme with block classification and sorting based on pearson's correlation coefficient,3.0,201.0,1.0,61.0,4.0,2.5,155.4,82,https://ieeexplore.ieee.org/iel7/83/4358840/06542017.pdf,"Fractal image compression (FIC) is an image coding technology based on the local similarity of image structure. It is widely used in many fields such as image retrieval, image denoising, image authentication, and encryption. FIC, however, suffers from the high computational complexity in encoding. Although many schemes are published to speed up encoding, they do not easily satisfy the encoding time or the reconstructed image quality requirements. In this paper, a new FIC scheme is proposed based on the fact that the affine similarity between two blocks in FIC is equivalent to the absolute value of Pearson's correlation coefficient (APCC) between them. First, all blocks in the range and domain pools are chosen and classified using an APCC-based block classification method to increase the matching probability. Second, by sorting the domain blocks with respect to APCCs between these domain blocks and a preset block in each class, the matching domain block for a range block can be searched in the selected domain set in which these APCCs are closer to APCC between the range block and the preset block. Experimental results show that the proposed scheme can significantly speed up the encoding process in FIC while preserving the reconstructed image quality well.",Ianofrimcoscwiblclansobaonpecoco,83.0,27.0,3.0
3481,Image Compression,67.0,region of interest coding techniques for medical image compression,4.0,201.0,1.0,184.0,3.0,2.5,155.7,83,https://www.researchgate.net/profile/Ilias-Maglogiannis/publication/3246417_Region_of_Interest_Coding_Techniques_for_Medical_Image_Compression/links/5bd2d8454585150b2b87d4ec/Region-of-Interest-Coding-Techniques-for-Medical-Image-Compression.pdf,"We have provided an overview of state-of-the-art ROI coding techniques applied to medical images. These techniques are classified according to the image type they apply to; thus the first class includes ROI coding schemes developed for two-dimensional (2-D) still medical images whereas the second class consists of ROI coding in the case of volumetric images. In the third class, a prototype ROI encoder for compression of angiogram video sequences is presented. ROI coding preserves image quality in diagnostically critical regions by performing advanced image compression, enabling better image examination and addressing issues regarding image handling and transmission in telemedicine systems. The mapping of the ROI from the spatial domain to the wavelet domain is dependent on the used wavelet filters and it is simplified for rectangular and circular regions. Therefore, ROI coding is considered quite important in distributed and networked electronic healthcare.",Ireofincotefomeimco,92.0,26.0,3.0
3482,Image Compression,180.0,a survey of image compression methods,3.0,201.0,1.0,80.0,4.0,2.5,158.4,84,https://www.researchgate.net/profile/Saravanan-Chandran/publication/266678305_A_Survey_of_Image_Compression_Methods/links/5437b2210cf2590375c53e20/A-Survey-of-Image-Compression-Methods.pdf,"research in the field of image compression is driven by the ever increasing bandwidth requirements for transmission of images in computer, mobile and internet environments. In this context, the survey summarizes the major image compression methods spanning across lossy and lossless image compression techniques and explains how the JPEG and JPEG2000 image compression techniques are distinct from each other. Further, the paper concludes that still research possibilities exist in this field to explore efficient image compression. General Terms Image compression, Huffman coding, low bit rate transmission.",Iasuofimcome,28.0,33.0,1.0
3483,Image Compression,1.0,image compression technique,5.0,201.0,1.0,201.0,1.0,2.2,141.0,85,http://arxiv.org/pdf/1410.2259v1,"Compression plays a significant role in a data storage and a transmission. If we speak about a generall data compression, it has to be a lossless one. It means, we are able to recover the original data 1:1 from the compressed file. Multimedia data (images, video, sound...), are a special case. In this area, we can use something called a lossy compression. Our main goal is not to recover data 1:1, but only keep them visually similar. This article is about an image compression, so we will be interested only in image compression. For a human eye, it is not a huge difference, if we recover RGB color with values [150,140,138] instead of original [151,140,137]. The magnitude of a difference determines the loss rate of the compression. The bigger difference usually means a smaller file, but also worse image quality and noticable differences from the original image. We want to cover compression techniques mainly from the last decade. Many of them are variations of existing ones, only some of them uses new principes.",Iimcote,24.0,33.0,0.0
3484,Image Compression,6.0,performance analysis of image compression using wavelets,5.0,201.0,1.0,201.0,1.0,2.2,142.5,86,https://www.academia.edu/download/53210242/41.92559620170520-19840-5ylfy5.pdf,"In this paper, we present a technique by which high-intensity feature vectors extracted from the Gabor wavelet transformation of frontal face images, is combined together with Independent Component Analysis (ICA) for enhanced face recognition. Firstly, the high-intensity feature vectors are automatically extracted using the local characteristics of each individual face from the Gabor transformed images. Then ICA is applied on these locally extracted high-intensity feature vectors of the facial images to obtain the independent high intensity feature (IHIF) vectors. These IHIF forms the basis of the work. Finally, the image classification is done using these IHIF vectors, which are considered as representatives of the images. The importance behind implementing ICA along with the high-intensity features of Gabor wavelet transformation is twofold. On the one hand, selecting peaks of the Gabor transformed face images exhibit strong characteristics of spatial locality, scale, and orientation selectivity. Thus these images produce salient local features that are most suitable for face recognition. On the other hand, as the ICA employs locally salient features from the high informative facial parts, it reduces redundancy and represents independent features explicitly. These independent features are most useful for subsequent facial discrimination and associative recall. The efficiency of IHIF method is demonstrated by the experiment on frontal facial images dataset, selected from the FERET, FRAV2D, and the ORL database.",Ipeanofimcouswa,355.0,40.0,13.0
3485,Image Compression,7.0,document and image compression,5.0,201.0,1.0,201.0,1.0,2.2,142.8,87,http://arxiv.org/pdf/1907.12219v1,"JPEG is one of the popular image compression algorithms that provide efficient storage and transmission capabilities in consumer electronics, and hence it is the most preferred image format over the internet world. In the present digital and Big-data era, a huge volume of JPEG compressed document images are being archived and communicated through consumer electronics on daily basis. Though it is advantageous to have data in the compressed form on one side, however, on the other side processing with off-the-shelf methods becomes computationally expensive because it requires decompression and recompression operations. Therefore, it would be novel and efficient, if the compressed data are processed directly in their respective compressed domains of consumer electronics. In the present research paper, we propose to demonstrate this idea taking the case study of printed text line segmentation. Since, JPEG achieves compression by dividing the image into non overlapping 8x8 blocks in the pixel domain and using Discrete Cosine Transform (DCT); it is very likely that the partitioned 8x8 DCT blocks overlap the contents of two adjacent text-lines without leaving any clue for the line separator, thus making text-line segmentation a challenging problem. Two approaches of segmentation have been proposed here using the DC projection profile and AC coefficients of each 8x8 DCT block. The first approach is based on the strategy of partial decompression of selected DCT blocks, and the second approach is with intelligent analysis of F10 and F11 AC coefficients and without using any type of decompression. The proposed methods have been tested with variable font sizes, font style and spacing between lines, and a good performance is reported.",Idoanimco,77.0,0.0,3.0
3487,Image Compression,13.0,wavelet-based image compression,5.0,201.0,1.0,201.0,1.0,2.2,144.60000000000002,88,http://www.dsp-book.narod.ru/TDCH/CH-06.PDF,"Due to the increasing requirements for transmission of images in computer, mobile environments, the research in the field of image compression has increased significantly. Image compression plays a crucial role in digital image processing, it is also very important for efficient transmission and storage of images. When we compute the number of bits per image resulting from typical sampling rates and quantization methods, we find that Image compression is needed. Therefore development of efficient techniques for image compression has become necessary .This paper is a survey for lossy image compression using Discrete Cosine Transform, it covers JPEG compression algorithm which is used for full-colour still image applications and describes all the components of it.",Iwaimco,10.0,32.0,0.0
3488,Image Compression,15.0,"line-based, reduced memory, wavelet image compression",5.0,201.0,1.0,201.0,1.0,2.2,145.2,89,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.9206&rep=rep1&type=pdf,"This paper proposes a joint framework wherein lifting-based, separable, image-matched wavelets are estimated from compressively sensed (CS) images and used for the reconstruction of the same. Matched wavelet can be easily designed if full image is available. Also matched wavelet may provide better reconstruction results in CS application compared to standard wavelet sparsifying basis. Since in CS application, we have compressively sensed image instead of full image, existing methods of designing matched wavelet cannot be used. Thus, we propose a joint framework that estimates matched wavelet from the compressively sensed images and also reconstructs full images. This paper has three significant contributions. First, lifting-based, image-matched separable wavelet is designed from compressively sensed images and is also used to reconstruct the same. Second, a simple sensing matrix is employed to sample data at sub-Nyquist rate such that sensing and reconstruction time is reduced considerably without any noticeable degradation in the reconstruction performance. Third, a new multi-level L-Pyramid wavelet decomposition strategy is provided for separable wavelet implementation on images that leads to improved reconstruction performance. Compared to CS-based reconstruction using standard wavelets with Gaussian sensing matrix and with existing wavelet decomposition strategy, the proposed methodology provides faster and better image reconstruction in compressive sensing application.",Iliremewaimco,241.0,20.0,11.0
3489,Image Compression,16.0,"jpeg2000 standard for image compression: concepts, algorithms and vlsi architectures",5.0,201.0,1.0,201.0,1.0,2.2,145.5,90,http://arxiv.org/abs/1702.01946v1,"For future solar missions as well as ground-based telescopes, efficient ways to return and process data have become increasingly important. Solar Orbiter, e.g., which is the next ESA/NASA mission to explore the Sun and the heliosphere, is a deep-space mission, which implies a limited telemetry rate that makes efficient onboard data compression a necessity to achieve the mission science goals. Missions like the Solar Dynamics Observatory (SDO) and future ground-based telescopes such as the Daniel K. Inouye Solar Telescope, on the other hand, face the challenge of making petabyte-sized solar data archives accessible to the solar community. New image compression standards address these challenges by implementing efficient and flexible compression algorithms that can be tailored to user requirements. We analyse solar images from the Atmospheric Imaging Assembly (AIA) instrument onboard SDO to study the effect of lossy JPEG2000 (from the Joint Photographic Experts Group 2000) image compression at different bit rates. To assess the quality of compressed images, we use the mean structural similarity (MSSIM) index as well as the widely used peak signal-to-noise ratio (PSNR) as metrics and compare the two in the context of solar EUV images. In addition, we perform tests to validate the scientific use of the lossily compressed images by analysing examples of an on-disk and off-limb coronal-loop oscillation time-series observed by AIA/SDO.",Ijpstfoimcocoalanvlar,382.0,0.0,22.0
3490,Image Compression,20.0,dct based high quality image compression,5.0,201.0,1.0,201.0,1.0,2.2,146.7,91,https://link.springer.com/content/pdf/10.1007/11499145_119.pdf,"This paper presents the performance of different blockbased discrete cosine transform (DCT) algorithms for compressing color image. In this RGB component of color image are converted to YCbCr before DCT transform is applied. Y is luminance component;Cb and Cr are chrominance components of the image. The modification of the image data is done based on the classification of image blocks to edge blocks and non-edge blocks, then the edge block of the image is compressed with low compression and the nonedge blocks is compressed with high compression. The analysis results have indicated that the performance of the suggested method is much better, where the constructed images are less distorted and compressed with higher factor.",Idcbahiquimco,105.0,13.0,1.0
3491,Image Compression,21.0,reversible integer-to-integer wavelet transforms for image compression: performance evaluation and analysis,5.0,201.0,1.0,201.0,1.0,2.2,147.0,92,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.6759&rep=rep1&type=pdf,"A method of lossless data hiding in images using integer wavelet transform and histogram shifting for gray scale images is proposed. The method shifts part of the histogram, to create space for embedding the watermark information bits. The method embeds watermark while maintaining the visual quality well. The method is completely reversible. The original image and the watermark data can be recovered without any loss.",Ireinwatrfoimcopeevanan,432.0,29.0,17.0
3492,Image Compression,22.0,a novel approach to medical image compression,5.0,201.0,1.0,201.0,1.0,2.2,147.3,93,https://vast.uccs.edu/~tboult/PAPERS/IJBRA-06-Zukowski-Boult-Medical-Image-Compression.pdf,"Mammography is currently the primary imaging modality for breast cancer screening and plays an important role in cancer diagnostics. A standard mammographic image acquisition always includes the compression of the breast prior x-ray exposure. The breast is compressed between two plates (the image receptor and the compression paddle) until a nearly uniform breast thickness is obtained. The breast flattening improves diagnostic image quality 1 and reduces the absorbed dose 2. However, this technique can also be a source of discomfort and might deter some women from attending breast screening by mammography 3,4. Therefore, the characterization of the pain perceived during breast compression is of potential interest to compare different compression approaches. The aim of this work is to develop simulation tools enabling the characterization of existing breast compression techniques in terms of patient comfort, dose delivered to the patient and resulting image quality. A 3D biomechanical model of the breast was developed providing physics-based predictions of tissue motion and internal stress and strain intensity. The internal stress and strain intensity are assumed to be directly correlated with the patient discomfort. The resulting compressed breast model is integrated in an image simulation framework to assess both image quality and average glandular dose. We present the results of compression simulations on two breast geometries, under different compression paddles (flex or rigid).",Ianoaptomeimco,68.0,28.0,2.0
3493,Image Compression,24.0,towards pde-based image compression,5.0,201.0,1.0,201.0,1.0,2.2,147.9,94,https://www.mia.uni-saarland.de/Publications/galic-vlsm05.pdf,"We present a neural video compression method based on generative adversarial networks (GANs) that outperforms previous neural video compression methods and is comparable to HEVC in a user study. We propose a technique to mitigate temporal error accumulation caused by recursive frame compression that uses randomized shifting and un-shifting, motivated by a spectral analysis. We present in detail the network design choices, their relative importance, and elaborate on the challenges of evaluating video compression methods in user studies.",Itopdimco,75.0,35.0,3.0
3494,Image Compression,25.0,adaptive downsampling to improve image compression at low bit rates,5.0,201.0,1.0,201.0,1.0,2.2,148.2,95,http://arxiv.org/pdf/2104.02322v1,"Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of H.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on a NVIDIA V100 GPU.",Iaddotoimimcoatlobira,104.0,18.0,6.0
3495,Image Compression,29.0,standard codecs: image compression to advanced video coding,5.0,201.0,1.0,201.0,1.0,2.2,149.4,96,http://arxiv.org/pdf/2105.09833v1,"In 2021, a new track has been initiated in the Challenge for Learned Image Compression~: the video track. This category proposes to explore technologies for the compression of short video clips at 1 Mbit/s. This paper proposes to generate coded videos using the latest standardized video coders, especially Versatile Video Coding (VVC). The objective is not only to measure the progress made by learning techniques compared to the state of the art video coders, but also to quantify their progress from years to years. With this in mind, this paper documents how to generate the video sequences fulfilling the requirements of this challenge, in a reproducible way, targeting the maximum performance for VVC.",Istcoimcotoadvico,388.0,17.0,29.0
3496,Image Compression,30.0,wavelet transform techniques for image compression-an evaluation,5.0,201.0,1.0,201.0,1.0,2.2,149.7,97,http://www.mecs-press.net/ijigsp/ijigsp-v6-n2/IJIGSP-V6-N2-7.pdf,"Spatial and spectral approaches are two major approaches for image processing tasks such as image classification and object recognition. Among many such algorithms, convolutional neural networks (CNNs) have recently achieved significant performance improvement in many challenging tasks. Since CNNs process images directly in the spatial domain, they are essentially spatial approaches. Given that spatial and spectral approaches are known to have different characteristics, it will be interesting to incorporate a spectral approach into CNNs. We propose a novel CNN architecture, wavelet CNNs, which combines a multiresolution analysis and CNNs into one model. Our insight is that a CNN can be viewed as a limited form of a multiresolution analysis. Based on this insight, we supplement missing parts of the multiresolution analysis via wavelet transform and integrate them as additional components in the entire architecture. Wavelet CNNs allow us to utilize spectral information which is mostly lost in conventional CNNs but useful in most image processing tasks. We evaluate the practical performance of wavelet CNNs on texture classification and image annotation. The experiments show that wavelet CNNs can achieve better accuracy in both tasks than existing models while having significantly fewer parameters than conventional CNNs.",Iwatrtefoimcoev,35.0,7.0,2.0
3497,Image Compression,31.0,a tutorial on modern lossy wavelet image compression: foundations of jpeg 2000,5.0,201.0,1.0,201.0,1.0,2.2,150.0,98,https://images-insite.sgp1.digitaloceanspaces.com/dunia_buku/koleksi-buku-lainnya/a-tutorial-on-modern-lossy-wavelet-image-compression-foundations-pdfdrivecom-96911582423395.pdf,"JPEG is one of the widely used lossy compression methods. JPEG-compressed images usually suffer from compression artifacts including blocking and blurring, especially at low bit-rates. Soft decoding is an effective solution to improve the quality of compressed images without changing codec or introducing extra coding bits. Inspired by the excellent performance of the deep convolutional neural networks (CNNs) on both low-level and high-level computer vision problems, we develop a dual pixel-wavelet domain deep CNNs-based soft decoding network for JPEG-compressed images, namely DPW-SDNet. The pixel domain deep network takes the four downsampled versions of the compressed image to form a 4-channel input and outputs a pixel domain prediction, while the wavelet domain deep network uses the 1-level discrete wavelet transformation (DWT) coefficients to form a 4-channel input to produce a DWT domain prediction. The pixel domain and wavelet domain estimates are combined to generate the final soft decoded result. Experimental results demonstrate the superiority of the proposed DPW-SDNet over several state-of-the-art compression artifacts reduction algorithms.",Iatuonmolowaimcofoofjp20,445.0,54.0,17.0
3498,Image Compression,34.0,image compression using coding of wavelet coefficients–a survey,5.0,201.0,1.0,201.0,1.0,2.2,150.9,99,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.1615&rep=rep1&type=pdf,"A variety of new and powerful algorithms have been developed for image compression over the years. Among them the wavelet-based image compression schemes have gained much popularity due to their overlapping nature which reduces the blocking artifacts that are common phenomena in JPEG compression and multiresolution character which leads to superior energy compaction with high quality reconstructed images. This paper provides a detailed survey on some of the popular wavelet coding techniques such as the Embedded Zerotree Wavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, the Set Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Coding with Optimized Truncation (EBCOT) algorithm. Other wavelet-based coding techniques like the Wavelet Difference Reduction (WDR) and the Adaptive Scanned Wavelet Difference Reduction (ASWDR) algorithms, the Space Frequency Quantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder (EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run (SR) coding and the recent Geometric Wavelet (GW) coding are also discussed. Based on the review, recommendations and discussions are presented for algorithm development and implementation.",Iimcouscoofwacosu,69.0,39.0,3.0
3499,Image Compression,40.0,digital image compression,5.0,201.0,1.0,201.0,1.0,2.2,152.7,100,"https://scholar.google.comftp://nozdr.ru/biblio/kolxoz/Cs/CsAl/Droste%20M.,%20Kuich%20W.,%20Vogler%20H.%20(eds.)%20Handbook%20of%20weighted%20automata%20(Springer,%202009)(ISBN%203642014917)(O)(610s)_CsAl_.pdf#page=457","We summarize the plans for and the current status of the Sloan Digital Sky Survey, a digital imaging and spectroscopic survey of $\pi$ steradians in the northern Galactic cap. The CCD photometric survey will produce images in five bands to limiting magnitudes of order 23. The spectroscopic survey will obtain redshifts of $10^6$ galaxies (a complete sample to a limiting magnitude $r' \sim 18$) and $10^5$ quasars ($g' \sim 19$). Repeated imaging of a 200 deg$^2$ strip in the southern Galactic cap will yield information about variable objects and a co-added photometric catalog roughly two magnitudes deeper than the northern survey. A dedicated 2.5-meter telescope, a large multi-CCD camera, and two fiber-fed double spectrographs are under construction and should be operational by fall of 1995. The main galaxy redshift sample will have a median redshift $\langle z \rangle \approx 0.1$.",Idiimco,8920.0,108.0,487.0
3793,Image Retrieval,43.0,learning with average precision: training image retrieval with a listwise loss,4.0,34.0,5.0,10.0,5.0,4.7,29.5,1,https://openaccess.thecvf.com/content_ICCV_2019/papers/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.pdf,"Image retrieval can be formulated as a ranking problem where the goal is to order database images by decreasing similarity to the query. Recent deep models for image retrieval have outperformed traditional methods by leveraging ranking-tailored loss functions, but important theoretical and practical problems remain. First, rather than directly optimizing the global ranking, they minimize an upper-bound on the essential loss, which does not necessarily result in an optimal mean average precision (mAP). Second, these methods require significant engineering efforts to work well, e.g., special pre-training and hard-negative mining. In this paper we propose instead to directly optimize the global mAP by leveraging recent advances in listwise loss formulations. Using a histogram binning approximation, the AP can be differentiated and thus employed to end-to-end learning. Compared to existing losses, the proposed method considers thousands of images simultaneously at each iteration and eliminates the need for ad hoc tricks. It also establishes a new state of the art on many standard retrieval benchmarks. Models and evaluation scripts have been made available at: https://europe.naverlabs.com/Deep-Image-Retrieval/.",Ilewiavprtrimrewialilo,98.0,63.0,15.0
3794,Image Retrieval,48.0,binary generative adversarial networks for image retrieval,4.0,138.0,3.0,21.0,5.0,3.9,75.89999999999999,2,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17150/15712,"The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107\% in terms of~mAP (See Table tab.res.map.comp) Our anonymous code is available at: this https URL.",Ibigeadnefoimre,111.0,60.0,11.0
3795,Image Retrieval,401.0,large-scale image retrieval with attentive deep local features,1.0,5.0,5.0,7.0,5.0,3.8,124.4,3,http://arxiv.org/pdf/1801.06267v1,"We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELE (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for key point selection, which shares most network layers with the descriptor. This frame-work can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives–in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELE outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins.",Ilaimrewiatdelofe,387.0,52.0,68.0
3796,Image Retrieval,401.0,deep image retrieval: learning global representations for image search,1.0,6.0,5.0,11.0,5.0,3.8,126.0,4,http://arxiv.org/pdf/1604.01325v2,"We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we leverage a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we use a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. Additional material is available at www.xrce.xerox.com/Deep-Image-Retrieval.",Ideimreleglrefoimse,596.0,68.0,67.0
3797,Image Retrieval,401.0,revisiting oxford and paris: large-scale image retrieval benchmarking,1.0,4.0,5.0,14.0,5.0,3.8,126.1,5,http://arxiv.org/pdf/1803.11285v1,"In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected. An extensive1 comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.",Ireoxanpalaimrebe,151.0,56.0,33.0
3798,Image Retrieval,401.0,fine-tuning cnn image retrieval with no human annotation,1.0,19.0,5.0,1.0,5.0,3.8,128.2,6,http://arxiv.org/pdf/2101.07945v1,"Image descriptors based on activations of Convolutional Neural Networks (CNNs) have become dominant in image retrieval due to their discriminative power, compactness of representation, and search efficiency. Training of CNNs, either from scratch or fine-tuning, requires a large amount of annotated data, where a high quality of annotation is often crucial. In this work, we propose to fine-tune CNNs for image retrieval on a large collection of unordered images in a fully automated manner. Reconstructed 3D models obtained by the state-of-the-art retrieval and structure-from-motion methods guide the selection of the training data. We show that both hard-positive and hard-negative examples, selected by exploiting the geometry and the camera positions available from the 3D models, enhance the performance of particular-object retrieval. CNN descriptor whitening discriminatively learned from the same training data outperforms commonly used PCA whitening. We propose a novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and average pooling and show that it boosts retrieval performance. Applying the proposed method to the VGG network achieves state-of-the-art performance on the standard benchmarks: Oxford Buildings, Paris, and Holidays datasets.",Ificnimrewinohuan,434.0,72.0,81.0
3799,Image Retrieval,401.0,cnn image retrieval learns from bow: unsupervised fine-tuning with hard examples,1.0,20.0,5.0,13.0,5.0,3.8,132.20000000000002,7,http://arxiv.org/pdf/1711.02512v2,"Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.",Icnimrelefrbounfiwihaex,460.0,59.0,53.0
3800,Image Retrieval,401.0,end-to-end learning of deep visual representations for image retrieval,1.0,35.0,5.0,8.0,5.0,3.8,136.70000000000002,8,http://arxiv.org/pdf/1607.08368v1,"While deep learning has become a key ingredient in the top performing methods for many computer vision tasks, it has failed so far to bring similar improvements to instance-level image retrieval. In this article, we argue that reasons for the underwhelming results of deep methods on image retrieval are threefold: (1) noisy training data, (2) inappropriate deep architecture, and (3) suboptimal training procedure. We address all three issues. First, we leverage a large-scale but noisy landmark dataset and develop an automatic cleaning method that produces a suitable training set for deep retrieval. Second, we build on the recent R-MAC descriptor, show that it can be interpreted as a deep and differentiable architecture, and present improvements to enhance it. Last, we train this network with a siamese architecture that combines three streams with a triplet loss. At the end of the training process, the proposed architecture produces a global image representation in a single forward pass that is well suited for image retrieval. Extensive experiments show that our approach significantly outperforms previous retrieval approaches, including state-of-the-art methods based on costly local descriptor indexing and spatial verification. On Oxford 5k, Paris 6k and Holidays, we respectively report 94.7, 96.6, and 94.8 mean average precision. Our representations can also be heavily compressed using product quantization with little loss in accuracy.",Ienleofdevirefoimre,340.0,90.0,52.0
3801,Image Retrieval,401.0,composing text and image for image retrieval - an empirical odyssey,1.0,52.0,4.0,4.0,5.0,3.4000000000000004,142.29999999999998,9,http://arxiv.org/pdf/2102.09249v1,"In this paper, we study the task of image retrieval, where the input query is specified in the form of an image plus some text that describes desired modifications to the input image. For example, we may present an image of the Eiffel tower, and ask the system to find images which are visually similar, but are modified in small ways, such as being taken at nighttime instead of during the day. o tackle this task, we embed the query (reference image plus modification text) and the target (images). The encoding function of the image text query learns a representation, such that the similarity with the target image representation is high iff it is a ``positive match''. We propose a new way to combine image and text through residual connection, that is designed for this retrieval task. We show this outperforms existing approaches on 3 different datasets, namely Fashion-200k, MIT-States and a new synthetic dataset we create based on CLEVR. We also show that our approach can be used to perform image classification with compositionally novel labels, and we outperform previous methods on MIT-States on this task.",Icoteanimfoimre-anemod,82.0,57.0,29.0
3802,Image Retrieval,401.0,deep sketch hashing: fast free-hand sketch-based image retrieval,1.0,79.0,4.0,16.0,5.0,3.4000000000000004,156.70000000000002,10,http://arxiv.org/pdf/1804.04804v1,"Free-hand sketch-based image retrieval (SBIR) is a specific cross-view retrieval task, in which queries are abstract and ambiguous sketches while the retrieval database is formed with natural images. Work in this area mainly focuses on extracting representative and shared features for sketches and natural images. However, these can neither cope well with the geometric distortion between sketches and images nor be feasible for large-scale SBIR due to the heavy continuous-valued distance computation. In this paper, we speed up SBIR by introducing a novel binary coding method, named Deep Sketch Hashing (DSH), where a semi-heterogeneous deep architecture is proposed and incorporated into an end-to-end binary coding framework. Specifically, three convolutional neural networks are utilized to encode free-hand sketches, natural images and, especially, the auxiliary sketch-tokens which are adopted as bridges to mitigate the sketch-image geometric distortion. The learned DSH codes can effectively capture the cross-view similarities as well as the intrinsic semantic correlations between different categories. To the best of our knowledge, DSH is the first hashing work specifically designed for category-level SBIR with an end-to-end deep architecture. The proposed DSH is comprehensively evaluated on two large-scale datasets of TU-Berlin Extension and Sketchy, and the experiments consistently show DSHs superior SBIR accuracies over several state-of-the-art methods, while achieving significantly reduced retrieval time and memory footprint.",Ideskhafafrskimre,158.0,83.0,29.0
3803,Image Retrieval,1.0,"image retrieval: ideas, influences, and trends of the new age",5.0,201.0,1.0,9.0,5.0,3.4,83.4,11,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.9078&rep=rep1&type=pdf,"We have witnessed great interest and a wealth of promise in content-based image retrieval as an emerging technology. While the last decade laid foundation to such promise, it also paved the way for a large number of new techniques and systems, got many new people involved, and triggered stronger association of weakly related fields. In this article, we survey almost 300 key theoretical and empirical contributions in the current decade related to image retrieval and automatic image annotation, and in the process discuss the spawning of related subfields. We also discuss significant challenges involved in the adaptation of existing image retrieval techniques to build systems that can be useful in the real world. In retrospect of what has been achieved so far, we also conjecture what the future may hold for image retrieval research.",Iimreidinantrofthneag,3580.0,324.0,196.0
3804,Image Retrieval,8.0,content-based image retrieval at the end of the early years,5.0,201.0,1.0,6.0,5.0,3.4,84.60000000000001,12,http://arxiv.org/pdf/1610.02984v1,"Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.",Icoimreatthenoftheaye,6594.0,434.0,347.0
3805,Image Retrieval,24.0,the earth mover's distance as a metric for image retrieval,5.0,201.0,1.0,5.0,5.0,3.4,89.10000000000001,13,http://robotics.stanford.edu/~rubner/papers/rubnerIjcv00.pdf,"We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one distribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length representations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the EMD is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the EMD with that of other distances.",Itheamodiasamefoimre,3913.0,58.0,504.0
3806,Image Retrieval,11.0,neural codes for image retrieval,5.0,201.0,1.0,31.0,5.0,3.4,93.0,14,https://link.springer.com/content/pdf/10.1007/978-3-319-10590-1_38.pdf,"This seminar report focuses on using convolutional neural networks for image retrieval. Firstly, we give a thorough discussion of several state-of-the-art techniques in image retrieval by considering the associated subproblems: image description, descriptor compression, nearest-neighbor search and query expansion. We discuss both the aggregation of local descriptors using clustering and metric learning techniques as well as global descriptors. Subsequently, we briefly introduce the basic concepts of deep convolutional neural networks, focusing on the architecture proposed by Krizhevsky et al. [KSH12]. We discuss different types of layers commonly used in recent architectures, for example convolutional layers, non-linearity and rectification layers, pooling layers as well as local contrast normalization layers. Finally, we shortly review supervised training techniques based on stochastic gradient descent and regularization techniques such as dropout and weight decay. Finally, following Babenko et al. [BSCL14], we discuss the use of feature activations in intermediate layers as image representation for image retrieval. After presenting experiments and comparing convolutional neural networks for image retrieval with other state-of-the-art techniques, we conclude by motivating the combined use of deep architectures and hand-crafted image representations for accurate and efficient image retrieval.",Inecofoimre,454.0,44.0,52.0
3807,Image Retrieval,401.0,particular object retrieval with integral max-pooling of cnn activations,1.0,36.0,5.0,198.0,3.0,3.2,194.1,15,http://arxiv.org/pdf/1511.05879v2,"Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.",Ipaobrewiinmaofcnac,699.0,55.0,150.0
3808,Image Retrieval,13.0,deep learning for content-based image retrieval: a comprehensive study,5.0,201.0,1.0,42.0,4.0,3.1,96.9,16,https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=3320&context=sis_research,"Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.",Idelefocoimreacost,683.0,62.0,35.0
3809,Image Retrieval,45.0,supervised hashing for image retrieval via image representation learning,4.0,201.0,1.0,20.0,5.0,3.1,99.9,17,https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/viewPDFInterstitial/8137/8861,"Hashing is a popular approximate nearest neighbor search approach for large-scale image retrieval. Supervised hashing, which incorporates similarity/ dissimilarity information on entity pairs to improve the quality of hashing function learning, has recently received increasing attention. However, in the existing supervised hashing methods for images, an input image is usually encoded by a vector of handcrafted visual features. Such hand-crafted feature vectors do not necessarily preserve the accurate semantic similarities of images pairs, which may often degrade the performance of hashing function learning. In this paper, we propose a supervised hashing method for image retrieval, in which we automatically learn a good image representation tailored to hashing as well as a set of hash functions. The proposed method has two stages. In the first stage, given the pairwise similarity matrix S over training images, we propose a scalable coordinate descent method to decompose S into a product of HHT where H is a matrix with each of its rows being the approximate hash code associated to a training image. In the second stage, we propose to simultaneously learn a good feature representation for the input images as well as a set of hash functions, via a deep convolutional network tailored to the learned hash codes in H and optionally the discrete class labels of the images. Extensive empirical evaluations on three benchmark datasets with different kinds of images show that the proposed method has superior performance gains over several state-of-the-art supervised and unsupervised hashing methods.",Isuhafoimreviimrele,680.0,24.0,143.0
3810,Image Retrieval,51.0,iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval,4.0,201.0,1.0,15.0,5.0,3.1,100.2,18,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.402.4453&rep=rep1&type=pdf,"This paper addresses the problem of learning similarity-preserving binary codes for efficient similarity search in large-scale image collections. We formulate this problem in terms of finding a rotation of zero-centered data so as to minimize the quantization error of mapping this data to the vertices of a zero-centered binary hypercube, and propose a simple and efficient alternating minimization algorithm to accomplish this task. This algorithm, dubbed iterative quantization (ITQ), has connections to multiclass spectral clustering and to the orthogonal Procrustes problem, and it can be used both with unsupervised data embeddings such as PCA and supervised embeddings such as canonical correlation analysis (CCA). The resulting binary codes significantly outperform several other state-of-the-art methods. We also show that further performance improvements can result from transforming the data with a nonlinear kernel mapping prior to PCA or CCA. Finally, we demonstrate an application of ITQ to learning binary attributes or ""classemes"" on the ImageNet data set.",Iitquapraptolebicofolaimre,1256.0,67.0,343.0
3811,Image Retrieval,47.0,deep convolutional learning for content based image retrieval,4.0,201.0,1.0,24.0,5.0,3.1,101.7,19,http://faratarjome.ir/u/media/shopping_files/store-EN-1520504599-9025.pdf,"In this paper we propose a model retraining method for learning more efficient convolutional representations for Content Based Image Retrieval. We employ a deep CNN model to obtain the feature representations from the activations of the convolutional layers using max-pooling, and subsequently we adapt and retrain the network, in order to produce more efficient compact image descriptors, which improve both the retrieval performance and the memory requirements, relying on the available information. Our method suggests three basic model retraining approaches. That is, the Fully Unsupervised Retraining, if no information except from the dataset itself is available, the Retraining with Relevance Information, if the labels of the training dataset are available, and the Relevance Feedback based Retraining, if feedback from users is available. The experimental evaluation on three publicly available image retrieval datasets indicates the effectiveness of the proposed method in learning more efficient representations for the retrieval task, outperforming other CNN-based retrieval techniques, as well as conventional hand-crafted feature-based approaches in all the used datasets.",Idecolefocobaimre,107.0,59.0,1.0
3812,Image Retrieval,9.0,a survey of content-based image retrieval with high-level semantics,5.0,201.0,1.0,97.0,4.0,3.1,112.2,20,http://www.baskent.edu.tr/~hogul/RA1.pdf,"In order to improve the retrieval accuracy of content-based image retrieval systems, research focus has been shifted from designing sophisticated low-level feature extraction algorithms to reducing the 'semantic gap' between the visual features and the richness of human semantics. This paper attempts to provide a comprehensive survey of the recent technical achievements in high-level semantic-based image retrieval. Major recent publications are included in this survey covering different aspects of the research in this area, including low-level image feature extraction, similarity measurement, and deriving high-level semantic features. We identify five major categories of the state-of-the-art techniques in narrowing down the 'semantic gap': (1) using object ontology to define high-level concepts; (2) using machine learning methods to associate low-level features with query concepts; (3) using relevance feedback to learn users' intention; (4) generating semantic template to support high-level image retrieval; (5) fusing the evidences from HTML text and the visual content of images for WWW image retrieval. In addition, some other related issues such as image test bed and retrieval performance evaluation are also discussed. Finally, based on existing technology and the demand from real-world applications, a few promising future research directions are suggested.",Iasuofcoimrewihise,1746.0,228.0,67.0
3813,Image Retrieval,401.0,dialog-based interactive image retrieval,1.0,115.0,3.0,25.0,5.0,3.0,173.8,21,http://arxiv.org/pdf/1911.03826v1,"Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",Idiinimre,81.0,66.0,17.0
3814,Image Retrieval,69.0,deep multi-view enhancement hashing for image retrieval,4.0,201.0,1.0,43.0,4.0,2.8,114.0,22,https://arxiv.org/pdf/2002.00169,"Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However, large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore, we try to introduce the multi-view deep neural network into the hash learning field, and design an efficient and innovative retrieval model, which has achieved a significant improvement in retrieval performance. In this paper, we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views, which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval, we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods.",Idemuenhafoimre,120.0,54.0,0.0
3815,Image Retrieval,52.0,local tetra patterns: a new feature descriptor for content-based image retrieval,4.0,201.0,1.0,62.0,4.0,2.8,114.6,23,https://www.researchgate.net/profile/Subrahmanyam-Murala/publication/224708699_Local_Tetra_Patterns_A_New_Feature_Descriptor_for_Content-Based_Image_Retrieval/links/02e7e5316529d07dd2000000/Local-Tetra-Patterns-A-New-Feature-Descriptor-for-Content-Based-Image-Retrieval.pdf,"In this paper, we propose a novel image indexing and retrieval algorithm using local tetra patterns (LTrPs) for content-based image retrieval (CBIR). The standard local binary pattern (LBP) and local ternary pattern (LTP) encode the relationship between the referenced pixel and its surrounding neighbors by computing gray-level difference. The proposed method encodes the relationship between the referenced pixel and its neighbors, based on the directions that are calculated using the first-order derivatives in vertical and horizontal directions. In addition, we propose a generic strategy to compute nth-order LTrP using (n - 1)th-order horizontal and vertical derivatives for efficient CBIR and analyze the effectiveness of our proposed algorithm by combining it with the Gabor transform. The performance of the proposed method is compared with the LBP, the local derivative patterns, and the LTP based on the results obtained using benchmark image databases viz., Corel 1000 database (DB1), Brodatz texture database (DB2), and MIT VisTex database (DB3). Performance analysis shows that the proposed method improves the retrieval result from 70.34%/44.9% to 75.9%/48.7% in terms of average precision/average recall on database DB1, and from 79.97% to 85.30% and 82.23% to 90.02% in terms of average retrieval rate on databases DB2 and DB3, respectively, as compared with the standard LBP.",Ilotepaanefedefocoimre,585.0,42.0,51.0
3816,Image Retrieval,102.0,solar: second-order loss and attention for image retrieval,3.0,72.0,4.0,201.0,1.0,2.8,119.7,24,https://arxiv.org/pdf/2001.08972,"This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.",Isoseloanatfoimre,22.0,71.0,2.0
3817,Image Retrieval,139.0,hierarchy-based image embeddings for semantic image retrieval,3.0,47.0,4.0,201.0,1.0,2.8,120.8,25,https://arxiv.org/pdf/1809.09924,"Deep neural networks trained for classification have been found to learn powerful image representations, which are also often used for other tasks such as comparing images w.r.t. their visual similarity. However, visual similarity does not imply semantic similarity. In order to learn semantically discriminative features, we propose to map images onto class embeddings whose pair-wise dot products correspond to a measure of semantic similarity between classes. Such an embedding does not only improve image retrieval results, but could also facilitate integrating semantics for other tasks, e.g., novelty detection or few-shot learning. We introduce a deterministic algorithm for computing the class centroids directly based on prior world-knowledge encoded in a hierarchy of classes such as WordNet. Experiments on CIFAR-100, NABirds, and ImageNet show that our learned semantic image embeddings improve the semantic consistency of image retrieval results by a large margin.",Ihiimemfoseimre,40.0,39.0,7.0
3819,Image Retrieval,26.0,content based image retrieval: survey,5.0,201.0,1.0,120.0,3.0,2.8,124.2,26,https://www.academia.edu/download/45970442/Content_Based_Image_Retrieval_Survey20160526-29818-1llo2j3.pdf,"The requirement for development of CBIR is enhanced due to tremendous growth in volume of images as well as the widespread application in multiple fields. Texture, color, shape and spatial layout are the underlying traits to represent and index the images. These peculiar features of images are extracted and implemented for a similarity check among images. The problem of content based image retrieval is based on generation of peculiar query. For relevant images that meet their information need, an automated search is initiated by drawing a sketch or with the submission of image having similar features. Similarity between extracted features can be measured by using different algorithms. The use of relevance feedback as a post retrieval step enhances the optimization of the process. The necessity to explore the ever growing volume of image and video is motivating the development of efficient CBIR algorithms. Different algorithms and models for the retrieval of images have been explored over the last twenty years. In this paper an analysis of visual contents of image is done with respect to features related to low level after extracting from image that are color, texture and shape. Here most popular algorithms of feature extraction and relevance feedback that try to bridge extracted low level features and features with high level semantics gap from image are discussed. A brief overview of an algorithm has been presented which is based on fuzzy logic and is used for the selection of peculiar features.",Icobaimresu,91.0,91.0,2.0
3820,Image Retrieval,38.0,medical image retrieval: past and present,5.0,201.0,1.0,128.0,3.0,2.8,130.20000000000002,27,http://arxiv.org/pdf/1802.07710v1,"With the widespread dissemination of picture archiving and communication systems (PACSs) in hospitals, the amount of imaging data is rapidly increasing. Effective image retrieval systems are required to manage these complex and large image databases. The authors reviewed the past development and the present state of medical image retrieval systems including text-based and content-based systems. In order to provide a more effective image retrieval service, the intelligent content-based retrieval systems combined with semantic systems are required.",Imeimrepaanpr,63.0,37.0,3.0
3821,Image Retrieval,37.0,large-scale image retrieval with compressed fisher vectors,5.0,201.0,1.0,139.0,3.0,2.8,133.2,28,https://www.academia.edu/download/30604790/Perronin.pdf,"The problem of large-scale image search has been traditionally addressed with the bag-of-visual-words (BOV). In this article, we propose to use as an alternative the Fisher kernel framework. We first show why the Fisher representation is well-suited to the retrieval problem: it describes an image by what makes it different from other images. One drawback of the Fisher vector is that it is high-dimensional and, as opposed to the BOV, it is dense. The resulting memory and computational costs do not make Fisher vectors directly amenable to large-scale retrieval. Therefore, we compress Fisher vectors to reduce their memory footprint and speed-up the retrieval. We compare three binarization approaches: a simple approach devised for this representation and two standard compression techniques. We show on two publicly available datasets that compressed Fisher vectors perform very well using as little as a few hundreds of bits per image, and significantly better than a very recent compressed BOV approach.",Ilaimrewicofive,736.0,29.0,84.0
3822,Image Retrieval,12.0,support vector machine active learning for image retrieval,5.0,201.0,1.0,167.0,3.0,2.8,134.1,29,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.2301&rep=rep1&type=pdf,"Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.",Isuvemaaclefoimre,1505.0,101.0,142.0
3823,Image Retrieval,34.0,content based image retrieval with lire,5.0,201.0,1.0,162.0,3.0,2.8,139.20000000000002,30,http://arxiv.org/pdf/1608.03811v1,"LIRe (Lucene Image Retrieval) is an open source library for content based image retrieval. Besides providing multiple common and state of the art retrieval mechanisms it allows for easy use on multiple platforms. LIRe is actively used for research, teaching and commercial applications. Due to its modular nature it can be used on process level (e.g. index images and search) as well as on image feature level. Developers and researchers can easily extend and modify LIRe to adapt it to their needs.",Icobaimrewili,55.0,26.0,2.0
3824,Image Retrieval,193.0,exploiting deep features for remote sensing image retrieval: a systematic investigation,3.0,201.0,1.0,3.0,5.0,2.8,139.20000000000002,31,https://arxiv.org/pdf/1707.07321,"Remote sensing (RS) image retrieval is of great significant for geological information mining. Over the past two decades, a large amount of research on this task has been carried out, which mainly focuses on the following three core issues: feature extraction, similarity metric, and relevance feedback. Due to the complexity and multiformity of ground objects in high-resolution remote sensing (HRRS) images, there is still room for improvement in the current retrieval approaches. In this article, we analyze the three core issues of RS image retrieval and provide a comprehensive review on existing methods. Furthermore, for the goal to advance the state-of-the-art in HRRS image retrieval, we focus on the feature extraction issue and delve how to use powerful deep representations to address this task. We conduct systematic investigation on evaluating correlative factors that may affect the performance of deep features. By optimizing each factor, we acquire remarkable retrieval results on publicly available HRRS datasets. Finally, we explain the experimental phenomenon in detail and draw conclusions according to our analysis. Our work can serve as a guiding role for the research of content-based RS image retrieval.",Iexdefeforeseimreasyin,65.0,174.0,6.0
3825,Image Retrieval,185.0,latent semantic minimal hashing for image retrieval,3.0,201.0,1.0,39.0,5.0,2.8,147.6,32,http://arxiv.org/pdf/2103.12328v1,"Hashing-based similarity search is an important technique for large-scale query-by-example image retrieval system, since it provides fast search with computation and memory efficiency. However, it is a challenge work to design compact codes to represent original features with good performance. Recently, a lot of unsupervised hashing methods have been proposed to focus on preserving geometric structure similarity of the data in the original feature space, but they have not yet fully refined image features and explored the latent semantic feature embedding in the data simultaneously. To address the problem, in this paper, a novel joint binary codes learning method is proposed to combine image feature to latent semantic feature with minimum encoding loss, which is referred as latent semantic minimal hashing. The latent semantic feature is learned based on matrix decomposition to refine original feature, thereby it makes the learned feature more discriminative. Moreover, a minimum encoding loss is combined with latent semantic feature learning process simultaneously, so as to guarantee the obtained binary codes are discriminative as well. Extensive experiments on several well-known large databases demonstrate that the proposed method outperforms most state-of-the-art hashing methods.",Ilasemihafoimre,100.0,60.0,4.0
3826,Image Retrieval,401.0,natural language object retrieval,1.0,76.0,4.0,200.0,3.0,2.8,210.7,33,http://arxiv.org/pdf/1511.04164v3,"In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",Inalaobre,365.0,35.0,52.0
3827,Image Retrieval,73.0,guided similarity separation for image retrieval,4.0,109.0,3.0,201.0,1.0,2.7,125.8,34,http://papers.nips.cc/paper/8434-guided-similarity-separation-for-image-retrieval.pdf,"Face image retrieval, which searches for images of the same identity from the query input face image, is drawing more attention as the size of the image database increases rapidly. In order to conduct fast and accurate retrieval, a compact hash code-based methods have been proposed, and recently, deep face image hashing methods with supervised classification training have shown outstanding performance. However, classification-based scheme has a disadvantage in that it cannot reveal complex similarities between face images into the hash code learning. In this paper, we attempt to improve the face image retrieval quality by proposing a Similarity Guided Hashing (SGH) method, which gently considers self and pairwise-similarity simultaneously. SGH employs various data augmentations designed to explore elaborate similarities between face images, solving both intra and inter identity-wise difficulties. Extensive experimental results on the protocols with existing benchmarks and an additionally proposed large scale higher resolution face image dataset demonstrate that our SGH delivers state-of-the-art retrieval performance.",Igusisefoimre,14.0,49.0,3.0
3828,Image Retrieval,401.0,unifying deep local and global features for image search,1.0,2.0,5.0,201.0,1.0,2.6,181.4,35,http://arxiv.org/pdf/2001.05027v4,"Image retrieval is the problem of searching an image database for items that are similar to a query image. To address this task, two main types of image representations have been studied: global and local image features. In this work, our key contribution is to unify global and local features into a single deep model, enabling accurate retrieval with efficient feature extraction. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads -- requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the Revisited Oxford and Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at https://github.com/tensorflow/models/tree/master/research/delf .",Iundeloanglfefoimse,42.0,68.0,9.0
3829,Image Retrieval,401.0,classification is a strong baseline for deep metric learning,1.0,8.0,5.0,201.0,1.0,2.6,183.8,36,http://arxiv.org/pdf/1909.02729v5,"Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the ""hardness"" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.",Iclisastbafodemele,80.0,38.0,15.0
3830,Image Retrieval,401.0,improving the hardnet descriptor,1.0,9.0,5.0,201.0,1.0,2.6,184.2,37,http://arxiv.org/pdf/2007.09699v2,"In the thesis we consider the problem of local feature descriptor learning for wide baseline stereo focusing on the HardNet descriptor, which is close to state-of-the-art. AMOS Patches dataset is introduced, which improves robustness to illumination and appearance changes. It is based on registered images from selected cameras from the AMOS dataset. We provide recommendations on the patch dataset creation process and evaluate HardNet trained on data of different modalities. We also introduce a dataset combination and reduction methods, that allow comparable performance on a significantly smaller dataset.   HardNet8, consistently outperforming the original HardNet, benefits from the architectural choices made: connectivity pattern, final pooling, receptive field, CNN building blocks found by manual or automatic search algorithms -- DARTS. We show impact of overlooked hyperparameters such as batch size and length of training on the descriptor quality. PCA dimensionality reduction further boosts performance and also reduces memory footprint.   Finally, the insights gained lead to two HardNet8 descriptors: one performing well on a variety of benchmarks -- HPatches, AMOS Patches and IMW Phototourism, the other is optimized for IMW Phototourism.",Iimthhade,3.0,73.0,0.0
3831,Image Retrieval,401.0,repeatability is not enough: learning affine regions via discriminability,1.0,11.0,5.0,201.0,1.0,2.6,185.0,38,http://arxiv.org/pdf/1711.06704v4,"A method for learning local affine-covariant regions is presented. We show that maximizing geometric repeatability does not lead to local regions, a.k.a features,that are reliably matched and this necessitates descriptor-based learning. We explore factors that influence such learning and registration: the loss function, descriptor type, geometric parametrization and the trade-off between matchability and geometric accuracy and propose a novel hard negative-constant loss function for learning of affine regions. The affine shape estimator -- AffNet -- trained with the hard negative-constant loss outperforms the state-of-the-art in bag-of-words image retrieval and wide baseline stereo. The proposed training process does not require precisely geometrically aligned patches.The source codes and trained weights are available at https://github.com/ducha-aiki/affnet",Ireisnoenleafrevidi,86.0,64.0,8.0
3832,Image Retrieval,401.0,working hard to know your neighbor's margins: local descriptor learning loss,1.0,12.0,5.0,201.0,1.0,2.6,185.4,39,http://arxiv.org/pdf/1705.10872v4,"We introduce a novel loss for learning local feature descriptors which is inspired by the Lowe's matching criterion for SIFT. We show that the proposed loss that maximizes the distance between the closest positive and closest negative patch in the batch is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor -- it has the same dimensionality as SIFT (128) that shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks. It is fast, computing a descriptor takes about 1 millisecond on a low-end GPU.",Iwohatoknyonemalodelelo,277.0,62.0,72.0
3833,Image Retrieval,401.0,circle loss: a unified perspective of pair similarity optimization,1.0,13.0,5.0,201.0,1.0,2.6,185.8,40,http://arxiv.org/pdf/2002.10857v2,"This paper provides a pair similarity optimization viewpoint on deep feature learning, aiming to maximize the within-class similarity $s_p$ and minimize the between-class similarity $s_n$. We find a majority of loss functions, including the triplet loss and the softmax plus cross-entropy loss, embed $s_n$ and $s_p$ into similarity pairs and seek to reduce $(s_n-s_p)$. Such an optimization manner is inflexible, because the penalty strength on every single similarity score is restricted to be equal. Our intuition is that if a similarity score deviates far from the optimum, it should be emphasized. To this end, we simply re-weight each similarity to highlight the less-optimized similarity scores. It results in a Circle loss, which is named due to its circular decision boundary. The Circle loss has a unified formula for two elemental deep feature learning approaches, i.e. learning with class-level labels and pair-wise labels. Analytically, we show that the Circle loss offers a more flexible optimization approach towards a more definite convergence target, compared with the loss functions optimizing $(s_n-s_p)$. Experimentally, we demonstrate the superiority of the Circle loss on a variety of deep feature learning tasks. On face recognition, person re-identification, as well as several fine-grained image retrieval datasets, the achieved performance is on par with the state of the art.",Iciloaunpeofpasiop,174.0,42.0,20.0
3834,Image Retrieval,401.0,improving zero-shot learning by mitigating the hubness problem,1.0,14.0,5.0,201.0,1.0,2.6,186.2,41,http://arxiv.org/pdf/1911.10097v1,"The hubness problem widely exists in high-dimensional embedding space and is a fundamental source of error for cross-modal matching tasks. In this work, we study the emergence of hubs in Visual Semantic Embeddings (VSE) with application to text-image matching. We analyze the pros and cons of two widely adopted optimization objectives for training VSE and propose a novel hubness-aware loss function (HAL) that addresses previous methods' defects. Unlike (Faghri et al.2018) which simply takes the hardest sample within a mini-batch, HAL takes all samples into account, using both local and global statistics to scale up the weights of ""hubs"". We experiment our method with various configurations of model architectures and datasets. The method exhibits exceptionally good robustness and brings consistent improvement on the task of text-image matching across all settings. Specifically, under the same model architectures as (Faghri et al. 2018) and (Lee at al. 2018), by switching only the learning objective, we report a maximum R@1improvement of 7.4% on MS-COCO and 8.3% on Flickr30k.",Iimzelebymithhupr,286.0,31.0,50.0
3835,Image Retrieval,401.0,the design and implementation of a real time visual search system on jd e-commerce platform,1.0,18.0,5.0,201.0,1.0,2.6,187.8,42,http://arxiv.org/pdf/1908.07389v1,"We present the design and implementation of a visual search system for real time image retrieval on JD.com, the world's third largest and China's largest e-commerce site. We demonstrate that our system can support real time visual search with hundreds of billions of product images at sub-second timescales and handle frequent image updates through distributed hierarchical architecture and efficient indexing methods. We hope that sharing our practice with our real production system will inspire the middleware community's interest and appreciation for building practical large scale systems for emerging applications, such as ecommerce visual search.",Ithdeanimofaretivisesyonjde-pl,8.0,36.0,1.0
3836,Image Retrieval,401.0,learning deep representations of fine-grained visual descriptions,1.0,23.0,5.0,201.0,1.0,2.6,189.8,43,http://arxiv.org/pdf/1611.05088v4,"Zero-shot learning (ZSL) models rely on learning a joint embedding space where both textual/semantic description of object classes and visual representation of object images can be projected to for nearest neighbour search. Despite the success of deep neural networks that learn an end-to-end model between text and images in other vision problems such as image captioning, very few deep ZSL model exists and they show little advantage over ZSL models that utilise deep feature representations but do not learn an end-to-end embedding. In this paper we argue that the key to make deep ZSL models succeed is to choose the right embedding space. Instead of embedding into a semantic space or an intermediate space, we propose to use the visual space as the embedding space. This is because that in this space, the subsequent nearest neighbour search would suffer much less from the hubness problem and thus become more effective. This model design also provides a natural mechanism for multiple semantic modalities (e.g., attributes and sentence descriptions) to be fused and optimised jointly in an end-to-end manner. Extensive experiments on four benchmarks show that our model significantly outperforms the existing models. Code is available at https://github.com/lzrobots/DeepEmbeddingModel_ZSL",Iledereoffivide,536.0,57.0,59.0
3837,Image Retrieval,401.0,cross-batch memory for embedding learning,1.0,24.0,5.0,201.0,1.0,2.6,190.2,44,http://arxiv.org/pdf/2103.06124v1,"Deep learning-based models are utilized to achieve state-of-the-art performance for recommendation systems. A key challenge for these models is to work with millions of categorical classes or tokens. The standard approach is to learn end-to-end, dense latent representations or embeddings for each token. The resulting embeddings require large amounts of memory that blow up with the number of tokens. Training and inference with these models create storage, and memory bandwidth bottlenecks leading to significant computing and energy consumption when deployed in practice. To this end, we present the problem of \textit{Memory Allocation} under budget for embeddings and propose a novel formulation of memory shared embedding, where memory is shared in proportion to the overlap in semantic information. Our formulation admits a practical and efficient randomized solution with Locality sensitive hashing based Memory Allocation (LMA). We demonstrate a significant reduction in the memory footprint while maintaining performance. In particular, our LMA embeddings achieve the same performance compared to standard embeddings with a 16$\times$ reduction in memory footprint. Moreover, LMA achieves an average improvement of over 0.003 AUC across different memory regimes than standard DLRM models on Criteo and Avazu datasets",Icrmefoemle,63.0,52.0,9.0
3838,Image Retrieval,401.0,multi-similarity loss with general pair weighting for deep metric learning,1.0,25.0,5.0,201.0,1.0,2.6,190.6,45,http://arxiv.org/pdf/1905.12837v1,"Deep metric learning aims at learning the distance metric between pair of samples, through the deep neural networks to extract the semantic feature embeddings where similar samples are close to each other while dissimilar samples are farther apart. A large amount of loss functions based on pair distances have been presented in the literature for guiding the training of deep metric learning. In this paper, we unify them in a general pair-based weighting loss function, where the minimizing objective loss is just the distances weighting of informative pairs. The general pair-based weighting loss includes two main aspects, (1) samples mining and (2) pairs weighting. Samples mining aims at selecting the informative positive and negative pair sets to exploit the structured relationship of samples in a mini-batch and also reduce the number of non-trivial pairs. Pair weighting aims at assigning different weights for different pairs according to the pair distances for discriminatively training the network. We detailedly review those existing pair-based losses inline with our general loss function, and explore some possible methods from the perspective of samples mining and pairs weighting. Finally, extensive experiments on three image retrieval datasets show that our general pair-based weighting loss obtains new state-of-the-art performance, demonstrating the effectiveness of the pair-based samples mining and pairs weighting for deep metric learning.",Imulowigepawefodemele,233.0,44.0,65.0
3839,Image Retrieval,401.0,google landmarks dataset v2 - a large-scale benchmark for instance-level recognition and retrieval,1.0,28.0,5.0,201.0,1.0,2.6,191.8,46,http://arxiv.org/pdf/2009.05132v1,"This paper presents the 1st place solution to the Google Landmark Retrieval 2020 Competition on Kaggle. The solution is based on metric learning to classify numerous landmark classes, and uses transfer learning with two train datasets, fine-tuning on bigger images, adjusting loss weight for cleaner samples, and esemble to enhance the model's performance further. Finally, it scored 0.38677 mAP@100 on the private leaderboard.",Igoladav2-alabefoinreanre,57.0,70.0,4.0
3840,Image Retrieval,401.0,two-stage discriminative re-ranking for large-scale landmark retrieval,1.0,29.0,5.0,201.0,1.0,2.6,192.2,47,http://arxiv.org/pdf/1906.04087v2,"The Google-Landmarks-v2 dataset is the biggest worldwide landmarks dataset characterized by a large magnitude of noisiness and diversity. We present a novel landmark retrieval/recognition system, robust to a noisy and diverse dataset, by our team, smlyaka. Our approach is based on deep convolutional neural networks with metric learning, trained by cosine-softmax based losses. Deep metric learning methods are usually sensitive to noise, and it could hinder to learn a reliable metric. To address this issue, we develop an automated data cleaning system. Besides, we devise a discriminative re-ranking method to address the diversity of the dataset for landmark retrieval. Using our methods, we achieved 1st place in the Google Landmark Retrieval 2019 challenge and 3rd place in the Google Landmark Recognition 2019 challenge on Kaggle.",Itwdirefolalare,7.0,57.0,1.0
3841,Image Retrieval,401.0,looking at outfit to parse clothing,1.0,30.0,5.0,201.0,1.0,2.6,192.6,48,http://arxiv.org/pdf/1703.01386v1,"This paper extends fully-convolutional neural networks (FCN) for the clothing parsing problem. Clothing parsing requires higher-level knowledge on clothing semantics and contextual cues to disambiguate fine-grained categories. We extend FCN architecture with a side-branch network which we refer outfit encoder to predict a consistent set of clothing labels to encourage combinatorial preference, and with conditional random field (CRF) to explicitly consider coherent label assignment to the given image. The empirical results using Fashionista and CFPD datasets show that our model achieves state-of-the-art performance in clothing parsing, without additional supervision during training. We also study the qualitative influence of annotation on the current clothing parsing benchmarks, with our Web-based tool for multi-scale pixel-wise annotation and manual refinement effort to the Fashionista dataset. Finally, we show that the image representation of the outfit encoder is useful for dress-up image retrieval application.",Iloatoutopacl,45.0,45.0,6.0
3842,Image Retrieval,401.0,deep triplet quantization,1.0,31.0,5.0,201.0,1.0,2.6,193.0,49,http://arxiv.org/pdf/1902.00153v1,"Deep hashing establishes efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We present a compact coding solution, focusing on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ), a novel approach to learning deep quantization models from the similarity triplets. To enable more effective triplet training, we design a new triplet selection approach, Group Hard, that randomly selects hard triplets in each image group. To generate compact binary codes, we further apply a triplet quantization with weak orthogonality during triplet training. The quantization loss reduces the codebook redundancy and enhances the quantizability of deep representations through back-propagation. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes, which yields state-of-the-art image retrieval performance on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO.",Idetrqu,46.0,47.0,11.0
3843,Image Retrieval,401.0,deep exemplar-based colorization,1.0,32.0,5.0,201.0,1.0,2.6,193.4,50,http://arxiv.org/pdf/1605.00075v1,"This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. We further develop an adaptive image clustering technique to incorporate the global image information. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.",Ideexco,113.0,86.0,17.0
3844,Image Retrieval,401.0,cnn features off-the-shelf: an astounding baseline for recognition,1.0,38.0,5.0,201.0,1.0,2.6,195.8,51,http://arxiv.org/pdf/1403.6382v3,"Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the \overfeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the \overfeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the \overfeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or $L2$ distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.",Icnfeofanasbafore,4079.0,59.0,235.0
3845,Image Retrieval,401.0,are gender-neutral queries really gender-neutral? mitigating gender bias in image search,1.0,39.0,5.0,201.0,1.0,2.6,196.2,52,http://arxiv.org/pdf/2109.05433v1,"Internet search affects people's cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.",Iargequregemigebiinimse,0.0,53.0,0.0
3846,Image Retrieval,401.0,stacked cross attention for image-text matching,1.0,40.0,5.0,201.0,1.0,2.6,196.6,53,http://arxiv.org/pdf/1803.08024v2,"In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: https://github.com/kuanghuei/SCAN.",Istcratfoimma,388.0,47.0,100.0
3847,Image Retrieval,115.0,feature extraction and image retrieval based on alexnet,3.0,201.0,1.0,55.0,4.0,2.5,131.4,54,http://arxiv.org/pdf/1904.02325v1,"Convolutional Neural Network is a hot research topic in image recognition. The latest research shows that Deep CNN model is good at extracting features and representing images. This capacity is applied to image retrieval in this paper. We study on the significance of each layer and do image retrieval experiments on the fusion features. Caffe framework and AlexNet model were used to extract the feature information about images. Two public image datasets, Inria Holidays and Oxford Buildings, were used in our experiment to search for the influence of different datasets. The results showed the fusion feature of Deep CNN model can improve the result of image retrieval and should apply different weights for different datasets.",Ifeexanimrebaonal,44.0,7.0,3.0
3848,Image Retrieval,57.0,fast wavelet-based image characterization for highly adaptive image retrieval,4.0,201.0,1.0,119.0,3.0,2.5,133.2,55,https://www.researchgate.net/profile/Gwenole-Quellec-2/publication/51920394_Fast_Wavelet-Based_Image_Characterization_for_Highly_Adaptive_Image_Retrieval/links/5874b6f408ae6eb871c96da7/Fast-Wavelet-Based-Image-Characterization-for-Highly-Adaptive-Image-Retrieval.pdf,"Adaptive wavelet-based image characterizations have been proposed in previous works for content-based image retrieval (CBIR) applications. In these applications, the same wavelet basis was used to characterize each query image: This wavelet basis was tuned to maximize the retrieval performance in a training data set. We take it one step further in this paper: A different wavelet basis is used to characterize each query image. A regression function, which is tuned to maximize the retrieval performance in the training data set, is used to estimate the best wavelet filter, i.e., in terms of expected retrieval performance, for each query image. A simple image characterization, which is based on the standardized moments of the wavelet coefficient distributions, is presented. An algorithm is proposed to compute this image characterization almost instantly for every possible separable or nonseparable wavelet filter. Therefore, using a different wavelet basis for each query image does not considerably increase computation times. On the other hand, significant retrieval performance increases were obtained in a medical image data set, a texture data set, a face recognition data set, and an object picture data set. This additional flexibility in wavelet adaptation paves the way to relevance feedback on image characterization itself and not simply on the way image characterizations are combined.",Ifawaimchfohiadimre,106.0,36.0,3.0
3849,Image Retrieval,67.0,an effective method for color image retrieval based on texture,4.0,201.0,1.0,125.0,3.0,2.5,138.0,56,http://arxiv.org/abs/1012.0223v1,"This paper presents an effective color image retrieval method based on texture, which uses the color co-occurrence matrix to extract the texture feature and measure the similarity of two color images. Due to the color information such as components and distribution is also taken into consideration, the feature obtained not only reflects the texture correlation but also represents the color information. As a result, our proposed method is superior to the gray-level co-occurrence matrix method and color histogram method, and it enhances the retrieval accuracy which is measured in terms of the recall and precision in the meanwhile. Research highlights This paper presents an effective color image retrieval method based on texture. We use color co-occurrence matrix to extract texture feature and measure similarity. Color information such as components and distribution is taken into consideration. It greatly enhances the retrieval accuracy.",Ianefmefocoimrebaonte,75.0,14.0,0.0
3850,Image Retrieval,80.0,content-based image retrieval by integrating color and texture features,4.0,201.0,1.0,121.0,3.0,2.5,140.7,57,http://arxiv.org/pdf/0908.4074v1,"Content-based image retrieval (CBIR) has been an active research topic in the last decade. Feature extraction and representation is one of the most important issues in the CBIR. In this paper, we propose a content-based image retrieval method based on an efficient integration of color and texture features. As its color features, pseudo-Zernike chromaticity distribution moments in opponent chromaticity space are used. As its texture features, rotation-invariant and scale-invariant image descriptor in steerable pyramid domain are adopted, which offers an efficient and flexible approximation of early processing in the human visual system. The integration of color and texture information provides a robust feature set for color image retrieval. Experimental results show that the proposed method yields higher retrieval accuracy than some conventional methods even though its feature vector dimension is not higher than those of the latter for different test DBs.",Icoimrebyincoantefe,83.0,40.0,2.0
3851,Image Retrieval,133.0,local diagonal extrema pattern: a new and efficient feature descriptor for ct image retrieval,3.0,201.0,1.0,70.0,4.0,2.5,141.3,58,http://arxiv.org/pdf/1808.01124v1,"The medical image retrieval plays an important role in medical diagnosis where a physician can retrieve most similar images from template images against a query image of a particular patient. In this letter, a new and efficient image features descriptor based on the local diagonal extrema pattern (LDEP) is proposed for CT image retrieval. The proposed approach finds the values and indexes of the local diagonal extremas to exploit the relationship among the diagonal neighbors of any center pixel of the image using first-order local diagonal derivatives. The intensity values of the local diagonal extremas are compared with the intensity value of the center pixel to utilize the relationship of central pixel with its neighbors. Finally, the descriptor is formed on the basis of the indexes and comparison of center pixel and local diagonal extremas. The consideration of only diagonal neighbors greatly reduces the dimension of the feature vector which speeds up the image retrieval task and solves the “Curse of dimensionality” problem also. The LDEP is tested for CT image retrieval over Emphysema-CT and NEMA-CT databases and compared with the existing approaches. The superiority in terms of performance and efficiency in terms of speedup of the proposed method are confirmed by the experiments.",Ilodiexpaaneaneffedefoctimre,104.0,30.0,7.0
3852,Image Retrieval,160.0,fusion of deep learning and compressed domain features for content-based image retrieval,3.0,201.0,1.0,49.0,4.0,2.5,143.1,59,http://arxiv.org/pdf/2006.10208v1,"This paper presents an effective image retrieval method by combining high-level features from convolutional neural network (CNN) model and low-level features from dot-diffused block truncation coding (DDBTC). The low-level features, e.g., texture and color, are constructed by vector quantization -indexed histogram from DDBTC bitmap, maximum, and minimum quantizers. Conversely, high-level features from CNN can effectively capture human perception. With the fusion of the DDBTC and CNN features, the extended deep learning two-layer codebook features is generated using the proposed two-layer codebook, dimension reduction, and similarity reweighting to improve the overall retrieval rate. Two metrics, average precision rate and average recall rate (ARR), are employed to examine various data sets. As documented in the experimental results, the proposed schemes can achieve superior performance compared with the state-of-the-art methods with either low-or high-level features in terms of the retrieval rate. Thus, it can be a strong candidate for various image retrieval related applications.",Ifuofdeleancodofefocoimre,55.0,58.0,3.0
3853,Image Retrieval,134.0,scalable mobile image retrieval by exploring contextual saliency,3.0,201.0,1.0,78.0,4.0,2.5,144.0,60,https://www.researchgate.net/profile/Xueming-Qian/publication/273637500_Scalable_Mobile_Image_Retrieval_by_Exploring_Contextual_Saliency/links/55472c130cf234bdb21dbb93/Scalable-Mobile-Image-Retrieval-by-Exploring-Contextual-Saliency.pdf,"Nowadays, it is very convenient to capture photos by a smart phone. As using, the smart phone is a convenient way to share what users experienced anytime and anywhere through social networks, it is very possible that we capture multiple photos to make sure the content is well photographed. In this paper, an effective scalable mobile image retrieval approach is proposed by exploring contextual salient information for the input query image. Our goal is to explore the high-level semantic information of an image by finding the contextual saliency from multiple relevant photos rather than solely using the input image. Thus, the proposed mobile image retrieval approach first determines the relevant photos according to visual similarity, then mines salient features by exploring contextual saliency from multiple relevant images, and finally determines contributions of salient features for scalable retrieval. Compared with the existing mobile-based image retrieval approaches, our approach requires less bandwidth and has better retrieval performance. We can carry out retrieval with <;200-B data, which is <;5% of existing approaches. Most importantly, when the bandwidth is limited, we can rank the transmitted features according to their contributions to retrieval. Experimental results show the effectiveness of the proposed approach.",Iscmoimrebyexcosa,74.0,48.0,3.0
3854,Image Retrieval,55.0,review of medical image retrieval systems and future directions,4.0,201.0,1.0,161.0,3.0,2.5,145.2,61,https://www.researchgate.net/profile/Sameer-Antani/publication/221030443_Review_of_medical_image_retrieval_systems_and_future_directions/links/0deec51cdce8b73c0d000000/Review-of-medical-image-retrieval-systems-and-future-directions.pdf,"This paper presents a review of online systems for content-based medical image retrieval (CBIR). The objective of this review is to evaluate the capabilities and gaps in these systems and to determine ways of improving relevance of multi-modal (text and image) information retrieval in the iMedline system, being developed at the National Library of Medicine (NLM). Seven medical information retrieval systems: Figuresearch, BioText, GoldMiner, Yale Image Finder, Yottalook, Image Retrieval for Medical Applications (IRMA), and iMedline have been evaluated here using the system of gaps defined in [1]. Not all of these systems take advantage of the visual information contained in biomedical literature as figures and illustrations. However, all attempt to extract metadata about the image from the full-text of the articles and retrieve figures/images in response to a query. iMedline aims to advance the state-of-the-art in multimodal information retrieval by unifying image and text features in computing relevance. We discuss the shortcomings of these current systems and discuss future directions and next steps in iMedline toward context-based medical image retrieval.",Ireofmeimresyanfudi,58.0,11.0,1.0
3855,Image Retrieval,172.0,content based image retrieval by using color descriptor and discrete wavelet transform,3.0,201.0,1.0,44.0,4.0,2.5,145.2,62,http://arxiv.org/pdf/1503.07816v1,"Due to recent development in technology, the complexity of multimedia is significantly increased and the retrieval of similar multimedia content is a open research problem. Content-Based Image Retrieval (CBIR) is a process that provides a framework for image search and low-level visual features are commonly used to retrieve the images from the image database. The basic requirement in any image retrieval process is to sort the images with a close similarity in term of visually appearance. The color, shape and texture are the examples of low-level image features. The feature plays a significant role in image processing. The powerful representation of an image is known as feature vector and feature extraction techniques are applied to get features that will be useful in classifying and recognition of images. As features define the behavior of an image, they show its place in terms of storage taken, efficiency in classification and obviously in time consumption also. In this paper, we are going to discuss various types of features, feature extraction techniques and explaining in what scenario, which features extraction technique will be better. The effectiveness of the CBIR approach is fundamentally based on feature extraction. In image processing errands like object recognition and image retrieval feature descriptor is an immense among the most essential step. The main idea of CBIR is that it can search related images to an image passed as query from a dataset got by using distance metrics. The proposed method is explained for image retrieval constructed on YCbCr color with canny edge histogram and discrete wavelet transform. The combination of edge of histogram and discrete wavelet transform increase the performance of image retrieval framework for content based search. The execution of different wavelets is additionally contrasted with discover the suitability of specific wavelet work for image retrieval. The proposed algorithm is prepared and tried to implement for Wang image database. For Image Retrieval Purpose, Artificial Neural Networks (ANN) is used and applied on standard dataset in CBIR domain. The execution of the recommended descriptors is assessed by computing both Precision and Recall values and compared with different other proposed methods with demonstrate the predominance of our method. The efficiency and effectiveness of the proposed approach outperforms the existing research in term of average precision and recall values.",Icobaimrebyuscodeandiwatr,60.0,66.0,2.0
3856,Image Retrieval,54.0,semi-supervised hashing for scalable image retrieval,4.0,201.0,1.0,166.0,3.0,2.5,146.4,63,https://research.google/pubs/pub36386.pdf,"Large scale image search has recently attracted considerable attention due to easy availability of huge amounts of data. Several hashing methods have been proposed to allow approximate but highly efficient search. Unsupervised hashing methods show good performance with metric distances but, in image search, semantic similarity is usually given in terms of labeled pairs of images. There exist supervised hashing methods that can handle such semantic similarity but they are prone to overfitting when labeled data is small or noisy. Moreover, these methods are usually very slow to train. In this work, we propose a semi-supervised hashing method that is formulated as minimizing empirical error on the labeled data while maximizing variance and independence of hash bits over the labeled and unlabeled data. The proposed method can handle both metric as well as semantic similarity. The experimental results on two large datasets (up to one million samples) demonstrate its superior performance over state-of-the-art supervised and unsupervised methods.",Isehafoscimre,613.0,17.0,76.0
3857,Image Retrieval,174.0,local bit-plane decoded pattern: a novel feature descriptor for biomedical image retrieval,3.0,201.0,1.0,47.0,4.0,2.5,146.7,64,http://arxiv.org/pdf/2009.10891v1,"A novel image feature descriptor based on the local bit-plane decoded pattern (LBDP) is introduced for indexing and retrieval of biomedical images in this paper. A local bit-plane transformation scheme is proposed to compute the local bit-plane transformed values for each image pixel from the bit-plane binary contents of its each neighboring pixels. The introduced LBDP is generated by finding a binary pattern using the difference of center pixel's intensity value with the local bit-plane transformed values. The efficacy of the LBDP is tested under biomedical image retrieval using average retrieval precision and average retrieval rate. Three benchmark databases Emphysema-CT, NEMA-CT, and Open Access Series of Imaging Studies magnetic resonance imaging are used for the evaluation and comparison of the proposed approach with recent state-of-art methods. The experimental results confirm the discriminative ability and the efficiency of the proposed LBDP for biomedical image indexing and retrieval and prove the outperformance of existing biomedical image retrieval approaches.",Ilobidepaanofedefobiimre,86.0,48.0,7.0
3858,Image Retrieval,63.0,a review of content-based image retrieval,4.0,201.0,1.0,186.0,3.0,2.5,155.10000000000002,65,https://www.researchgate.net/profile/Wai-Lok-Woo/publication/221093029_A_review_of_content-based_image_retrieval/links/09e4150f1891bad6fb000000/A-review-of-content-based-image-retrieval.pdf,"A comprehensive survey on patch recognition, which is a crucial part of content-based image retrieval (CBIR), is presented. CBIR can be viewed as a methodology in which three correlated modules including patch sampling, characterizing, and recognizing are employed. This paper aims to evaluate meaningful models for one of the most challenging problems in image understanding, specifically, for the effective and efficient mapping between image visual features and high-level semantic concepts. To achieve this, the latest classification, clustering, and interactive methods have been meticulously discussed. Finally, several recommendations for future research issues have been suggested based on the weaknesses of recent technologies.",Iareofcoimre,84.0,74.0,2.0
3859,Image Retrieval,88.0,content based image retrieval using hierarchical and k-means clustering techniques,4.0,201.0,1.0,190.0,3.0,2.5,163.8,66,https://www.academia.edu/download/2048425/utx3oaiyovgtxs.pdf,"In this paper we present an image retrieval system that takes an image as the input query and retrieves images based on image content. Content Based Image Retrieval is an approach for retrieving semantically-relevant images from an image database based on automatically-derived image features. The unique aspect of the system is the utilization of hierarchical and k-means clustering techniques. The proposed procedure consists of two stages. First, here we are going to filter most of the images in the hierarchical clustering and then apply the clustered images to KMeans, so that we can get better favored image results.",Icobaimreushiank-clte,78.0,7.0,4.0
3860,Image Retrieval,140.0,matchable image retrieval by learning from surface reconstruction,3.0,119.0,3.0,201.0,1.0,2.4,149.89999999999998,67,https://arxiv.org/pdf/1811.10343,"Visual localization to compute 6DoF camera pose from a given image has wide applications such as in robotics, virtual reality, augmented reality, etc. Two kinds of descriptors are important for the visual localization. One is global descriptors that extract the whole feature from each image. The other is local descriptors that extract the local feature from each image patch usually enclosing a key point. More and more methods of the visual localization have two stages: at first to perform image retrieval by global descriptors and then from the retrieval feedback to make 2D-3D point correspondences by local descriptors. The two stages are in serial for most of the methods. This simple combination has not achieved superiority of fusing local and global descriptors. The 3D points obtained from the retrieval feedback are as the nearest neighbor candidates of the 2D image points only by global descriptors. Each of the 2D image points is also called a query local feature when performing the 2D-3D point correspondences. In this paper, we propose a novel parallel search framework, which leverages advantages of both local and global descriptors to get nearest neighbor candidates of a query local feature. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the query local feature. We propose a new probabilistic model and a new deep learning based local descriptor when constructing the random trees. A weighted Hamming regularization term to keep discriminativeness after binarization is given in the loss function for the proposed local descriptor. The loss function co-trains both real and binary descriptors of which the results are integrated into the random trees.",Imaimrebylefrsure,17.0,49.0,2.0
3861,Image Retrieval,108.0,a modulation module for multi-task learning with applications in image retrieval,3.0,160.0,3.0,201.0,1.0,2.4,156.7,68,https://openaccess.thecvf.com/content_ECCV_2018/papers/Xiangyun_Zhao_A_Modulation_Module_ECCV_2018_paper.pdf,"Multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual tasks, under the assumption that those tasks are correlated and complementary to each other. However, the relationships between the tasks are complicated in practice, especially when the number of involved tasks scales up. When two tasks are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the tasks. This will raise destructive interference which decreases learning efficiency of shared parameters and lead to low quality loss local optimum w.r.t. shared parameters. To address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant tasks while disentangling the learning of irrelevant tasks with minor parameters addition. Equipped with this module, gradient directions from different tasks can be enforced to be consistent for those shared parameters, which benefits multi-task joint training. The module is end-to-end learnable without ad-hoc design for specific tasks, and can naturally handle many tasks at the same time. We apply our approach on two retrieval tasks, face retrieval on the CelebA dataset [1] and product retrieval on the UT-Zappos50K dataset [2, 3], and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency.",Iamomofomulewiapinimre,48.0,38.0,8.0
3862,Image Retrieval,2.0,features for image retrieval: an experimental comparison,5.0,201.0,1.0,201.0,1.0,2.2,141.3,69,http://thomas.deselaers.de/publications/papers/deselaers_infret08.pdf,"Learning a metric of natural image patches is an important tool for analyzing images. An efficient means is to train a deep network to map an image patch to a vector space, in which the Euclidean distance reflects patch similarity. Previous attempts learned such an embedding in a supervised manner, requiring the availability of many annotated images. In this paper, we present an unsupervised embedding of natural image patches, avoiding the need for annotated images. The key idea is that the similarity of two patches can be learned from the prevalence of their spatial proximity in natural images. Clearly, relying on this simple principle, many spatially nearby pairs are outliers, however, as we show, the outliers do not harm the convergence of the metric learning. We show that our unsupervised embedding approach is more effective than a supervised one or one that uses deep patch representations. Moreover, we show that it naturally leads itself to an efficient self-supervised domain adaptation technique onto a target domain that contains a common foreground object.",Ifefoimreanexco,625.0,105.0,47.0
3863,Image Retrieval,3.0,boosting image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,141.60000000000002,70,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.7646&rep=rep1&type=pdf,"Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. The performance of k-means has been enhanced from different perspectives over the years. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is driven by an explicit objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. The procedure of k-means becomes simpler and converges to a considerably better local optima. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.",Iboimre,2.0,32.0,0.0
3864,Image Retrieval,4.0,fundamentals of content-based image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,141.9,71,http://arxiv.org/pdf/2103.03770v2,"Matching plays a vital role in the rational allocation of resources in many areas, ranging from market operation to people's daily lives. In economics, the term matching theory is coined for pairing two agents in a specific market to reach a stable or optimal state. In computer science, all branches of matching problems have emerged, such as the question-answer matching in information retrieval, user-item matching in a recommender system, and entity-relation matching in the knowledge graph. A preference list is the core element during a matching process, which can either be obtained directly from the agents or generated indirectly by prediction. Based on the preference list access, matching problems are divided into two categories, i.e., explicit matching and implicit matching. In this paper, we first introduce the matching theory's basic models and algorithms in explicit matching. The existing methods for coping with various matching problems in implicit matching are reviewed, such as retrieval matching, user-item matching, entity-relation matching, and image matching. Furthermore, we look into representative applications in these areas, including marriage and labor markets in explicit matching and several similarity-based matching problems in implicit matching. Finally, this survey paper concludes with a discussion of open issues and promising future directions in the field of matching.",Ifuofcoimre,615.0,94.0,38.0
3865,Image Retrieval,5.0,performance evaluation in content-based image retrieval: overview and proposals,5.0,201.0,1.0,201.0,1.0,2.2,142.2,72,https://archive-ouverte.unige.ch/unige:48034/ATTACHMENT01,"Generative Adversarial Networks (GANs) have shown great success in many applications. In this work, we present a novel method that leverages human annotations to improve the quality of generated images. Unlike previous paradigms that directly ask annotators to distinguish between real and fake data in a straightforward way, we propose and annotate a set of carefully designed attributes that encode important image information at various levels, to understand the differences between fake and real images. Specifically, we have collected an annotated dataset that contains 600 fake images and 400 real images. These images are evaluated by 10 workers from the Amazon Mechanical Turk (AMT) based on eight carefully defined attributes. Statistical analyses have revealed different distributions of the proposed attributes between real and fake images. These attributes are shown to be useful in discriminating fake images from real ones, and deep neural networks are developed to automatically predict the attributes. We further utilize the information by integrating the attributes into GANs to generate better images. Experimental results evaluated by multiple metrics show performance improvement of the proposed model.",Ipeevincoimreovanpr,632.0,67.0,27.0
3866,Image Retrieval,6.0,content-based image retrieval: approaches and trends of the new age,5.0,201.0,1.0,201.0,1.0,2.2,142.5,73,https://www.academia.edu/download/48029357/Content-based_image_retrieval_approaches20160813-7126-1eo3b22.pdf,"Bone Age Assessment (BAA) is a task performed by radiologists to diagnose abnormal growth in a child. In manual approaches, radiologists take into account different identity markers when calculating bone age, i.e., chronological age and gender. However, the current automated Bone Age Assessment methods do not completely exploit the information present in the patient's metadata. With this lack of available methods as motivation, we present SIMBA: Specific Identity Markers for Bone Age Assessment. SIMBA is a novel approach for the task of BAA based on the use of identity markers. For this purpose, we build upon the state-of-the-art model, fusing the information present in the identity markers with the visual features created from the original hand radiograph. We then use this robust representation to estimate the patient's relative bone age: the difference between chronological age and bone age. We validate SIMBA on the Radiological Hand Pose Estimation dataset and find that it outperforms previous state-of-the-art methods. SIMBA sets a trend of a new wave of Computer-aided Diagnosis methods that incorporate all of the data that is available regarding a patient. To promote further research in this area and ensure reproducibility we will provide the source code as well as the pre-trained models of SIMBA.",Icoimreapantrofthneag,533.0,125.0,31.0
3867,Image Retrieval,7.0,"the bayesian image retrieval system, pichunter: theory, implementation, and psychophysical experiments",5.0,201.0,1.0,201.0,1.0,2.2,142.8,74,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.9375&rep=rep1&type=pdf,"In psychophysical experiments time and the limited goodwill of participants is usually a major constraint. This has been the main motivation behind the early development of adaptive methods for the measurements of psychometric thresholds. More recently methods have been developed to measure whole psychometric functions in an adaptive way. Here we describe a Bayesian method to measure adaptively any aspect of a psychophysical function, taking inspiration from Kontsevich and Tyler's optimal Bayesian measurement method. Our method is implemented in a complete and easy-to-use MATLAB package.",Ithbaimresypithimanpsex,818.0,90.0,54.0
3868,Image Retrieval,10.0,features for image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,143.7,75,http://www-i6.informatik.rwth-aachen.de/publications/download/10/DeselaersThomas--FeaturesforImageRetrieval--2003.pdf,"Vision-based localization of an agent in a map is an important problem in robotics and computer vision. In that context, localization by learning matchable image features is gaining popularity due to recent advances in machine learning. Features that uniquely describe the visual contents of images have a wide range of applications, including image retrieval and understanding. In this work, we propose a method that learns image features targeted for image-retrieval-based localization. Retrieval-based localization has several benefits, such as easy maintenance and quick computation. However, the state-of-the-art features only provide visual similarity scores which do not explicitly reveal the geometric distance between query and retrieved images. Knowing this distance is highly desirable for accurate localization, especially when the reference images are sparsely distributed in the scene. Therefore, we propose a novel loss function for learning image features which are both visually representative and geometrically relatable. This is achieved by guiding the learning process such that the feature and geometric distances between images are directly proportional. In our experiments we show that our features not only offer significantly better localization accuracy, but also allow to estimate the trajectory of a query sequence in absence of the reference images.",Ifefoimre,483.0,28.0,77.0
3869,Image Retrieval,14.0,one-class svm for learning in image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,144.9,76,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.2423&rep=rep1&type=pdf,"Good results on image classification and retrieval using support vector machines (SVM) with local binary patterns (LBPs) as features have been extensively reported in the literature where an entire image is retrieved or classified. In contrast, in medical imaging, not all parts of the image may be equally significant or relevant to the image retrieval application at hand. For instance, in lung x-ray image, the lung region may contain a tumour, hence being highly significant whereas the surrounding area does not contain significant information from medical diagnosis perspective. In this paper, we propose to detect salient regions of images during training and fold the data to reduce the effect of irrelevant regions. As a result, smaller image areas will be used for LBP features calculation and consequently classification by SVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify the performance of the proposed approach. The results demonstrate the benefits of saliency-based folding approach that delivers comparable classification accuracies with state-of-the-art but exhibits lower computational cost and storage requirements, factors highly important for big data analytics.",Ionsvfoleinimre,621.0,16.0,49.0
3870,Image Retrieval,15.0,relevance feedback in image retrieval: a comprehensive review,5.0,201.0,1.0,201.0,1.0,2.2,145.2,77,https://www.fi.muni.cz/~xkohout7/Research/clanky_cizi/relevance_feedback/review2002.pdf,"Emergence of various vertical search engines highlights the fact that a single ranking technology cannot deal with the complexity and scale of search problems. For example, technology behind video and image search is very different from general web search. Their ranking functions share few features. Question answering websites (e.g., Yahoo! Answer) can make use of text matching and click features developed for general web, but they have unique page structures and rich user feedback, e.g., thumbs up and thumbs down ratings in Yahoo! answer, which greatly benefit their own ranking. Even for those features shared by answer and general web, the correlation between features and relevance could be very different. Therefore, dedicated functions are needed in order to better rank documents within individual domains. These dedicated functions are defined on distinct feature spaces. However, having one search box for each domain, is neither efficient nor scalable. Rather than typing the same query two times into both Yahoo! Search and Yahoo! Answer and retrieving two ranking lists, we would prefer putting it only once but receiving a comprehensive list of documents from both domains on the subject. This situation calls for new technology that blends documents from different sources into a single ranking list. Despite the content richness of the blended list, it has to be sorted by relevance none the less. We call such technology blending, which is the main subject of this paper.",Irefeinimreacore,971.0,79.0,48.0
3871,Image Retrieval,16.0,diversifying the image retrieval results,5.0,201.0,1.0,201.0,1.0,2.2,145.5,78,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.1226&rep=rep1&type=pdf,"Literature search is arguably one of the most important phases of the academic and non-academic research. The increase in the number of published papers each year makes manual search inefficient and furthermore insufficient. Hence, automatized methods such as search engines have been of interest in the last thirty years. Unfortunately, these traditional engines use keyword-based approaches to solve the search problem, but these approaches are prone to ambiguity and synonymy. On the other hand, bibliographic search techniques based only on the citation information are not prone to these problems since they do not consider textual similarity. For many particular research areas and topics, the amount of knowledge to humankind is immense, and obtaining the desired information is as hard as looking for a needle in a haystack. Furthermore, sometimes, what we are looking for is a set of documents where each one is different than the others, but at the same time, as a whole we want them to cover all the important parts of the literature relevant to our search. This paper targets the problem of result diversification in citation-based bibliographic search. It surveys a set of techniques which aim to find a set of papers with satisfactory quality and diversity. We enhance these algorithms with a direction-awareness functionality to allow the users to reach either old, well-cited, well-known research papers or recent, less-known ones. We also propose a set of novel techniques for a better diversification of the results. All the techniques considered are compared by performing a rigorous experimentation. The results show that some of the proposed techniques are very successful in practice while performing a search in a bibliographic database.",Idithimrere,90.0,16.0,3.0
3872,Image Retrieval,17.0,evaluating image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,145.8,79,https://www.academia.edu/download/42417807/Evaluating_image_retrieval20160208-16433-yvfjxx.pdf,"Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric (CIDEr) that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.",Ievimre,53.0,67.0,0.0
3873,Image Retrieval,18.0,intelligent image retrieval techniques: a survey,5.0,201.0,1.0,201.0,1.0,2.2,146.10000000000002,80,http://arxiv.org/pdf/2006.10553v1,"Computers have been used to analyze and create music since they were first introduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the Internet and large scale platforms for music recommendation and retrieval have made music an increasingly prevalent domain of machine learning and artificial intelligence research. While still nascent, several different approaches have been employed to tackle what may broadly be referred to as ""musical intelligence."" This article provides a definition of musical intelligence, introduces a taxonomy of its constituent components, and surveys the wide range of AI methods that can be, and have been, brought to bear in its pursuit, with a particular emphasis on machine learning methods.",Iinimreteasu,44.0,130.0,0.0
3874,Image Retrieval,19.0,image retrieval in multimedia databases: a survey,5.0,201.0,1.0,201.0,1.0,2.2,146.4,81,http://arxiv.org/pdf/1706.06064v2,"The explosive increase and ubiquitous accessibility of visual data on the Web have led to the prosperity of research activity in image search or retrieval. With the ignorance of visual content as a ranking clue, methods with text search techniques for visual retrieval may suffer inconsistency between the text words and visual content. Content-based image retrieval (CBIR), which makes use of the representation of visual content to identify relevant images, has attracted sustained attention in recent two decades. Such a problem is challenging due to the intention gap and the semantic gap problems. Numerous techniques have been developed for content-based image retrieval in the last decade. The purpose of this paper is to categorize and evaluate those algorithms proposed during the period of 2003 to 2016. We conclude with several promising directions for future research.",Iimreinmudaasu,62.0,41.0,3.0
3876,Image Retrieval,21.0,a review of content-based image retrieval systems in medical applications—clinical benefits and future directions,5.0,201.0,1.0,201.0,1.0,2.2,147.0,82,http://www.baskent.edu.tr/~hogul/RA8.pdf,"With the advances in both stable interest region detectors and robust and distinctive descriptors, local feature-based image or object retrieval has become a popular research topic. %All of the local feature-based image retrieval system involves two important processes: local feature extraction and image representation. The other key technology for image retrieval systems is image representation such as the bag-of-visual words (BoVW), Fisher vector, or Vector of Locally Aggregated Descriptors (VLAD) framework. In this paper, we review local features and image representations for image retrieval. Because many and many methods are proposed in this area, these methods are grouped into several classes and summarized. In addition, recent deep learning-based approaches for image retrieval are briefly reviewed.",Iareofcoimresyinmeapbeanfudi,1591.0,218.0,62.0
3878,Image Retrieval,23.0,a survey on emotional semantic image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,147.60000000000002,83,https://www.researchgate.net/profile/Qianhua-He-2/publication/224358996_A_survey_on_emotional_semantic_image_retrieval/links/0deec536c89e23e198000000/A-survey-on-emotional-semantic-image-retrieval.pdf,"Most research on emotion analysis from text focuses on the task of emotion classification or emotion intensity regression. Fewer works address emotions as a phenomenon to be tackled with structured learning, which can be explained by the lack of relevant datasets. We fill this gap by releasing a dataset of 5000 English news headlines annotated via crowdsourcing with their associated emotions, the corresponding emotion experiencers and textual cues, related emotion causes and targets, as well as the reader's perception of the emotion of the headline. This annotation task is comparably challenging, given the large number of classes and roles to be identified. We therefore propose a multiphase annotation procedure in which we first find relevant instances with emotional content and then annotate the more fine-grained aspects. Finally, we develop a baseline for the task of automatic prediction of semantic role structures and discuss the results. The corpus we release enables further research on emotion classification, emotion intensity prediction, emotion cause detection, and supports further qualitative studies.",Iasuonemseimre,120.0,36.0,5.0
3879,Image Retrieval,25.0,private content based image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,148.2,84,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.562.127&rep=rep1&type=pdf,"Most popular web browsers include ""reader modes"" that improve the user experience by removing un-useful page elements. Reader modes reformat the page to hide elements that are not related to the page's main content. Such page elements include site navigation, advertising related videos and images, and most JavaScript. The intended end result is that users can enjoy the content they are interested in, without distraction.   In this work, we consider whether the ""reader mode"" can be widened to also provide performance and privacy improvements. Instead of its use as a post-render feature to clean up the clutter on a page we propose SpeedReader as an alternative multistep pipeline that is part of the rendering pipeline. Once the tool decides during the initial phase of a page load that a page is suitable for reader mode use, it directly applies document tree translation before the page is rendered.   Based on our measurements, we believe that SpeedReader can be continuously enabled in order to drastically improve end-user experience, especially on slower mobile connections. Combined with our approach to predicting which pages should be rendered in reader mode with 91% accuracy, it achieves drastic speedups and bandwidth reductions of up to 27x and 84x respectively on average. We further find that our novel ""reader mode"" approach brings with it significant privacy improvements to users. Our approach effectively removes all commonly recognized trackers, issuing 115 fewer requests to third parties, and interacts with 64 fewer trackers on average, on transformed pages.",Iprcobaimre,89.0,19.0,5.0
3880,Image Retrieval,27.0,application of svm and ann for image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,148.8,85,http://arxiv.org/pdf/2011.03759v1,"The amyloid state of proteins is widely studied with relevancy in neurology, biochemistry, and biotechnology. In contrast with amorphous aggregation, the amyloid state has a well-defined structure, consisting of parallel and anti-parallel $\beta$-sheets in a periodically repeated formation. The understanding of the amyloid state is growing with the development of novel molecular imaging tools, like cryogenic electron microscopy. Sequence-based amyloid predictors were developed by using mostly artificial neural networks (ANNs) as the underlying computational techniques. From a good neural network-based predictor, it is a very difficult task to identify those attributes of the input amino acid sequence, which implied the decision of the network. Here we present a Support Vector Machine (SVM)-based predictor for hexapeptides with correctness higher than 84\%, i.e., it is at least as good as the published ANN-based tools. Unlike the artificial neural networks, the decision of the SVMs are much easier to analyze, and from a good predictor, we can infer rich biochemical knowledge.   Availability and Implementation: The Budapest Amyloid Predictor webserver is freely available at https://pitgroup.org/bap.",Iapofsvananfoimre,86.0,25.0,0.0
3882,Image Retrieval,29.0,content-based image retrieval research,5.0,201.0,1.0,201.0,1.0,2.2,149.4,86,https://www.sciencedirect.com/science/article/pii/S1875389211007279/pdf?md5=2d721645f858d6d3ea445829b4efb990&pid=1-s2.0-S1875389211007279-main.pdf&_valck=1,"Content-based multimedia information retrieval is an interesting research area since it allows retrieval based on inherent characteristic of multimedia objects. For example retrieval based on visual characteristics such as colour, shapes or textures of objects in images or retrieval based on spatial relationships among objects in the media (images or video clips). This paper reviews some work done in image and video retrieval and then proposes an integrated model that can handle images and video clips uniformly. Using this model retrieval on images or video clips can be done based on the same framework.",Icoimrere,45.0,4.0,2.0
3883,Image Retrieval,30.0,word image retrieval using binary features,5.0,201.0,1.0,201.0,1.0,2.2,149.7,87,http://arxiv.org/pdf/1604.05907v2,"Digital libraries store images which can be highly degraded and to index this kind of images we resort to word spot- ting as our information retrieval system. Information retrieval for handwritten document images is more challenging due to the difficulties in complex layout analysis, large variations of writing styles, and degradation or low quality of historical manuscripts. This paper presents a simple innovative learning-free method for word spotting from large scale historical documents combining Local Binary Pattern (LBP) and spatial sampling. This method offers three advantages: firstly, it operates in completely learning free paradigm which is very different from unsupervised learning methods, secondly, the computational time is significantly low because of the LBP features which are very fast to compute, and thirdly, the method can be used in scenarios where annotations are not available. Finally we compare the results of our proposed retrieval method with the other methods in the literature.",Iwoimreusbife,103.0,0.0,2.0
3884,Image Retrieval,31.0,facing the reality of semantic image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,150.0,88,http://arxiv.org/pdf/1706.00631v2,"Face retrieval has received much attention over the past few decades, and many efforts have been made in retrieving face images against pose, illumination, and expression variations. However, the conventional works fail to meet the requirements of a potential and novel task --- retrieving a person's face image at a specific age, especially when the specific 'age' is not given as a numeral, i.e. 'retrieving someone's image at the similar age period shown by another person's image'. To tackle this problem, we propose a dual reference face retrieval framework in this paper, where the system takes two inputs: an identity reference image which indicates the target identity and an age reference image which reflects the target age. In our framework, the raw images are first projected on a joint manifold, which preserves both the age and identity locality. Then two similarity metrics of age and identity are exploited and optimized by utilizing our proposed quartet-based model. The experiments show promising results, outperforming hierarchical methods.",Ifathreofseimre,71.0,31.0,2.0
3885,Image Retrieval,32.0,efficient image retrieval through vantage objects,5.0,201.0,1.0,201.0,1.0,2.2,150.3,89,http://www.cs.uu.nl/groups/MG/multimedia/publications/art/pr2002.pdf,"Detecting spliced images is one of the emerging challenges in computer vision. Unlike prior methods that focus on detecting low-level artifacts generated during the manipulation process, we use an image retrieval approach to tackle this problem. When given a spliced query image, our goal is to retrieve the original image from a database of authentic images. To achieve this goal, we propose representing an image by its constituent objects based on the intuition that the finest granularity of manipulations is oftentimes at the object-level. We introduce a framework, object embeddings for spliced image retrieval (OE-SIR), that utilizes modern object detectors to localize object regions. Each region is then embedded and collectively used to represent the image. Further, we propose a student-teacher training paradigm for learning discriminative embeddings within object regions to avoid expensive multiple forward passes. Detailed analysis of the efficacy of different feature embedding models is also provided in this study. Extensive experimental results show that the OE-SIR achieves state-of-the-art performance in spliced image retrieval.",Iefimrethvaob,90.0,25.0,3.0
3886,Image Retrieval,33.0,efficient near-duplicate detection and sub-image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,150.60000000000002,90,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.5010&rep=rep1&type=pdf,"Systematic checkpointing of the machine state makes restart of execution from a safe state possible upon detection of an error. The time and energy overhead of checkpointing, however, grows with the frequency of checkpointing. Amortizing this overhead becomes especially challenging, considering the growth of expected error rates, as checkpointing frequency tends to increase with increasing error rates. Based on the observation that due to imbalanced technology scaling, recomputing a data value can be more energy efficient than retrieving (i.e., loading) a stored copy, this paper explores how recomputation of data values (which otherwise would be read from a checkpoint from memory or secondary storage) can reduce the machine state to be checkpointed, and thereby reduce the checkpointing overhead. Specifically, the resulting amnesic checkpointing framework AmnesiCHK can reduce the storage overhead by up to 23.91%; time overhead, by 11.92%; and energy overhead, by 12.53%, respectively, even in a relatively small scale system.",Iefnedeansure,385.0,18.0,25.0
3887,Image Retrieval,35.0,research and improvement of content-based image retrieval framework,5.0,201.0,1.0,201.0,1.0,2.2,151.2,91,http://arxiv.org/pdf/2005.06380v2,"The world has seen in 2020 an unprecedented global outbreak of SARS-CoV-2, a new strain of coronavirus, causing the COVID-19 pandemic, and radically changing our lives and work conditions. Many scientists are working tirelessly to find a treatment and a possible vaccine. Furthermore, governments, scientific institutions and companies are acting quickly to make resources available, including funds and the opening of large-volume data repositories, to accelerate innovation and discovery aimed at solving this pandemic. In this paper, we develop a novel automated theme-based visualisation method, combining advanced data modelling of large corpora, information mapping and trend analysis, to provide a top-down and bottom-up browsing and search interface for quick discovery of topics and research resources. We apply this method on two recently released publications datasets (Dimensions' COVID-19 dataset and the Allen Institute for AI's CORD-19). The results reveal intriguing information including increased efforts in topics such as social distancing; cross-domain initiatives (e.g. mental health and education); evolving research in medical topics; and the unfolding trajectory of the virus in different territories through publications. The results also demonstrate the need to quickly and automatically enable search and browsing of large corpora. We believe our methodology will improve future large volume visualisation and discovery systems but also hope our visualisation interfaces will currently aid scientists, researchers, and the general public to tackle the numerous issues in the fight against the COVID-19 pandemic.",Ireanimofcoimrefr,31.0,12.0,0.0
3888,Image Retrieval,36.0,"approaches, challenges and future direction of image retrieval",5.0,201.0,1.0,201.0,1.0,2.2,151.5,92,https://arxiv.org/pdf/1006.4568,"This paper attempts to discuss the evolution of the retrieval approaches focusing on development, challenges and future direction of the image retrieval. It highlights both the already addressed and outstanding issues. The explosive growth of image data leads to the need of research and development of Image Retrieval. However, Image retrieval researches are moving from keyword, to low level features and to semantic features. Drive towards semantic features is due to the problem of the keywords which can be very subjective and time consuming while low level features cannot always describe high level concepts in the users' mind. Hence, introducing an interpretation inconsistency between image descriptors and high level semantics that known as the semantic gap. This paper also discusses the semantic gap issues, user query mechanisms as well as common ways used to bridge the gap in image retrieval.",Iapchanfudiofimre,37.0,57.0,2.0
3890,Image Retrieval,40.0,a study of color histogram based image retrieval,5.0,201.0,1.0,201.0,1.0,2.2,152.7,93,https://www.academia.edu/download/33692806/A_Study_of_Color_Histogram_Based_Image_Retrieval.pdf,"This article discuss the problem of color image content comparison. Particularly, methods of image content comparison are analyzed, restrictions of color histogram are described and a modified method of images content comparison is proposed. This method uses the color histograms and considers color locations. Testing and analyzing of based and modified algorithms are performed. The modified method shows 97% average precision for a collection containing about 700 images without loss of the advantages of based method, i.e. scale and rotation invariant.",Iastofcohibaimre,109.0,19.0,4.0
3891,Image Retrieval,117.0,sketch-based image retrieval on a large scale database,3.0,201.0,1.0,132.0,3.0,2.2,155.1,94,https://projet.liris.cnrs.fr/imagine/pub/proceedings/ACM-MULTIMEDIA-2012/mm/p973.pdf,"The paper presents a simple and effective sketch-based algorithm for large scale image retrieval. One of the main challenges in image retrieval is to localize a region in an image which would be matched with the query image in contour. To tackle this problem, we use the human perception mechanism to identify two types of regions in one image: the first type of region (the main region) is defined by a weighted center of image features, suggesting that we could retrieve objects in images regardless of their sizes and positions. The second type of region, called region of interests (ROI), is to find the most salient part of an image, and is helpful to retrieve images with objects similar to the query in a complicated scene. So using the two types of regions as candidate regions for feature extraction, our algorithm could increase the retrieval rate dramatically. Besides, to accelerate the retrieval speed, we first extract orientation features and then organize them in a hierarchal way to generate global-to-local features. Based on this characteristic, a hierarchical database index structure could be built which makes it possible to retrieve images on a very large scale image database online. Finally a real-time image retrieval system on 4.5 million database is developed to verify the proposed algorithm. The experiment results show excellent retrieval performance of the proposed algorithm and comparisons with other algorithms are also given.",Iskimreonalascda,56.0,7.0,4.0
3892,Image Retrieval,155.0,scalable face image retrieval using attribute-enhanced sparse codewords,3.0,201.0,1.0,106.0,3.0,2.2,158.7,95,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.9366&rep=rep1&type=pdf,"Photos with people (e.g., family, friends, celebrities, etc.) are the major interest of users. Thus, with the exponentially growing photos, large-scale content-based face image retrieval is an enabling technology for many emerging applications. In this work, we aim to utilize automatically detected human attributes that contain semantic cues of the face photos to improve content-based face retrieval by constructing semantic codewords for efficient large-scale face retrieval. By leveraging human attributes in a scalable and systematic framework, we propose two orthogonal methods named attribute-enhanced sparse coding and attribute-embedded inverted indexing to improve the face retrieval in the offline and online stages. We investigate the effectiveness of different attributes and vital factors essential for face retrieval. Experimenting on two public datasets, the results show that the proposed methods can achieve up to 43.5% relative improvement in MAP compared to the existing methods.",Iscfaimreusatspco,98.0,42.0,5.0
3893,Image Retrieval,189.0,embedding neural networks for semantic association in content based image retrieval,3.0,201.0,1.0,118.0,3.0,2.2,172.5,96,https://www.researchgate.net/profile/Aun-Irtaza/publication/257627617_Embedding_neural_networks_for_semantic_association_in_content_based_image_retrieval/links/5551fc9a08aeaaff3befdefe/Embedding-neural-networks-for-semantic-association-in-content-based-image-retrieval.pdf,"Content based image retrieval (CBIR) systems provide potential solution of retrieving semantically similar images from large image repositories against any query image. The research community are competing for more effective ways of content based image retrieval, so they can be used in serving time critical applications in scientific and industrial domains. In this paper a Neural Network based architecture for content based image retrieval is presented. To enhance the capabilities of proposed work, an efficient feature extraction method is presented which is based on the concept of in-depth texture analysis. For this wavelet packets and Eigen values of Gabor filters are used for image representation purposes. To ensure semantically correct image retrieval, a partial supervised learning scheme is introduced which is based on K-nearest neighbors of a query image, and ensures the retrieval of images in a robust way. To elaborate the effectiveness of the presented work, the proposed method is compared with several existing CBIR systems, and it is proved that the proposed method has performed better then all of the comparative systems.",Iemnenefoseasincobaimre,61.0,43.0,5.0
3894,Image Retrieval,158.0,image retrieval system based on color layout descriptor and gabor filters,3.0,201.0,1.0,154.0,3.0,2.2,174.0,97,http://arxiv.org/abs/1605.04478v1,"The current paper presents a content-based image retrieval (CBIR) system using the image features extracted by a color layout descriptor (CLD) and Gabor texture descriptor. CLD represents the spatial distribution of colors with a few nonlinear quantized DCT coefficients of grid-based average colors, whereas the Gabor filter works as a bandpass filter for the local spatial frequency distribution. These two descriptors are very powerful for CBIR systems. Furthermore, combining the color and texture features in CBIR systems leads to more accurate results for image retrieval. To compare the performance of image retrieval method, average precision and recall are computed for all queries. The results showed an improved performance (higher precision and recall values) compared with the performance using other CBIR methods.",Iimresybaoncoladeangafi,87.0,20.0,6.0
3895,Image Retrieval,200.0,relevance feedback based on genetic programming for image retrieval,3.0,201.0,1.0,157.0,3.0,2.2,187.5,98,https://www.academia.edu/download/58631319/ferreira2011prl.pdf,"This paper presents two content-based image retrieval frameworks with relevance feedback based on genetic programming. The first framework exploits only the user indication of relevant images. The second one considers not only the relevant but also the images indicated as non-relevant. Several experiments were conducted to validate the proposed frameworks. These experiments employed three different image databases and color, shape, and texture descriptors to represent the content of database images. The proposed frameworks were compared, and outperformed six other relevance feedback methods regarding their effectiveness and efficiency in image retrieval tasks.",Irefebaongeprfoimre,82.0,53.0,1.0
3896,Image Retrieval,197.0,mammosys: a content-based image retrieval system using breast density patterns,3.0,201.0,1.0,191.0,3.0,2.2,196.8,99,https://www.academia.edu/download/34346276/CMPB_2010-xxx.pdf,"In this paper, we present a content-based image retrieval system designed to retrieve mammographies from large medical image database. The system is developed based on breast density, according to the four categories defined by the American College of Radiology, and is integrated to the database of the Image Retrieval in Medical Applications (IRMA) project, that provides images with classification ground truth. Two-dimensional principal component analysis is used in breast density texture characterization, in order to effectively represent texture and allow for dimensionality reduction. A support vector machine is used to perform the retrieval process. Average precision rates are in the range from 83% to 97% considering a data set of 5024 images. The results indicate the potential of the system as the first stage of a computer-aided diagnosis framework.",Imaacoimresyusbrdepa,88.0,32.0,4.0
3897,Image Retrieval,401.0,compounding the performance improvements of assembled techniques in a convolutional neural network,1.0,41.0,4.0,201.0,1.0,2.2,197.0,100,http://arxiv.org/pdf/2001.06268v2,"Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g. ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3\% to 82.78\%, mCE from 76.0\% to 48.9\% and mFR from 57.7\% to 32.3\% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at https://github.com/clovaai/assembled-cnn",Icothpeimofasteinaconene,35.0,41.0,4.0
4271,Image augmentation,3.0,image augmentation is all you need: regularizing deep reinforcement learning from pixels,5.0,16.0,5.0,13.0,5.0,5.0,11.2,1,https://arxiv.org/pdf/2004.13649,"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at this https URL.",Iimauisalyonerederelefrpi,149.0,73.0,40.0
4272,Image augmentation,30.0,data augmentation using random image cropping and patching for deep cnns,5.0,34.0,5.0,14.0,5.0,5.0,26.8,2,https://arxiv.org/pdf/1811.09030,"Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage of the soft labels. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of 2.19% on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet, an image-caption retrieval task using Microsoft COCO, and other computer vision tasks.",Idaauusraimcranpafodecn,89.0,49.0,5.0
4273,Image augmentation,27.0,learning raw image denoising with bayer pattern unification and bayer preserving augmentation,5.0,25.0,5.0,47.0,4.0,4.7,32.2,3,http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Liu_Learning_Raw_Image_Denoising_With_Bayer_Pattern_Unification_and_Bayer_CVPRW_2019_paper.pdf,"In this paper, we present new data pre-processing and augmentation techniques for DNN-based raw image denoising. Compared with traditional RGB image denoising, performing this task on direct camera sensor readings presents new challenges such as how to effectively handle various Bayer patterns from different data sources, and subsequently how to perform valid data augmentation with raw images. To address the first problem, we propose a Bayer pattern unification (BayerUnify) method to unify different Bayer patterns. This allows us to fully utilize a heterogeneous dataset to train a single denoising model instead of training one model for each pattern. Furthermore, while it is essential to augment the dataset to improve model generalization and performance, we discovered that it is error-prone to modify raw images by adapting augmentation methods designed for RGB images. Towards this end, we present a Bayer preserving augmentation (BayerAug) method as an effective approach for raw image augmentation. Combining these data processing technqiues with a modified U-Net, our method achieves a PSNR of 52.11 and a SSIM of 0.9969 in NTIRE 2019 Real Image Denoising Challenge, demonstrating the state-of-the-art performance.",Ileraimdewibapaunanbaprau,28.0,36.0,2.0
4274,Image augmentation,11.0,albumentations: fast and flexible image augmentations,5.0,3.0,5.0,99.0,4.0,4.7,34.2,4,https://www.mdpi.com/2078-2489/11/2/125/pdf,"Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.",Ialfaanflimau,349.0,92.0,10.0
4275,Image augmentation,41.0,autoaugment: learning augmentation policies from data,4.0,1.0,5.0,98.0,4.0,4.4,42.1,5,https://arxiv.org/pdf/1805.09501,"Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.",Iauleaupofrda,742.0,81.0,115.0
4276,Image augmentation,90.0,random erasing data augmentation,4.0,2.0,5.0,57.0,4.0,4.4,44.9,6,https://ojs.aaai.org/index.php/AAAI/article/view/7000/6854,"In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: this https URL.",Iraerdaau,1109.0,56.0,101.0
4277,Image augmentation,74.0,learn to augment: joint data augmentation and network optimization for text recognition,4.0,15.0,5.0,100.0,4.0,4.4,58.2,7,https://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Learn_to_Augment_Joint_Data_Augmentation_and_Network_Optimization_for_CVPR_2020_paper.pdf,"Handwritten text and scene text suffer from various shapes and distorted patterns. Thus training a robust recognition model requires a large amount of data to cover diversity as much as possible. In contrast to data collection and annotation, data augmentation is a low cost way. In this paper, we propose a new method for text image augmentation. Different from traditional augmentation methods such as rotation, scaling and perspective transformation, our proposed augmentation method is designed to learn proper and efï¬cient data augmentation which is more effective and specific for training a robust recognizer. By using a set of custom ï¬ducial points, the proposed augmentation method is ï¬‚exible and controllable. Furthermore, we bridge the gap between the isolated processes of data augmentation and network optimization by joint learning. An agent network learns from the output of the recognition network and controls the ï¬ducial points to generate more proper training samples for the recognition network. Extensive experiments on various benchmarks, including regular scene text, irregular scene text and handwritten text, show that the proposed augmentation and the joint learning methods signiï¬cantly boost the performance of the recognition networks. A general toolkit for geometric augmentation is available.",Iletoaujodaauanneopfotere,22.0,49.0,6.0
4278,Image augmentation,29.0,anatomical data augmentation via fluid-based image registration,5.0,22.0,5.0,201.0,1.0,3.8,77.8,8,https://arxiv.org/pdf/2007.02447,"In this paper, we present a novel approach for image retrieval based on extraction of low level features using techniques such as Directional Binary Code, Haar Wavelet transform and Histogram of Oriented Gradients. The DBC texture descriptor captures the spatial relationship between any pair of neighbourhood pixels in a local region along a given direction, while Local Binary Patterns descriptor considers the relationship between a given pixel and its surrounding neighbours. Therefore, DBC captures more spatial information than LBP and its variants, also it can extract more edge information than LBP. Hence, we employ DBC technique in order to extract grey level texture feature from each RGB channels individually and computed texture maps are further combined which represents colour texture features of an image. Then, we decomposed the extracted colour texture map and original image using Haar wavelet transform. Finally, we encode the shape and local features of wavelet transformed images using Histogram of Oriented Gradients for content based image retrieval. The performance of proposed method is compared with existing methods on two databases such as Wang's corel image and Caltech 256. The evaluation results show that our approach outperforms the existing methods for image retrieval.",Iandaauviflimre,5.0,42.0,0.0
4279,Image augmentation,81.0,data augmentation based malware detection using convolutional neural networks,4.0,24.0,5.0,201.0,1.0,3.5,94.2,9,http://arxiv.org/pdf/2010.01862v1,"Recently, cyber-attacks have been extensively seen due to the everlasting increase of malware in the cyber world. These attacks cause irreversible damage not only to end-users but also to corporate computer systems. Ransomware attacks such as WannaCry and Petya specifically targets to make critical infrastructures such as airports and rendered operational processes inoperable. Hence, it has attracted increasing attention in terms of volume, versatility, and intricacy. The most important feature of this type of malware is that they change shape as they propagate from one computer to another. Since standard signature-based detection software fails to identify this type of malware because they have different characteristics on each contaminated computer. This paper aims at providing an image augmentation enhanced deep convolutional neural network (CNN) models for the detection of malware families in a metamorphic malware environment. The main contributions of the paper's model structure consist of three components, including image generation from malware samples, image augmentation, and the last one is classifying the malware families by using a convolutional neural network model. In the first component, the collected malware samples are converted binary representation to 3-channel images using windowing technique. The second component of the system create the augmented version of the images, and the last component builds a classification model. In this study, five different deep convolutional neural network model for malware family detection is used.",Idaaubamadeusconene,9.0,38.0,0.0
4280,Image augmentation,401.0,learning data augmentation strategies for object detection,1.0,8.0,5.0,82.0,4.0,3.5,148.1,10,http://arxiv.org/pdf/1906.11172v1,"Data augmentation is a critical component of training deep learning models. Although data augmentation has been shown to significantly improve image classification, its potential has not been thoroughly investigated for object detection. Given the additional cost for annotating images for object detection, data augmentation may be of even greater importance for this computer vision task. In this work, we study the impact of data augmentation on object detection. We first demonstrate that data augmentation operations borrowed from image classification may be helpful for training detection models, but the improvement is limited. Thus, we investigate how learned, specialized data augmentation policies improve generalization performance for detection models. Importantly, these augmentation policies only affect training and leave a trained model unchanged during evaluation. Experiments on the COCO dataset indicate that an optimized data augmentation policy improves detection accuracy by more than +2.3 mAP, and allow a single inference model to achieve a state-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on COCO may be transferred unchanged to other detection datasets and models to improve predictive accuracy. For example, the best augmentation policy identified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our results also reveal that a learned augmentation policy is superior to state-of-the-art architecture regularization methods for object detection, even when considering strong baselines. Code for training with the learned policy is available online at this https URL",Iledaaustfoobde,156.0,67.0,8.0
4281,Image augmentation,1.0,biomedical image augmentation using augmentor,5.0,201.0,1.0,5.0,5.0,3.4,82.2,11,http://arxiv.org/pdf/1505.04597v1,"MOTIVATION
Image augmentation is a frequently used technique in computer vision and has been seeing increased interest since the popularity of deep learning. Its usefulness is becoming more and more recognised due to deep neural networks requiring larger amounts of data to train, and because in certain fields, such as biomedical imaging, large amounts of labelled data are difficult to come by or expensive to produce. In biomedical imaging, features specific to this domain need to be addressed.


RESULTS
Here we present the Augmentor software package for image augmentation. It provides a stochastic, pipeline-based approach to image augmentation with a number of features that are relevant to biomedical imaging, such as z-stack augmentation and randomised elastic distortions. The software has been designed to be highly extensible, meaning an operation that might be specific to a highly specialised task can easily be added to the library, even at runtime. Although it has been designed as a general software library, it has features that are particularly relevant to biomedical imaging and the techniques required for this domain.


AVAILABILITY
Augmentor is a Python package made available under the terms of the MIT licence. Source code can be found on GitHub under https://github.com/mdbloice/Augmentor and installation is via the pip package manager*.


SUPPLEMENTARY INFORMATION
The GitHub repository contains supplementary information, code examples, and Jupyter notebooks. Extensive documentation is hosted on Read the Docs under https://augmentor.readthedocs.io. For continuous integration tests see https://travis-ci.org/mdbloice/Augmentor.",Ibiimauusau,84.0,7.0,2.0
4282,Image augmentation,5.0,a survey on image data augmentation for deep learning,5.0,201.0,1.0,1.0,5.0,3.4,82.2,12,http://arxiv.org/pdf/2009.13120v2,"Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.",Iasuonimdaaufodele,1601.0,158.0,34.0
4283,Image augmentation,4.0,gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification,5.0,201.0,1.0,4.0,5.0,3.4,82.80000000000001,13,https://arxiv.org/pdf/1803.01229.pdf?source=post_page---------------------------,"Abstract Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results increased to 85.7% sensitivity and 92.4% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists’ efforts to improve diagnosis.",Igasymeimaufoincnpeinlilecl,634.0,43.0,13.0
4284,Image augmentation,9.0,an image augmentation approach using two-stage generative adversarial network for nuclei image segmentation,5.0,201.0,1.0,2.0,5.0,3.4,83.7,14,http://arxiv.org/pdf/1810.10850v2,"Abstract The major challenge in applying deep neural network techniques in the medical imaging domain is how to cope with small datasets and the limited amount of annotated samples. Data augmentation procedures that include conventional geometrical transformation based augmentation techniques and the recent image synthesis techniques using generative adversarial networks (GANs) can be employed to artificially increase the number of training images. This paper is focused on data augmentation for image segmentation task, which has an inherent challenge when compared to the conventional image classification task, due to its requirement to produce a corresponding mask for each generated image. To tackle the challenge of image-mask pair augmentation for image segmentation, this paper proposes a novel two-stage generative adversarial network. The proposed approach first employs a GAN to generate a synthesized binary mask, then incorporates this synthesized mask into the second GAN to perform a conditional generation of the synthesized image. Thus, these two GANs collaborate to generate the synthesized image-mask pairs, which are used to improve the performance of the conventional image segmentation approaches. The proposed approach is evaluated using the cell nuclei image segmentation task and demonstrates the superior performance to outperform both the traditional augmentation methods and the existing GAN-based augmentation methods in extensive results conducted using the benchmark Kaggle cell nuclei image segmentation dataset.",Ianimauapustwgeadnefonuimse,17.0,29.0,0.0
4285,Image augmentation,2.0,combining noise-to-image and image-to-image gans: brain mr image augmentation for tumor detection,5.0,201.0,1.0,12.0,5.0,3.4,84.6,15,https://ieeexplore.ieee.org/iel7/6287639/8600701/08869751.pdf,"Convolutional Neural Networks (CNNs) achieve excellent computer-assisted diagnosis with sufficient annotated training data. However, most medical imaging datasets are small and fragmented. In this context, Generative Adversarial Networks (GANs) can synthesize realistic/diverse additional training images to fill the data lack in the real image distribution; researchers have improved classification by augmenting data with noise-to-image (e.g., random noise samples to diverse pathological images) or image-to-image GANs (e.g., a benign image to a malignant one). Yet, no research has reported results combining noise-to-image and image-to-image GANs for further performance boost. Therefore, to maximize the DA effect with the GAN combinations, we propose a two-step GAN-based DA that generates and refines brain Magnetic Resonance (MR) images with/without tumors separately: ( ${i}$ ) Progressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN for high-resolution MR image generation, first generates realistic/diverse $256\times 256$ images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT) that combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focused GAN loss, further refines the texture/shape of the PGGAN-generated images similarly to the real ones. We thoroughly investigate CNN-based tumor classification results, also considering the influence of pre-training on ImageNet and discarding weird-looking GAN-generated images. The results show that, when combined with classic DA, our two-step GAN-based DA can significantly outperform the classic DA alone, in tumor detection (i.e., boosting sensitivity 93.67% to 97.48%) and also in other medical imaging tasks.",Iconoanimgabrmrimaufotude,63.0,54.0,1.0
4286,Image augmentation,13.0,synthesizing diverse lung nodules wherever massively: 3d multi-conditional gan-based ct image augmentation for object detection,5.0,201.0,1.0,8.0,5.0,3.4,86.70000000000002,16,https://arxiv.org/pdf/1906.04962,"Accurate Computer-Assisted Diagnosis, relying on large-scale annotated pathological images, can alleviate the risk of overlooking the diagnosis. Unfortunately, in medical imaging, most available datasets are small/fragmented. To tackle this, as a Data Augmentation (DA) method, 3D conditional Generative Adversarial Networks (GANs) can synthesize desired realistic/diverse 3D images as additional training data. However, no 3D conditional GAN-based DA approach exists for general bounding box-based 3D object detection, while it can locate disease areas with physicians' minimum annotation cost, unlike rigorous 3D segmentation. Moreover, since lesions vary in position/size/attenuation, further GAN-based DA performance requires multiple conditions. Therefore, we propose 3D Multi-Conditional GAN (MCGAN) to generate realistic/diverse 32 × 32 × 32 nodules placed naturally on lung Computed Tomography images to boost sensitivity in 3D object detection. Our MCGAN adopts two discriminators for conditioning: the context discriminator learns to classify real vs synthetic nodule/surrounding pairs with noise box-centered surroundings; the nodule discriminator attempts to classify real vs synthetic nodules with size/attenuation conditions. The results show that 3D Convolutional Neural Network-based detection can achieve higher sensitivity under any nodule size/attenuation at fixed False Positive rates and overcome the medical data paucity with the MCGAN-generated realistic nodules–even expert physicians fail to distinguish them from the real ones in Visual Turing Test.",Isydilunowhma3dmugactimaufoobde,45.0,43.0,1.0
4287,Image augmentation,23.0,improved image augmentation for convolutional neural networks by copyout and copypairing,5.0,53.0,4.0,201.0,1.0,3.4,88.4,17,https://arxiv.org/pdf/1909.00390,"The identification of lesion within medical image data is necessary for diagnosis, treatment and prognosis. Segmentation and classification approaches are mainly based on supervised learning with well-paired image-level or voxel-level labels. However, labeling the lesion in medical images is laborious requiring highly specialized knowledge. We propose a medical image synthesis model named abnormal-to-normal translation generative adversarial network (ANT-GAN) to generate a normal-looking medical image based on its abnormal-looking counterpart without the need for paired training data. Unlike typical GANs, whose aim is to generate realistic samples with variations, our more restrictive model aims at producing a normal-looking image corresponding to one containing lesions, and thus requires a special design. Being able to provide a ""normal"" counterpart to a medical image can provide useful side information for medical imaging tasks like lesion segmentation or classification validated by our experiments. In the other aspect, the ANT-GAN model is also capable of producing highly realistic lesion-containing image corresponding to the healthy one, which shows the potential in data augmentation verified in our experiments.",Iimimaufoconenebycoanco,1.0,9.0,0.0
4288,Image augmentation,8.0,image augmentation using radial transform for training deep neural networks,5.0,201.0,1.0,24.0,5.0,3.4,90.00000000000001,18,"https://arxiv.org/pdf/1708.04347.pdf,","Deep learning models have a large number of free parameters that must be estimated by efficient training of the models on a large number of training data samples to increase their generalization performance. In real-world applications, the data available to train these networks is often limited or imbalanced. We propose a sampling method based on the radial transform in a polar coordinate system for image augmentation to facilitate the training of deep learning models from limited source data. This pixel-wise transform provides representations of the original image in the polar coordinate system by generating a new image from each pixel. This technique can generate radial transformed images up to the number of pixels in the original image to increase the diversity of poorly represented image classes. Our experiments show improved generalization performance in training deep convolutional neural networks with radial transformed images.",Iimauusratrfotrdenene,35.0,24.0,0.0
4289,Image augmentation,19.0,detection of diabetic retinopathy and maculopathy in eye fundus images using deep learning and image augmentation,5.0,201.0,1.0,20.0,5.0,3.4,92.1,19,http://arxiv.org/pdf/2003.02261v1,"Diabetic retinopathy is a significant complication of diabetes, produced by high blood sugar level, which causes damage to the retina. Effective diabetic retinopathy screening is required because diabetic retinopathy does not show any symptoms in the initial stages, and can cause blindness if it is not diagnosed and treated promptly. This paper presents a novel diabetic retinopathy automatic detection in retinal images by implementing efficient image processing and deep learning techniques. Besides diabetic retinopathy detection, the developed system integrates a novel detection of maculopathy into one detection system. Maculopathy is the damage to the macula, the eye part that is responsible for central vision. Therefore, the combined detection of diabetic retinopathy and maculopathy is essential for an effective screening of diabetic retinopathy. The paper investigates the capability of image pre-processing techniques based on data augmentation as well as deep learning for diabetic retinopathy and maculopathy detection. Computer-assisted clinical decision-making is inevitably transforming the diabetic retinopathy detection and management today, which is crucial for clinicians and patients alike. Therefore, a high degree of accuracy, with which computer algorithms can detect the diabetic retinopathy and maculopathy, is absolutely necessary.",Ideofdireanmaineyfuimusdeleanimau,13.0,47.0,0.0
4290,Image augmentation,20.0,rethinking data augmentation for image super-resolution: a comprehensive analysis and a new strategy,5.0,201.0,1.0,19.0,5.0,3.4,92.1,20,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yoo_Rethinking_Data_Augmentation_for_Image_Super-resolution_A_Comprehensive_Analysis_and_CVPR_2020_paper.pdf,"Data augmentation is an effective way to improve the performance of deep networks. Unfortunately, current methods are mostly developed for high-level vision tasks (e.g., classification) and few are studied for low-level vision tasks (e.g., image restoration). In this paper, we provide a comprehensive analysis of the existing augmentation methods applied to the super-resolution task. We find that the methods discarding or manipulating the pixels or features too much hamper the image restoration, where the spatial relationship is very important. Based on our analyses, we propose CutBlur that cuts a low-resolution patch and pastes it to the corresponding high-resolution image region and vice versa. The key intuition of CutBlur is to enable a model to learn not only ""how"" but also ""where"" to super-resolve an image. By doing so, the model can understand ""how much"", instead of blindly learning to apply super-resolution to every given pixel. Our method consistently and significantly improves the performance across various scenarios, especially when the model size is big and the data is collected under real-world environments. We also show that our method improves other low-level vision tasks, such as denoising and compression artifact removal.",Iredaaufoimsuacoanananest,32.0,45.0,5.0
4291,Image augmentation,10.0,the effectiveness of data augmentation in image classification using deep learning,5.0,201.0,1.0,37.0,5.0,3.4,94.5,21,https://arxiv.org/pdf/1712.04621.pdf?source=post_page---------------------------,"In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.",Ithefofdaauinimclusdele,1411.0,17.0,29.0
4292,Image augmentation,25.0,a full stage data augmentation method in deep convolutional neural network for natural image classification,5.0,201.0,1.0,27.0,5.0,3.4,96.0,22,http://arxiv.org/pdf/1712.00967v1,"Nowadays, deep learning has achieved remarkable results in many computer vision related tasks, among which the support of big data is essential. In this paper, we propose a full stage data augmentation framework to improve the accuracy of deep convolutional neural networks, which can also play the role of implicit model ensemble without introducing additional model training costs. Simultaneous data augmentation during training and testing stages can ensure network optimization and enhance its generalization ability. Augmentation in two stages needs to be consistent to ensure the accurate transfer of specific domain information. Furthermore, this framework is universal for any network architecture and data augmentation strategy and therefore can be applied to a variety of deep learning based tasks. Finally, experimental results about image classification on the coarsegrained dataset CIFAR-10 (93.41%) and fine-grained dataset CIFAR-100 (70.22%) demonstrate the effectiveness of the framework by comparing with state-of-the-art results.",Iafustdaaumeindeconenefonaimcl,17.0,48.0,1.0
4293,Image augmentation,35.0,automatic data augmentation for 3d medical image segmentation,5.0,201.0,1.0,39.0,5.0,3.4,102.6,23,https://arxiv.org/pdf/2010.11695,"Data augmentation is an effective and universal technique for improving generalization performance of deep neural networks. It could enrich diversity of training samples that is essential in medical image segmentation tasks because 1) the scale of medical image dataset is typically smaller, which may increase the risk of overfitting; 2) the shape and modality of different objects such as organs or tumors are unique, thus requiring customized data augmentation policy. However, most data augmentation implementations are hand-crafted and suboptimal in medical image processing. To fully exploit the potential of data augmentation, we propose an efficient algorithm to automatically search for the optimal augmentation strategies. We formulate the coupled optimization w.r.t. network weights and augmentation parameters into a differentiable form by means of stochastic relaxation. This formulation allows us to apply alternative gradient-based methods to solve it, i.e. stochastic natural gradient method with adaptive step-size. To the best of our knowledge, it is the first time that differentiable automatic data augmentation is employed in medical image segmentation tasks. Our numerical experiments demonstrate that the proposed approach significantly outperforms existing build-in data augmentation of state-of-the-art models.",Iaudaaufo3dmeimse,11.0,24.0,0.0
4294,Image augmentation,401.0,simple copy-paste is a strong data augmentation method for instance segmentation,1.0,7.0,5.0,132.0,3.0,3.2,162.7,24,http://arxiv.org/pdf/2012.07177v2,"Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (e.g., [13], [12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.1",Isicoisastdaaumefoinse,68.0,78.0,7.0
4295,Image augmentation,401.0,learning to compose domain-specific transformations for data augmentation,1.0,17.0,5.0,187.0,3.0,3.2,183.2,25,http://arxiv.org/pdf/1709.01643v3,"Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.",Iletocodotrfodaau,200.0,37.0,11.0
4296,Image augmentation,7.0,the effectiveness of image augmentation in deep learning networks for detecting covid-19: a geometric transformation perspective,5.0,201.0,1.0,43.0,4.0,3.1,95.4,26,http://arxiv.org/pdf/2108.01625v1,"Chest X-ray imaging technology used for the early detection and screening of COVID-19 pneumonia is both accessible worldwide and affordable compared to other non-invasive technologies. Additionally, deep learning methods have recently shown remarkable results in detecting COVID-19 on chest X-rays, making it a promising screening technology for COVID-19. Deep learning relies on a large amount of data to avoid overfitting. While overfitting can result in perfect modeling on the original training dataset, on a new testing dataset it can fail to achieve high accuracy. In the image processing field, an image augmentation step (i.e., adding more training data) is often used to reduce overfitting on the training dataset, and improve prediction accuracy on the testing dataset. In this paper, we examined the impact of geometric augmentations as implemented in several recent publications for detecting COVID-19. We compared the performance of 17 deep learning algorithms with and without different geometric augmentations. We empirically examined the influence of augmentation with respect to detection accuracy, dataset diversity, augmentation methodology, and network size. Contrary to expectation, our results show that the removal of recently used geometrical augmentation steps actually improved the Matthews correlation coefficient (MCC) of 17 models. The MCC without augmentation (MCC = 0.51) outperformed four recent geometrical augmentations (MCC = 0.47 for Data Augmentation 1, MCC = 0.44 for Data Augmentation 2, MCC = 0.48 for Data Augmentation 3, and MCC = 0.49 for Data Augmentation 4). When we retrained a recently published deep learning without augmentation on the same dataset, the detection accuracy significantly increased, with a χMcNemar′s statistic2=163.2 and a p-value of 2.23 × 10−37. This is an interesting finding that may improve current deep learning algorithms using geometrical augmentations for detecting COVID-19. We also provide clinical perspectives on geometric augmentation to consider regarding the development of a robust COVID-19 X-ray-based detector.",Ithefofimauindelenefodecoagetrpe,8.0,53.0,0.0
4297,Image augmentation,6.0,data augmentation for improving deep learning in image classification problem,5.0,201.0,1.0,55.0,4.0,3.1,98.7,27,https://www.researchgate.net/profile/Agnieszka-Mikolajczyk-3/publication/325920702_Data_augmentation_for_improving_deep_learning_in_image_classification_problem/links/5d5d5569458515210257607c/Data-augmentation-for-improving-deep-learning-in-image-classification-problem.pdf,"These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.",Idaaufoimdeleinimclpr,394.0,44.0,5.0
4298,Image augmentation,49.0,realistic adversarial data augmentation for mr image segmentation,4.0,201.0,1.0,28.0,5.0,3.1,103.5,28,https://arxiv.org/pdf/2006.13322,"Neural network-based approaches can achieve high accuracy in various medical image segmentation tasks. However, they generally require large labelled datasets for supervised learning. Acquiring and manually labelling a large medical dataset is expensive and sometimes impractical due to data sharing and privacy issues. In this work, we propose an adversarial data augmentation method for training neural networks for medical image segmentation. Instead of generating pixel-wise adversarial attacks, our model generates plausible and realistic signal corruptions, which models the intensity inhomogeneities caused by a common type of artefacts in MR imaging: bias field. The proposed method does not rely on generative networks, and can be used as a plug-in module for general segmentation networks in both supervised and semi-supervised learning. Using cardiac MR imaging we show that such an approach can improve the generalization ability and robustness of models as well as provide significant improvements in low-data scenarios.",Ireaddaaufomrimse,20.0,38.0,0.0
4299,Image augmentation,28.0,medical image synthesis for data augmentation and anonymization using generative adversarial networks,5.0,201.0,1.0,52.0,4.0,3.1,104.4,29,https://arxiv.org/pdf/1807.10225,"Data diversity is critical to success when training deep learning models. Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models. In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI. We demonstrate two unique benefits that the synthetic images provide. First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation. Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data. Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.",Imeimsyfodaauananusgeadne,275.0,21.0,9.0
4300,Image augmentation,39.0,analysis and best parameters selection for person recognition based on gait model using cnn algorithm and image augmentation,5.0,201.0,1.0,42.0,4.0,3.1,104.7,30,http://arxiv.org/abs/1809.06653v2,"Person Recognition based on Gait Model (PRGM) and motion features is are indeed a challenging and novel task due to their usages and to the critical issues of human pose variation, human body occlusion, camera view variation, etc. In this project, a deep convolution neural network (CNN) was modified and adapted for person recognition with Image Augmentation (IA) technique depending on gait features. Adaptation aims to get best values for CNN parameters to get best CNN model. In Addition to the CNN parameters Adaptation, the design of CNN model itself was adapted to get best model structure; Adaptation in the design was affected the type, the number of layers in CNN and normalization between them. After choosing best parameters and best design, Image augmentation was used to increase the size of train dataset with many copies of the image to boost the number of different images that will be used to train Deep learning algorithms. The tests were achieved using known dataset (Market dataset). The dataset contains sequential pictures of people in different gait status. The image in CNN model as matrix is extracted to many images or matrices by the convolution, so dataset size may be bigger by hundred times to make the problem a big data issue. In this project, results show that adaptation has improved the accuracy of person recognition using gait model comparing to model without adaptation. In addition, dataset contains images of person carrying things. IA technique improved the model to be robust to some variations such as image dimensions (quality and resolution), rotations and carried things by persons. Results for 200 persons recognition, validation accuracy was about 82% without IA and 96.23 with IA. For 800 persons recognition, validation accuracy was 93.62% without IA.",Iananbepasefoperebaongamouscnalanimau,9.0,38.0,0.0
4301,Image augmentation,37.0,data augmentation for hyperspectral image classification with deep cnn,5.0,201.0,1.0,46.0,4.0,3.1,105.3,31,https://my.ece.msstate.edu/faculty/du/GRSL-DataAugment.pdf,"Convolutional neural network (CNN) has been widely used in hyperspectral imagery (HSI) classification. Data augmentation is proven to be quite effective when training data size is relatively small. In this letter, extensive comparison experiments are conducted with common data augmentation methods, which draw an observation that common methods can produce a limited and up-bounded performance. To address this problem, a new data augmentation method, named as pixel-block pair (PBP), is proposed to greatly increase the number of training samples. The proposed method takes advantage of deep CNN to extract PBP features, and decision fusion is utilized for final label assignment. Experimental results demonstrate that the proposed method can outperform the existing ones.",Idaaufohyimclwidecn,43.0,26.0,1.0
4302,Image augmentation,18.0,a new image classification method using cnn transfer learning and web data augmentation,5.0,201.0,1.0,66.0,4.0,3.1,105.6,32,http://arxiv.org/pdf/2103.05110v1,"Abstract Since Convolutional Neural Network (CNN) won the image classification competition 202 (ILSVRC12), a lot of attention has been paid to deep layer CNN study. The success of CNN is attributed to its superior multi-scale high-level image representations as opposed to hand-engineering low-level features. However, estimating millions of parameters of a deep CNN requires a large number of annotated samples, which currently prevents many superior deep CNNs (such as AlexNet, VGG, ResNet) being applied to problems with limited training data. To address this problem, a novel two-phase method combining CNN transfer learning and web data augmentation is proposed. With our method, the useful feature presentation of pre-trained network can be efficiently transferred to target task, and the original dataset can be augmented with the most valuable Internet images for classification. Our method not only greatly reduces the requirement of a large training data, but also effectively expand the training dataset. Both of method features contribute to the considerable over-fitting reduction of deep CNNs on small dataset. In addition, we successfully apply Bayesian optimization to solve the tuff problem, hyper-parameter tuning, in network fine-tuning. Our solution is applied to six public small datasets. Extensive experiments show that, comparing to traditional methods, our solution can assist the popular deep CNNs to achieve better performance. Particularly, ResNet can outperform all the state-of-the-art models on six small datasets. The experiment results prove that the proposed solution will be the great tool for dealing with practice problems which are related to use deep CNNs on small dataset.",Ianeimclmeuscntrleanwedaau,191.0,44.0,4.0
4303,Image augmentation,34.0,image block augmentation for one-shot learning,5.0,201.0,1.0,51.0,4.0,3.1,105.9,33,https://ojs.aaai.org/index.php/AAAI/article/download/4212/4090,"Given one or a few training instances of novel classes, oneshot learning task requires that the classifier generalizes to these novel classes. Directly training one-shot classifier may suffer from insufficient training instances in one-shot learning. Previous one-shot learning works investigate the metalearning or metric-based algorithms; in contrast, this paper proposes a Self-Training Jigsaw Augmentation (Self-Jig) method for one-shot learning. Particularly, we solve one-shot learning by directly augmenting the training images through leveraging the vast unlabeled instances. Precisely our proposed Self-Jig algorithm can synthesize new images from the labeled probe and unlabeled gallery images. The labels of gallery images are predicted to help the augmentation process, which can be taken as a self-training scheme. Intrinsically, we argue that we provide a very useful way of directly generating massive amounts of training images for novel classes. Extensive experiments and ablation study not only evaluate the efficacy but also reveal the insights, of the proposed Self-Jig method.",Iimblaufoonle,29.0,36.0,1.0
4304,Image augmentation,16.0,research on data augmentation for image classification based on convolution neural networks,5.0,201.0,1.0,81.0,4.0,3.1,109.5,34,http://arxiv.org/pdf/1904.09737v3,"The performance of deep convolution neural networks will be further enhanced with the expansion of the training data set. For the image classification tasks, it is necessary to expand the insufficient training image samples through various data augmentation methods. This paper explores the impact of various data augmentation methods on image classification tasks with deep convolution Neural network, in which Alexnet is employed as the pre-training network model and a subset of CIFAR10 and ImageNet (10 categories) are selected as the original data set. The data augmentation methods used in this paper include: GAN/WGAN, Flipping, Cropping, Shifting, PCA jittering, Color jittering, Noise, Rotation, and some combinations. Experimental results show that, under the same condition of multiple increasing, the performance evaluation on small-scale data sets is more obvious, the four individual methods (Cropping, Flipping, WGAN, Rotation) perform generally better than others, and some appropriate combination methods are slightly more effective than the individuals.",Ireondaaufoimclbaonconene,86.0,12.0,2.0
4305,Image augmentation,26.0,tracking by detection for interactive image augmentation in laparoscopy,5.0,201.0,1.0,73.0,4.0,3.1,110.1,35,https://cs.adelaide.edu.au/~jaehak/paper/Kim_et_al_WBIR2012.pdf,"We present a system for marking, tracking and visually augmenting a deformable surgical site by the robust automatic detection of natural landmarks (image features) in laparoscopic surgery. In our system, the surgeon first selects a frame containing an organ of interest, and this is used by our system both to detect every instance of the organ in a laparoscopic video feed, and to recover the nonrigid deformations. The system then augments the video with customizable visual information such as the location of hidden or weakly visible structures (cysts, vessels, etc), or planned incision points, acquired from pre-operative or intra-operative data. Frame-rate organ detection is performed via a novel procedure that matches the current frame to the reference frame. Because laparoscopic images are known to be extremely difficult to match, we propose to use Shape-from-Shading and conformal flattening to cancel out much of the variation in appearance due to perspective foreshortening, and we then apply robust matching to the flattened surfaces. Experiments show robust tracking and detection results on a laparoscopic procedure with the uterus as target organ. As our system detects the organ in every frame, it is not impaired by target loss, contrary to most previous methods.",Itrbydefoinimauinla,22.0,20.0,3.0
4306,Image augmentation,36.0,data augmentation on plant leaf disease image dataset using image manipulation and deep learning techniques,5.0,201.0,1.0,63.0,4.0,3.1,110.1,36,http://arxiv.org/pdf/1807.10931v3,"A large volume of data will increase the performance of machine learning algorithms and avoid overfitting problems. Collecting a large amount of training data in the agricultural field for designing plant leaf disease detection and diagnosis model is a highly challenging task which takes more time and resources. Data augmentation increases the diversity of training data for machine learning algorithms without collecting new data. In this article, augmented plant leaf disease datasets was developed using basic image manipulation and deep learning based image augmentation techniques such as Image flipping, cropping, rotation, color transformation, PCA color augmentation, noise injection, Generative Adversarial Networks (GANs) and Neural Style Transfer (NST) techniques. Performance of the data augmentation techniques was studied using state-of-the-art transfer learning techniques, for instance, VGG16, ResNet, and InceptionV3. An extensive simulation shows that the augmented dataset using GAN and NST techniques achieves better accuracy than the original dataset using a basic image manipulation based augmented dataset. Furthermore, a combination of Deep learning, Color, and Position augmentation dataset gives the maximum classification performance than all other datasets.",Idaauonpllediimdausimmaandelete,9.0,28.0,0.0
4307,Image augmentation,15.0,data-driven color augmentation techniques for deep skin image analysis,5.0,201.0,1.0,84.0,4.0,3.1,110.1,37,https://arxiv.org/pdf/1703.03702,"Dermoscopic skin images are often obtained with different imaging devices, under varying acquisition conditions. In this work, instead of attempting to perform intensity and color normalization, we propose to leverage computational color constancy techniques to build an artificial data augmentation technique suitable for this kind of images. Specifically, we apply the \emph{shades of gray} color constancy technique to color-normalize the entire training set of images, while retaining the estimated illuminants. We then draw one sample from the distribution of training set illuminants and apply it on the normalized image. We employ this technique for training two deep convolutional neural networks for the tasks of skin lesion segmentation and skin lesion classification, in the context of the ISIC 2017 challenge and without using any external dermatologic image set. Our results on the validation set are promising, and will be supplemented with extended results on the hidden test set when available.",Idacoautefodeskiman,42.0,22.0,1.0
4308,Image augmentation,33.0,deep feature augmentation for occluded image classification,5.0,201.0,1.0,83.0,4.0,3.1,115.20000000000002,38,https://arxiv.org/pdf/2011.00768,"Abstract Due to the difficulty in acquiring massive task-specific occluded images, the classification of occluded images with deep convolutional neural networks (CNNs) remains highly challenging. To alleviate the dependency on large-scale occluded image datasets, we propose a novel approach to improve the classification accuracy of occluded images by fine-tuning the pre-trained models with a set of augmented deep feature vectors (DFVs). The set of augmented DFVs is composed of original DFVs and pseudo-DFVs. The pseudo-DFVs are generated by randomly adding difference vectors (DVs), extracted from a small set of clean and occluded image pairs, to the real DFVs. In the fine-tuning, the back-propagation is conducted on the DFV data flow to update the network parameters. The experiments on various datasets and network structures show that the deep feature augmentation significantly improves the classification accuracy of occluded images without a noticeable influence on the performance of clean images. Specifically, on the ILSVRC2012 dataset with synthetic occluded images, the proposed approach achieves 11.21% and 9.14% average increases in classification accuracy for the ResNet50 networks fine-tuned on the occlusion-exclusive and occlusion-inclusive training sets, respectively.",Idefeaufoocimcl,18.0,53.0,0.0
4310,Image augmentation,52.0,end-to-end automatic image annotation based on deep cnn and multi-label data augmentation,4.0,201.0,1.0,50.0,4.0,2.8,111.0,39,http://arxiv.org/pdf/1904.00838v3,"Automatic image annotation is a key step in image retrieval and image understanding. In this paper, we present an end-to-end automatic image annotation method based on a deep convolutional neural network (CNN) and multi-label data augmentation. Different from traditional annotation models that usually perform feature extraction and annotation as two independent tasks, we propose an end-to-end automatic image annotation model based on deep CNN (E2E-DCNN). E2E-DCNN transforms the image annotation problem into a multi-label learning problem. It uses a deep CNN structure to carry out the adaptive feature learning before constructing the end-to-end annotation structure using multiple cross-entropy loss functions for training. It is difficult to train a deep CNN model using small-scale datasets or scale up multi-label datasets using traditional data augmentation methods; hence, we propose a multi-label data augmentation method based on Wasserstein generative adversarial networks (ML-WGAN). The ML-WGAN generator can approximate the data distribution of a single multi-label image. The images generated by ML-WGAN can assist in the reduction of the over-fitting problem of training a deep CNN model and enhance the generalization ability of the trained CNN model. We optimize the network structure by using deformable convolution and spatial pyramid pooling. We experiment the proposed E2E-DCNN model with data augmentation by the proposed ML-WGAN on several public datasets. The experimental results demonstrate that the proposed model outperforms the state-of-the-art automatic image annotation models.",Ienauimanbaondecnanmudaau,31.0,49.0,1.0
4311,Image augmentation,53.0,hyperspectral image classification using random occlusion data augmentation,4.0,201.0,1.0,53.0,4.0,2.8,112.20000000000002,40,https://www.umbc.edu/rssipl/people/aplaza/Papers/Journals/2019.GRSL.Occlusion.pdf,"Convolutional neural networks (CNNs) have become a powerful tool for remotely sensed hyperspectral image (HSI) classification due to their great generalization ability and high accuracy. However, owing to the huge amount of parameters that need to be learned and to the complex nature of HSI data itself, these approaches must deal with the important problem of overfitting, which can lead to inadequate generalization and loss of accuracy. In order to mitigate this problem, in this letter, we adopt random occlusion, a recently developed data augmentation (DA) method for training CNNs, in which the pixels of different rectangular spatial regions in the HSI are randomly occluded, generating training images with various levels of occlusion and reducing the risk of overfitting. Our results with two well-known HSIs reveal that the proposed method helps to achieve better classification accuracy with low computational cost.",Ihyimclusraocdaau,25.0,22.0,0.0
4312,Image augmentation,47.0,data augmentation via image registration,4.0,201.0,1.0,68.0,4.0,2.8,114.9,41,http://arxiv.org/pdf/2007.02447v1,"Data augmentation helps improve generalization of deep neural networks, and can be perceived as implicit regularization. It is pivotal in scenarios in which the amount of ground-truth data is limited, and acquiring new examples is costly and time-consuming. This is a common problem in medical image analysis, especially tumor delineation—in this paper, we focus on brain-tumor segmentation from magnetic resonance imaging (MRI), and propose a novel augmentation technique which exploits image registration to benefit from subtle spatial and/or tissue characteristics captured within the training set. We used a set of MRI scans of 44 low-grade glioma patients, augmented it using the proposed technique, and exploited it to train U-Net-based deep networks. The results show that our augmentation delivers statistically important boost of performance without sacrificing inference speed.",Idaauviimre,9.0,19.0,0.0
4313,Image augmentation,42.0,ricap: random image cropping and patching data augmentation for deep cnns,4.0,201.0,1.0,75.0,4.0,2.8,115.5,42,http://proceedings.mlr.press/v95/takahashi18a/takahashi18a.pdf,"Deep convolutional neural networks (CNNs) have demonstrated remarkable results in image recognition owing to their rich expression ability and numerous parameters. However, an excessive expression ability compared to the variety of training images often has a risk of overfitting. Data augmentation techniques have been proposed to address this problem as they enrich datasets by flipping, cropping, resizing, and color-translating images. They enable deep CNNs to achieve an impressive performance. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP), which randomly crops four images and patches them to construct a new training image. Hence, RICAP randomly picks up subsets of original features among the four images and discard others, enriching the variety of training images. Also, RICAP mixes the class labels of the four images and enjoys a benefit similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., shake-shake regularization model) and achieved a new state-of-the-art test error of 2.23% on CIFAR-10 among competitive data augmentation techniques such as cutout and mixup. We also confirmed that deep CNNs with RICAP achieved better results on CIFAR-100 and ImageNet than those results obtained by other techniques.",Iriraimcranpadaaufodecn,43.0,28.0,7.0
4314,Image augmentation,69.0,privacy-preserving deep neural networks with pixel-based image encryption considering data augmentation in the encrypted domain,4.0,201.0,1.0,48.0,4.0,2.8,115.5,43,https://arxiv.org/pdf/1905.01827,"We present a novel privacy-preserving scheme for deep neural networks (DNNs) that enables us not to only apply images without visual information to DNNs for both training and testing but to also consider data augmentation in the encrypted domain for the first time. In this paper, a novel pixel-based image encryption method is first proposed for privacy-preserving DNNs. In addition, a novel adaptation network is considered that reduces the influence of image encryption. In an experiment, the proposed method is applied to a well-known network, ResNet-18, for image classification. The experimental results demonstrate that conventional privacy-preserving machine learning methods including the state-of-the-arts cannot be applied to data augmentation in the encrypted domain and that the proposed method outperforms them in terms of classification accuracy.",Iprdenenewipiimencodaauinthendo,37.0,28.0,1.0
4315,Image augmentation,43.0,data augmentation via latent space interpolation for image classification,4.0,201.0,1.0,76.0,4.0,2.8,116.1,44,https://www.researchgate.net/profile/Xiaofeng-Liu-33/publication/329312472_Data_Augmentation_via_Latent_Space_Interpolation_for_Image_Classification/links/5c0898aca6fdcc494fdcadbd/Data-Augmentation-via-Latent-Space-Interpolation-for-Image-Classification.pdf,"Effective training of the deep neural networks requires much data to avoid underdetermined and poor generalization. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, cropping a patch from the original samples. In this paper, we introduce the adversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification. As a possible “recognition via generation” framework, it has potentials for several other classification tasks. Our experiments on the ILSVRC 2012, CIFAR-10 datasets show that the latent space interpolation (LSI) improves the generalization and performance of state-of-the-art deep neural networks.",Idaauvilaspinfoimcl,41.0,41.0,2.0
4316,Image augmentation,112.0,the influence of image augmentation on breast lesion classification using transfer learning,3.0,201.0,1.0,9.0,5.0,2.8,116.7,45,http://arxiv.org/pdf/1804.02119v1,"Transfer learning and data augmentation techniques play an essential role in implementing an effective deep learning model when there is a lack of sufficient training samples. Popular pre-trained networks such as AlexNet, VGG, GoogLeNet, and ResNet showed high level of accuracy on many image classification tasks, however these networks were trained on general purpose datasets such as ImageNet which does not contain labeled images related to breast lesions. In this paper, we have fine-tuned the pre-trained VGG-19 network on breast lesions images before and after applying the augmentation methods to differentiate between benign and malignant type of cancers. Image augmentation methods are carefully chosen to further overcome the lack of breast x-ray images, improve the classification accuracy, and reduce over-fitting by generating new training samples based on the existing breast x-ray images modifications. These augmentation methods include rotation, flipping, zooming, brightness, and contrast. The performance of VGG-19 classification is compared with similar pre-trained models used in the literature such as ResNet50 and VGG-16. Our approach surpassed the previous approaches with classification accuracy, sensitivity, specificity, and AUC of 90.4%, 94.01%, 85.9%, and 95.9% respectively.",Ithinofimauonbrleclustrle,3.0,15.0,0.0
4317,Image augmentation,59.0,a preliminary study on data augmentation of deep learning for image classification,4.0,201.0,1.0,67.0,4.0,2.8,118.2,46,https://arxiv.org/pdf/1906.11887,"Deep learning models have a large number of free parameters that need to be calculated by effective training of the models on a great deal of training data to improve their generalization performance. However, data obtaining and labeling is expensive in practice. Data augmentation is one of the methods to alleviate this problem. In this paper, we conduct a preliminary study on how four variables (augmentation method, augmentation rate, size of basic dataset per label, and method combination) can affect the accuracy of deep learning for image classification. The study provides some guidelines: (1) altering the geometry of the images is not always better than those just lighting and color. (2) 2-3 times augmentation rate is good enough for training. (3) the combination of two geometry methods degrade the performance, while combinations with at least one photometric method, will improve the performance, especially when one method is a photometric method and another is a geometry method. (4) the sequence of methods in combination has little effect on the performance.",Iaprstondaauofdelefoimcl,13.0,39.0,0.0
4318,Image augmentation,65.0,synthetic augmentation and feature-based filtering for improved cervical histopathology image classification,4.0,201.0,1.0,64.0,4.0,2.8,119.1,47,https://arxiv.org/pdf/1907.10655,"Cervical intraepithelial neoplasia (CIN) grade of histopathology images is a crucial indicator in cervical biopsy results. Accurate CIN grading of epithelium regions helps pathologists with precancerous lesion diagnosis and treatment planning. Although an automated CIN grading system has been desired, supervised training of such a system would require a large amount of expert annotations, which are expensive and time-consuming to collect. In this paper, we investigate the CIN grade classification problem on segmented epithelium patches. We propose to use conditional Generative Adversarial Networks (cGANs) to expand the limited training dataset, by synthesizing realistic cervical histopathology images. While the synthetic images are visually appealing, they are not guaranteed to contain meaningful features for data augmentation. To tackle this issue, we propose a synthetic-image filtering mechanism based on the divergence in feature space between generated images and class centroids in order to control the feature quality of selected synthetic images for data augmentation. Our models are evaluated on a cervical histopathology image dataset with a limited number of patch-level CIN grade annotations. Extensive experimental results show a significant improvement of classification accuracy from 66.3% to 71.7% using the same ResNet18 baseline classifier after leveraging our cGAN generated images with feature-based filtering, which demonstrates the effectiveness of our models.",Isyauanfefifoimcehiimcl,16.0,17.0,0.0
4319,Image augmentation,44.0,training cnns for image registration from few samples with model-based data augmentation,4.0,201.0,1.0,86.0,4.0,2.8,119.4,48,http://www.imiweb.uni-luebeck.de/sites/default/files/project/paper333_web.pdf,"Convolutional neural networks (CNNs) have been successfully used for fast and accurate estimation of dense correspondences between images in computer vision applications. However, much of their success is based on the availability of large training datasets with dense ground truth correspondences, which are only rarely available in medical applications. In this paper, we, therefore, address the problem of CNNs learning from few training data for medical image registration. Our contributions are threefold: (1) We present a novel approach for learning highly expressive appearance models from few training samples, (2) we show that this approach can be used to synthesize huge amounts of realistic ground truth training data for CNN-based medical image registration, and (3) we adapt the FlowNet architecture for CNN-based optical flow estimation to the medical image registration problem. This pipeline is applied to two medical data sets with less than 40 training images. We show that CNNs learned from the proposed generative model outperform those trained on random deformations or displacement fields estimated via classical image registration.",Itrcnfoimrefrfesawimodaau,52.0,18.0,4.0
4320,Image augmentation,128.0,evaluating gan-based image augmentation for threat detection in large-scale xray security images,3.0,201.0,1.0,10.0,5.0,2.8,121.8,49,https://www.mdpi.com/2076-3417/11/1/36/pdf,"The inherent imbalance in the data distribution of X-ray security images is one of the most challenging aspects of computer vision algorithms applied in this domain. Most of the prior studies in this field have ignored this aspect, limiting their application in the practical setting. This paper investigates the effect of employing Generative Adversarial Networks (GAN)-based image augmentation, or image synthesis, in improving the performance of computer vision algorithms on an imbalanced X-ray dataset. We used Deep Convolutional GAN (DCGAN) to generate new X-ray images of threat objects and Cycle-GAN to translate camera images of threat objects to X-ray images. We synthesized new X-ray security images by combining threat objects with background X-ray images, which are used to augment the dataset. Then, we trained various Faster (Region Based Convolutional Neural Network) R-CNN models using different augmentation approaches and evaluated their performance on a large-scale practical X-ray image dataset. Experiment results show that image synthesis is an effective approach to combating the imbalance problem by significantly reducing the false-positive rate (FPR) by up to 15.3%. The FPR is further improved by up to 19.9% by combining image synthesis and conventional image augmentation. Meanwhile, a relatively high true positive rate (TPR) of about 94% was maintained regardless of the augmentation method used.",Ievgaimaufothdeinlaxrseim,3.0,19.0,0.0
4321,Image augmentation,115.0,efficient training for automatic defect classification by image augmentation,3.0,201.0,1.0,35.0,5.0,2.8,125.4,50,http://arxiv.org/pdf/2103.16493v1,"At semiconductor wafer production sites, an automatic defect classification (ADC) system is used to analyze defects. The ADC system automatically classifies defect images into user-defined defect categories. Every manufacturing process requires the ADC system to be set up by acquiring training samples and teaching defect categories in advance since different defects occur for each process. Since the setup is time-consuming, there is a need for more efficient training from a small number of training samples. This paper describes an ADC system that can be efficiently trained by image augmentation to improve the efficiency of the defect analysis. The image augmentation of the proposed ADC system includes rotation/flip and distortion processing. The rotation/flip processing restricts rotation angles and flip axes to generate new images as if the images were acquired by actual tools. The distortion processing generates new images of defects that are likely to exist by considering distortion amount and continuity. These processes increase effective appearance variations for the training while maintaining the consistency of defect appearances in each category. Experimental results demonstrate that the proposed ADC system maintains the classification accuracy even with half the number of actual samples for the training.",Ieftrfoaudeclbyimau,3.0,11.0,0.0
4322,Image augmentation,85.0,underwater image classification using deep convolutional neural networks and data augmentation,4.0,201.0,1.0,90.0,4.0,2.8,132.9,51,http://arxiv.org/pdf/1807.03528v1,"The classification accuracy of underwater image, which have the special image characteristic, is lower than the corresponding result of images in the air. A study was carried out to underwater image classification with deep convolutional neural networks and the classification ability was improved with two data augmentation methods. The experiments showed that the submarine image classification with the ILSVRC Championship GoogLenet model was still relatively low confidence. The classification probability can be improved by two augmentation methods. The first was optical transformation of raw data such as scale and aspect ratio augmentation and Color augmentation. The second was increasing the virtue data generated by Generative Adversarial Nets. The results of the study validated the effectiveness of two data augmentation methods. Especially, the generative adversarial nets approach gave a new path to increasing the train data.",Iunimclusdeconeneandaau,18.0,14.0,0.0
4323,Image augmentation,31.0,deep cnn and data augmentation for skin lesion classification,5.0,201.0,1.0,183.0,3.0,2.8,144.6,52,https://www.researchgate.net/profile/Hoang-Van-Dung/publication/323161914_Deep_CNN_and_Data_Augmentation_for_Skin_Lesion_Classification/links/5b36fc8ba6fdcc8506dfb30c/Deep-CNN-and-Data-Augmentation-for-Skin-Lesion-Classification.pdf,"Deep CNN techniques have dramatically become the state of the art in image classification. However, applying high-capacity Deep CNN in medical image analysis has been impeded because of scarcity of labeled data. This study has two primary contributions: first, we propose a classification model to improve performance of classification of skin lesion using Deep CNN and Data Augmentation. Second, we demonstrate the use of image data augmentation for overcoming the problem of data limitation and examine the influence of different number of augmented samples on the performance of different classifiers. The proposed classification system is evaluated using the largest public skin lesion testing dataset, containing 600 testing images, and 6,162 training images. New state-of-the-art performance result is archived with AUC (89.2% vs. 87.4%), AP (73.9% vs. 71.5%), and ACC (89.0% vs. 87.2%). In additional, we explore the influence of each image augmentation on the three classifiers and observe that performance of each classifier is influenced differently by each augmentation and has better results comparing with traditional methods. Thus, it is suggested that the performance of skin cancer classification and medial image classification could be improved further by applying data augmentation.",Idecnandaaufosklecl,51.0,12.0,1.0
4324,Image augmentation,401.0,differentiable data augmentation with kornia,1.0,4.0,5.0,201.0,1.0,2.6,182.2,53,http://arxiv.org/pdf/2011.09832v1,"In this paper we present a review of the Kornia differentiable data augmentation (DDA) module for both for spatial (2D) and volumetric (3D) tensors. This module leverages differentiable computer vision solutions from Kornia, with an aim of integrating data augmentation (DA) pipelines and strategies to existing PyTorch components (e.g. autograd for differentiability, optim for optimization). In addition, we provide a benchmark comparing different DA frameworks and a short review for a number of approaches that make use of Kornia DDA.",Ididaauwiko,0.0,12.0,0.0
4325,Image augmentation,401.0,a survey on kornia: an open source differentiable computer vision library for pytorch,1.0,5.0,5.0,201.0,1.0,2.6,182.6,54,http://arxiv.org/pdf/2009.10521v1,"This work presents Kornia, an open source computer vision library built upon a set of differentiable routines and modules that aims to solve generic computer vision problems. The package uses PyTorch as its main backend, not only for efficiency but also to take advantage of the reverse auto-differentiation engine to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be integrated into neural networks to train models to perform a wide range of operations including image transformations,camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations on graphical processing units, generating faster systems. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.",Iasuonkoanopsodicovilifopy,80.0,93.0,3.0
4326,Image augmentation,401.0,improved regularization of convolutional neural networks with cutout,1.0,9.0,5.0,201.0,1.0,2.6,184.2,55,https://arxiv.org/pdf/1708.04552,"Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56%, 15.20%, and 1.30% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout",Iimreofconenewicu,1270.0,24.0,206.0
4327,Image augmentation,401.0,unsupervised data augmentation for consistency training,1.0,10.0,5.0,201.0,1.0,2.6,184.6,56,http://arxiv.org/pdf/1904.12848v6,"Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",Iundaaufocotr,590.0,84.0,86.0
4328,Image augmentation,401.0,fast autoaugment,1.0,11.0,5.0,201.0,1.0,2.6,185.0,57,http://arxiv.org/pdf/1905.00397v2,"Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet.",Ifaau,195.0,43.0,41.0
4329,Image augmentation,401.0,implicit semantic data augmentation for deep networks,1.0,13.0,5.0,201.0,1.0,2.6,185.8,58,http://arxiv.org/pdf/1909.12220v5,"In this paper, we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping, translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds. As a consequence, translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently, we first perform an online estimate of the covariance matrix of deep features for each class, which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly, instead of augmenting the samples explicitly, we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set, leading to a highly efficient algorithm. In fact, we show that the proposed ISDA amounts to minimizing a novel robust CE loss, which adds negligible extra computational cost to a normal training procedure. Although being simple, ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet. Code for reproducing our results is available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.",Iimsedaaufodene,34.0,44.0,3.0
4330,Image augmentation,401.0,population based augmentation: efficient learning of augmentation policy schedules,1.0,14.0,5.0,201.0,1.0,2.6,186.2,59,http://arxiv.org/pdf/1905.05393v1,"A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.",Ipobaauefleofauposc,169.0,50.0,29.0
4331,Image augmentation,401.0,what else can fool deep learning? addressing color constancy errors on deep neural network performance,1.0,20.0,5.0,201.0,1.0,2.6,188.6,60,http://arxiv.org/pdf/1912.06960v1,"There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into producing incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets.",Iwhelcafodeleadcocoerondenenepe,34.0,74.0,1.0
4332,Image augmentation,401.0,data augmentation for scene text recognition,1.0,23.0,5.0,201.0,1.0,2.6,189.8,61,http://arxiv.org/pdf/2003.06606v1,"Handwritten text and scene text suffer from various shapes and distorted patterns. Thus training a robust recognition model requires a large amount of data to cover diversity as much as possible. In contrast to data collection and annotation, data augmentation is a low cost way. In this paper, we propose a new method for text image augmentation. Different from traditional augmentation methods such as rotation, scaling and perspective transformation, our proposed augmentation method is designed to learn proper and efficient data augmentation which is more effective and specific for training a robust recognizer. By using a set of custom fiducial points, the proposed augmentation method is flexible and controllable. Furthermore, we bridge the gap between the isolated processes of data augmentation and network optimization by joint learning. An agent network learns from the output of the recognition network and controls the fiducial points to generate more proper training samples for the recognition network. Extensive experiments on various benchmarks, including regular scene text, irregular scene text and handwritten text, show that the proposed augmentation and the joint learning methods significantly boost the performance of the recognition networks. A general toolkit for geometric augmentation is available.",Idaaufosctere,0.0,44.0,0.0
4333,Image augmentation,401.0,parallel grid pooling for data augmentation,1.0,27.0,5.0,201.0,1.0,2.6,191.4,62,http://arxiv.org/pdf/1803.11370v1,"Convolutional neural network (CNN) architectures utilize downsampling layers, which restrict the subsequent layers to learn spatially invariant features while reducing computational costs. However, such a downsampling operation makes it impossible to use the full spectrum of input features. Motivated by this observation, we propose a novel layer called parallel grid pooling (PGP) which is applicable to various CNN models. PGP performs downsampling without discarding any intermediate feature. It works as data augmentation and is complementary to commonly used data augmentation techniques. Furthermore, we demonstrate that a dilated convolution can naturally be represented using PGP operations, which suggests that the dilated convolution can also be regarded as a type of data augmentation technique. Experimental results based on popular image classification benchmarks demonstrate the effectiveness of the proposed method. Code is available at: https://github.com/akitotakeki",Ipagrpofodaau,4.0,54.0,0.0
4334,Image augmentation,401.0,self-supervised pretraining improves self-supervised pretraining,1.0,28.0,5.0,201.0,1.0,2.6,191.8,63,http://arxiv.org/pdf/2104.08027v2,"Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years. However, previous work has indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective universal lexical and sentence encoders even without any additional data and without any supervision. We propose an extremely simple, fast and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds without any additional external knowledge. Mirror-BERT relies on fully identical or slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in both lexical-level and sentence-level tasks, across different domains and different languages. Notably, in the standard sentence semantic similarity (STS) tasks, our self-supervised Mirror-BERT model even matches the performance of the task-tuned Sentence-BERT models from prior work. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple approach can yield effective universal lexical and sentence encoders.",Iseprimsepr,10.0,72.0,0.0
4335,Image augmentation,401.0,an efficient and scalable deep learning approach for road damage detection,1.0,29.0,5.0,201.0,1.0,2.6,192.2,64,http://arxiv.org/pdf/2011.09577v3,"Pavement condition evaluation is essential to time the preventative or rehabilitative actions and control distress propagation. Failing to conduct timely evaluations can lead to severe structural and financial loss of the infrastructure and complete reconstructions. Automated computer-aided surveying measures can provide a database of road damage patterns and their locations. This database can be utilized for timely road repairs to gain the minimum cost of maintenance and the asphalt's maximum durability. This paper introduces a deep learning-based surveying scheme to analyze the image-based distress data in real-time. A database consisting of a diverse population of crack distress types such as longitudinal, transverse, and alligator cracks, photographed using mobile-device is used. Then, a family of efficient and scalable models that are tuned for pavement crack detection is trained, and various augmentation policies are explored. Proposed models, resulted in F1-scores, ranging from 52% to 56%, and average inference time from 178-10 images per second. Finally, the performance of the object detectors are examined, and error analysis is reported against various images. The source code is available at https://github.com/mahdi65/roadDamageDetection2020.",Ianefanscdeleapforodade,3.0,40.0,0.0
4336,Image augmentation,401.0,efficient method for categorize animals in the wild,1.0,30.0,5.0,201.0,1.0,2.6,192.6,65,http://arxiv.org/pdf/1907.13037v1,"Automatic species classification in camera traps would greatly help the biodiversity monitoring and species analysis in the earth. In order to accelerate the development of automatic species classification task, ""Microsoft AI for Earth"" have prepared a challenge in FGVC6 workshop at CVPR 2019, which called ""iWildCam 2019 competition"". In this work, we propose an efficient method for categorizing animals in the wild. We transfer the state-of-the-art ImagaNet pretrained models to the problem. To improve the generalization and robustness of the model, we utilize efficient image augmentation and regularization strategies, like cutout, mixup and label-smoothing. Finally, we use ensemble learning to increase the performance of the model. Thanks to advanced regularization strategies and ensemble learning, we got top 7/336 places in the final leaderboard. Source code of this work is available at https://github.com/Walleclipse/iWildCam_2019_FGVC6",Iefmefocaaninthwi,1.0,18.0,0.0
4337,Image augmentation,401.0,an open-source tool for hyperspectral image augmentation in tensorflow,1.0,32.0,5.0,201.0,1.0,2.6,193.4,66,https://arxiv.org/pdf/2003.13502,"Satellite imagery allows a plethora of applications ranging from weather forecasting to land surveying. The rapid development of computer vision systems could open new horizons to the utilization of satellite data due to the abundance of large volumes of data. However, current state-of-the-art computer vision systems mainly cater to applications that mainly involve natural images. While useful, those images exhibit a different distribution from satellite images in addition to having more spectral channels. This allows the use of pretrained deep learning models only in a subset of spectral channels that are equivalent to natural images thus discarding valuable information from other spectral channels. This calls for research effort to optimize deep learning models for satellite imagery to enable the assessment of their utility in the domain of remote sensing. Tensorflow tool allows for rapid prototyping and testing of deep learning models, however, its built-in image generator is designed to handle a maximum of four spectral channels. This manuscript introduces an open-source tool that allows the implementation of image augmentation for hyperspectral images in Tensorflow. Given how accessible and easy-to-use Tensorflow is, this tool would provide many researchers with the means to implement, test, and deploy deep learning models for remote sensing applications.",Ianoptofohyimauinte,1.0,10.0,0.0
4338,Image augmentation,401.0,densenet models for tiny imagenet classification,1.0,33.0,5.0,201.0,1.0,2.6,193.8,67,http://arxiv.org/pdf/1904.10429v2,"In this paper, we present two image classification models on the Tiny ImageNet dataset. We built two very different networks from scratch based on the idea of Densely Connected Convolution Networks. The architecture of the networks is designed based on the image resolution of this specific dataset and by calculating the Receptive Field of the convolution layers. We also used some non-conventional techniques related to image augmentation and Cyclical Learning Rate to improve the accuracy of our models. The networks are trained under high constraints and low computation resources. We aimed to achieve top-1 validation accuracy of 60%; the results and error analysis are also presented.",Idemofotiimcl,7.0,7.0,0.0
4339,Image augmentation,401.0,can ai help in screening viral and covid-19 pneumonia?,1.0,36.0,5.0,201.0,1.0,2.6,195.0,68,https://ieeexplore.ieee.org/iel7/6287639/8948470/09144185.pdf,"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively.",Icaaiheinscviancopn,344.0,100.0,35.0
4340,Image augmentation,401.0,improved mixed-example data augmentation,1.0,37.0,5.0,201.0,1.0,2.6,195.4,69,http://arxiv.org/pdf/2007.04206v1,"Modern deep neural networks can produce badly calibrated predictions, especially when train and test distributions are mismatched. Training an ensemble of models and averaging their predictions can help alleviate these issues. We propose a simple technique to improve calibration, using a different data augmentation for each ensemble member. We additionally use the idea of `mixing' un-augmented and augmented inputs to improve calibration when test and training distributions are the same. These simple techniques improve calibration and accuracy over strong baselines on the CIFAR10 and CIFAR100 benchmarks, and out-of-domain data from their corrupted versions.",Iimmidaau,53.0,28.0,3.0
4341,Image augmentation,401.0,adversarial policy gradient for deep learning image augmentation,1.0,38.0,5.0,201.0,1.0,2.6,195.8,70,http://arxiv.org/pdf/1912.11188v1,"Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.",Iadpogrfodeleimau,2.0,13.0,1.0
4342,Image augmentation,401.0,adversarial augmentation for enhancing classification of mammography images,1.0,39.0,5.0,201.0,1.0,2.6,196.2,71,http://arxiv.org/pdf/1902.07762v1,"Supervised deep learning relies on the assumption that enough training data is available, which presents a problem for its application to several fields, like medical imaging. On the example of a binary image classification task (breast cancer recognition), we show that pretraining a generative model for meaningful image augmentation helps enhance the performance of the resulting classifier. By augmenting the data, performance on downstream classification tasks could be improved even with a relatively small training set. We show that this ""adversarial augmentation"" yields promising results compared to classical image augmentation on the example of breast cancer classification.",Iadaufoenclofmaim,2.0,37.0,0.0
4343,Image augmentation,401.0,multi-disease detection in retinal imaging based on ensembling heterogeneous deep learning models,1.0,40.0,5.0,201.0,1.0,2.6,196.6,72,http://arxiv.org/pdf/2103.14660v1,"Preventable or undiagnosed visual impairment and blindness affect billion of people worldwide. Automated multi-disease detection models offer great potential to address this problem via clinical decision support in diagnosis. In this work, we proposed an innovative multi-disease detection pipeline for retinal imaging which utilizes ensemble learning to combine the predictive capabilities of several heterogeneous deep convolutional neural network models. Our pipeline includes state-of-the-art strategies like transfer learning, class weighting, real-time image augmentation and Focal loss utilization. Furthermore, we integrated ensemble learning techniques like heterogeneous deep learning models, bagging via 5-fold cross-validation and stacked logistic regression models. Through internal and external evaluation, we were able to validate and demonstrate high accuracy and reliability of our pipeline, as well as the comparability with other state-of-the-art pipelines for retinal disease prediction.",Imudeinreimbaonenhedelemo,1.0,20.0,0.0
4344,Image augmentation,86.0,see better before looking closer: weakly supervised data augmentation network for fine-grained visual classification,4.0,201.0,1.0,119.0,3.0,2.5,141.9,73,https://arxiv.org/pdf/1901.09891.pdf%C3%A2%E2%82%AC%E2%80%B9,"Data augmentation is usually adopted to increase the amount of training data, prevent overfitting and improve the performance of deep models. However, in practice, random data augmentation, such as random image cropping, is low-efficiency and might introduce many uncontrolled background noises. In this paper, we propose Weakly Supervised Data Augmentation Network (WS-DAN) to explore the potential of data augmentation. Specifically, for each training image, we first generate attention maps to represent the object's discriminative parts by weakly supervised learning. Next, we augment the image guided by these attention maps, including attention cropping and attention dropping. The proposed WS-DAN improves the classification accuracy in two folds. In the first stage, images can be seen better since more discriminative parts' features will be extracted. In the second stage, attention regions provide accurate location of object, which ensures our model to look at the object closer and further improve the performance. Comprehensive experiments in common fine-grained visual classification datasets show that our WS-DAN surpasses the state-of-the-art methods, which demonstrates its effectiveness.",Isebebeloclwesudaaunefofivicl,82.0,49.0,12.0
4345,Image augmentation,80.0,classification of skin lesions using transfer learning and augmentation with alex-net,4.0,201.0,1.0,127.0,3.0,2.5,142.5,74,http://arxiv.org/pdf/2105.07592v2,"Skin cancer is one of most deadly diseases in humans. According to the high similarity between melanoma and nevus lesions, physicians take much more time to investigate these lesions. The automated classification of skin lesions will save effort, time and human life. The purpose of this paper is to present an automatic skin lesions classification system with higher classification rate using the theory of transfer learning and the pre-trained deep neural network. The transfer learning has been applied to the Alex-net in different ways, including fine-tuning the weights of the architecture, replacing the classification layer with a softmax layer that works with two or three kinds of skin lesions, and augmenting dataset by fixed and random rotation angles. The new softmax layer has the ability to classify the segmented color image lesions into melanoma and nevus or into melanoma, seborrheic keratosis, and nevus. The three well-known datasets, MED-NODE, Derm (IS & Quest) and ISIC, are used in testing and verifying the proposed method. The proposed DCNN weights have been fine-tuned using the training and testing dataset from ISIC in addition to 10-fold cross validation for MED-NODE and DermIS—DermQuest. The accuracy, sensitivity, specificity, and precision measures are used to evaluate the performance of the proposed method and the existing methods. For the datasets, MED-NODE, Derm (IS & Quest) and ISIC, the proposed method has achieved accuracy percentages of 96.86%, 97.70%, and 95.91% respectively. The performance of the proposed method has outperformed the performance of the existing classification methods of skin cancer.",Iclofskleustrleanauwial,76.0,51.0,4.0
4346,Image augmentation,46.0,stada: style transfer as data augmentation,4.0,201.0,1.0,170.0,3.0,2.5,145.2,75,https://arxiv.org/pdf/1909.01056,"The success of training deep Convolutional Neural Networks (CNNs) heavily depends on a significant amount of labelled data. Recent research has found that neural style transfer algorithms can apply the artistic style of one image to another image without changing the latter's high-level semantic content, which makes it feasible to employ neural style transfer as a data augmentation method to add more variation to the training dataset. The contribution of this paper is a thorough evaluation of the effectiveness of the neural style transfer as a data augmentation method for image classification tasks. We explore the state-of-the-art neural style transfer algorithms and apply them as a data augmentation method on Caltech 101 and Caltech 256 dataset, where we found around 2% improvement from 83% to 85% of the image classification accuracy with VGG16, compared with traditional data augmentation strategies. We also combine this new method with conventional data augmentation approaches to further improve the performance of image classification. This work shows the potential of neural style transfer in computer vision field, such as helping us to reduce the difficulty of collecting sufficient labelled data and improve the performance of generic image-based deep learning algorithms.",Iststtrasdaau,19.0,35.0,0.0
4347,Image augmentation,180.0,polarimetric image augmentation,3.0,201.0,1.0,45.0,4.0,2.5,147.9,76,https://arxiv.org/pdf/2005.11044,"This paper deals with new augmentation methods for an unconventional imaging modality sensitive to the physics of the observed scene called polarimetry. In nature, polarized light is obtained by reflection or scattering. Robotics applications in urban environments are subject to many obstacles that can be specular and therefore provide polarized light. These areas are prone to segmentation errors using standard modalities but could be solved using information carried by the polarized light. Deep Convolutional Neural Networks (DCNNs) have shown excellent segmentation results, but require a significant amount of data to achieve best performances. The lack of data is usually overcomed by using augmentation methods. However, unlike RGB images, polarization images are not only scalar (intensity) images and standard augmentation techniques cannot be applied straightforwardly. We propose enhancing deep learning models through a regularized augmentation procedure applied to polarimetric data in order to characterize scenes more effectively under challenging conditions. We subsequently observe an average of 18.1 % improvement in IoU between not augmented and regularized training procedures on real world data.",Ipoimau,1.0,39.0,0.0
4348,Image augmentation,45.0,a new color augmentation method for deep learning segmentation of histological images,4.0,201.0,1.0,182.0,3.0,2.5,148.5,77,https://core.ac.uk/download/pdf/222877678.pdf,"This paper addresses the problem of labeled data insufficiency in neural network training for semantic segmentation of color-stained histological images acquired via Whole Slide Imaging. It proposes an efficient image augmentation method to alleviate the demand for a large amount of labeled data and improve the network's generalization capacity. Typical image augmentation in bioimaging involves geometric transformation. Here, we propose a new image augmentation technique by combining the structure of one image with the color appearance of another image to construct augmented images on-the-fly for each training iteration. We show that it improves performance in the segmentation of histological images of human skin, and also offers better results when combined with geometric transformation.",Ianecoaumefodeleseofhiim,7.0,30.0,0.0
4349,Image augmentation,77.0,data augmentation by pairing samples for images classification,4.0,201.0,1.0,150.0,3.0,2.5,148.5,78,https://arxiv.org/pdf/1801.02929,"Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate N^2 new samples from N training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.",Idaaubypasafoimcl,192.0,17.0,15.0
4351,Image augmentation,79.0,facial expression recognition using convolutional neural network with data augmentation,4.0,201.0,1.0,162.0,3.0,2.5,152.70000000000002,79,https://www.diva-portal.org/smash/get/diva2:1299002/FULLTEXT01.pdf,"Detecting emotion from facial expression has become an urgent need because of its immense applications in artificial intelligence such as human-computer collaboration, data-driven animation, human-robot communication etc. Since it is a demanding and interesting problem in computer vision, several works had been conducted regarding this topic. The objective of this research is to develop a facial expression recognition system based on convolutional neural network with data augmentation. This approach enables to classify seven basic emotions consist of angry, disgust, fear, happy, neutral, sad and surprise from image data. Convolutional neural network with data augmentation leads to higher validation accuracy than the other existing models (which is 96.24%) as well as helps to overcome their limitations.",Ifaexreusconenewidaau,22.0,26.0,1.0
4352,Image augmentation,68.0,assessment of data augmentation strategies toward performance improvement of abnormality classification in chest radiographs,4.0,201.0,1.0,175.0,3.0,2.5,153.3,80,https://lhncbc.nlm.nih.gov/LHC-publications/PDF/pub9938.pdf,"Image augmentation is a commonly performed technique to prevent class imbalance in datasets to compensate for insufficient training samples, or to prevent model overfitting. Traditional augmentation (TA) techniques include various image transformations, such as rotation, translation, channel splitting, etc. Alternatively, Generative Adversarial Network (GAN), due to its proven ability to synthesize convincingly-realistic images, has been used to perform image augmentation as well. However, it is unclear whether GAN augmentation (GA) strategy provides an advantage over TA for medical image classification tasks. In this paper, we study the usefulness of TA and GA for classifying abnormal chest X-ray (CXR) images. We first trained a progressive-growing GAN (PG-GAN) to synthesize high-resolution CXRs for performing GA. Then, we trained an abnormality classifier using three training sets individually – training set with TA, with GA and with no augmentation (NA). Finally, we analyzed the abnormality classifier’s performance for the three training cases, which led to the following conclusions: (1) GAN strategy is not always superior to TA for improving the classifier’s performance; (2) in comparison to NA, however, both TA and GA leads to a significant performance improvement; and, (3) increasing the quantity of images in TA and GA strategies also improves the classifier’s performance.",Iasofdaausttopeimofabclinchra,9.0,13.0,1.0
4353,Image augmentation,154.0,faster autoaugment: learning augmentation strategies using backpropagation,3.0,201.0,1.0,94.0,4.0,2.5,154.79999999999998,81,https://arxiv.org/pdf/1911.06987,"Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.",Ifaauleaustusba,49.0,42.0,9.0
4354,Image augmentation,87.0,augmentation of cbct reconstructed from under-sampled projections using deep learning,4.0,201.0,1.0,185.0,3.0,2.5,162.0,82,http://arxiv.org/pdf/2006.15713v2,"Edges tend to be over-smoothed in total variation (TV) regularized under-sampled images. In this paper, symmetric residual convolutional neural network (SR-CNN), a deep learning based model, was proposed to enhance the sharpness of edges and detailed anatomical structures in under-sampled cone-beam computed tomography (CBCT). For training, CBCT images were reconstructed using TV-based method from limited projections simulated from the ground truth CT, and were fed into SR-CNN, which was trained to learn a restoring pattern from under-sampled images to the ground truth. For testing, under-sampled CBCT was reconstructed using TV regularization and was then augmented by SR-CNN. Performance of SR-CNN was evaluated using phantom and patient images of various disease sites acquired at different institutions both qualitatively and quantitatively using structure similarity (SSIM) and peak signal-to-noise ratio (PSNR). SR-CNN substantially enhanced image details in the TV-based CBCT across all experiments. In the patient study using real projections, SR-CNN augmented CBCT images reconstructed from as low as 120 half-fan projections to image quality comparable to the reference fully-sampled FDK reconstruction using 900 projections. In the tumor localization study, improvements in the tumor localization accuracy were made by the SR-CNN augmented images compared with the conventional FDK and TV-based images. SR-CNN demonstrated robustness against noise levels and projection number reductions and generalization for various disease sites and datasets from different institutions. Overall, the SR-CNN-based image augmentation technique was efficient and effective in considerably enhancing edges and anatomical structures in under-sampled 3D/4D-CBCT, which can be very valuable for image-guided radiotherapy.",Iauofcbrefrunprusdele,15.0,95.0,0.0
4355,Image augmentation,199.0,concatenated image completion via tensor augmentation and completion,3.0,201.0,1.0,96.0,4.0,2.5,168.89999999999998,83,https://arxiv.org/pdf/1607.03967,"This paper proposes a novel framework called concatenated image completion via tensor augmentation and completion (ICTAC), which recovers missing entries of color images with high accuracy. Typical images are second-or third-order tensors (2D/3D) depending if they are grayscale or color, hence tensor completion algorithms are ideal for their recovery. The proposed framework performs image completion by concatenating copies of a single image that has missing entries into a third-order tensor, applying a dimensionality augmentation technique to the tensor, utilizing a tensor completion algorithm for recovering its missing entries, and finally extracting the recovered image from the tensor. The solution relies on two key components that have been recently proposed to take advantage of the tensor train (TT) rank: A tensor augmentation tool called ket augmentation (KA) that represents a low-order tensor by a higher-order tensor, and the algorithm tensor completion by parallel matrix factorization via tensor train (TMac-TT), which has been demonstrated to outperform state-of-the-art tensor completion algorithms. Simulation results for color image recovery show the clear advantage of our framework against current state-of-the-art tensor completion algorithms.",Icoimcoviteauanco,10.0,29.0,0.0
4356,Image augmentation,14.0,deformation estimation of elastic bodies using multiple silhouette images for endoscopic image augmentation,5.0,201.0,1.0,201.0,1.0,2.2,144.9,84,http://www.bme.sys.i.kyoto-u.ac.jp/~meg/doc/2015_ISMAR_DefromationFromSilhouette.pdf,"The purpose of this paper is to design a solution to the problem of facial recognition by use of convolutional neural networks, with the intention of applying the solution in a camera-based home-entry access control system. More specifically, the paper focuses on solving the supervised classification problem of taking images of people as input and classifying the person in the image as one of the authors or not. Two approaches are proposed: (1) building and training a neural network called WoodNet from scratch and (2) leveraging transfer learning by utilizing a network pre-trained on the ImageNet database and adapting it to this project's data and classes. In order to train the models to recognize the authors, a dataset containing more than 150 000 images has been created, balanced over the authors and others. Image extraction from videos and image augmentation techniques were instrumental for dataset creation. The results are two models classifying the individuals in the dataset with high accuracy, achieving over 99% accuracy on held-out test data. The pre-trained model fitted significantly faster than WoodNet, and seems to generalize better. However, these results come with a few caveats. Because of the way the dataset was compiled, as well as the high accuracy, one has reason to believe the models over-fitted to the data to some degree. An added consequence of the data compilation method is that the test dataset may not be sufficiently different from the training data, limiting its ability to validate generalization of the models. However, utilizing the models in a web-cam based system, classifying faces in real-time, shows promising results and indicates that the models generalized fairly well for at least some of the classes (see the accompanying video).",Ideesofelbousmusiimfoenimau,20.0,4.0,1.0
4357,Image augmentation,17.0,an enhanced framework of generative adversarial networks (ef-gans) for environmental microorganism image augmentation with limited rotation-invariant training …,5.0,201.0,1.0,201.0,1.0,2.2,145.8,85,https://ieeexplore.ieee.org/iel7/6287639/8948470/09223631.pdf,"Synthesis of high resolution images using Generative Adversarial Networks (GANs) is challenging, which usually requires numbers of high-end graphic cards with large memory and long time of training. In this paper, we propose a two-stage framework to accelerate the training process of synthesizing high resolution images. High resolution images are first transformed to small codes via the trained encoder and decoder networks. The code in latent space is times smaller than the original high resolution images. Then, we train a code generation network to learn the distribution of the latent codes. In this way, the generator only learns to generate small latent codes instead of large images. Finally, we decode the generated latent codes to image space via the decoder networks so as to output the synthesized high resolution images. Experimental results show that the proposed method accelerates the training process significantly and increases the quality of the generated samples. The proposed acceleration framework makes it possible to generate high resolution images using less training time with limited hardware resource. After using the proposed acceleration method, it takes only 3 days to train a 1024 *1024 image generator on Celeba-HQ dataset using just one NVIDIA P100 graphic card.",Ianenfrofgeadne(efoenmiimauwilirotr…,5.0,55.0,0.0
4358,Image augmentation,38.0,tropical fruits classification using an alexnet-type convolutional neural network and image augmentation,5.0,201.0,1.0,201.0,1.0,2.2,152.10000000000002,86,http://arxiv.org/pdf/2103.02096v2,"Convolutional neural networks (CNNs) have been used in many machine learning fields. In practical applications, the computational cost of convolutional neural networks is often high with the deepening of the network and the growth of data volume, mostly due to a large amount of multiplication operations of floating-point numbers in convolution operations. To reduce the amount of multiplications, we propose a new type of CNNs called Tropical Convolutional Neural Networks (TCNNs) which are built on tropical convolutions in which the multiplications and additions in conventional convolutional layers are replaced by additions and min/max operations respectively. In addition, since tropical convolution operators are essentially nonlinear operators, we expect TCNNs to have higher nonlinear fitting ability than conventional CNNs. In the experiments, we test and analyze several different architectures of TCNNs for image classification tasks in comparison with similar-sized conventional CNNs. The results show that TCNN can achieve higher expressive power than ordinary convolutional layers on the MNIST and CIFAR10 image data set. In different noise environments, there are wins and losses in the robustness of TCNN and ordinary CNNs.",Itrfrclusanalconeneanimau,6.0,9.0,2.0
4359,Image augmentation,40.0,conditional generative adversarial network-based data augmentation for enhancement of iris recognition accuracy,5.0,201.0,1.0,201.0,1.0,2.2,152.7,87,https://ieeexplore.ieee.org/iel7/6287639/8600701/08815758.pdf,"Cross-spectral iris recognition is emerging as a promising biometric approach to authenticating the identity of individuals. However, matching iris images acquired at different spectral bands shows significant performance degradation when compared to single-band near-infrared (NIR) matching due to the spectral gap between iris images obtained in the NIR and visual-light (VIS) spectra. Although researchers have recently focused on deep-learning-based approaches to recover invariant representative features for more accurate recognition performance, the existing methods cannot achieve the expected accuracy required for commercial applications. Hence, in this paper, we propose a conditional coupled generative adversarial network (CpGAN) architecture for cross-spectral iris recognition by projecting the VIS and NIR iris images into a low-dimensional embedding domain to explore the hidden relationship between them. The conditional CpGAN framework consists of a pair of GAN-based networks, one responsible for retrieving images in the visible domain and other responsible for retrieving images in the NIR domain. Both networks try to map the data into a common embedding subspace to ensure maximum pair-wise similarity between the feature vectors from the two iris modalities of the same subject. To prove the usefulness of our proposed approach, extensive experimental results obtained on the PolyU dataset are compared to existing state-of-the-art cross-spectral recognition methods.",Icogeadnedaaufoenofirreac,18.0,62.0,0.0
4360,Image augmentation,113.0,learning more with less: conditional pggan-based data augmentation for brain metastases detection using highly-rough annotation on mr images,3.0,201.0,1.0,141.0,3.0,2.2,156.60000000000002,88,https://arxiv.org/pdf/1902.09856,"Accurate Computer-Assisted Diagnosis, associated with proper data wrangling, can alleviate the risk of overlooking the diagnosis in a clinical environment. Towards this, as a Data Augmentation (DA) technique, Generative Adversarial Networks (GANs) can synthesize additional training data to handle the small/fragmented medical imaging datasets collected from various scanners; those images are realistic but completely different from the original ones, filling the data lack in the real image distribution. However, we cannot easily use them to locate disease areas, considering expert physicians' expensive annotation cost. Therefore, this paper proposes Conditional Progressive Growing of GANs (CPGGANs), incorporating highly-rough bounding box conditions incrementally into PGGANs to place brain metastases at desired positions/sizes on 256 × 256 Magnetic Resonance (MR) images, for Convolutional Neural Network-based tumor detection; this first GAN-based medical DA using automatic bounding box annotation improves the training robustness. The results show that CPGGAN-based DA can boost 10% sensitivity in diagnosis with clinically acceptable additional False Positives. Surprisingly, further tumor realism, achieved with additional normal brain MR images for CPGGAN training, does not contribute to detection performance, while even three physicians cannot accurately distinguish them from the real ones in Visual Turing Test.",Ilemowilecopgdaaufobrmedeushianonmrim,49.0,65.0,2.0
4361,Image augmentation,137.0,semantic equivalent adversarial data augmentation for visual question answering,3.0,201.0,1.0,125.0,3.0,2.2,159.0,89,https://arxiv.org/pdf/2007.09592,"Visual Question Answering (VQA) has achieved great success thanks to the fast development of deep neural networks (DNN). On the other hand, the data augmentation, as one of the major tricks for DNN, has been widely used in many computer vision tasks. However, there are few works studying the data augmentation problem for VQA and none of the existing image based augmentation schemes (such as rotation and flipping) can be directly applied to VQA due to its semantic structure -- an $\langle image, question, answer\rangle$ triplet needs to be maintained correctly. For example, a direction related Question-Answer (QA) pair may not be true if the associated image is rotated or flipped. In this paper, instead of directly manipulating images and questions, we use generated adversarial examples for both images and questions as the augmented data. The augmented examples do not change the visual properties presented in the image as well as the \textbf{semantic} meaning of the question, the correctness of the $\langle image, question, answer\rangle$ is thus still maintained. We then use adversarial learning to train a classic VQA model (BUTD) with our augmented data. We find that we not only improve the overall performance on VQAv2, but also can withstand adversarial attack effectively, compared to the baseline model. The source code is available at this https URL.",Iseeqaddaaufoviquan,13.0,48.0,1.0
4362,Image augmentation,111.0,auggan: cross domain adaptation with gan-based data augmentation,3.0,201.0,1.0,179.0,3.0,2.2,167.4,90,https://openaccess.thecvf.com/content_ECCV_2018/papers/Sheng-Wei_Huang_AugGAN_Cross_Domain_ECCV_2018_paper.pdf,"Deep learning based image-to-image translation methods aim at learning the joint distribution of the two domains and finding transformations between them. Despite recent GAN (Generative Adversarial Network) based methods have shown compelling results, they are prone to fail at preserving image-objects and maintaining translation consistency, which reduces their practicality on tasks such as generating large-scale training data for different domains. To address this problem, we purpose a structure-aware image-to-image translation network, which is composed of encoders, generators, discriminators and parsing nets for the two domains, respectively, in a unified framework. The purposed network generates more visually plausible images compared to competing methods on different image-translation tasks. In addition, we quantitatively evaluate different methods by training Faster-RCNN and YOLO with datasets generated from the image-translation results and demonstrate significant improvement on the detection accuracies by using the proposed image-object preserving network.",Iaucrdoadwigadaau,106.0,21.0,4.0
4363,Image augmentation,190.0,data augmentation generative adversarial networks,3.0,201.0,1.0,130.0,3.0,2.2,176.4,91,https://arxiv.org/pdf/1711.04340,"Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).",Idaaugeadne,509.0,41.0,44.0
4364,Image augmentation,193.0,a novel scene classification model combining resnet based transfer learning and data augmentation with a filter,3.0,201.0,1.0,140.0,3.0,2.2,180.3,92,http://arxiv.org/pdf/2101.02919v1,"Abstract Scene classification is a significant aspect of computer vision. Convolutional neural networks (CNNs), a development of deep learning, are a well-understood tool for image classification. But training CNNs requires large-scale datasets. Transfer learning addresses this problem and produces a solution for small-scale datasets. Because scene image classification is more complex than common image classification. We propose a novel ResNet based transfer learning model utilizing multi-layer feature fusion, taking full advantage of interlayer discriminating features and fusing them for classification by softmax regression. In addition, a novel data augmentation method with a filter useful for small-scale datasets is presented. New image patches are generated by sliding block cropping of a raw image, which are then filtered to insure that the new images sufficiently represent the original categorization. Our new ResNet based transfer learning model with enhanced data augmentation is evaluated on six benchmark scene datasets (LF, OT, FP, LS, MIT67, SUN397). Extensive experimental results show that on the six datasets our method obtains better accuracy than other state-of-the-art models.",Ianoscclmocorebatrleandaauwiafi,50.0,69.0,1.0
4365,Image augmentation,401.0,visual question generation from radiology images,1.0,41.0,4.0,201.0,1.0,2.2,197.0,93,http://arxiv.org/pdf/1611.08669v5,"We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.   We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org",Iviqugefrraim,6.0,29.0,0.0
4366,Image augmentation,401.0,salient objects in clutter,1.0,42.0,4.0,201.0,1.0,2.2,197.4,94,http://arxiv.org/pdf/2105.03053v1,"This paper identifies and addresses a serious design bias of existing salient object detection (SOD) datasets, which unrealistically assume that each image should contain at least one clear and uncluttered salient object. This design bias has led to a saturation in performance for state-of-the-art SOD models when evaluated on existing datasets. However, these models are still far from satisfactory when applied to real-world scenes. Based on our analyses, we propose a new high-quality dataset and update the previous saliency benchmark. Specifically, our dataset, called Salient Objects in Clutter (SOC), includes images with both salient and non-salient objects from several common object categories. In addition to object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes, which can help provide deeper insight into the SOD problem. Further, with a given saliency encoder, e.g., the backbone network, existing saliency models are designed to achieve mapping from the training image set to the training ground-truth set. We, therefore, argue that improving the dataset can yield higher performance gains than focusing only on the decoder design. With this in mind, we investigate several dataset-enhancement strategies, including label smoothing to implicitly emphasize salient boundaries, random image augmentation to adapt saliency models to various scenarios, and self-supervised learning as a regularization strategy to learn from small datasets. Our extensive results demonstrate the effectiveness of these tricks. We also provide a comprehensive benchmark for SOD, which can be found in our repository: http://dpfan.net/SOCBenchmark.",Isaobincl,220.0,51.0,18.0
4367,Image augmentation,401.0,image augmentation for multitask few-shot learning: agricultural domain use-case,1.0,43.0,4.0,201.0,1.0,2.2,197.8,95,http://arxiv.org/pdf/2102.12295v1,"Large datasets' availability is catalyzing a rapid expansion of deep learning in general and computer vision in particular. At the same time, in many domains, a sufficient amount of training data is lacking, which may become an obstacle to the practical application of computer vision techniques. This paper challenges small and imbalanced datasets based on the example of a plant phenomics domain. We introduce an image augmentation framework, which enables us to extremely enlarge the number of training samples while providing the data for such tasks as object detection, semantic segmentation, instance segmentation, object counting, image denoising, and classification. We prove that our augmentation method increases model performance when only a few training samples are available. In our experiment, we use the DeepLabV3 model on semantic segmentation tasks with Arabidopsis and Nicotiana tabacum image dataset. The obtained result shows a 9% relative increase in model performance compared to the basic image augmentation techniques.",Iimaufomufeleagdous,1.0,32.0,0.0
4368,Image augmentation,401.0,learning optimal data augmentation policies via bayesian optimization for image classification tasks,1.0,45.0,4.0,201.0,1.0,2.2,198.6,96,http://arxiv.org/pdf/1905.02610v2,"In recent years, deep learning has achieved remarkable achievements in many fields, including computer vision, natural language processing, speech recognition and others. Adequate training data is the key to ensure the effectiveness of the deep models. However, obtaining valid data requires a lot of time and labor resources. Data augmentation (DA) is an effective alternative approach, which can generate new labeled data based on existing data using label-preserving transformations. Although we can benefit a lot from DA, designing appropriate DA policies requires a lot of expert experience and time consumption, and the evaluation of searching the optimal policies is costly. So we raise a new question in this paper: how to achieve automated data augmentation at as low cost as possible? We propose a method named BO-Aug for automating the process by finding the optimal DA policies using the Bayesian optimization approach. Our method can find the optimal policies at a relatively low search cost, and the searched policies based on a specific dataset are transferable across different neural network architectures or even different datasets. We validate the BO-Aug on three widely used image classification datasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show that the proposed method can achieve state-of-the-art or near advanced classification accuracy. Code to reproduce our experiments is available at https://github.com/zhangxiaozao/BO-Aug.",Ileopdaaupovibaopfoimclta,1.0,58.0,0.0
4369,Image augmentation,401.0,data augmentation via levy processes,1.0,46.0,4.0,201.0,1.0,2.2,199.0,97,http://arxiv.org/pdf/1603.06340v1,"If a document is about travel, we may expect that short snippets of the document should also be about travel. We introduce a general framework for incorporating these types of invariances into a discriminative classifier. The framework imagines data as being drawn from a slice of a Levy process. If we slice the Levy process at an earlier point in time, we obtain additional pseudo-examples, which can be used to train the classifier. We show that this scheme has two desirable properties: it preserves the Bayes decision boundary, and it is equivalent to fitting a generative model in the limit where we rewind time back to 0. Our construction captures popular schemes such as Gaussian feature noising and dropout training, as well as admitting new generalizations.",Idaauvilepr,5.0,29.0,0.0
4370,Image augmentation,401.0,learning convolutional neural networks using hybrid orthogonal projection and estimation,1.0,47.0,4.0,201.0,1.0,2.2,199.4,98,http://arxiv.org/pdf/1606.05929v4,"Convolutional neural networks (CNNs) have yielded the excellent performance in a variety of computer vision tasks, where CNNs typically adopt a similar structure consisting of convolution layers, pooling layers and fully connected layers. In this paper, we propose to apply a novel method, namely Hybrid Orthogonal Projection and Estimation (HOPE), to CNNs in order to introduce orthogonality into the CNN structure. The HOPE model can be viewed as a hybrid model to combine feature extraction using orthogonal linear projection with mixture models. It is an effective model to extract useful information from the original high-dimension feature vectors and meanwhile filter out irrelevant noises. In this work, we present three different ways to apply the HOPE models to CNNs, i.e., {\em HOPE-Input}, {\em single-HOPE-Block} and {\em multi-HOPE-Blocks}. For {\em HOPE-Input} CNNs, a HOPE layer is directly used right after the input to de-correlate high-dimension input feature vectors. Alternatively, in {\em single-HOPE-Block} and {\em multi-HOPE-Blocks} CNNs, we consider to use HOPE layers to replace one or more blocks in the CNNs, where one block may include several convolutional layers and one pooling layer. The experimental results on both Cifar-10 and Cifar-100 data sets have shown that the orthogonal constraints imposed by the HOPE layers can significantly improve the performance of CNNs in these image classification tasks (we have achieved one of the best performance when image augmentation has not been applied, and top 5 performance with image augmentation).",Ileconeneushyorpranes,10.0,38.0,1.0
4371,Image augmentation,401.0,"perturb, predict & paraphrase: semi-supervised learning using noisy student for image captioning",1.0,48.0,4.0,201.0,1.0,2.2,199.8,99,http://arxiv.org/abs/0801.0251v1,"The Berger model of perturbative fragmentation of quarks to pions is improved by providing an absolute normalization and keeping all terms in a (1-z) expansion, which makes the calculation valid at all values of fractional pion momentum z. We also replace the nonrelativistic wave function of a loosely bound pion by the more realistic procedure of projecting to the light-cone pion wave function, which in turn is taken from well known models. The full calculation does not confirm the (1-z)^2 behavior of the fragmentation function (FF) predicted in for $z>0.5$, and only works at very large z>0.95, where it is in reasonable agreement with phenomenological FFs. Otherwise, we observe quite a different z-dependence which grossly underestimates data at smaller z. The disagreement is reduced after the addition of pions from decays of light vector mesons, but still remains considerable. The process dependent higher twist terms are also calculated exactly and found to be important at large z and/or pT.",Ipepr&paseleusnostfoimca,0.0,38.0,0.0
4372,Image augmentation,401.0,image augmentation using a task guided generative adversarial network for age estimation on brain mri,1.0,49.0,4.0,201.0,1.0,2.2,200.2,100,http://arxiv.org/pdf/2108.01659v1,"Brain age estimation based on magnetic resonance imaging (MRI) is an active research area in early diagnosis of some neurodegenerative diseases (e.g. Alzheimer, Parkinson, Huntington, etc.) for elderly people or brain underdevelopment for the young group. Deep learning methods have achieved the state-of-the-art performance in many medical image analysis tasks, including brain age estimation. However, the performance and generalisability of the deep learning model are highly dependent on the quantity and quality of the training data set. Both collecting and annotating brain MRI data are extremely time-consuming. In this paper, to overcome the data scarcity problem, we propose a generative adversarial network (GAN) based image synthesis method. Different from the existing GAN-based methods, we integrate a task-guided branch (a regression model for age estimation) to the end of the generator in GAN. By adding a task-guided loss to the conventional GAN loss, the learned low-dimensional latent space and the synthesised images are more task-specific. It helps to boost the performance of the down-stream task by combining the synthesised images and real images for model training. The proposed method was evaluated on a public brain MRI data set for age estimation. Our proposed method outperformed (statistically significant) a deep convolutional neural network based regression model and the GAN-based image synthesis method without the task-guided branch. More importantly, it enables the identification of age-related brain regions in the image space. The code is available on GitHub (https://github.com/ruizhe-l/tgb-gan).",Iimauusatagugeadnefoagesonbrmr,0.0,27.0,0.0
4604,Image classification,401.0,bag of tricks for image classification with convolutional neural networks,1.0,87.0,4.0,8.0,5.0,3.4000000000000004,157.5,1,http://arxiv.org/pdf/1812.01187v2,"Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.",Ibaoftrfoimclwiconene,461.0,42.0,39.0
4605,Image classification,8.0,residual attention network for image classification,5.0,201.0,1.0,3.0,5.0,3.4,83.70000000000002,2,https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Residual_Attention_Network_CVPR_2017_paper.pdf,"In this work, we propose Residual Attention Network, a convolutional neural network using attention mechanism which can incorporate with state-of-art feed forward network architecture in an end-to-end training fashion. Our Residual Attention Network is built by stacking Attention Modules which generate attention-aware features. The attention-aware features from different modules change adaptively as layers going deeper. Inside each Attention Module, bottom-up top-down feedforward structure is used to unfold the feedforward and feedback attention process into a single feedforward process. Importantly, we propose attention residual learning to train very deep Residual Attention Networks which can be easily scaled up to hundreds of layers. Extensive analyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the effectiveness of every module mentioned above. Our Residual Attention Network achieves state-of-the-art object recognition performance on three benchmark datasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and ImageNet (4.8% single model and single crop, top-5 error). Note that, our method achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69% forward FLOPs comparing to ResNet-200. The experiment also demonstrates that our network is robust against noisy labels.",Ireatnefoimcl,1667.0,41.0,123.0
4606,Image classification,13.0,the effectiveness of data augmentation in image classification using deep learning,5.0,201.0,1.0,11.0,5.0,3.4,87.60000000000001,3,https://arxiv.org/pdf/1712.04621.pdf?source=post_page---------------------------,"In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.",Ithefofdaauinimclusdele,1411.0,17.0,29.0
4607,Image classification,15.0,deep convolutional neural networks for image classification: a comprehensive review,5.0,201.0,1.0,12.0,5.0,3.4,88.5,4,https://www.researchgate.net/profile/Zenghui-Wang-6/publication/317496930_Deep_Convolutional_Neural_Networks_for_Image_Classification_A_Comprehensive_Review/links/59f814630f7e9b553ebefe27/Deep-Convolutional-Neural-Networks-for-Image-Classification-A-Comprehensive-Review.pdf,"Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges.",Ideconenefoimclacore,1190.0,315.0,36.0
4608,Image classification,17.0,multi-column deep neural networks for image classification,5.0,201.0,1.0,10.0,5.0,3.4,88.5,5,https://arxiv.org/pdf/1202.2745,"Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.",Imudenenefoimcl,3211.0,44.0,154.0
4609,Image classification,22.0,locality-constrained linear coding for image classification,5.0,201.0,1.0,9.0,5.0,3.4,89.7,6,http://www.rogerioferis.com/VisualRecognitionAndSearch2013/material/Class3Sparse3.pdf,"The traditional SPM approach based on bag-of-features (BoF) requires nonlinear classifiers to achieve good image classification performance. This paper presents a simple but effective coding scheme called Locality-constrained Linear Coding (LLC) in place of the VQ coding in traditional SPM. LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation. With linear classifier, the proposed approach performs remarkably better than the traditional nonlinear SPM, achieving state-of-the-art performance on several benchmarks. Compared with the sparse coding strategy [22], the objective function used by LLC has an analytical solution. In addition, the paper proposes a fast approximated LLC method by first performing a K-nearest-neighbor search and then solving a constrained least square fitting problem, bearing computational complexity of O(M + K2). Hence even with very large codebooks, our system can still process multiple frames per second. This efficiency significantly adds to the practical values of LLC for real applications.",Ilolicofoimcl,3213.0,29.0,595.0
4610,Image classification,31.0,deep inside convolutional networks: visualising image classification models and saliency maps,5.0,201.0,1.0,1.0,5.0,3.4,90.0,7,"https://arxiv.org/pdf/1312.6034.pdf,","This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].",Ideinconeviimclmoansama,3859.0,17.0,392.0
4611,Image classification,12.0,improving the fisher kernel for large-scale image classification,5.0,201.0,1.0,22.0,5.0,3.4,90.6,8,https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_11.pdf,"The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.",Iimthfikefolaimcl,2603.0,32.0,364.0
4612,Image classification,37.0,deep convolutional neural network based medical image classification for disease diagnosis,5.0,201.0,1.0,17.0,5.0,3.4,96.6,9,http://arxiv.org/pdf/1803.02544v3,"Medical image classification plays an essential role in clinical treatment and teaching tasks. However, the traditional method has reached its ceiling on performance. Moreover, by using them, much time and effort need to be spent on extracting and selecting classification features. The deep neural network is an emerging machine learning method that has proven its potential for different classification tasks. Notably, the convolutional neural network dominates with the best results on varying image classification tasks. However, medical image datasets are hard to collect because it needs a lot of professional expertise to label them. Therefore, this paper researches how to apply the convolutional neural network (CNN) based algorithm on a chest X-ray dataset to classify pneumonia. Three techniques are evaluated through experiments. These are linear support vector machine classifier with local rotation and orientation free features, transfer learning on two convolutional neural network models: Visual Geometry Group i.e., VGG16 and InceptionV3, and a capsule network training from scratch. Data augmentation is a data preprocessing method applied to all three methods. The results of the experiments show that data augmentation generally is an effective way for all three algorithms to improve performance. Also, Transfer learning is a more useful classification method on a small dataset compared to a support vector machine with oriented fast and rotated binary (ORB) robust independent elementary features and capsule network. In transfer learning, retraining specific features on a new target dataset is essential to improve performance. And, the second important factor is a proper network complexity that matches the scale of the dataset.",Ideconenebameimclfodidi,169.0,45.0,1.0
4613,Image classification,39.0,linear spatial pyramid matching using sparse coding for image classification,5.0,201.0,1.0,39.0,5.0,3.4,103.8,10,http://webia.lip6.fr/~thomen/Teaching/RDFIA/CVPR09-ScSPM.pdf,"Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.",Ilisppymausspcofoimcl,1970.0,27.0,290.0
4614,Image classification,401.0,very deep convolutional networks for large-scale image recognition,1.0,19.0,5.0,128.0,3.0,3.2,166.29999999999998,11,http://arxiv.org/pdf/1507.02159v1,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",Ivedeconefolaimre,55065.0,55.0,9657.0
4615,Image classification,401.0,deep residual learning for image recognition,1.0,15.0,5.0,148.0,3.0,3.2,170.7,12,http://arxiv.org/pdf/1612.05400v1,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",Iderelefoimre,79301.0,61.0,15628.0
4616,Image classification,4.0,image classification with the fisher vector: theory and practice,5.0,201.0,1.0,45.0,4.0,3.1,95.1,13,https://hal.inria.fr/hal-00779493/file/RR-8209.pdf,"A standard approach to describe an image for classification and retrieval purposes is to extract a set of local patch descriptors, encode them into a high dimensional vector and pool them into an image-level signature. The most common patch encoding strategy consists in quantizing the local descriptors into a finite set of prototypical elements. This leads to the popular Bag-of-Visual words representation. In this work, we propose to use the Fisher Kernel framework as an alternative patch encoding strategy: we describe patches by their deviation from an “universal” generative Gaussian mixture model. This representation, which we call Fisher vector has many advantages: it is efficient to compute, it leads to excellent results even with efficient linear classifiers, and it can be compressed with a minimal loss of accuracy using product quantization. We report experimental results on five standard datasets—PASCAL VOC 2007, Caltech 256, SUN 397, ILSVRC 2010 and ImageNet10K—with up to 9M images and 10K classes, showing that the FV framework is a state-of-the-art patch encoding technique.",Iimclwithfivethanpr,1416.0,112.0,188.0
4617,Image classification,53.0,a baseline for few-shot image classification,4.0,201.0,1.0,2.0,5.0,3.1,96.9,14,https://arxiv.org/pdf/1909.02729,"Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the ""hardness"" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.",Iabafofeimcl,165.0,79.0,28.0
4618,Image classification,3.0,between-class learning for image classification,5.0,201.0,1.0,56.0,4.0,3.1,98.1,15,http://openaccess.thecvf.com/content_cvpr_2018/papers/Tokozume_Between-Class_Learning_for_CVPR_2018_paper.pdf,"In this paper, we propose a novel learning method for image classification called Between-Class learning (BC learning)1. We generate between-class images by mixing two images belonging to different classes with a random ratio. We then input the mixed image to the model and train the model to output the mixing ratio. BC learning has the ability to impose constraints on the shape of the feature distributions, and thus the generalization ability is improved. BC learning is originally a method developed for sounds, which can be digitally mixed. Mixing two image data does not appear to make sense; however, we argue that because convolutional neural networks have an aspect of treating input data as waveforms, what works on sounds must also work on images. First, we propose a simple mixing method using internal divisions, which surprisingly proves to significantly improve performance. Second, we propose a mixing method that treats the images as waveforms, which leads to a further improvement in performance. As a result, we achieved 19.4% and 2.26% top-1 errors on ImageNet-1K and CIFAR-10, respectively.2",Ibelefoimcl,114.0,32.0,10.0
4619,Image classification,47.0,visual attention consistency under image transforms for multi-label image classification,4.0,201.0,1.0,24.0,5.0,3.1,101.7,16,https://openaccess.thecvf.com/content_CVPR_2019/papers/Guo_Visual_Attention_Consistency_Under_Image_Transforms_for_Multi-Label_Image_Classification_CVPR_2019_paper.pdf,"Human visual perception shows good consistency for many multi-label image classification tasks under certain spatial transforms, such as scaling, rotation, flipping and translation. This has motivated the data augmentation strategy widely used in CNN classifier training -- transformed images are included for training by assuming the same class labels as their original images. In this paper, we further propose the assumption of perceptual consistency of visual attention regions for classification under such transforms, i.e., the attention region for a classification follows the same transform if the input image is spatially transformed. While the attention regions of CNN classifiers can be derived as an attention heatmap in middle layers of the network, we find that their consistency under many transforms are not preserved. To address this problem, we propose a two-branch network with an original image and its transformed image as inputs and introduce a new attention consistency loss that measures the attention heatmap consistency between two branches. This new loss is then combined with multi-label image classification loss for network training. Experiments on three datasets verify the superiority of the proposed network by achieving new state-of-the-art classification performance.",Iviatcounimtrfomuimcl,77.0,69.0,11.0
4620,Image classification,62.0,fundus image classification using vgg-19 architecture with pca and svd,4.0,201.0,1.0,15.0,5.0,3.1,103.5,17,https://www.mdpi.com/2073-8994/11/1/1/pdf,"Automated medical image analysis is an emerging field of research that identifies the disease with the help of imaging technology. Diabetic retinopathy (DR) is a retinal disease that is diagnosed in diabetic patients. Deep neural network (DNN) is widely used to classify diabetic retinopathy from fundus images collected from suspected persons. The proposed DR classification system achieves a symmetrically optimized solution through the combination of a Gaussian mixture model (GMM), visual geometry group network (VGGNet), singular value decomposition (SVD) and principle component analysis (PCA), and softmax, for region segmentation, high dimensional feature extraction, feature selection and fundus image classification, respectively. The experiments were performed using a standard KAGGLE dataset containing 35,126 images. The proposed VGG-19 DNN based DR model outperformed the AlexNet and spatial invariant feature transform (SIFT) in terms of classification accuracy and computational time. Utilization of PCA and SVD feature selection with fully connected (FC) layers demonstrated the classification accuracies of 92.21%, 98.34%, 97.96%, and 98.13% for FC7-PCA, FC7-SVD, FC8-PCA, and FC8-SVD, respectively.",Ifuimclusvgarwipcansv,245.0,49.0,9.0
4621,Image classification,11.0,advancements in image classification using convolutional neural network,5.0,201.0,1.0,67.0,4.0,3.1,103.8,18,https://arxiv.org/pdf/1905.03288,"Convolutional Neural Network (CNN) is the state-of-the-art for image classification task. Here we have briefly discussed different components of CNN. In this paper, We have explained different CNN architectures for image classification. Through this paper, we have shown advancements in CNN from LeNet-5 to latest SENet model. We have discussed the model description and training details of each model. We have also drawn a comparison among those models.",Iadinimclusconene,82.0,47.0,1.0
4622,Image classification,78.0,resmlp: feedforward networks for image classification with data-efficient training,4.0,54.0,4.0,201.0,1.0,3.1,105.3,19,https://arxiv.org/pdf/2105.03404.pdf?ref=https://githubhelp.com,"The clothing fashion reflects the common aesthetics that people share with each other in dressing. To recognize the fashion time of a clothing is meaningful for both an individual and the industry. In this paper, under the assumption that the clothing fashion changes year by year, the fashion-time recognition problem is mapped into a clothing-fashion classification problem. Specifically, a novel deep neural network is proposed which achieves accurate human body segmentation by fusing multi-scale convolutional features in a fully convolutional network, and then feature learning and fashion classification are performed on the segmented parts avoiding the influence of image background. In the experiments, 9,339 fashion images from 8 continuous years are collected for performance evaluation. The results demonstrate the effectiveness of the proposed body segmentation and fashion classification methods.",Irefenefoimclwidatr,39.0,71.0,12.0
4623,Image classification,29.0,pcanet: a simple deep learning baseline for image classification?,5.0,201.0,1.0,55.0,4.0,3.1,105.6,20,https://arxiv.org/pdf/1404.3606,"In this paper, we propose a very simple deep learning network for image classification that is based on very basic data processing components: 1) cascaded principal component analysis (PCA); 2) binary hashing; and 3) blockwise histograms. In the proposed architecture, the PCA is employed to learn multistage filter banks. This is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus called the PCA network (PCANet) and can be extremely easily and efficiently designed and learned. For comparison and to provide a better understanding, we also introduce and study two simple variations of PCANet: 1) RandNet and 2) LDANet. They share the same topology as PCANet, but their cascaded filters are either randomly selected or learned from linear discriminant analysis. We have extensively tested these basic networks on many benchmark visual data sets for different tasks, including Labeled Faces in the Wild (LFW) for face verification; the MultiPIE, Extended Yale B, AR, Facial Recognition Technology (FERET) data sets for face recognition; and MNIST for hand-written digit recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state-of-the-art features either prefixed, highly hand-crafted, or carefully learned [by deep neural networks (DNNs)]. Even more surprisingly, the model sets new records for many classification tasks on the Extended Yale B, AR, and FERET data sets and on MNIST variations. Additional experiments on other public data sets also demonstrate the potential of PCANet to serve as a simple but highly competitive baseline for texture classification and object recognition.",Ipcasidelebafoimcl,1055.0,76.0,81.0
4624,Image classification,23.0,a study on cnn transfer learning for image classification,5.0,201.0,1.0,63.0,4.0,3.1,106.20000000000002,21,https://www.researchgate.net/profile/Jordan-Bird/publication/325803364_A_Study_on_CNN_Transfer_Learning_for_Image_Classification/links/5bd8874b92851c6b279a23ea/A-Study-on-CNN-Transfer-Learning-for-Image-Classification.pdf,"Many image classification models have been introduced to help tackle the foremost issue of recognition accuracy. Image classification is one of the core problems in Computer Vision field with a large variety of practical applications. Examples include: object recognition for robotic manipulation, pedestrian or obstacle detection for autonomous vehicles, among others. A lot of attention has been associated with Machine Learning, specifically neural networks such as the Convolutional Neural Network (CNN) winning image classification competitions. This work proposes the study and investigation of such a CNN architecture model (i.e. Inception-v3) to establish whether it would work best in terms of accuracy and efficiency with new image datasets via Transfer Learning. The retrained model is evaluated, and the results are compared to some state-of-the-art approaches.",Iastoncntrlefoimcl,110.0,13.0,2.0
4625,Image classification,64.0,hierarchical convolutional neural networks for fashion image classification,4.0,201.0,1.0,26.0,5.0,3.1,107.4,22,http://arxiv.org/pdf/1803.03415v2,"Abstract Deep learning can be applied in various business fields for better performance. Especially, fashion-related businesses have started to apply deep learning techniques on their e-commerce such as apparel recognition, apparel search and retrieval engine, and automatic product recommendation. The most important backbone of these applications is the image classification task. However, apparel classification can be difficult due to its various apparel properties, and complexity in the depth of categorization. In other words, multi-class apparel classification can be hard and ambiguous to separate among similar classes. Here, we find the need of image classification reflecting hierarchical structure of apparel categories. In most of the previous studies, hierarchy has not been considered in image classification when using Convolutional Neural Networks (CNN), and not even in fashion image classification using other methodologies. In this paper, we propose to apply Hierarchical Convolutional Neural Networks (H CNN) on apparel classification. This study has contribution in that this is the first trial to apply hierarchical classification of apparel using CNN and has significance in that the proposed model is a knowledge embedded classifier outputting hierarchical information. We implement H CNN using VGGNet on Fashion-MNIST dataset. Results have shown that when using H CNN model, the loss gets decreased and the accuracy gets improved than the base model without hierarchical structure. We conclude that H CNN brings better performance in classifying apparel.",Ihiconenefofaimcl,65.0,23.0,2.0
4626,Image classification,83.0,capsule networks for hyperspectral image classification,4.0,201.0,1.0,21.0,5.0,3.1,111.6,23,http://repositori.uji.es/xmlui/bitstream/handle/10234/178002/IEEE+TRANSACTIONS+ON+GEOSCIENCE+AND+REMOTE+SENSING4.pdf?sequence=1,"Convolutional neural networks (CNNs) have recently exhibited an excellent performance in hyperspectral image classification tasks. However, the straightforward CNN-based network architecture still finds obstacles when effectively exploiting the relationships between hyperspectral imaging (HSI) features in the spectral–spatial domain, which is a key factor to deal with the high level of complexity present in remotely sensed HSI data. Despite the fact that deeper architectures try to mitigate these limitations, they also find challenges with the convergence of the network parameters, which eventually limit the classification performance under highly demanding scenarios. In this paper, we propose a new CNN architecture based on spectral–spatial capsule networks in order to achieve a highly accurate classification of HSIs while significantly reducing the network design complexity. Specifically, based on Hinton’s capsule networks, we develop a CNN model extension that redefines the concept of capsule units to become spectral–spatial units specialized in classifying remotely sensed HSI data. The proposed model is composed by several building blocks, called spectral–spatial capsules, which are able to learn HSI spectral–spatial features considering their corresponding spatial positions in the scene, their associated spectral signatures, and also their possible transformations. Our experiments, conducted using five well-known HSI data sets and several state-of-the-art classification methods, reveal that our HSI classification approach based on spectral–spatial capsules is able to provide competitive advantages in terms of both classification accuracy and computational time.",Icanefohyimcl,108.0,57.0,6.0
4627,Image classification,36.0,preprocessing for image classification by convolutional neural networks,5.0,201.0,1.0,99.0,4.0,3.1,120.9,24,http://arxiv.org/pdf/2005.03824v1,"In recent times, the Convolutional Neural Networks have become the most powerful method for image classification. Various researchers have shown the importance of network architecture in achieving better performances by making changes in different layers of the network. Some have shown the importance of the neuron's activation by using various types of activation functions. But here we have shown the importance of preprocessing techniques for image classification using the CIFAR10 dataset and three variations of the Convolutional Neural Network. The results that we have achieved, clearly shows that the Zero Component Analysis(ZCA) outperforms both the Mean Normalization and Standardization techniques for all the three networks and thus it is the most important preprocessing technique for image classification with Convolutional Neural Networks.",Iprfoimclbyconene,79.0,16.0,4.0
4628,Image classification,401.0,multi-scale dense networks for resource efficient image classification,1.0,174.0,3.0,14.0,5.0,3.0,194.1,25,http://arxiv.org/pdf/1703.09844v5,"In this paper we investigate image classification with computational resource limits at test time. Two such settings are: 1. anytime classification, where the network's prediction for a test example is progressively updated, facilitating the output of a prediction at any time; and 2. budgeted batch classification, where a fixed amount of computation is available to classify a set of examples that can be spent unevenly across ""easier"" and ""harder"" inputs. In contrast to most prior work, such as the popular Viola and Jones algorithm, our approach is based on convolutional neural networks. We train multiple classifiers with varying resource demands, which we adaptively apply during test time. To maximally re-use computation between the classifiers, we incorporate them as early-exits into a single deep convolutional neural network and inter-connect them with dense connectivity. To facilitate high quality classification early on, we use a two-dimensional multi-scale network architecture that maintains coarse and fine level features all-throughout the network. Experiments on three image-classification tasks demonstrate that our framework substantially improves the existing state-of-the-art in both settings.",Imudeneforeefimcl,315.0,40.0,55.0
4629,Image classification,1.0,a survey of image classification methods and techniques for improving classification performance,5.0,201.0,1.0,104.0,3.0,2.8,111.9,26,https://www.tandfonline.com/doi/pdf/10.1080/01431160600746456,"Image classification is a complex process that may be affected by many factors. This paper examines current practices, problems, and prospects of image classification. The emphasis is placed on the summarization of major advanced classification approaches and the techniques used for improving classification accuracy. In addition, some important issues affecting classification performance are discussed. This literature review suggests that designing a suitable image‐processing procedure is a prerequisite for a successful classification of remotely sensed data into a thematic map. Effective use of multiple features of remotely sensed data and the selection of a suitable classification method are especially significant for improving classification accuracy. Non‐parametric classifiers such as neural network, decision tree classifier, and knowledge‐based classification have increasingly become important approaches for multisource data classification. Integration of remote sensing, geographical information systems (GIS), and expert system emerges as a new research frontier. More research, however, is needed to identify and reduce uncertainties in the image‐processing chain to improve classification accuracy.",Iasuofimclmeantefoimclpe,2339.0,444.0,122.0
4630,Image classification,51.0,deep learning for remote sensing image classification: a survey,4.0,201.0,1.0,64.0,4.0,2.8,114.9,27,https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1264,"Remote sensing (RS) image classification plays an important role in the earth observation technology using RS data, having been widely exploited in both military and civil fields. However, due to the characteristics of RS data such as high dimensionality and relatively small amounts of labeled samples available, performing RS image classification faces great scientific and practical challenges. In recent years, as new deep learning (DL) techniques emerge, approaches to RS image classification with DL have achieved significant breakthroughs, offering novel opportunities for the research and development of RS image classification. In this paper, a brief overview of typical DL models is presented first. This is followed by a systematic review of pixel‐wise and scene‐wise RS image classification approaches that are based on the use of DL. A comparative analysis regarding the performances of typical DL‐based RS methods is also provided. Finally, the challenges and potential directions for further research are discussed.",Ideleforeseimclasu,109.0,126.0,1.0
4631,Image classification,61.0,deep convolutional neural networks for diabetic retinopathy detection by image classification,4.0,201.0,1.0,57.0,4.0,2.8,115.8,28,http://arxiv.org/pdf/2003.02261v1,"Abstract Diabetic retinopathy (DR) is a common complication of diabetes and one of the major causes of blindness in the active population. Many of the complications of DR can be prevented by blood glucose control and timely treatment. Since the varieties and the complexities of DR, it is really difficult for DR detection in the time-consuming manual diagnosis. This paper is to attempt towards finding an automatic way to classify a given set of fundus images. We bring convolutional neural networks (CNNs) power to DR detection, which includes 3 major difficult challenges: classification, segmentation and detection. Coupled with transfer learning and hyper-parameter tuning, we adopt AlexNet, VggNet, GoogleNet, ResNet, and analyze how well these models do with the DR image classification. We employ publicly available Kaggle platform for training these models. The best classification accuracy is 95.68% and the results have demonstrated the better accuracy of CNNs and transfer learning on DR image classification.",Ideconenefodiredebyimcl,118.0,27.0,5.0
4632,Image classification,63.0,multilabel image classification with regional latent semantic dependencies,4.0,201.0,1.0,59.0,4.0,2.8,117.0,29,https://arxiv.org/pdf/1612.01082,"Deep convolution neural networks (CNNs) have demonstrated advanced performance on single-label image classification, and various progress also has been made to apply CNN methods on multilabel image classification, which requires annotating objects, attributes, scene categories, etc., in a single shot. Recent state-of-the-art approaches to the multilabel image classification exploit the label dependencies in an image, at the global level, largely improving the labeling capacity. However, predicting small objects and visual concepts is still challenging due to the limited discrimination of the global visual features. In this paper, we propose a regional latent semantic dependencies model (RLSD) to address this problem. The utilized model includes a fully convolutional localization architecture to localize the regions that may contain multiple highly dependent labels. The localized regions are further sent to the recurrent neural networks to characterize the latent semantic dependencies at the regional level. Experimental results on several benchmark datasets show that our proposed model achieves the best performance compared to the state-of-the-art models, especially for predicting small objects occurring in the images. Also, we set up an upper bound model (RLSD+ft-RPN) using bounding-box coordinates during training, and the experimental results also show that our RLSD can approach the upper bound without using the bounding-box annotations, which is more realistic in the real world.",Imuimclwirelasede,95.0,70.0,10.0
4633,Image classification,43.0,graph convolutional networks for hyperspectral image classification,4.0,201.0,1.0,83.0,4.0,2.8,118.20000000000002,30,https://arxiv.org/pdf/2008.02457,"Convolutional neural networks (CNNs) have been attracting increasing attention in hyperspectral (HS) image classification due to their ability to capture spatial–spectral feature representations. Nevertheless, their ability in modeling relations between the samples remains limited. Beyond the limitations of grid sampling, graph convolutional networks (GCNs) have been recently proposed and successfully applied in irregular (or nongrid) data representation and analysis. In this article, we thoroughly investigate CNNs and GCNs (qualitatively and quantitatively) in terms of HS image classification. Due to the construction of the adjacency matrix on all the data, traditional GCNs usually suffer from a huge computational cost, particularly in large-scale remote sensing (RS) problems. To this end, we develop a new minibatch GCN (called miniGCN hereinafter), which allows to train large-scale GCNs in a minibatch fashion. More significantly, our miniGCN is capable of inferring out-of-sample data without retraining networks and improving classification performance. Furthermore, as CNNs and GCNs can extract different types of HS features, an intuitive solution to break the performance bottleneck of a single model is to fuse them. Since miniGCNs can perform batchwise network training (enabling the combination of CNNs and GCNs), we explore three fusion strategies: additive fusion, elementwise multiplicative fusion, and concatenation fusion to measure the obtained performance gain. Extensive experiments, conducted on three HS data sets, demonstrate the advantages of miniGCNs over GCNs and the superiority of the tested fusion strategies with regard to the single CNN or GCN models. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_GCN for the sake of reproducibility.",Igrconefohyimcl,108.0,52.0,3.0
4634,Image classification,5.0,a survey of image classification methods and techniques,5.0,201.0,1.0,141.0,3.0,2.8,124.2,31,https://www.researchgate.net/profile/Nilanjan-Dey-2/publication/269984702_A_survey_of_image_classification_methods_and_techniques/links/54a0b32a0cf257a636021a50/A-survey-of-image-classification-methods-and-techniques.pdf,"In this paper, we review the current activity of image classification methodologies and techniques. Image classification is a complex process which depends upon various factors. Here, we discuss about the current techniques, problems as well as prospects of image classification. The main focus will be on advanced classification techniques which are used for improving classification accuracy. Additionally, some important issues relating to classification performance are also discussed.",Iasuofimclmeante,97.0,14.0,6.0
4635,Image classification,57.0,a novel image classification method with cnn-xgboost model,4.0,201.0,1.0,89.0,4.0,2.8,124.2,32,http://arxiv.org/pdf/1908.03651v1,"Image classification problem is one of most important research directions in image processing and has become the focus of research in many years due to its diversity and complexity of image information. In view of the existing image classification models’ failure to fully utilize the information of images, this paper proposes a novel image classification method of combining the Convolutional Neural Network (CNN) and eXtreme Gradient Boosting (XGBoost), which are two outstanding classifiers. The presented CNN-XGBoost model provides more precise output by integrating CNN as a trainable feature extractor to automatically obtain features from input and XGBoost as a recognizer in the top level of the network to produce results. Experiments are implemented on the well-known MNIST and CIFAR-10 databases. The results prove that the new method performs better compared with other methods on the same databases, which verify the effectiveness of the proposed method in image classification problem.",Ianoimclmewicnmo,67.0,19.0,2.0
4636,Image classification,30.0,hyperspectral image classification using dictionary-based sparse representation,5.0,201.0,1.0,122.0,3.0,2.8,126.0,33,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.4143&rep=rep1&type=pdf,"A new sparsity-based algorithm for the classification of hyperspectral imagery is proposed in this paper. The proposed algorithm relies on the observation that a hyperspectral pixel can be sparsely represented by a linear combination of a few training samples from a structured dictionary. The sparse representation of an unknown pixel is expressed as a sparse vector whose nonzero entries correspond to the weights of the selected training samples. The sparse vector is recovered by solving a sparsity-constrained optimization problem, and it can directly determine the class label of the test sample. Two different approaches are proposed to incorporate the contextual information into the sparse recovery optimization problem in order to improve the classification performance. In the first approach, an explicit smoothing constraint is imposed on the problem formulation by forcing the vector Laplacian of the reconstructed image to become zero. In this approach, the reconstructed pixel of interest has similar spectral characteristics to its four nearest neighbors. The second approach is via a joint sparsity model where hyperspectral pixels in a small neighborhood around the test pixel are simultaneously represented by linear combinations of a few common training samples, which are weighted with a different set of coefficients for each pixel. The proposed sparsity-based algorithm is applied to several real hyperspectral images for classification. Experimental results show that our algorithm outperforms the classical supervised classifier support vector machines in most cases.",Ihyimclusdispre,895.0,68.0,149.0
4637,Image classification,10.0,satellite image classification methods and techniques: a review,5.0,201.0,1.0,144.0,3.0,2.8,126.6,34,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.695.1415&rep=rep1&type=pdf,Satellite image classification process involves grouping the image pixel values into meaningful categories. Several satellite image classification methods and techniques are available. Satellite image classification methods can be broadly classified into three categories 1) automatic 2) manual and 3) hybrid. All three methods have their own advantages and disadvantages. Majority of the satellite image classification methods fall under first category. Satellite image classification needs selection of appropriate classification method based on the requirements. The current research work is a study on satellite image classification methods and techniques. The research work also compares various researcher’s comparative results on satellite image classification methods.,Isaimclmeanteare,69.0,55.0,3.0
4638,Image classification,99.0,supervised deep feature extraction for hyperspectral image classification,4.0,201.0,1.0,60.0,4.0,2.8,128.10000000000002,35,http://arxiv.org/pdf/2012.10932v1,"Hyperspectral image classification has become a research focus in recent literature. However, well-designed features are still open issues that impact on the performance of classifiers. In this paper, a novel supervised deep feature extraction method based on siamese convolutional neural network (S-CNN) is proposed to improve the performance of hyperspectral image classification. First, a CNN with five layers is designed to directly extract deep features from hyperspectral cube, where the CNN can be intended as a nonlinear transformation function. Then, the siamese network composed by two CNNs is trained to learn features that show a low intraclass and high interclass variability. The important characteristic of the presented approach is that the S-CNN is supervised with a margin ranking loss function, which can extract more discriminative features for classification tasks. To demonstrate the effectiveness of the proposed feature extraction method, the features extracted from three widely used hyperspectral data sets are fed into a linear support vector machine (SVM) classifier. The experimental results demonstrate that the proposed feature extraction method in conjunction with a linear SVM classifier can obtain better classification performance than that of the conventional methods.",Isudefeexfohyimcl,113.0,44.0,7.0
4639,Image classification,19.0,"flexible, high performance convolutional neural networks for image classification",5.0,201.0,1.0,152.0,3.0,2.8,131.70000000000002,36,https://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewPDFInterstitial/3098/3425,"We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs, respectively.",Iflhipeconenefoimcl,1107.0,44.0,40.0
4640,Image classification,2.0,in defense of nearest-neighbor based image classification,5.0,201.0,1.0,185.0,3.0,2.8,136.5,37,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.450.4683&rep=rep1&type=pdf,"State-of-the-art image classification methods require an intensive learning/training stage (using SVM, Boosting, etc.) In contrast, non-parametric nearest-neighbor (NN) based image classifiers require no training time and have other favorable properties. However, the large performance gap between these two families of approaches rendered NN-based image classifiers useless. We claim that the effectiveness of non-parametric NN-based image classification has been considerably undervalued. We argue that two practices commonly used in image classification methods, have led to the inferior performance of NN-based image classifiers: (i) Quantization of local image descriptors (used to generate ""bags-of-words "", codebooks). (ii) Computation of 'image-to-image' distance, instead of 'image-to-class' distance. We propose a trivial NN-based classifier - NBNN, (Naive-Bayes nearest-neighbor), which employs NN- distances in the space of the local image descriptors (and not in the space of images). NBNN computes direct 'image- to-class' distances without descriptor quantization. We further show that under the Naive-Bayes assumption, the theoretically optimal image classifier can be accurately approximated by NBNN. Although NBNN is extremely simple, efficient, and requires no learning/training phase, its performance ranks among the top leading learning-based image classifiers. Empirical comparisons are shown on several challenging databases (Caltech-101 ,Caltech-256 and Graz-01).",Iindeofnebaimcl,1193.0,37.0,152.0
4641,Image classification,34.0,image classification using support vector machine and artificial neural network,5.0,201.0,1.0,194.0,3.0,2.8,148.8,38,http://imslab.org/mcml2012/data/paper/Group3_Image%20Classification%20using%20Support%20Vector%20Machine%20and%20Artificial%20Neural%20Network.pdf,"Image classification is one of classical problems of concern in image processing. There are various approaches for solving this problem. The aim of this paper is bring together two areas in which are Artificial Neural Network (ANN) and Support Vector Machine (SVM) applying for image classification. Firstly, we separate the image into many sub-images based on the features of images. Each sub-image is classified into the responsive class by an ANN. Finally, SVM has been compiled all the classify result of ANN. Our proposal classification model has brought together many ANN and one SVM. Let it denote ANN_SVM. ANN_SVM has been applied for Roman numerals recognition application and the precision rate is 86%. The experimental results show the feasibility of our proposal model.",Iimclussuvemaanarnene,103.0,21.0,4.0
4642,Image classification,401.0,an image is worth 16x16 words: transformers for image recognition at scale,1.0,2.0,5.0,201.0,1.0,2.6,181.4,39,http://arxiv.org/pdf/2010.11929v2,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",Ianimiswo16wotrfoimreatsc,1338.0,52.0,382.0
4643,Image classification,401.0,a simple framework for contrastive learning of visual representations,1.0,3.0,5.0,201.0,1.0,2.6,181.8,40,http://arxiv.org/pdf/2002.05709v3,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",Iasifrfocoleofvire,2455.0,68.0,774.0
4644,Image classification,401.0,searching for mobilenetv3,1.0,4.0,5.0,201.0,1.0,2.6,182.2,41,http://arxiv.org/pdf/1905.02244v5,"We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",Isefomo,1040.0,61.0,208.0
4645,Image classification,401.0,encoder-decoder with atrous separable convolution for semantic image segmentation,1.0,6.0,5.0,201.0,1.0,2.6,183.0,42,http://arxiv.org/pdf/1706.05587v3,"In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",Ienwiatsecofoseimse,3836.0,92.0,670.0
4646,Image classification,401.0,progressive neural architecture search,1.0,8.0,5.0,201.0,1.0,2.6,183.8,43,http://arxiv.org/pdf/1808.00391v1,"This paper addresses the difficult problem of finding an optimal neural architecture design for a given image classification task. We propose a method that aggregates two main results of the previous state-of-the-art in neural architecture search. These are, appealing to the strong sampling efficiency of a search scheme based on sequential model-based optimization (SMBO), and increasing training efficiency by sharing weights among sampled architectures. Sequential search has previously demonstrated its capabilities to find state-of-the-art neural architectures for image classification. However, its computational cost remains high, even unreachable under modest computational settings. Affording SMBO with weight-sharing alleviates this problem. On the other hand, progressive search with SMBO is inherently greedy, as it leverages a learned surrogate function to predict the validation error of neural architectures. This prediction is directly used to rank the sampled neural architectures. We propose to attenuate the greediness of the original SMBO method by relaxing the role of the surrogate function so it predicts architecture sampling probability instead. We demonstrate with experiments on the CIFAR-10 dataset that our method, denominated Efficient progressive neural architecture search (EPNAS), leads to increased search efficiency, while retaining competitiveness of found architectures.",Iprnearse,1149.0,52.0,167.0
4647,Image classification,401.0,learning transferable architectures for scalable image recognition,1.0,9.0,5.0,201.0,1.0,2.6,184.2,44,http://arxiv.org/pdf/1707.07012v4,"Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the ""NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or ""cell"") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named ""NASNet architecture"". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.",Iletrarfoscimre,2953.0,81.0,512.0
4648,Image classification,401.0,pyramid scene parsing network,1.0,10.0,5.0,201.0,1.0,2.6,184.6,45,http://arxiv.org/pdf/1612.01105v2,"Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",Ipyscpane,4603.0,47.0,771.0
4649,Image classification,401.0,wide residual networks,1.0,12.0,5.0,201.0,1.0,2.6,185.4,46,http://arxiv.org/pdf/1605.07146v4,"Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks",Iwirene,3617.0,40.0,746.0
4650,Image classification,401.0,identity mappings in deep residual networks,1.0,13.0,5.0,201.0,1.0,2.6,185.8,47,http://arxiv.org/pdf/1603.05027v3,"Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers",Iidmainderene,5611.0,33.0,924.0
4651,Image classification,401.0,"inception-v4, inception-resnet and the impact of residual connections on learning",1.0,14.0,5.0,201.0,1.0,2.6,186.2,48,http://arxiv.org/pdf/1602.07261v2,"Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge",Iininanthimofrecoonle,6952.0,26.0,773.0
4652,Image classification,401.0,rethinking the inception architecture for computer vision,1.0,16.0,5.0,201.0,1.0,2.6,187.0,49,http://arxiv.org/pdf/1710.07991v1,"Deep convolutional semantic segmentation (DCSS) learning doesn't converge to an optimal local minimum with random parameters initializations; a pre-trained model on the same domain becomes necessary to achieve convergence.In this work, we propose a joint cooperative end-to-end learning method for DCSS. It addresses many drawbacks with existing deep semantic segmentation learning; the proposed approach simultaneously learn both segmentation and classification; taking away the essential need of the pre-trained model for learning convergence. We present an improved inception based architecture with partial attention gating (PAG) over encoder information. The PAG also adds to achieve faster convergence and better accuracy for segmentation task. We will show the effectiveness of this learning on a diabetic retinopathy classification and segmentation dataset.",Irethinarfocovi,13238.0,28.0,1785.0
4653,Image classification,401.0,batch normalization: accelerating deep network training by reducing internal covariate shift,1.0,17.0,5.0,201.0,1.0,2.6,187.4,50,http://arxiv.org/pdf/1709.09603v3,"Batch Normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets.",Ibanoacdenetrbyreincosh,25984.0,33.0,1397.0
4654,Image classification,401.0,going deeper with convolutions,1.0,18.0,5.0,201.0,1.0,2.6,187.8,51,http://arxiv.org/pdf/1409.4842v1,"We propose a deep convolutional neural network architecture codenamed ""Inception"", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",Igodewico,26591.0,277.0,2983.0
4655,Image classification,401.0,searching for efficient multi-scale architectures for dense image prediction,1.0,22.0,5.0,201.0,1.0,2.6,189.4,52,http://arxiv.org/pdf/1809.04184v1,"The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.",Isefoefmuarfodeimpr,235.0,100.0,21.0
4656,Image classification,401.0,the inaturalist species classification and detection dataset,1.0,23.0,5.0,201.0,1.0,2.6,189.8,53,http://arxiv.org/pdf/1707.06642v2,"Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.",Ithinspclandeda,367.0,50.0,31.0
4657,Image classification,401.0,learning to remember rare events,1.0,24.0,5.0,201.0,1.0,2.6,190.2,54,http://arxiv.org/pdf/1703.03129v1,"Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training.   Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.",Iletoreraev,248.0,36.0,34.0
4658,Image classification,401.0,neural architecture search with reinforcement learning,1.0,25.0,5.0,201.0,1.0,2.6,190.6,55,http://arxiv.org/pdf/1808.00193v3,"Neural Architecture Search (NAS) is an important yet challenging task in network design due to its high computational consumption. To address this issue, we propose the Reinforced Evolutionary Neural Architecture Search (RE- NAS), which is an evolutionary method with the reinforced mutation for NAS. Our method integrates reinforced mutation into an evolution algorithm for neural architecture exploration, in which a mutation controller is introduced to learn the effects of slight modifications and make mutation actions. The reinforced mutation controller guides the model population to evolve efficiently. Furthermore, as child models can inherit parameters from their parents during evolution, our method requires very limited computational resources. In experiments, we conduct the proposed search method on CIFAR-10 and obtain a powerful network architecture, RENASNet. This architecture achieves a competitive result on CIFAR-10. The explored network architecture is transferable to ImageNet and achieves a new state-of-the-art accuracy, i.e., 75.7% top-1 accuracy with 5.36M parameters on mobile ImageNet. We further test its performance on semantic segmentation with DeepLabv3 on the PASCAL VOC. RENASNet outperforms MobileNet-v1, MobileNet-v2 and NASNet. It achieves 75.83% mIOU without being pre-trained on COCO.",Inearsewirele,2965.0,72.0,369.0
4659,Image classification,401.0,training data-efficient image transformers & distillation through attention,1.0,28.0,5.0,201.0,1.0,2.6,191.8,56,http://arxiv.org/pdf/2105.04070v1,"Previous robustness approaches for deep learning models such as data augmentation techniques via data transformation or adversarial training cannot capture real-world variations that preserve the semantics of the input, such as a change in lighting conditions. To bridge this gap, we present NaTra, an adversarial training scheme that is designed to improve the robustness of image classification algorithms. We target attributes of the input images that are independent of the class identification, and manipulate those attributes to mimic real-world natural transformations (NaTra) of the inputs, which are then used to augment the training dataset of the image classifier. Specifically, we apply \textit{Batch Inverse Encoding and Shifting} to map a batch of given images to corresponding disentangled latent codes of well-trained generative models. \textit{Latent Codes Expansion} is used to boost image reconstruction quality through the incorporation of extended feature maps. \textit{Unsupervised Attribute Directing and Manipulation} enables identification of the latent directions that correspond to specific attribute changes, and then produce interpretable manipulations of those attributes, thereby generating natural transformations to the input data. We demonstrate the efficacy of our scheme by utilizing the disentangled latent representations derived from well-trained GANs to mimic transformations of an image that are similar to real-world natural variations (such as lighting conditions or hairstyle), and train models to be invariant to these natural transformations. Extensive experiments show that our method improves generalization of classification models and increases its robustness to various real-world distortions",Itrdaimtr&dithat,397.0,84.0,141.0
4660,Image classification,401.0,extreme memorization via scale of initialization,1.0,30.0,5.0,201.0,1.0,2.6,192.6,57,http://arxiv.org/pdf/2008.13363v2,"We construct an experimental setup in which changing the scale of initialization strongly impacts the implicit regularization induced by SGD, interpolating from good generalization performance to completely memorizing the training set while making little progress on the test set. Moreover, we find that the extent and manner in which generalization ability is affected depends on the activation and loss function used, with $\sin$ activation demonstrating extreme memorization. In the case of the homogeneous ReLU activation, we show that this behavior can be attributed to the loss function. Our empirical investigation reveals that increasing the scale of initialization correlates with misalignment of representations and gradients across examples in the same class. This insight allows us to devise an alignment measure over gradients and representations which can capture this phenomenon. We demonstrate that our alignment measure correlates with generalization of deep models trained on image classification tasks.",Iexmeviscofin,2.0,54.0,0.0
4661,Image classification,401.0,scalable second order optimization for deep learning,1.0,31.0,5.0,201.0,1.0,2.6,193.0,58,http://arxiv.org/pdf/2002.09018v2,"Optimization in machine learning, both theoretical and applied, is presently dominated by first-order gradient methods such as stochastic gradient descent. Second-order optimization methods, that involve second derivatives and/or second order statistics of the data, are far less prevalent despite strong theoretical properties, due to their prohibitive computation, memory and communication costs. In an attempt to bridge this gap between theoretical and practical optimization, we present a scalable implementation of a second-order preconditioned method (concretely, a variant of full-matrix Adagrad), that along with several critical algorithmic and numerical improvements, provides significant convergence and wall-clock time improvements compared to conventional first-order methods on state-of-the-art deep models. Our novel design effectively utilizes the prevalent heterogeneous hardware architecture for training deep models, consisting of a multicore CPU coupled with multiple accelerator units. We demonstrate superior performance compared to state-of-the-art on very large learning tasks such as machine translation with Transformers, language modeling with BERT, click-through rate prediction on Criteo, and image classification on ImageNet with ResNet-50.",Iscseoropfodele,0.0,50.0,0.0
4662,Image classification,401.0,beyond synthetic noise: deep learning on controlled noisy labels,1.0,32.0,5.0,201.0,1.0,2.6,193.4,59,http://arxiv.org/pdf/1911.09781v3,"Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link: http://www.lujiang.info/cnlw.html",Ibesynodeleonconola,63.0,72.0,12.0
4663,Image classification,401.0,dataset distillation with infinitely wide convolutional networks,1.0,33.0,5.0,201.0,1.0,2.6,193.8,60,http://arxiv.org/pdf/2107.13034v2,"The effectiveness of machine learning algorithms arises from being able to extract useful features from large amounts of data. As model and dataset sizes increase, dataset distillation methods that compress large datasets into significantly smaller yet highly performant ones will become valuable in terms of training efficiency and useful feature extraction. To that end, we apply a novel distributed kernel based meta-learning framework to achieve state-of-the-art results for dataset distillation using infinitely wide convolutional neural networks. For instance, using only 10 datapoints (0.02% of original dataset), we obtain over 64% test accuracy on CIFAR-10 image classification task, a dramatic improvement over the previous best test accuracy of 40%. Our state-of-the-art results extend across many other settings for MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and SVHN. Furthermore, we perform some preliminary analyses of our distilled datasets to shed light on how they differ from naturally occurring data.",Idadiwiinwicone,0.0,66.0,0.0
4664,Image classification,401.0,can weight sharing outperform random architecture search? an investigation with tunas,1.0,35.0,5.0,201.0,1.0,2.6,194.6,61,http://arxiv.org/pdf/2008.06120v1,"Efficient Neural Architecture Search methods based on weight sharing have shown good promise in democratizing Neural Architecture Search for computer vision models. There is, however, an ongoing debate whether these efficient methods are significantly better than random search. Here we perform a thorough comparison between efficient and random search methods on a family of progressively larger and more challenging search spaces for image classification and detection on ImageNet and COCO. While the efficacies of both methods are problem-dependent, our experiments demonstrate that there are large, realistic tasks where efficient search methods can provide substantial gains over random search. In addition, we propose and evaluate techniques which improve the quality of searched architectures and reduce the need for manual hyper-parameter tuning.   Source code and experiment data are available at https://github.com/google-research/google-research/tree/master/tunas",Icaweshouraarseaninwitu,50.0,51.0,7.0
4665,Image classification,401.0,milking cowmask for semi-supervised image classification,1.0,36.0,5.0,201.0,1.0,2.6,195.0,62,http://arxiv.org/pdf/2003.12022v3,"Consistency regularization is a technique for semi-supervised learning that underlies a number of strong results for classification with few labeled data. It works by encouraging a learned model to be robust to perturbations on unlabeled data. Here, we present a novel mask-based augmentation method called CowMask. Using it to provide perturbations for semi-supervised consistency regularization, we achieve a state-of-the-art result on ImageNet with 10% labeled data, with a top-5 error of 8.76% and top-1 error of 26.06%. Moreover, we do so with a method that is much simpler than many alternatives. We further investigate the behavior of CowMask for semi-supervised learning by running many smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets, where we achieve results competitive with the state of the art, indicating that CowMask is widely applicable. We open source our code at https://github.com/google-research/google-research/tree/master/milking_cowmask",Imicofoseimcl,17.0,45.0,1.0
4666,Image classification,401.0,meta pseudo labels,1.0,37.0,5.0,201.0,1.0,2.6,195.4,63,http://arxiv.org/pdf/2003.10580v4,"We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2% on ImageNet, which is 1.6% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at https://github.com/google-research/google-research/tree/master/meta_pseudo_labels.",Imepsla,75.0,116.0,9.0
4667,Image classification,401.0,distilling effective supervision from severe label noise,1.0,38.0,5.0,201.0,1.0,2.6,195.8,64,http://arxiv.org/pdf/1910.00701v5,"Collecting large-scale data with clean labels for supervised training of neural networks is practically challenging. Although noisy labels are usually cheap to acquire, existing methods suffer a lot from label noise. This paper targets at the challenge of robust training at high label noise regimes. The key insight to achieve this goal is to wisely leverage a small trusted set to estimate exemplar weights and pseudo labels for noisy data in order to reuse them for supervised training. We present a holistic framework to train deep neural networks in a way that is highly invulnerable to label noise. Our method sets the new state of the art on various types of label noise and achieves excellent performance on large-scale datasets with real-world label noise. For instance, on CIFAR100 with a $40\%$ uniform noise ratio and only 10 trusted labeled data per class, our method achieves $80.2{\pm}0.3\%$ classification accuracy, where the error rate is only $1.4\%$ higher than a neural network trained without label noise. Moreover, increasing the noise ratio to $80\%$, our method still maintains a high accuracy of $75.5{\pm}0.2\%$, compared to the previous best accuracy $48.2\%$.   Source code available: https://github.com/google-research/google-research/tree/master/ieg",Idiefsufrselano,50.0,56.0,7.0
4668,Image classification,401.0,a benchmark for interpretability methods in deep neural networks,1.0,40.0,5.0,201.0,1.0,2.6,196.6,65,http://arxiv.org/pdf/1806.10758v3,"We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.",Iabefoinmeindenene,185.0,48.0,22.0
4669,Image classification,59.0,hyperspectral image classification via contextual deep learning,4.0,201.0,1.0,126.0,3.0,2.5,135.9,66,http://arxiv.org/pdf/2107.02988v1,"Because the reliability of feature for every pixel determines the accuracy of classification, it is important to design a specialized feature mining algorithm for hyperspectral image classification. We propose a feature learning algorithm, contextual deep learning, which is extremely effective for hyperspectral image classification. On the one hand, the learning-based feature extraction algorithm can characterize information better than the pre-defined feature extraction algorithm. On the other hand, spatial contextual information is effective for hyperspectral image classification. Contextual deep learning explicitly learns spectral and spatial features via a deep learning architecture and promotes the feature extractor using a supervised fine-tune strategy. Extensive experiments show that the proposed contextual deep learning algorithm is an excellent feature learning algorithm and can achieve good performance with only a simple classifier.",Ihyimclvicodele,104.0,38.0,3.0
4670,Image classification,54.0,convolutional neural networks for document image classification,4.0,201.0,1.0,139.0,3.0,2.5,138.3,67,https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICPR-2014/data/5209d168.pdf,"This paper presents a Convolutional Neural Network (CNN) for document image classification. In particular, document image classes are defined by the structural similarity. Previous approaches rely on hand-crafted features for capturing structural information. In contrast, we propose to learn features from raw image pixels using CNN. The use of CNN is motivated by the the hierarchical nature of document layout. Equipped with rectified linear units and trained with dropout, our CNN performs well even when document layouts present large inner-class variations. Experiments on public challenging datasets demonstrate the effectiveness of the proposed approach.",Iconenefodoimcl,102.0,26.0,7.0
4671,Image classification,104.0,robust transfer metric learning for image classification,3.0,201.0,1.0,90.0,4.0,2.5,138.60000000000002,68,https://par.nsf.gov/servlets/purl/10064744,"Metric learning has attracted increasing attention due to its critical role in image analysis and classification. Conventional metric learning always assumes that the training and test data are sampled from the same or similar distribution. However, to build an effective distance metric, we need abundant supervised knowledge (i.e., side/label information), which is generally inaccessible in practice, because of the expensive labeling cost. In this paper, we develop a robust transfer metric learning (RTML) framework to effectively assist the unlabeled target learning by transferring the knowledge from the well-labeled source domain. Specifically, RTML exploits knowledge transfer to mitigate the domain shift in two directions, i.e., sample space and feature space. In the sample space, domain-wise and class-wise adaption schemes are adopted to bridge the gap of marginal and conditional distribution disparities across two domains. In the feature space, our metric is built in a marginalized denoising fashion and low-rank constraint, which make it more robust to tackle noisy data in reality. Furthermore, we design an explicit rank constraint regularizer to replace the rank minimization NP-hard problem to guide the low-rank metric learning. Experimental results on several standard benchmarks demonstrate the effectiveness of our proposed RTML by comparing it with the state-of-the-art transfer learning and metric learning algorithms.",Irotrmelefoimcl,112.0,47.0,7.0
4672,Image classification,105.0,involvement of machine learning for breast cancer image classification: a survey,3.0,201.0,1.0,93.0,4.0,2.5,139.8,69,http://arxiv.org/pdf/2110.14013v1,"Breast cancer is one of the largest causes of women's death in the world today. Advance engineering of natural image classification techniques and Artificial Intelligence methods has largely been used for the breast-image classification task. The involvement of digital image classification allows the doctor and the physicians a second opinion, and it saves the doctors' and physicians' time. Despite the various publications on breast image classification, very few review papers are available which provide a detailed description of breast cancer image classification techniques, feature extraction and selection procedures, classification measuring parameterizations, and image classification findings. We have put a special emphasis on the Convolutional Neural Network (CNN) method for breast image classification. Along with the CNN method we have also described the involvement of the conventional Neural Network (NN), Logic Based classifiers such as the Random Forest (RF) algorithm, Support Vector Machines (SVM), Bayesian methods, and a few of the semisupervised and unsupervised methods which have been used for breast image classification.",Iinofmalefobrcaimclasu,61.0,202.0,4.0
4673,Image classification,48.0,mining mid-level features for image classification,4.0,201.0,1.0,155.0,3.0,2.5,141.3,70,https://hal.archives-ouvertes.fr/hal-00968299/file/ijcv2014.pdf,"Mid-level or semi-local features learnt using class-level information are potentially more distinctive than the traditional low-level local features constructed in a purely bottom-up fashion. At the same time they preserve some of the robustness properties with respect to occlusions and image clutter. In this paper we propose a new and effective scheme for extracting mid-level features for image classification, based on relevant pattern mining. In particular, we mine relevant patterns of local compositions of densely sampled low-level features. We refer to the new set of obtained patterns as Frequent Local Histograms or FLHs. During this process, we pay special attention to keeping all the local histogram information and to selecting the most relevant reduced set of FLH patterns for classification. The careful choice of the visual primitives and an extension to exploit both local and global spatial information allow us to build powerful bag-of-FLH-based image representations. We show that these bag-of-FLHs are more discriminative than traditional bag-of-words and yield state-of-the-art results on various image classification benchmarks, including Pascal VOC.",Imimifefoimcl,70.0,63.0,7.0
4674,Image classification,111.0,multi-scale 3d deep convolutional neural network for hyperspectral image classification,3.0,201.0,1.0,97.0,4.0,2.5,142.8,71,http://arxiv.org/abs/2009.11948v3,"Research in deep neural network (DNN) and deep learning has great progress for 1D (speech), 2D (image) and 3D (3D-object) recognition/classification problems. As HSI that with 2D spatial and 1D spectral information is quite different from 3D object image, the existing DNN cannot be directly extended to hyperspectral image (HSI) classification. A Multiscale 3D deep convolutional neural network (M3D-DCNN) is proposed for HSI classification, which could jointly learn both 2D Multi-scale spatial feature and 1D spectral feature from HSI data in an end-to-end approach, promising to achieve better results with large-scale dataset. Although without any hand-craft features or pre/post-processing like PCA, sparse coding etc, we achieve the state-of-the-art results on the standard datasets, which shows the technical validity and advancement of our method.",Imu3ddeconenefohyimcl,82.0,18.0,4.0
4675,Image classification,55.0,spatial pooling of heterogeneous features for image classification,4.0,201.0,1.0,154.0,3.0,2.5,143.1,72,http://arxiv.org/pdf/1611.05138v1,"In image classification tasks, one of the most successful algorithms is the bag-of-features (BoFs) model. Although the BoF model has many advantages, such as simplicity, generality, and scalability, it still suffers from several drawbacks, including the limited semantic description of local descriptors, lack of robust structures upon single visual words, and missing of efficient spatial weighting. To overcome these shortcomings, various techniques have been proposed, such as extracting multiple descriptors, spatial context modeling, and interest region detection. Though they have been proven to improve the BoF model to some extent, there still lacks a coherent scheme to integrate each individual module together. To address the problems above, we propose a novel framework with spatial pooling of complementary features. Our model expands the traditional BoF model on three aspects. First, we propose a new scheme for combining texture and edge-based local features together at the descriptor extraction level. Next, we build geometric visual phrases to model spatial context upon complementary features for midlevel image representation. Finally, based on a smoothed edgemap, a simple and effective spatial weighting scheme is performed to capture the image saliency. We test the proposed framework on several benchmark data sets for image classification. The extensive results show the superior performance of our algorithm over the state-of-the-art methods.",Isppoofhefefoimcl,88.0,55.0,3.0
4676,Image classification,41.0,latent dirichlet allocation models for image classification,4.0,201.0,1.0,181.0,3.0,2.5,147.0,73,http://arxiv.org/pdf/1511.02821v2,"Two new extensions of latent Dirichlet allocation (LDA), denoted topic-supervised LDA (ts-LDA) and class-specific-simplex LDA (css-LDA), are proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, ts--LDA models are introduced which replace the automated topic discovery of LDA with specified topics, identical to the classes of interest for classification. While this results in improvements in classification accuracy over existing LDA models, it compromises the ability of LDA to discover unanticipated structure of interest. This limitation is addressed by the introduction of css-LDA, an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e., a single set of topics shared across classes is replaced by multiple class-specific topic sets. The css-LDA model is shown to combine the labeling strength of topic-supervision with the flexibility of topic-discovery. Its effectiveness is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform existing LDA-based image classification approaches.",Iladialmofoimcl,85.0,30.0,5.0
4677,Image classification,159.0,deep cnns for microscopic image classification by exploiting transfer learning and feature concatenation,3.0,201.0,1.0,66.0,4.0,2.5,147.9,74,https://dr.ntu.edu.sg/bitstream/10356/136682/2/Deep%20CNNs%20for%20microscopic%20image%20classification%20by%20exploiting%20transfer%20learning%20and%20feature%20concatenation.pdf,"Deep convolutional neural networks (CNNs) have become one of the state-of-the-art methods for image classification in various domains. For biomedical image classification where the number of training images is generally limited, transfer learning using CNNs is often applied. Such technique extracts generic image features from nature image datasets and these features can be directly adopted for feature extraction in smaller datasets. In this paper, we propose a novel deep neural network architecture based on transfer learning for microscopic image classification. In our proposed network, we concatenate the features extracted from three pretrained deep CNNs. The concatenated features are then used to train two fully-connected layers to perform classification. In the experiments on both the 2D-Hela and the PAP-smear datasets, our proposed network architecture produces significant performance gains comparing to the neural network structure that uses only features extracted from single CNN and several traditional classification methods.",Idecnfomiimclbyextrleanfeco,86.0,26.0,4.0
4678,Image classification,94.0,spectral–spatial hyperspectral image classification based on knn,4.0,201.0,1.0,135.0,3.0,2.5,149.10000000000002,75,http://arxiv.org/pdf/2107.10638v1,"Fusion of spectral and spatial information is an effective way in improving the accuracy of hyperspectral image classification. In this paper, a novel spectral–spatial hyperspectral image classification method based on K nearest neighbor (KNN) is proposed, which consists of the following steps. First, the support vector machine is adopted to obtain the initial classification probability maps which reflect the probability that each hyperspectral pixel belongs to different classes. Then, the obtained pixel-wise probability maps are refined with the proposed KNN filtering algorithm that is based on matching and averaging nonlocal neighborhoods. The proposed method does not need sophisticated segmentation and optimization strategies while still being able to make full use of the nonlocal principle of real images by using KNN, and thus, providing competitive classification with fast computation. Experiments performed on two real hyperspectral data sets show that the classification results obtained by the proposed method are comparable to several recently proposed hyperspectral image classification methods.",Isphyimclbaonkn,79.0,30.0,4.0
4679,Image classification,44.0,discriminative feature fusion for image classification,4.0,201.0,1.0,199.0,3.0,2.5,153.3,76,https://hal.archives-ouvertes.fr/hal-00690244/file/CVPR2012.pdf,"Bag-of-words-based image classification approaches mostly rely on low level local shape features. However, it has been shown that combining multiple cues such as color, texture, or shape is a challenging and promising task which can improve the classification accuracy. Most of the state-of-the-art feature fusion methods usually aim to weight the cues without considering their statistical dependence in the application at hand. In this paper, we present a new logistic regression-based fusion method, called LRFF, which takes advantage of the different cues without being tied to any of them. We also design a new marginalized kernel by making use of the output of the regression model. We show that such kernels, surprisingly ignored so far by the computer vision community, are particularly well suited to achieve image classification tasks. We compare our approach with existing methods that combine color and shape on three datasets. The proposed learning-based feature fusion process clearly outperforms the state-of-the art fusion methods for image classification.",Idifefufoimcl,78.0,35.0,4.0
4680,Image classification,71.0,ensemble projection for semi-supervised image classification,4.0,201.0,1.0,179.0,3.0,2.5,155.4,77,http://openaccess.thecvf.com/content_iccv_2013/papers/Dai_Ensemble_Projection_for_2013_ICCV_paper.pdf,"This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) and performs plain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification, (2) our method lets itself combine well with these methods, and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as labeled ones, but rather from a random collection of images.",Ienprfoseimcl,82.0,36.0,15.0
4681,Image classification,172.0,discriminant deep belief network for high-resolution sar image classification,3.0,201.0,1.0,79.0,4.0,2.5,155.7,78,http://arxiv.org/pdf/1912.10803v1,"Classification plays an important role in many fields of synthetic aperture radar (SAR) image understanding and interpretation. Many scholars have devoted to design features to characterize the content of SAR images. However, it is still a challenge to design discriminative and robust features for SAR image classification. Recently, the deep learning has attracted much attention and has been successfully applied in many fields of computer vision. In this paper, a novel feature learning approach that is called discriminant deep belief network (DisDBN) is proposed to learning high-level features for SAR image classification, in which the discriminant features are learned by combining ensemble learning with a deep belief network in an unsupervised manner. Firstly, some subsets of SAR image patches are selected and marked with pseudo-labels to train weak classifiers. Secondly, the specific SAR image patch is characterized by a set of projection vectors that are obtained by projecting the SAR image patch onto each weak decision space spanned by each weak classifier. Finally, the discriminant features are generated by feeding the projection vectors to a DBN for SAR image classification. Experimental results demonstrate that better classification performance can be achieved by the proposed approach than the other state-of-the-art approaches. HighlightsA DisDBN is proposed to characterize SAR image patches in an unsupervised manner.Both the CPL and IPL are investigated to produce prototypes of SAR image patches.Some weak decision spaces are constructed based on the learned prototypes.A high-level feature is learned for the SAR image patch in a hierarchy manner.We show that our method can achieve a better classification performance.",Ididebenefohisaimcl,101.0,74.0,3.0
4682,Image classification,73.0,learning dictionary on manifolds for image classification,4.0,201.0,1.0,187.0,3.0,2.5,158.4,79,http://arxiv.org/abs/1112.5640v5,"At present, dictionary based models have been widely used in image classification. The image features are approximated as a linear combination of bases selected from the dictionary in a sparse space, resulting in compact patterns. The features applied to image classification usually reside on low dimensional manifolds embedded in a high dimensional ambient space; traditional sparse coding algorithm, however, does not consider this topological structure. It can be characterized naturally by linear coefficients that reconstruct each data point from its neighbors. One of the central issues here is how to determine the neighbors and learn the coefficients. In this paper, the geometrical structures are encoded in two situations. In simple cases when data points distribute on a single manifold, it is explicitly modeled by locally linear embedding algorithm combined with k-nearest neighbors. Nevertheless, in real-world scenarios, complex data points often lie on multiple manifolds. Sparse representation algorithm combined with k-nearest neighbors is instead utilized to construct the topological structures, because it is capable of approximating the data point by selecting its homogenous neighbors adaptively to guarantee the smoothness of each manifold. After obtaining the local fitting relationship, these two topological structures are then embedded into sparse coding algorithm as regularization terms to formulate the corresponding objective functions of dictionary learning on single manifold (DLSM) and dictionary learning on multiple manifolds (DLMM), respectively. Upon this, a coordinate descent scheme is proposed to solve the unified optimization problems. Experimental results on several benchmark data sets, such as Caltech-256, Caltech-101, Scene 15, and UIUC-Sports, show that our proposed algorithms equal or outperform other state-of-the-art image classification algorithms.",Iledionmafoimcl,70.0,38.0,2.0
4683,Image classification,72.0,effective use of frequent itemset mining for image classification,4.0,201.0,1.0,191.0,3.0,2.5,159.3,80,https://link.springer.com/content/pdf/10.1007/978-3-642-33718-5_16.pdf,"In this paper we propose a new and effective scheme for applying frequent itemset mining to image classification tasks. We refer to the new set of obtained patterns as Frequent Local Histograms or FLHs. During the construction of the FLHs, we pay special attention to keep all the local histogram information during the mining process and to select the most relevant reduced set of FLH patterns for classification. The careful choice of the visual primitives and some proposed extensions to exploit other visual cues such as colour or global spatial information allow us to build powerful bag-of-FLH-based image representations. We show that these bag-of-FLHs are more discriminative than traditional bag-of-words and yield state-of-the art results on various image classification benchmarks.",Iefusoffritmifoimcl,78.0,36.0,7.0
4684,Image classification,81.0,interpretable aesthetic features for affective image classification,4.0,201.0,1.0,182.0,3.0,2.5,159.3,81,https://hcsi.cs.tsinghua.edu.cn/Paper/Paper13/XiaohuiWang_ICIP2013.pdf,"Images can not only display contents themselves, but also convey emotions, e.g., excitement, sadness. Affective image classification is useful and hot in many fields such as computer vision and multimedia. Current researches usually consider the relationship model between images and emotions as a black box. They extract the traditional discursive visual features such as SIFT and wavelet textures, and use them directly upon various classification algorithms. However, these visual features are not interpretable, and people cannot know why such a set of features induce a particular emotion. And due to the highly subjective nature of images, the classification accuracies on these visual features are not satisfactory for a long time. We propose the interpretable aesthetic features to describe images inspired by art theories, which are intuitive, discriminative and easily understandable. Affective image classification based on these features can achieve higher accuracy, compared with the state-of-the-art. Specifically, the features can also intuitively explain why an image tends to convey a certain emotion. We also develop an emotion guided image gallery to demonstrate the proposed feature collection.",Iinaefefoafimcl,71.0,13.0,12.0
4685,Image classification,69.0,image classification based on effective extreme learning machine,4.0,201.0,1.0,200.0,3.0,2.5,161.10000000000002,82,http://arxiv.org/abs/1809.04120v3,"In this work, a new image classification method is proposed based on extreme k-means (EKM) and effective extreme learning machine (EELM). The proposed method has image decomposition with curvelet transform, reduces dimensionality with discriminative locality alignment (DLA), generates a set of distinctive features with EKM, and has a classification with EELM. Since EKM has a better clustering performance than k-means and EELM has a better accuracy than ELM, the proposed EKM-EELM algorithm has a significant improvement in classification rate. Extensive experiments are performed using challenging databases and results are compared against state of the art techniques. Experimental results show that the proposed method has superior performances on classification rate than some other traditional methods for image classification.",Iimclbaonefexlema,60.0,33.0,0.0
4686,Image classification,401.0,some improvements on deep convolutional neural network based image classification,1.0,187.0,3.0,117.0,3.0,2.4000000000000004,230.2,83,http://arxiv.org/pdf/1312.5402v1,"Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner.",Isoimondeconenebaimcl,331.0,9.0,11.0
4687,Image classification,6.0,image classification for content-based indexing,5.0,201.0,1.0,201.0,1.0,2.2,142.5,84,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.7732&rep=rep1&type=pdf,"Geometric feature extraction is a crucial component of point cloud registration pipelines. Recent work has demonstrated how supervised learning can be leveraged to learn better and more compact 3D features. However, those approaches' reliance on ground-truth annotation limits their scalability. We propose BYOC: a self-supervised approach that learns visual and geometric features from RGB-D video without relying on ground-truth pose or correspondence. Our key observation is that randomly-initialized CNNs readily provide us with good correspondences; allowing us to bootstrap the learning of both visual and geometric features. Our approach combines classic ideas from point cloud registration with more recent representation learning approaches. We evaluate our approach on indoor scene datasets and find that our method outperforms traditional and learned descriptors, while being competitive with current state-of-the-art supervised approaches.",Iimclfocoin,863.0,47.0,22.0
4688,Image classification,7.0,eigenregions for image classification,5.0,201.0,1.0,201.0,1.0,2.2,142.8,85,https://infoscience.epfl.ch/record/33820/files/01343851.pdf,"The improvement of the accuracy of image query retrieval used image classification technique. Image classification is well known technique of supervised learning. The improved method of image classification increases the working efficiency of image query retrieval. For the improvements of classification technique we used RBF neural network function for better prediction of feature used in image retrieval.Colour content is represented by pixel values in image classification using radial base function(RBF) technique. This approach provides better result compare to SVM technique in image representation.Image is represented by matrix though RBF using pixel values of colour intensity of image. Firstly we using RGB colour model. In this colour model we use red, green and blue colour intensity values in matrix.SVM with partical swarm optimization for image classification is implemented in content of images which provide better Results based on the proposed approach are found encouraging in terms of color image classification accuracy.",Ieifoimcl,63.0,24.0,2.0
4689,Image classification,9.0,kernel-based methods for hyperspectral image classification,5.0,201.0,1.0,201.0,1.0,2.2,143.4,86,http://rslab.disi.unitn.it/papers/R36-TGARS-kernel-hyper.pdf,"In this paper, we proposed a novel pipeline for image-level classification in the hyperspectral images. By doing this, we show that the discriminative spectral information at image-level features lead to significantly improved performance in a face recognition task. We also explored the potential of traditional feature descriptors in the hyperspectral images. From our evaluations, we observe that SIFT features outperform the state-of-the-art hyperspectral face recognition methods, and also the other descriptors. With the increasing deployment of hyperspectral sensors in a multitude of applications, we believe that our approach can effectively exploit the spectral information in hyperspectral images, thus beneficial to more accurate classification.",Ikemefohyimcl,1301.0,55.0,74.0
4690,Image classification,14.0,composite kernels for hyperspectral image classification,5.0,201.0,1.0,201.0,1.0,2.2,144.9,87,https://www.researchgate.net/profile/Javier-Calpe-2/publication/3449668_Composite_Kernels_for_Hyperspectral_Image_Classification/links/0c9605177d3376bd19000000/Composite-Kernels-for-Hyperspectral-Image-Classification.pdf,"With the emergence of passive and active optical sensors available for geospatial imaging, information fusion across sensors is becoming ever more important. An important aspect of single (or multiple) sensor geospatial image analysis is feature extraction - the process of finding ""optimal"" lower dimensional subspaces that adequately characterize class-specific information for subsequent analysis tasks, such as classification, change and anomaly detection etc. In recent work, we proposed and developed an angle-based discriminant analysis approach that projected data onto subspaces with maximal ""angular"" separability in the input (raw) feature space and Reproducible Kernel Hilbert Space (RKHS). We also developed an angular locality preserving variant of this algorithm. In this letter, we advance this work and make it suitable for information fusion - we propose and validate a composite kernel local angular discriminant analysis projection, that can operate on an ensemble of feature sources (e.g. from different sources), and project the data onto a unified space through composite kernels where the data are maximally separated in an angular sense. We validate this method with the multi-sensor University of Houston hyperspectral and LiDAR dataset, and demonstrate that the proposed method significantly outperforms other composite kernel approaches to sensor (information) fusion.",Icokefohyimcl,950.0,29.0,118.0
4691,Image classification,16.0,a relative evaluation of multiclass image classification by support vector machines,5.0,201.0,1.0,201.0,1.0,2.2,145.5,88,https://core.ac.uk/download/pdf/162671153.pdf,"This paper adresses the problem of interactive multiclass segmentation. We propose a fast and efficient new interactive segmentation method called Superpixel Classification-based Interactive Segmentation (SCIS). From a few strokes drawn by a human user over an image, this method extracts relevant semantic objects. To get a fast calculation and an accurate segmentation, SCIS uses superpixel over-segmentation and support vector machine classification. In this paper, we demonstrate that SCIS significantly outperfoms competing algorithms by evaluating its performances on the reference benchmarks of McGuinness and Santner.",Iareevofmuimclbysuvema,905.0,47.0,44.0
4692,Image classification,18.0,convolutional neural networks for image classification,5.0,201.0,1.0,201.0,1.0,2.2,146.10000000000002,89,http://arxiv.org/pdf/1810.03946v1,"Currently, increasingly deeper neural networks have been applied to improve their accuracy. In contrast, We propose a novel wider Convolutional Neural Networks (CNN) architecture, motivated by the Multi-column Deep Neural Networks and the Network In Network(NIN), aiming for higher accuracy without input data transmutation. In our architecture, namely ""CNN In Convolution""(CNNIC), a small CNN, instead of the original generalized liner model(GLM) based filters, is convoluted as kernel on the original image, serving as feature extracting layer of this networks. And further classifications are then carried out by a global average pooling layer and a softmax layer. Dropout and orthonormal initialization are applied to overcome training difficulties including slow convergence and over-fitting. Persuasive classification performance is demonstrated on MNIST.",Iconenefoimcl,4.0,15.0,0.0
4693,Image classification,20.0,image classification at supercomputer scale,5.0,201.0,1.0,201.0,1.0,2.2,146.7,90,https://arxiv.org/pdf/1811.06992,"Deep learning is extremely computationally intensive, and hardware vendors have responded by building faster accelerators in large clusters. Training deep learning models at petaFLOPS scale requires overcoming both algorithmic and systems software challenges. In this paper, we discuss three systems-related optimizations: (1) distributed batch normalization to control per-replica batch sizes, (2) input pipeline optimizations to sustain model throughput, and (3) 2-D torus all-reduce to speed up gradient summation. We combine these optimizations to train ResNet-50 on ImageNet to 76.3% accuracy in 2.2 minutes on a 1024-chip TPU v3 Pod with a training throughput of over 1.05 million images/second and no accuracy drop.",Iimclatsusc,89.0,24.0,12.0
4694,Image classification,21.0,sampling strategies for bag-of-features image classification,5.0,201.0,1.0,201.0,1.0,2.2,147.0,91,https://link.springer.com/content/pdf/10.1007/11744085_38.pdf,"Transformers with powerful global relation modeling abilities have been introduced to fundamental computer vision tasks recently. As a typical example, the Vision Transformer (ViT) directly applies a pure transformer architecture on image classification, by simply splitting images into tokens with a fixed length, and employing transformers to learn relations between these tokens. However, such naive tokenization could destruct object structures, assign grids to uninterested regions such as background, and introduce interference signals. To mitigate the above issues, in this paper, we propose an iterative and progressive sampling strategy to locate discriminative regions. At each iteration, embeddings of the current sampling step are fed into a transformer encoder layer, and a group of sampling offsets is predicted to update the sampling locations for the next step. The progressive sampling is differentiable. When combined with the Vision Transformer, the obtained PS-ViT network can adaptively learn where to look. The proposed PS-ViT is both effective and efficient. When trained from scratch on ImageNet, PS-ViT performs 3.8% higher than the vanilla ViT in terms of top-1 accuracy with about $4\times$ fewer parameters and $10\times$ fewer FLOPs. Code is available at https://github.com/yuexy/PS-ViT.",Isastfobaimcl,1100.0,25.0,74.0
4695,Image classification,24.0,efficient hik svm learning for image classification,5.0,201.0,1.0,201.0,1.0,2.2,147.9,92,http://arxiv.org/abs/1202.2194v4,"Supervised statistical classification is a vital tool for satellite image processing. It is useful not only when a discrete result, such as feature extraction or surface type, is required, but also for continuum retrievals by dividing the quantity of interest into discrete ranges. Because of the high resolution of modern satellite instruments and because of the requirement for real-time processing, any algorithm has to be fast to be useful. Here we describe an algorithm based on kernel estimation called Adaptive Gaussian Filtering that incorporates several innovations to produce superior efficiency as compared to three other popular methods: k-nearest-neighbour (KNN), Learning Vector Quantization (LVQ) and Support Vector Machines (SVM). This efficiency is gained with no compromises: accuracy is maintained, while estimates of the conditional probabilities are returned. These are useful not only to gauge the accuracy of an estimate in the absence of its true value, but also to re-calibrate a retrieved image and as a proxy for a discretized continuum variable. The algorithm is demonstrated and compared with the other three on a pair of synthetic test classes and to map the waterways of the Netherlands. Software may be found at: http://libagf.sourceforge.net.",Iefhisvlefoimcl,70.0,34.0,5.0
4696,Image classification,25.0,survey on svm and their application in image classification,5.0,201.0,1.0,201.0,1.0,2.2,148.2,93,http://arxiv.org/abs/1203.5111v2,"Sky surveys represent a fundamental data basis for astronomy. We use them to map in a systematic way the universe and its constituents, and to discover new types of objects or phenomena. We review the subject, with an emphasis on the wide-field imaging surveys, placing them in a broader scientific and historical context. Surveys are the largest data generators in astronomy, propelled by the advances in information and computation technology, and have transformed the ways in which astronomy is done. We describe the variety and the general properties of surveys, the ways in which they may be quantified and compared, and offer some figures of merit that can be used to compare their scientific discovery potential. Surveys enable a very wide range of science; that is perhaps their key unifying characteristic. As new domains of the observable parameter space open up thanks to the advances in technology, surveys are often the initial step in their exploration. Science can be done with the survey data alone or a combination of different surveys, or with a targeted follow-up of potentially interesting selected sources. Surveys can be used to generate large, statistical samples of objects that can be studied as populations, or as tracers of larger structures. They can be also used to discover or generate samples of rare or unusual objects, and may lead to discoveries of some previously unknown types. We discuss a general framework of parameter spaces that can be used for an assessment and comparison of different surveys, and the strategies for their scientific exploration. As we move into the Petascale regime, an effective processing and scientific exploitation of such large data sets and data streams poses many challenges, some of which may be addressed in the framework of Virtual Observatory and Astroinformatics, with a broader application of data mining and knowledge discovery technologies.",Isuonsvanthapinimcl,34.0,68.0,0.0
4697,Image classification,26.0,image classification using random forests and ferns,5.0,201.0,1.0,201.0,1.0,2.2,148.5,94,http://course1.winona.edu/thooks/Media/Random%20Forest%20Ferns%20%28no%20abstract%29.pdf,"In this paper, we first apply random ferns for classification of real music recordings of a jazz band. No initial segmentation of audio data is assumed, i.e., no onset, offset, nor pitch data are needed. The notion of random ferns is described in the paper, to familiarize the reader with this classification algorithm, which was introduced quite recently and applied so far in image recognition tasks. The performance of random ferns is compared with random forests for the same data. The results of experiments are presented in the paper, and conclusions are drawn.",Iimclusrafoanfe,1347.0,31.0,135.0
4698,Image classification,27.0,a survey on artificial intelligence approaches for medical image classification,5.0,201.0,1.0,201.0,1.0,2.2,148.8,95,https://sciresol.s3.us-east-2.amazonaws.com/IJST/Articles/2011/Issue-11/Article34.pdf,"Medical image fusion is the process of registering and combining multiple images from single or multiple imaging modalities to improve the imaging quality and reduce randomness and redundancy in order to increase the clinical applicability of medical images for diagnosis and assessment of medical problems. Multi-modal medical image fusion algorithms and devices have shown notable achievements in improving clinical accuracy of decisions based on medical images. This review article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field of medical image fusion. We characterize the medical image fusion research based on (1) the widely used image fusion methods, (2) imaging modalities, and (3) imaging of organs that are under study. This review concludes that even though there exists several open ended technological and scientific challenges, the fusion of medical images has proved to be useful for advancing the clinical reliability of using medical imaging for medical diagnostics and analysis, and is a scientific discipline that has the potential to significantly grow in the coming years.",Iasuonarinapfomeimcl,99.0,76.0,2.0
4699,Image classification,28.0,transfer learning for image classification,5.0,201.0,1.0,201.0,1.0,2.2,149.10000000000002,96,http://arxiv.org/abs/1806.02682v1,"The field of image classification has shown an outstanding success thanks to the development of deep learning techniques. Despite the great performance obtained, most of the work has focused on natural images ignoring other domains like artistic depictions. In this paper, we use transfer learning techniques to propose a new classification network with better performance in illustration images. Starting from the deep convolutional network VGG19, pre-trained with natural images, we propose two novel models which learn object representations in the new domain. Our optimized network will learn new low-level features of the images (colours, edges, textures) while keeping the knowledge of the objects and shapes that it already learned from the ImageNet dataset. Thus, requiring much less data for the training. We propose a novel dataset of illustration images labelled by content where our optimized architecture achieves $\textbf{86.61\%}$ of top-1 and $\textbf{97.21\%}$ of top-5 precision. We additionally demonstrate that our model is still able to recognize objects in photographs.",Itrlefoimcl,55.0,117.0,0.0
4700,Image classification,32.0,a reliable method for cell phenotype image classification,5.0,201.0,1.0,201.0,1.0,2.2,150.3,97,https://www.academia.edu/download/50131452/A_reliable_method_for_cell_phenotype_ima20161105-24317-58cc7o.pdf,"Background: Single-cell RNA sequencing (scRNA-seq) yields valuable insights about gene expression and gives critical information about complex tissue cellular composition. In the analysis of single-cell RNA sequencing, the annotations of cell subtypes are often done manually, which is time-consuming and irreproducible. Garnett is a cell-type annotation software based the on elastic net method. Besides cell-type annotation, supervised machine learning methods can also be applied to predict other cell phenotypes from genomic data. Despite the popularity of such applications, there is no existing study to systematically investigate the performance of those supervised algorithms in various sizes of scRNA-seq data sets.   Methods and Results: This study evaluates 13 popular supervised machine learning algorithms to classify cell phenotypes, using published real and simulated data sets with diverse cell sizes. The benchmark contained two parts. In the first part, we used real data sets to assess the popular supervised algorithms' computing speed and cell phenotype classification performance. The classification performances were evaluated using AUC statistics, F1-score, precision, recall, and false-positive rate. In the second part, we evaluated gene selection performance using published simulated data sets with a known list of real genes.   Conclusion: The study outcomes showed that ElasticNet with interactions performed best in small and medium data sets. NB was another appropriate method for medium data sets. In large data sets, XGB works excellent. Ensemble algorithms were not significantly superior to individual machine learning methods. Adding interactions to ElasticNet can help, and the improvement was significant in small data sets.",Iaremefocephimcl,73.0,28.0,4.0
4701,Image classification,33.0,support vector machines for remote sensing image classification,5.0,201.0,1.0,201.0,1.0,2.2,150.60000000000002,98,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.11.5830&rep=rep1&type=pdf,"Support Vector Machines (SVMs) are a relatively new supervised classification technique to the land cover mapping community. They have their roots in Statistical Learning Theory and have gained prominence because they are robust, accurate and are effective even when using a small training sample. By their nature SVMs are essentially binary classifiers, however, they can be adopted to handle the multiple classification tasks common in remote sensing studies. The two approaches commonly used are the One-Against-One (1A1) and One-Against-All (1AA) techniques. In this paper, these approaches are evaluated in as far as their impact and implication for land cover mapping. The main finding from this research is that whereas the 1AA technique is more predisposed to yielding unclassified and mixed pixels, the resulting classification accuracy is not significantly different from 1A1 approach. It is the authors conclusions that ultimately the choice of technique adopted boils down to personal preference and the uniqueness of the dataset at hand.",Isuvemaforeseimcl,68.0,15.0,3.0
4702,Image classification,35.0,building powerful image classification models using very little data,5.0,201.0,1.0,201.0,1.0,2.2,151.2,99,https://deeplearning.lipingyang.org/wp-content/uploads/2016/12/Building-powerful-image-classification-models-using-very-little-data.pdf,"Although deep learning can provide promising results in medical image analysis, the lack of very large annotated datasets confines its full potential. Furthermore, limited positive samples also create unbalanced datasets which limit the true positive rates of trained models. As unbalanced datasets are mostly unavoidable, it is greatly beneficial if we can extract useful knowledge from negative samples to improve classification accuracy on limited positive samples. To this end, we propose a new strategy for building medical image analysis pipelines that target disease detection. We train a discriminative segmentation model only on normal images to provide a source of knowledge to be transferred to a disease detection classifier. We show that using the feature maps of a trained segmentation network, deviations from normal anatomy can be learned by a two-class classification network on an extremely unbalanced training dataset with as little as one positive for 17 negative samples. We demonstrate that even though the segmentation network is only trained on normal cardiac computed tomography images, the resulting feature maps can be used to detect pericardial effusion and cardiac septal defects with two-class convolutional classification networks.",Ibupoimclmousvelida,9.0,0.0,0.0
4703,Image classification,40.0,generic image classification using visual knowledge on the web,5.0,201.0,1.0,201.0,1.0,2.2,152.7,100,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.329.2585&rep=rep1&type=pdf,"Tables on the web constitute a valuable data source for many applications, like factual search and knowledge base augmentation. However, as genuine tables containing relational knowledge only account for a small proportion of tables on the web, reliable genuine web table classification is a crucial first step of table extraction. Previous works usually rely on explicit feature construction from the HTML code. In contrast, we propose an approach for web table classification by exploiting the full visual appearance of a table, which works purely by applying a convolutional neural network on the rendered image of the web table. Since these visual features can be extracted automatically, our approach circumvents the need for explicit feature construction. A new hand labeled gold standard dataset containing HTML source code and images for 13,112 tables was generated for this task. Transfer learning techniques are applied to well known VGG16 and ResNet50 architectures. The evaluation of CNN image classification with fine tuned ResNet50 (F1 93.29%) shows that this approach achieves results comparable to previous solutions using explicitly defined HTML code based features. By combining visual and explicit features, an F-measure of 93.70% can be achieved by Random Forest classification, which beats current state of the art methods.",Igeimclusviknonthwe,72.0,20.0,3.0
5061,Image generation,1.0,conditional image generation with pixelcnn decoders,5.0,57.0,4.0,2.0,5.0,4.6,23.700000000000003,1,https://arxiv.org/pdf/1606.05328),"This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",Icoimgewipide,1410.0,39.0,145.0
5062,Image generation,3.0,high-fidelity image generation with fewer labels,5.0,58.0,4.0,37.0,5.0,4.6,35.2,2,http://proceedings.mlr.press/v97/lucic19a/lucic19a.pdf,"Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.",Ihiimgewifela,90.0,45.0,7.0
5063,Image generation,2.0,draw: a recurrent neural network for image generation,5.0,113.0,3.0,23.0,5.0,4.2,52.7,3,http://proceedings.mlr.press/v37/gregor15.pdf,"This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",Idrarenenefoimge,1539.0,42.0,116.0
5064,Image generation,23.0,infinite nature: perpetual view generation of natural scenes from a single image,5.0,5.0,5.0,201.0,1.0,3.8,69.19999999999999,4,http://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Infinite_Nature_Perpetual_View_Generation_of_Natural_Scenes_From_a_ICCV_2021_paper.pdf,"We introduce the problem of perpetual view generation - long-range generation of novel views corresponding to an arbitrarily long camera trajectory given a single image. This is a challenging problem that goes far beyond the capabilities of current view synthesis methods, which quickly degenerate when presented with large camera motions. Methods for video generation also have limited ability to produce long sequences and are often agnostic to scene geometry. We take a hybrid approach that integrates both geometry and image synthesis in an iterative `\emph{render}, \emph{refine} and \emph{repeat}' framework, allowing for long-range generation that cover large distances after hundreds of frames. Our approach can be trained from a set of monocular video sequences. We propose a dataset of aerial footage of coastal scenes, and compare our method with recent view synthesis and conditional video generation baselines, showing that it can generate plausible scenes for much longer time horizons over large camera trajectories compared to existing methods. Project page at https://infinite-nature.github.io/.",Iinnapevigeofnascfrasiim,9.0,53.0,2.0
5065,Image generation,38.0,self-attention generative adversarial networks,5.0,10.0,5.0,201.0,1.0,3.8,75.7,5,http://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf,"Convolution has been arguably the most important feature transform for modern neural networks, leading to the advance of deep learning. Recent emergence of Transformer networks, which replace convolution layers with self-attention blocks, has revealed the limitation of stationary convolution kernels and opened the door to the era of dynamic feature transforms. The existing dynamic transforms, including self-attention, however, are all limited for video understanding where correspondence relations in space and time, i.e., motion information, are crucial for effective representation. In this work, we introduce a relational feature transform, dubbed the relational self-attention (RSA), that leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating relational contexts. Our experiments and ablation studies show that the RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1 & V2, Diving48, and FineGym.",Isegeadne,1667.0,57.0,192.0
5066,Image generation,30.0,instance normalization: the missing ingredient for fast stylization,5.0,23.0,5.0,201.0,1.0,3.8,78.5,6,https://arxiv.org/pdf/1607.08022,"It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.",Iinnothmiinfofast,1825.0,5.0,106.0
5067,Image generation,50.0,high-resolution image synthesis and semantic manipulation with conditional gans,4.0,32.0,5.0,201.0,1.0,3.5,88.1,7,https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_High-Resolution_Image_Synthesis_CVPR_2018_paper.pdf,"SN 1996cr, located in the Circinus Galaxy (3.7 Mpc, z ~ 0.001) was non-detected in X-rays at ~ 1000 days yet brightened to ~ 4 x 10^{39} erg/s (0.5-8 keV) after 10 years (Bauer et al. 2008). A 1-D hydrodynamic model of the ejecta-CSM interaction produces good agreement with the measured X-ray light curves and spectra at multiple epochs. We conclude that the progenitor of SN 1996cr could have been a massive star, M > 30 M_solar, which went from an RSG to a brief W-R phase before exploding within its ~ 0.04 pc wind-blown shell (Dwarkadas et al. 2010). Further analysis of the deep Chandra HETG observations allows line-shape fitting of a handful of bright Si and Fe lines in the spectrum. The line shapes are well fit by axisymmetric emission models with an axis orientation ~ 55 degrees to our line-of-sight. In the deep 2009 epoch the higher ionization Fe XXVI emission is constrained to high lattitudes: the Occam-est way to get the Fe H-like emission coming from high latitude/polar regions is to have more CSM at/around the poles than at mid and lower lattitudes, along with a symmetric ejecta explosion/distribution. Similar CSM/ejecta characterization may be possible for other SNe and, with higher-throughput X-ray observations, for gamma-ray burst remnants as well.",Ihiimsyansemawicoga,1870.0,76.0,289.0
5068,Image generation,95.0,gans trained by a two time-scale update rule converge to a local nash equilibrium,4.0,11.0,5.0,201.0,1.0,3.5,93.2,8,http://papers.nips.cc/paper/7240-gans-trained-by-a-two-time-scale-update-rule.pdf,"Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.",Igatrbyatwtiuprucotoalonaeq,3439.0,72.0,1152.0
5069,Image generation,401.0,zero-shot text-to-image generation,1.0,34.0,5.0,71.0,4.0,3.5,155.20000000000002,9,http://arxiv.org/pdf/1205.3915v1,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",Izetege,143.0,56.0,23.0
5070,Image generation,401.0,image generation from scene graphs,1.0,76.0,4.0,20.0,5.0,3.4000000000000004,156.7,10,http://arxiv.org/pdf/2110.11918v1,"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.",Iimgefrscgr,392.0,64.0,61.0
5071,Image generation,20.0,"show, attend and tell: neural image caption generation with visual attention",5.0,201.0,1.0,8.0,5.0,3.4,88.80000000000001,11,http://proceedings.mlr.press/v37/xuc15.pdf,"Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.",Ishatanteneimcagewiviat,6618.0,66.0,586.0
5072,Image generation,40.0,generative adversarial text to image synthesis,5.0,62.0,4.0,201.0,1.0,3.4,97.1,12,http://proceedings.mlr.press/v48/reed16.pdf,"As a sub-domain of text-to-image synthesis, text-to-face generation has huge potentials in public safety domain. With lack of dataset, there are almost no related research focusing on text-to-face synthesis. In this paper, we propose a fully-trained Generative Adversarial Network (FTGAN) that trains the text encoder and image decoder at the same time for fine-grained text-to-face generation. With a novel fully-trained generative network, FTGAN can synthesize higher-quality images and urge the outputs of the FTGAN are more relevant to the input sentences. In addition, we build a dataset called SCU-Text2face for text-to-face synthesis. Through extensive experiments, the FTGAN shows its superiority in boosting both generated images' quality and similarity to the input descriptions. The proposed FTGAN outperforms the previous state of the art, boosting the best reported Inception Score to 4.63 on the CUB dataset. On SCU-text2face, the face images generated by our proposed FTGAN just based on the input descriptions is of average 59% similarity to the ground-truth, which set a baseline for text-to-face synthesis.",Igeadtetoimsy,1931.0,48.0,162.0
5073,Image generation,401.0,deep image prior,1.0,28.0,5.0,128.0,3.0,3.2,169.9,13,http://arxiv.org/pdf/1712.05016v2,"Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, superresolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity.",Ideimpr,1079.0,34.0,215.0
5074,Image generation,4.0,unsupervised learning of object landmarks through conditional image generation,5.0,201.0,1.0,58.0,4.0,3.1,99.0,14,http://papers.nips.cc/paper/7657-unsupervised-learning-ofobject-landmarks-through-conditional-image-generation.pdf,"We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.",Iunleofoblathcoimge,120.0,58.0,25.0
5075,Image generation,5.0,gan-based synthetic brain mr image generation,5.0,201.0,1.0,57.0,4.0,3.1,99.0,15,https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_action_common_download&block_id=8&page_id=13&item_id=187503&item_no=1&attribute_id=1&file_no=1,"In medical imaging, it remains a challenging and valuable goal how to generate realistic medical images completely different from the original ones; the obtained synthetic images would improve diagnostic reliability, allowing for data augmentation in computer-assisted diagnosis as well as physician training. In this paper, we focus on generating synthetic multi-sequence brain Magnetic Resonance (MR) images using Generative Adversarial Networks (GANs). This involves difficulties mainly due to low contrast MR images, strong consistency in brain anatomy, and intra-sequence variability. Our novel realistic medical image generation approach shows that GANs can generate 128 χ 128 brain MR images avoiding artifacts. In our preliminary validation, even an expert physician was unable to accurately distinguish the synthetic images from the real samples in the Visual Turing Test.",Igasybrmrimge,130.0,22.0,3.0
5076,Image generation,19.0,scene graph generation with external knowledge and image reconstruction,5.0,201.0,1.0,85.0,4.0,3.1,111.6,16,http://openaccess.thecvf.com/content_CVPR_2019/papers/Gu_Scene_Graph_Generation_With_External_Knowledge_and_Image_Reconstruction_CVPR_2019_paper.pdf,"Scene graph generation has received growing attention with the advancements in image understanding tasks such as object detection, attributes and relationship prediction, etc. However, existing datasets are biased in terms of object and relationship labels, or often come with noisy and missing annotations, which makes the development of a reliable scene graph prediction model very challenging. In this paper, we propose a novel scene graph generation algorithm with external knowledge and image reconstruction loss to overcome these dataset issues. In particular, we extract commonsense knowledge from the external knowledge base to refine object and phrase features for improving generalizability in scene graph generation. To address the bias of noisy object annotations, we introduce an auxiliary image reconstruction path to regularize the scene graph generation network. Extensive experiments show that our framework can generate better scene graphs, achieving the state-of-the-art performance on two benchmark datasets: Visual Relationship Detection and Visual Genome datasets.",Iscgrgewiexknanimre,110.0,49.0,6.0
5077,Image generation,9.0,synthetic off-line signature image generation,5.0,201.0,1.0,97.0,4.0,3.1,112.2,17,https://www.researchgate.net/profile/Aythami-Morales/publication/257964484_Synthetic_Off-Line_Signature_Image_Generation/links/55265fc50cf2ee9bad77c81e/Synthetic-Off-Line-Signature-Image-Generation.pdf,"This paper proposes a novel methodology to generate static/off-line signatures of new identities. The signature of the new synthetic identity is obtained particularizing the random variables of a statistical distribution of global signature properties. The results mimic real signature shapes and writing style properties, which are estimated from static signature databases. New instances, as well as forgeries, from the synthetic identities are obtained introducing a natural variability from the synthetic individual properties. As additional novelty, an ink deposition model based on a ballpoint is developed for realistic static signature image generation. The range of the static signature generator has been established matching the performance obtained with the synthetic databases and those obtained with two public databases.",Isyofsiimge,56.0,22.0,4.0
5078,Image generation,21.0,hybrid retrieval-generation reinforced agent for medical image report generation,5.0,201.0,1.0,100.0,4.0,3.1,116.7,18,https://arxiv.org/pdf/1805.08298,"Generating long and coherent reports to describe medical images poses challenges to bridging visual patterns with informative human linguistic descriptions. We propose a novel Hybrid Retrieval-Generation Reinforced Agent (HRGR-Agent) which reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. HRGR-Agent employs a hierarchical decision-making procedure. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence. HRGR-Agent is updated via reinforcement learning, guided by sentence-level and word-level rewards. Experiments show that our approach achieves the state-of-the-art results on two medical report datasets, generating well-balanced structured sentences with robust coverage of heterogeneous medical report contents. In addition, our model achieves the highest detection accuracy of medical terminologies, and improved human evaluation performance.",Ihyrereagfomeimrege,92.0,56.0,17.0
5079,Image generation,401.0,unsupervised cross-domain image generation,1.0,53.0,4.0,43.0,4.0,3.1,154.4,19,http://arxiv.org/pdf/1801.03318v1,"We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.",Iuncrimge,678.0,31.0,36.0
5080,Image generation,401.0,stylespace analysis: disentangled controls for stylegan image generation,1.0,47.0,4.0,76.0,4.0,3.1,161.9,20,http://arxiv.org/pdf/2011.12799v2,"We explore and analyze the latent style space of Style-GAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and dis-entangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.",Istandicofostimge,36.0,45.0,6.0
5081,Image generation,401.0,pose-normalized image generation for person re-identification,1.0,81.0,4.0,41.0,4.0,3.1,165.0,21,http://arxiv.org/pdf/1801.00881v3,"Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id feature free of the influence of pose variations. We show that this feature is strong on its own and complementary to features learned with the original images. Importantly, under the transfer learning setting, we show that our model generalizes well to any new re-id dataset without the need for collecting any training data for model fine-tuning. The model thus has the potential to make re-id model truly scalable.",Ipoimgefopere,250.0,112.0,12.0
5082,Image generation,401.0,inspirational adversarial image generation,1.0,69.0,4.0,82.0,4.0,3.1,172.5,22,http://arxiv.org/pdf/1906.11661v2,"The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user’s choosing by performing several optimization steps to recover optimal parameters from the model’s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so they can even be used without numerical criterion nor inspirational image, only with human preferences. Thus, by iterating on one’s preferences we can make robust facial composite or fashion generation algorithms. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.",Iinadimge,8.0,79.0,0.0
5083,Image generation,7.0,visual object networks: image generation with disentangled 3d representation,5.0,114.0,3.0,201.0,1.0,3.0,108.0,23,https://arxiv.org/pdf/1812.02725,"Recent progress in deep generative models has led to tremendous breakthroughs in image generation. However, while existing models can synthesize photorealistic images, they lack an understanding of our underlying 3D world. We present a new generative model, Visual Object Networks (VON), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel our image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches to generate natural images. The VON not only generates images that are more realistic than state-of-the-art 2D image synthesis methods, but also enables many 3D operations such as changing the viewpoint of a generated image, editing of shape and texture, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.",Iviobneimgewidi3dre,113.0,66.0,11.0
5084,Image generation,401.0,progressive pose attention transfer for person image generation,1.0,101.0,3.0,22.0,5.0,3.0,167.29999999999998,24,http://arxiv.org/pdf/2103.11622v1,"This paper proposes a new generative adversarial network to the problem of pose transfer, i.e., transferring the pose of a given person to a target one. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency.",Iprpoattrfopeimge,115.0,58.0,32.0
5085,Image generation,401.0,deep image spatial transformation for person image generation,1.0,127.0,3.0,3.0,5.0,3.0,172.0,25,http://arxiv.org/pdf/2008.12606v1,"Pose-guided person image generation is to transform a source person image to a target pose. This task requires spatial manipulations of source data. However, Convolutional Neural Networks are limited by the lack of ability to spatially transform the inputs. In this paper, we propose a differentiable global-flow local-attention framework to reassemble the inputs at the feature level. Specifically, our model first calculates the global correlations between sources and targets to predict flow fields. Then, the flowed local patch pairs are extracted from the feature maps to calculate the local attention coefficients. Finally, we warp the source features using a content-aware sampling method with the obtained local attention coefficients. The results of both subjective and objective experiments demonstrate the superiority of our model. Besides, additional results in video animation and view synthesis show that our model is applicable to other tasks requiring spatial transformation. Our source code is available at https://github.com/RenYurui/Global-Flow-Local-Attention.",Ideimsptrfopeimge,45.0,49.0,13.0
5086,Image generation,401.0,disentangled and controllable face image generation via 3d imitative-contrastive learning,1.0,129.0,3.0,1.0,5.0,3.0,172.20000000000002,26,http://arxiv.org/pdf/2006.07810v1,"We propose an approach for face image generation of virtual people with disentangled, precisely-controllable latent representations for identity of non-existing people, expression, pose, and illumination. We embed 3D priors into adversarial learning and train the network to imitate the image formation of an analytic 3D face deformation and rendering process. To deal with the generation freedom induced by the domain gap between real and rendered faces, we further introduce contrastive learning to promote disentanglement by comparing pairs of generated images. Experiments show that through our imitative-contrastive learning, the factor variations are very well disentangled and the properties of a generated face can be precisely controlled. We also analyze the learned latent space and present several meaningful properties supporting factor disentanglement. Our method can also be used to embed real images into the disentangled latent space. We hope our method could provide new understandings of the relationship between physical properties and deep image synthesis.",Idiancofaimgevi3dimle,57.0,56.0,8.0
5088,Image generation,401.0,guided image generation with conditional invertible neural networks,1.0,128.0,3.0,29.0,5.0,3.0,180.2,27,http://arxiv.org/pdf/1907.02392v3,"In this work, we address the task of natural image generation guided by a conditioning input. We introduce a new architecture called conditional invertible neural network (cINN). The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning input into useful features. All parameters of the cINN are jointly optimized with a stable, maximum likelihood-based training procedure. By construction, the cINN does not experience mode collapse and generates diverse samples, in contrast to e.g. cGANs. At the same time our model produces sharp images since no reconstruction loss is required, in contrast to e.g. VAEs. We demonstrate these properties for the tasks of MNIST digit generation and image colorization. Furthermore, we take advantage of our bi-directional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.",Iguimgewicoinnene,82.0,46.0,5.0
5089,Image generation,401.0,deformable gans for pose-based human image generation,1.0,142.0,3.0,19.0,5.0,3.0,182.8,28,http://arxiv.org/pdf/2104.11599v1,"In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.",Idegafopohuimge,242.0,31.0,56.0
5090,Image generation,401.0,xinggan for person image generation,1.0,181.0,3.0,5.0,5.0,3.0,194.2,29,http://arxiv.org/pdf/2007.09278v1,"We propose a novel Generative Adversarial Network (XingGAN or CrossingGAN) for person image generation tasks, i.e., translating the pose of a given person to a desired one. The proposed Xing generator consists of two generation branches that model the person's appearance and shape information, respectively. Moreover, we propose two novel blocks to effectively transfer and update the person's shape and appearance embeddings in a crossing way to mutually improve each other, which has not been considered by any other existing GAN-based image generation work. Extensive experiments on two challenging datasets, i.e., Market-1501 and DeepFashion, demonstrate that the proposed XingGAN advances the state-of-the-art performance both in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at this https URL.",Ixifopeimge,40.0,49.0,6.0
5091,Image generation,401.0,disentangled person image generation,1.0,200.0,3.0,34.0,5.0,3.0,210.5,30,http://arxiv.org/pdf/1712.02621v4,"Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task1.",Idipeimge,297.0,50.0,38.0
5092,Image generation,28.0,image-grounded conversations: multimodal context for natural question and response generation,5.0,201.0,1.0,113.0,3.0,2.8,122.70000000000002,31,https://arxiv.org/pdf/1701.08251,"The popularity of image sharing on social media and the engagement it creates between users reflect the important role that visual context plays in everyday conversations. We present a novel task, Image Grounded Conversations (IGC), in which natural-sounding conversations are generated about a shared image. To benchmark progress, we introduce a new multiple reference dataset of crowd-sourced, event-centric conversations on images. IGC falls on the continuum between chit-chat and goal-directed conversation models, where visual grounding constrains the topic of conversation to event-driven utterances. Experiments with models trained on social media data show that the combination of visual and textual context enhances the quality of generated conversational turns. In human evaluation, the gap between human performance and that of both neural and retrieval architectures suggests that multi-modal IGC presents an interesting challenge for dialog research.",Iimcomucofonaquanrege,112.0,51.0,7.0
5093,Image generation,176.0,spectral normalization for generative adversarial networks,3.0,86.0,4.0,201.0,1.0,2.8,147.5,32,https://arxiv.org/pdf/1802.05957,"Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization performance due to adversarial training. In this work, we extend the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN's weight matrices. We also provide a computationally-efficient method for normalizing the spectral norm of convolutional layers with arbitrary stride and padding schemes in deep convolutional networks. We evaluate the power of spectral normalization extensively on combinations of datasets, network architectures, and adversarial training schemes. The code is available at https://github.com/jessemzhang/dl_spectral_normalization.",Ispnofogeadne,2343.0,41.0,400.0
5094,Image generation,401.0,visual object networks: image generation with disentangled 3d representations,1.0,115.0,3.0,59.0,4.0,2.7,184.0,33,http://arxiv.org/pdf/1812.02725v1,"Recent progress in deep generative models has led to tremendous breakthroughs in image generation. While being able to synthesize photorealistic images, existing models lack an understanding of our underlying 3D world. Different from previous works built on 2D datasets and models, we present a new generative model, Visual Object Networks (VONs), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel the image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shape and 2D texture. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic textures to these 2.5D sketches to generate realistic images. The VON not only generates images that are more realistic than the state-of-the-art 2D image synthesis methods but also enables many 3D operations such as changing the viewpoint of a generated image, shape and texture editing, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.",Iviobneimgewidi3dre,113.0,66.0,11.0
5095,Image generation,401.0,rethinking attention with performers,1.0,1.0,5.0,201.0,1.0,2.6,181.0,34,http://arxiv.org/pdf/2009.14794v3,"We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.",Ireatwipe,196.0,60.0,36.0
5096,Image generation,401.0,density estimation using real nvp,1.0,2.0,5.0,201.0,1.0,2.6,181.4,35,http://arxiv.org/pdf/1605.08803v3,"Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",Ideesusrenv,1474.0,84.0,346.0
5097,Image generation,401.0,improved techniques for training gans,1.0,3.0,5.0,201.0,1.0,2.6,181.8,36,http://arxiv.org/pdf/1606.03498v1,"We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",Iimtefotrga,4920.0,34.0,654.0
5098,Image generation,401.0,autoregressive diffusion models,1.0,6.0,5.0,201.0,1.0,2.6,183.0,37,http://arxiv.org/pdf/2110.02037v1,"We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.",Iaudimo,0.0,59.0,0.0
5099,Image generation,401.0,a general and adaptive robust loss function,1.0,8.0,5.0,201.0,1.0,2.6,183.8,38,http://arxiv.org/pdf/1701.03077v10,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.",Iageanadrolofu,160.0,44.0,13.0
5100,Image generation,401.0,training with quantization noise for extreme model compression,1.0,9.0,5.0,201.0,1.0,2.6,184.2,39,http://arxiv.org/pdf/2004.07320v3,"We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work beyond int8 fixed-point quantization with extreme compression methods where the approximations introduced by STE are severe, such as Product Quantization. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.",Itrwiqunofoexmoco,48.0,79.0,6.0
5101,Image generation,401.0,aggregating nested transformers,1.0,12.0,5.0,201.0,1.0,2.6,185.4,40,http://arxiv.org/pdf/2105.12723v2,"Although hierarchical structures are popular in recent vision transformers, they require sophisticated designs and massive datasets to work well. In this work, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture with minor code changes upon the original vision transformer and obtains improved performance compared to existing methods. Our empirical results show that the proposed method NesT converges faster and requires much less training data to achieve good generalization. For example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves $82.3\%/83.8\%$ accuracy evaluated on $224\times 224$ image size, outperforming previous methods with up to $57\%$ parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves $96\%$ accuracy using a single GPU, setting a new state of the art for vision transformers. Beyond image classification, we extend the key idea to image generation and show NesT leads to a strong decoder that is 8$\times$ faster than previous transformer based generators. Furthermore, we also propose a novel method for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer.",Iagnetr,14.0,68.0,2.0
5102,Image generation,401.0,evolving normalization-activation layers,1.0,13.0,5.0,201.0,1.0,2.6,185.8,41,http://arxiv.org/pdf/2004.02967v5,"Normalization layers and activation functions are fundamental components in deep networks and typically co-locate with each other. Here we propose to design them using an automated approach. Instead of designing them separately, we unify them into a single tensor-to-tensor computation graph, and evolve its structure starting from basic mathematical functions. Examples of such mathematical functions are addition, multiplication and statistical moments. The use of low-level mathematical functions, in contrast to the use of high-level modules in mainstream NAS, leads to a highly sparse and large search space which can be challenging for search methods. To address the challenge, we develop efficient rejection protocols to quickly filter out candidate layers that do not work well. We also use multi-objective evolution to optimize each layer's performance across many architectures to prevent overfitting. Our method leads to the discovery of EvoNorms, a set of new normalization-activation layers with novel, and sometimes surprising structures that go beyond existing design patterns. For example, some EvoNorms do not assume that normalization and activation functions must be applied sequentially, nor need to center the feature maps, nor require explicit activation functions. Our experiments show that EvoNorms work well on image classification models including ResNets, MobileNets and EfficientNets but also transfer well to Mask R-CNN with FPN/SpineNet for instance segmentation and to BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers in many cases.",Ievnola,30.0,72.0,7.0
5103,Image generation,401.0,making convolutional networks shift-invariant again,1.0,14.0,5.0,201.0,1.0,2.6,186.2,42,http://arxiv.org/pdf/1904.11486v2,"Modern convolutional networks are not shift-invariant, as small input shifts or translations can cause drastic changes in the output. Commonly used downsampling methods, such as max-pooling, strided-convolution, and average-pooling, ignore the sampling theorem. The well-known signal processing fix is anti-aliasing by low-pass filtering before downsampling. However, simply inserting this module into deep networks degrades performance; as a result, it is seldomly used today. We show that when integrated correctly, it is compatible with existing architectural components, such as max-pooling and strided-convolution. We observe \textit{increased accuracy} in ImageNet classification, across several commonly-used architectures, such as ResNet, DenseNet, and MobileNet, indicating effective regularization. Furthermore, we observe \textit{better generalization}, in terms of stability and robustness to input corruptions. Our results demonstrate that this classical signal processing technique has been undeservingly overlooked in modern deep networks. Code and anti-aliased versions of popular networks are available at https://richzhang.github.io/antialiased-cnns/ .",Imaconeshag,287.0,74.0,47.0
5104,Image generation,401.0,a style-based generator architecture for generative adversarial networks,1.0,15.0,5.0,201.0,1.0,2.6,186.6,43,http://arxiv.org/pdf/2109.02532v1,"Adversarial training is a computationally expensive task and hence searching for neural network architectures with robustness as the criterion can be challenging. As a step towards practical automation, this work explores the efficacy of a simple post processing step in yielding robust deep learning model. To achieve this, we adopt adversarial training as a post-processing step for optimised network architectures obtained from a neural architecture search algorithm. Specific policies are adopted for tuning the hyperparameters of the different steps, resulting in a fully automated pipeline for generating adversarially robust deep learning models. We evidence the usefulness of the proposed pipeline with extensive experimentation across 11 image classification and 9 text classification tasks.",Iastgearfogeadne,2601.0,68.0,592.0
5105,Image generation,401.0,the relativistic discriminator: a key element missing from standard gan,1.0,17.0,5.0,201.0,1.0,2.6,187.4,44,http://arxiv.org/pdf/1807.00734v3,"In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs.   We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function.   Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.",Ithrediakeelmifrstga,457.0,27.0,61.0
5106,Image generation,401.0,improved training of wasserstein gans,1.0,19.0,5.0,201.0,1.0,2.6,188.2,45,https://arxiv.org/pdf/1704.00028.pdf],"Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",Iimtrofwaga,4953.0,41.0,997.0
5107,Image generation,401.0,wasserstein gan,1.0,20.0,5.0,201.0,1.0,2.6,188.6,46,http://arxiv.org/pdf/1902.09323v1,"To address the challenges in learning deep generative models (e.g.,the blurriness of variational auto-encoder and the instability of training generative adversarial networks, we propose a novel deep generative model, named Wasserstein-Wasserstein auto-encoders (WWAE). We formulate WWAE as minimization of the penalized optimal transport between the target distribution and the generated distribution. By noticing that both the prior $P_Z$ and the aggregated posterior $Q_Z$ of the latent code Z can be well captured by Gaussians, the proposed WWAE utilizes the closed-form of the squared Wasserstein-2 distance for two Gaussians in the optimization process. As a result, WWAE does not suffer from the sampling burden and it is computationally efficient by leveraging the reparameterization trick. Numerical results evaluated on multiple benchmark datasets including MNIST, fashion- MNIST and CelebA show that WWAE learns better latent structures than VAEs and generates samples of better visual quality and higher FID scores than VAEs and GANs.",Iwaga,0.0,42.0,0.0
5108,Image generation,401.0,conditional image synthesis with auxiliary classifier gans,1.0,21.0,5.0,201.0,1.0,2.6,189.0,47,http://proceedings.mlr.press/v70/odena17a/odena17a.pdf,"Building on top of the success of generative adversarial networks (GANs), conditional GANs attempt to better direct the data generation process by conditioning with certain additional information. Inspired by the most recent AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In addition to the real/fake classifier used in vanilla GANs, our discriminator has an advanced auxiliary classifier which distinguishes each real class from an extra `fake' class. The `fake' class avoids mixing generated data with real data, which can potentially confuse the classification of real data as AC-GAN does, and makes the advanced auxiliary classifier behave as another real/fake classifier. As a result, FC-GAN can accelerate the process of differentiation of all classes, thus boost the convergence speed. Experimental results on image synthesis demonstrate our model is competitive in the quality of images generated while achieving a faster convergence rate.",Icoimsywiauclga,1872.0,44.0,298.0
5109,Image generation,401.0,generating diverse high-fidelity images with vq-vae-2,1.0,24.0,5.0,201.0,1.0,2.6,190.2,48,http://arxiv.org/pdf/1811.11389v3,"Despite significant recent progress on generative models, controlled generation of images depicting multiple and complex object layouts is still a difficult problem. Among the core challenges are the diversity of appearance a given object may possess and, as a result, exponential set of images consistent with a specified layout. To address these challenges, we propose a novel approach for layout-based image generation; we call it Layout2Im. Given the coarse spatial layout (bounding boxes + object categories), our model can generate a set of realistic images which have the correct objects in the desired locations. The representation of each object is disentangled into a specified/certain part (category) and an unspecified/uncertain part (appearance). The category is encoded using a word embedding and the appearance is distilled into a low-dimensional vector sampled from a normal distribution. Individual object representations are composed together using convolutional LSTM, to obtain an encoding of the complete layout, and then decoded to an image. Several loss terms are introduced to encourage accurate and diverse generation. The proposed Layout2Im model significantly outperforms the previous state of the art, boosting the best reported inception score by 24.66% and 28.57% on the very challenging COCO-Stuff and Visual Genome datasets, respectively. Extensive experiments also demonstrate our method's ability to generate complex and diverse images with multiple objects.",Igedihiimwivq,350.0,41.0,50.0
5110,Image generation,401.0,training language gans from scratch,1.0,25.0,5.0,201.0,1.0,2.6,190.6,49,http://arxiv.org/pdf/1905.09922v2,"Generative Adversarial Networks (GANs) enjoy great success at image generation, but have proven difficult to train in the domain of natural language. Challenges with gradient estimation, optimization instability, and mode collapse have lead practitioners to resort to maximum likelihood pre-training, followed by small amounts of adversarial fine-tuning. The benefits of GAN fine-tuning for language generation are unclear, as the resulting models produce comparable or worse samples than traditional language models. We show it is in fact possible to train a language GAN from scratch -- without maximum likelihood pre-training. We combine existing techniques such as large batch sizes, dense rewards and discriminator regularization to stabilize and improve language GANs. The resulting model, ScratchGAN, performs comparably to maximum likelihood training on EMNLP2017 News and WikiText-103 corpora according to quality and diversity metrics.",Itrlagafrsc,47.0,58.0,11.0
5111,Image generation,401.0,on noise injection in generative adversarial networks,1.0,26.0,5.0,201.0,1.0,2.6,191.0,50,http://arxiv.org/pdf/2006.05891v3,"Noise injection has been proved to be one of the key technique advances in generating high-fidelity images. Despite its successful usage in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on the geodesic normal coordinates. Guided by our theories, we find that the existing method is incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.",Ionnoiningeadne,3.0,39.0,0.0
5112,Image generation,401.0,analyzing and improving the image quality of stylegan,1.0,27.0,5.0,201.0,1.0,2.6,191.4,51,http://arxiv.org/pdf/1912.04958v2,"The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",Iananimthimquofst,971.0,61.0,287.0
5113,Image generation,401.0,semantic image synthesis with spatially-adaptive normalization,1.0,29.0,5.0,201.0,1.0,2.6,192.2,52,http://arxiv.org/pdf/2003.13898v1,"We propose a novel Edge guided Generative Adversarial Network (EdgeGAN) for photo-realistic image synthesis from semantic layouts. Although considerable improvement has been achieved, the quality of synthesized images is far from satisfactory due to two largely unresolved challenges. First, the semantic labels do not provide detailed structural information, making it difficult to synthesize local details and structures. Second, the widely adopted CNN operations such as convolution, down-sampling and normalization usually cause spatial resolution loss and thus are unable to fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects). To tackle the first challenge, we propose to use the edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. Edge information is produced by a convolutional generator and introduces detailed structure information. Further, to preserve the semantic information, we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout. Extensive experiments on two challenging datasets show that the proposed EdgeGAN can generate significantly better results than state-of-the-art methods. The source code and trained models are available at https://github.com/Ha0Tang/EdgeGAN.",Iseimsywispno,901.0,77.0,211.0
5114,Image generation,401.0,"progressive growing of gans for improved quality, stability, and variation",1.0,31.0,5.0,201.0,1.0,2.6,193.0,53,https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&__hssc=200028081.1.1524009600084&__hsfp=1773666937,"We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",Iprgrofgafoimqustanva,3287.0,61.0,570.0
5115,Image generation,401.0,denoising diffusion probabilistic models,1.0,33.0,5.0,201.0,1.0,2.6,193.8,54,http://arxiv.org/pdf/2006.11239v2,"We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",Idediprmo,128.0,74.0,47.0
5116,Image generation,401.0,axiom-based grad-cam: towards accurate visualization and explanation of cnns,1.0,35.0,5.0,201.0,1.0,2.6,194.6,55,http://arxiv.org/pdf/2008.02312v4,"To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. In spite of the reasonable visualization, lack of clear and sufficient theoretical support is the main limitation of these methods. In this paper, we introduce two axioms -- Conservation and Sensitivity -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of conservation and sensitivity. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. The code is available at https://github.com/Fu0511/XGrad-CAM.",Iaxgrtoacvianexofcn,20.0,38.0,7.0
5117,Image generation,401.0,combining markov random fields and convolutional neural networks for image synthesis,1.0,36.0,5.0,201.0,1.0,2.6,195.0,56,http://arxiv.org/pdf/1601.04589v1,"This paper studies a combination of generative Markov random field (MRF) models and discriminatively trained deep convolutional neural networks (dCNNs) for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNN feature pyramid, controling the image layout at an abstract level. We apply the method to both photographic and non-photo-realistic (artwork) synthesis tasks. The MRF regularizer prevents over-excitation artifacts and reduces implausible feature mixtures common to previous dCNN inversion approaches, permitting synthezing photographic content with increased visual plausibility. Unlike standard MRF-based texture synthesis, the combined system can both match and adapt local features with considerable variability, yielding results far out of reach of classic generative MRF methods.",Icomarafianconenefoimsy,480.0,37.0,58.0
5118,Image generation,401.0,adversarial latent autoencoders,1.0,37.0,5.0,201.0,1.0,2.6,195.4,57,http://arxiv.org/pdf/1703.01220v4,"Unsupervised learning is of growing interest because it unlocks the potential held in vast amounts of unlabelled data to learn useful representations for inference. Autoencoders, a form of generative model, may be trained by learning to reconstruct unlabelled input data from a latent representation space. More robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones. Representations may be further improved by introducing regularisation during training to shape the distribution of the encoded data in latent space. We suggest denoising adversarial autoencoders, which combine denoising and regularisation, shaping the distribution of latent space using adversarial training. We introduce a novel analysis that shows how denoising may be incorporated into the training and sampling of adversarial autoencoders. Experiments are performed to assess the contributions that denoising makes to the learning of representations for classification and sample synthesis. Our results suggest that autoencoders trained using a denoising criterion achieve higher classification performance, and can synthesise samples that are more consistent with the input data than those trained without a corruption process.",Iadlaau,82.0,56.0,16.0
5119,Image generation,401.0,joint discriminative and generative learning for person re-identification,1.0,39.0,5.0,201.0,1.0,2.6,196.2,58,http://arxiv.org/pdf/2006.14937v3,"Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative learners and tools of central importance to the everyday machine learning practitioner and data scientist. Due to their discriminative nature, however, they lack principled methods to process inputs with missing features or to detect outliers, which requires pairing them with imputation techniques or a separate generative model. In this paper, we demonstrate that DTs and RFs can naturally be interpreted as generative models, by drawing a connection to Probabilistic Circuits, a prominent class of tractable probabilistic models. This reinterpretation equips them with a full joint distribution over the feature space and leads to Generative Decision Trees (GeDTs) and Generative Forests (GeFs), a family of novel hybrid generative-discriminative models. This family of models retains the overall characteristics of DTs and RFs while additionally being able to handle missing features by means of marginalisation. Under certain assumptions, frequently made for Bayes consistency results, we show that consistency in GeDTs and GeFs extend to any pattern of missing input features, if missing at random. Empirically, we show that our models often outperform common routines to treat missing data, such as K-nearest neighbour imputation, and moreover, that our models can naturally detect outliers by monitoring the marginal probability of input features.",Ijodiangelefopere,316.0,66.0,22.0
5120,Image generation,401.0,alias-free generative adversarial networks,1.0,40.0,5.0,201.0,1.0,2.6,196.6,59,http://arxiv.org/pdf/1810.12576v1,"Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing. Codes for the project are available at https://github.com/aam-at/adversary_critic.",Ialgeadne,12.0,77.0,1.0
5121,Image generation,66.0,language generation with recurrent generative adversarial networks without pre-training,4.0,201.0,1.0,180.0,3.0,2.5,154.2,60,https://arxiv.org/pdf/1706.01399,"Generative Adversarial Networks (GANs) have shown great promise recently in image generation. Training GANs for language generation has proven to be more difficult, because of the non-differentiable nature of generating text with recurrent neural networks. Consequently, past work has either resorted to pre-training with maximum-likelihood or used convolutional networks for generation. In this work, we show that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. We empirically show that our approach vastly improves the quality of generated sequences compared to a convolutional baseline.",Ilagewiregeadnewipr,87.0,24.0,3.0
5122,Image generation,401.0,a variational u-net for conditional appearance and shape generation,1.0,122.0,3.0,123.0,3.0,2.4000000000000004,206.0,61,http://arxiv.org/pdf/1804.04694v1,"Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net [30] for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO [20], DeepFashion [21, 23], shoes [43], Market-1501 [47] and handbags [49] the approach demonstrates significant improvements over the state-of-the-art.",Iavau-focoapanshge,260.0,53.0,46.0
5123,Image generation,401.0,mode seeking generative adversarial networks for diverse image synthesis,1.0,136.0,3.0,133.0,3.0,2.4000000000000004,214.6,62,http://arxiv.org/pdf/1903.05628v6,"Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality.",Imosegeadnefodiimsy,161.0,44.0,31.0
5124,Image generation,112.0,pixel recurrent neural networks,3.0,190.0,3.0,201.0,1.0,2.4,169.89999999999998,63,http://proceedings.mlr.press/v48/oord16.pdf,"Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.",Ipirenene,1568.0,38.0,163.0
5125,Image generation,6.0,stereoscopic image generation based on depth images,5.0,201.0,1.0,201.0,1.0,2.2,142.5,64,http://arxiv.org/abs/2110.07726v1,"Stereoscopic projection mapping (PM) allows a user to see a three-dimensional (3D) computer-generated (CG) object floating over physical surfaces of arbitrary shapes around us using projected imagery. However, the current stereoscopic PM technology only satisfies binocular cues and is not capable of providing correct focus cues, which causes a vergence--accommodation conflict (VAC). Therefore, we propose a multifocal approach to mitigate VAC in stereoscopic PM. Our primary technical contribution is to attach electrically focus-tunable lenses (ETLs) to active shutter glasses to control both vergence and accommodation. Specifically, we apply fast and periodical focal sweeps to the ETLs, which causes the ""virtual image'"" (as an optical term) of a scene observed through the ETLs to move back and forth during each sweep period. A 3D CG object is projected from a synchronized high-speed projector only when the virtual image of the projected imagery is located at a desired distance. This provides an observer with the correct focus cues required. In this study, we solve three technical issues that are unique to stereoscopic PM: (1) The 3D CG object is displayed on non-planar and even moving surfaces; (2) the physical surfaces need to be shown without the focus modulation; (3) the shutter glasses additionally need to be synchronized with the ETLs and the projector. We also develop a novel compensation technique to deal with the ""lens breathing"" artifact that varies the retinal size of the virtual image through focal length modulation. Further, using a proof-of-concept prototype, we demonstrate that our technique can present the virtual image of a target 3D CG object at the correct depth. Finally, we validate the advantage provided by our technique by comparing it with conventional stereoscopic PM using a user study on a depth-matching task.",Istimgebaondeim,143.0,7.0,10.0
5126,Image generation,8.0,renderbots—multi‐agent systems for direct image generation,5.0,201.0,1.0,201.0,1.0,2.2,143.10000000000002,65,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.9040&rep=rep1&type=pdf,"Image manipulation can be considered a special case of image generation where the image to be produced is a modification of an existing image. Image generation and manipulation have been, for the most part, tasks that operate on raw pixels. However, the remarkable progress in learning rich image and object representations has opened the way for tasks such as text-to-image or layout-to-image generation that are mainly driven by semantics. In our work, we address the novel problem of image manipulation from scene graphs, in which a user can edit images by merely applying changes in the nodes or edges of a semantic graph that is generated from the image. Our goal is to encode image information in a given constellation and from there on generate new constellations, such as replacing objects or even changing relationships between objects, while respecting the semantics and style from the original image. We introduce a spatio-semantic scene graph network that does not require direct supervision for constellation changes or image edits. This makes it possible to train the system from existing real-world datasets with no additional annotation effort.",Iresyfodiimge,93.0,39.0,3.0
5127,Image generation,12.0,image generation from text with entity information fusion,5.0,201.0,1.0,201.0,1.0,2.2,144.3,66,http://arxiv.org/pdf/2107.11970v1,"Entity-aware image captioning aims to describe named entities and events related to the image by utilizing the background knowledge in the associated article. This task remains challenging as it is difficult to learn the association between named entities and visual cues due to the long-tail distribution of named entities. Furthermore, the complexity of the article brings difficulty in extracting fine-grained relationships between entities to generate informative event descriptions about the image. To tackle these challenges, we propose a novel approach that constructs a multi-modal knowledge graph to associate the visual objects with named entities and capture the relationship between entities simultaneously with the help of external knowledge collected from the web. Specifically, we build a text sub-graph by extracting named entities and their relationships from the article, and build an image sub-graph by detecting the objects in the image. To connect these two sub-graphs, we propose a cross-modal entity matching module trained using a knowledge base that contains Wikipedia entries and the corresponding images. Finally, the multi-modal knowledge graph is integrated into the captioning model via a graph attention mechanism. Extensive experiments on both GoodNews and NYTimes800k datasets demonstrate the effectiveness of our method.",Iimgefrtewieninfu,0.0,28.0,0.0
5128,Image generation,14.0,free viewpoint image generation using multipass dynamic programming,5.0,201.0,1.0,201.0,1.0,2.2,144.9,67,http://arxiv.org/pdf/1211.4767v1,"We study an interactive live streaming scenario where multiple peers pull streams of the same free viewpoint video that are synchronized in time but not necessarily in view. In free viewpoint video, each user can periodically select a virtual view between two anchor camera views for display. The virtual view is synthesized using texture and depth videos of the anchor views via depth-image-based rendering (DIBR). In general, the distortion of the virtual view increases with the distance to the anchor views, and hence it is beneficial for a peer to select the closest anchor views for synthesis. On the other hand, if peers interested in different virtual views are willing to tolerate larger distortion in using more distant anchor views, they can collectively share the access cost of common anchor views.   Given anchor view access cost and synthesized distortion of virtual views between anchor views, we study the optimization of anchor view allocation for collaborative peers. We first show that, if the network reconfiguration costs due to view-switching are negligible, the problem can be optimally solved in polynomial time using dynamic programming. We then consider the case of non-negligible reconfiguration costs (e.g., large or frequent view-switching leading to anchor-view changes). In this case, the view allocation problem becomes NP-hard. We thus present a locally optimal and centralized allocation algorithm inspired by Lloyd's algorithm in non-uniform scalar quantization. We also propose a distributed algorithm with guaranteed convergence where each peer group independently make merge-and-split decisions with a well-defined fairness criteria. The results show that depending on the problem settings, our proposed algorithms achieve respective optimal and close-to-optimal performance in terms of total cost, and outperform a P2P scheme without collaborative anchor selection.",Ifrviimgeusmudypr,2.0,16.0,0.0
5129,Image generation,15.0,unsupervised image-to-image translation networks,5.0,201.0,1.0,201.0,1.0,2.2,145.2,68,http://papers.nips.cc/paper/6672-unsupervised-image-to-imagetranslation-networks.pdf,"Unsupervised image-to-image translation is a recently proposed task of translating an image to a different style or domain given only unpaired image examples at training time. In this paper, we formulate a new task of unsupervised video-to-video translation, which poses its own unique challenges. Translating video implies learning not only the appearance of objects and scenes but also realistic motion and transitions between consecutive frames.We investigate the performance of per-frame video-to-video translation using existing image-to-image translation networks, and propose a spatio-temporal 3D translator as an alternative solution to this problem. We evaluate our 3D method on multiple synthetic datasets, such as moving colorized digits, as well as the realistic segmentation-to-video GTA dataset and a new CT-to-MRI volumetric images translation dataset. Our results show that frame-wise translation produces realistic results on a single frame level but underperforms significantly on the scale of the whole video compared to our three-dimensional translation approach, which is better able to learn the complex structure of video and motion and continuity of object appearance.",Iunimtrne,1596.0,42.0,235.0
5130,Image generation,16.0,image-to-image translation with conditional adversarial networks,5.0,201.0,1.0,201.0,1.0,2.2,145.5,69,https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf,"Image-to-image translation is a long-established and a difficult problem in computer vision. In this paper we propose an adversarial based model for image-to-image translation. The regular deep neural-network based methods perform the task of image-to-image translation by comparing gram matrices and using image segmentation which requires human intervention. Our generative adversarial network based model works on a conditional probability approach. This approach makes the image translation independent of any local, global and content or style features. In our approach we use a bidirectional reconstruction model appended with the affine transform factor that helps in conserving the content and photorealism as compared to other models. The advantage of using such an approach is that the image-to-image translation is semi-supervised, independant of image segmentation and inherits the properties of generative adversarial networks tending to produce realistic. This method has proven to produce better results than Multimodal Unsupervised Image-to-image translation.",Iimtrwicoadne,9583.0,65.0,1723.0
5131,Image generation,17.0,the generation and maintenance of visual mental images: evidence from image type and aging,5.0,201.0,1.0,201.0,1.0,2.2,145.8,70,http://arxiv.org/pdf/2107.13683v1,"We build on the empirical finding that a human being's mental age is normally distributed around the chronological age. This opposes the frequent societal assumption ""mental = chronological"" which is known to be false in general but entertained for simplicity due to lack of methodology; hence disregarding that, f.e., people of different chronological ages can be much closer in their mental ages. As a quantitative approach on a scientific basis, we set up a general formula for the probability that two individuals of given ages are mentally within a certain range of years and investigate its implications i.a. by critically analyzing popular assumptions on age and computing statistical expectations within populations.",Ithgeanmaofvimeimevfrimtyanag,44.0,20.0,1.0
5132,Image generation,22.0,a method of epipolar image generation based on pos data,5.0,201.0,1.0,201.0,1.0,2.2,147.3,71,http://arxiv.org/pdf/1703.09725v3,"Computing the epipolar geometry from feature points between cameras with very different viewpoints is often error prone, as an object's appearance can vary greatly between images. For such cases, it has been shown that using motion extracted from video can achieve much better results than using a static image. This paper extends these earlier works based on the scene dynamics. In this paper we propose a new method to compute the epipolar geometry from a video stream, by exploiting the following observation: For a pixel p in Image A, all pixels corresponding to p in Image B are on the same epipolar line. Equivalently, the image of the line going through camera A's center and p is an epipolar line in B. Therefore, when cameras A and B are synchronized, the momentary images of two objects projecting to the same pixel, p, in camera A at times t1 and t2, lie on an epipolar line in camera B. Based on this observation we achieve fast and precise computation of epipolar lines. Calibrating cameras based on our method of finding epipolar lines is much faster and more robust than previous methods.",Iameofepimgebaonpoda,4.0,0.0,0.0
5133,Image generation,24.0,a novel cnn based security guaranteed image watermarking generation scenario for smart city applications,5.0,201.0,1.0,201.0,1.0,2.2,147.9,72,http://arxiv.org/pdf/1402.7341v1,"Digital multimedia watermarking technology had suggested in the last decade to embed copyright information in digital objects such as images, audio and video. However, the increasing use of relational database systems in many real-life applications created an ever-increasing need for watermarking database systems. As a result, watermarking relational database system is now emerging as a research area that deals with the legal issue of copyright protection of database systems. The main goal of database watermarking is to generate robust and impersistent watermark for database. In this paper we propose a method, based on image as watermark and this watermark is embedded over the database at two different attribute of tuple, one in the numeric attribute of tuple and another in the date attribute's time (seconds) field. Our approach can be applied for numerical and categorical database.",Ianocnbaseguimwagescfosmciap,134.0,43.0,0.0
5134,Image generation,25.0,generation of digital phantoms of cell nuclei and simulation of image formation in 3d image cytometry,5.0,201.0,1.0,201.0,1.0,2.2,148.2,73,https://onlinelibrary.wiley.com/doi/pdf/10.1002/cyto.a.20714,"Increasing demand for understanding the vast heterogeneity of cellular phenotypes has driven the development of imaging flow cytometry (IFC), that combines features of flow cytometry with fluorescence and bright field microscopy. IFC combines the throughput and statistical advantage of flow cytometry with the ability to discretely measure events based on a real or computational image, as well as conventional flow cytometry metrics. A limitation of existing IFC systems is that, regardless of detection methodology, only two-dimensional (2D) cell images are obtained. Without tomographic three-dimensional (3D) resolution the projection problem remains: collapsing 3D information onto a 2D image, limiting the reliability of spot counting or co-localization crucial to cell phenotyping. Here we present a solution to the projection problem: three-dimensional imaging flow cytometry (3D-IFC), a high-throughput 3D cell imager based on optical sectioning microscopy. We combine orthogonal light-sheet scanning illumination with our previous spatiotemporal transformation detection to produce 3D cell image reconstruction from a cameraless single-pixel photodetector readout. We further demonstrate this capability by co-capturing 3D fluorescence and label-free side-scattering images of single cells in flow at a velocity of 0.2 m s-1, corresponding to a throughput of approximately 500 cells per second with 60,000 voxels (resized subsequently to 106 voxels) for each cell image at a resolution of less than 1 micron in X, Y, and Z dimensions. Improved high-throughput imaging tools are needed to phenotype-genotype recognized heterogeneity in the fields of immunology, oncology, cell- and gene- therapy, and drug discovery.",Igeofdiphofcenuansiofimfoin3dimcy,107.0,58.0,5.0
5135,Image generation,27.0,elemental image generation algorithm using reverse iteration along optical path,5.0,201.0,1.0,201.0,1.0,2.2,148.8,74,http://arxiv.org/pdf/2009.10072v1,"In this paper, a topology optimization framework utilizing automatic differentiation is presented as an efficient way for solving 2D density-based topology optimization problem by calculating gradients through the fully differentiable finite element solver. The optimization framework with the differentiable physics solver is proposed and tested on several classical topology optimization examples. The differentiable solver is implemented in Julia programming language and can be automatically differentiated in reverse mode to provide the pullback functions of every single operation. The entire end-to-end gradient information can be then backed up by utilizing chain rule. This framework incorporates a generator built from convolutional layers with a set of learnable parameters to propose new designs for every iteration. Since the whole process is differentiable, the parameters of the generator can be updated using any optimization algorithm given the gradient information from automatic differentiation. The proposed optimization framework is demonstrated on designing a half MBB beam and compared to the results with the ones from the efficient 88-line code. By only changing the objective function and the boundary conditions, it can run an optimization for designing a compliant mechanism, e.g. a force inverter where the output displacement is in the opposite direction of the input.",Ielimgealusreitaloppa,1.0,26.0,0.0
5136,Image generation,29.0,generation of geometrically and radiometrically terrain corrected sar image products,5.0,201.0,1.0,201.0,1.0,2.2,149.4,75,http://arxiv.org/abs/2010.06819v2,"Since by the International Telecommunications Union (ITU) regulatory the radio spectrum available to spaceborne synthetic aperture radar (SAR) is restricted to certain limited frequency intervals, there are many different spaceborne SAR systems sharing common frequency bands. Due to this reason, it is reported that two spaceborne SARs at orbit cross positions can potentially cause severe mutual interference. Specifically, the transmitting signal of a SAR, typically linear frequency modulated (LFM), can be directly received by the side or back lobes of another SAR's antenna, causing radiometric artefacts in the focused image. This paper tries to model and characterize the artefacts, and study efficient methods for mitigating them. To this end, we formulate an analytical model for describing the artefact, which reveals that the mutual interference can introduce a two-dimensional LFM radiometric artefact in image domain with a limited spatial extent. We show that the artefact is low-rank based on range-azimuth decoupling analysis and two-dimensional high-order Taylor expansion. Based on the low rank model, we show that two methods, i.e., principle component analysis and its robust variant, can be adopted to efficiently mitigate the artefact via processing in image domain. The former method has the advantage of fast processing speed, for example, a sub-swath of Sentinel-1 interferometric wide swath image can be processed within 70 seconds via block-wise operation, whereas the latter provides improved accuracy for sparse point-like scatterers. Experiment results demonstrate that the radiometric artefacts caused by mutual interference in Sentinel-1 level-1 images can be efficiently mitigated via the proposed methods.",Igeofgeanratecosaimpr,66.0,17.0,4.0
5137,Image generation,31.0,view generation for multiview maximum disagreement based active learning for hyperspectral image classification,5.0,201.0,1.0,201.0,1.0,2.2,150.0,76,http://arxiv.org/abs/1212.4455v2,"In multiview applications, multiple cameras acquire the same scene from different viewpoints and generally produce correlated video streams. This results in large amounts of highly redundant data. In order to save resources, it is critical to handle properly this correlation during encoding and transmission of the multiview data. In this work, we propose a correlation-aware packet scheduling algorithm for multi-camera networks, where information from all cameras are transmitted over a bottleneck channel to clients that reconstruct the multiview images. The scheduling algorithm relies on a new rate-distortion model that captures the importance of each view in the scene reconstruction. We propose a problem formulation for the optimization of the packet scheduling policies, which adapt to variations in the scene content. Then, we design a low complexity scheduling algorithm based on a trellis search that selects the subset of candidate packets to be transmitted towards effective multiview reconstruction at clients. Extensive simulation results confirm the gain of our scheduling algorithm when inter-source correlation information is used in the scheduler, compared to scheduling policies with no information about the correlation or non-adaptive scheduling policies. We finally show that increasing the optimization horizon in the packet scheduling algorithm improves the transmission performance, especially in scenarios where the level of correlation rapidly varies with time.",Ivigefomumadibaaclefohyimcl,97.0,47.0,5.0
5138,Image generation,33.0,a short review on image caption generation with deep learning,5.0,201.0,1.0,201.0,1.0,2.2,150.60000000000002,77,http://arxiv.org/pdf/2107.13114v1,"Image Captioning is a task that combines computer vision and natural language processing, where it aims to generate descriptive legends for images. It is a two-fold process relying on accurate image understanding and correct language understanding both syntactically and semantically. It is becoming increasingly difficult to keep up with the latest research and findings in the field of image captioning due to the growing amount of knowledge available on the topic. There is not, however, enough coverage of those findings in the available review papers. We perform in this paper a run-through of the current techniques, datasets, benchmarks and evaluation metrics used in image captioning. The current research on the field is mostly focused on deep learning-based methods, where attention mechanisms along with deep reinforcement and adversarial learning appear to be in the forefront of this research topic. In this paper, we review recent methodologies such as UpDown, OSCAR, VIVO, Meta Learning and a model that uses conditional generative adversarial nets. Although the GAN-based model achieves the highest score, UpDown represents an important basis for image captioning and OSCAR and VIVO are more useful as they use novel object captioning. This review paper serves as a roadmap for researchers to keep up to date with the latest contributions made in the field of image caption generation.",Iashreonimcagewidele,10.0,47.0,0.0
5139,Image generation,34.0,segmentation of kidneys using a new active shape model generation technique based on non-rigid image registration,5.0,201.0,1.0,201.0,1.0,2.2,150.9,78,http://www5.informatik.uni-erlangen.de/Forschung/Publikationen/2008/Spiegel08-SOK.pdf,"It remains challenging to automatically segment kidneys in clinical ultrasound (US) images due to the kidneys' varied shapes and image intensity distributions, although semi-automatic methods have achieved promising performance. In this study, we propose subsequent boundary distance regression and pixel classification networks to segment the kidneys, informed by the fact that the kidney boundaries have relatively homogenous texture patterns across images. Particularly, we first use deep neural networks pre-trained for classification of natural images to extract high-level image features from US images, then these features are used as input to learn kidney boundary distance maps using a boundary distance regression network, and finally the predicted boundary distance maps are classified as kidney pixels or non-kidney pixels using a pixel classification network in an end-to-end learning fashion. We also adopted a data-augmentation method based on kidney shape registration to generate enriched training data from a small number of US images with manually segmented kidney labels. Experimental results have demonstrated that our method could effectively improve the performance of automatic kidney segmentation, significantly better than deep learning-based pixel classification networks.",Iseofkiusaneacshmogetebaonnoimre,72.0,31.0,3.0
5140,Image generation,35.0,image quality and radiation dose of low tube voltage 3 rd generation dual-source coronary ct angiography in obese patients: a phantom study,5.0,201.0,1.0,201.0,1.0,2.2,151.2,79,http://arxiv.org/abs/1806.09748v3,"In coronary CT angiography, a series of CT images are taken at different levels of radiation dose during the examination. Although this reduces the total radiation dose, the image quality during the low-dose phases is significantly degraded. To address this problem, here we propose a novel semi-supervised learning technique that can remove the noises of the CT images obtained in the low-dose phases by learning from the CT images in the routine dose phases. Although a supervised learning approach is not possible due to the differences in the underlying heart structure in two phases, the images in the two phases are closely related so that we propose a cycle-consistent adversarial denoising network to learn the non-degenerate mapping between the low and high dose cardiac phases. Experimental results showed that the proposed method effectively reduces the noise in the low-dose CT image while the preserving detailed texture and edge information. Moreover, thanks to the cyclic consistency and identity loss, the proposed network does not create any artificial features that are not present in the input images. Visual grading and quality evaluation also confirm that the proposed method provides significant improvement in diagnostic quality.",Iimquanradooflotuvo3rdgeducoctaninobpaaphst,73.0,18.0,2.0
5141,Image generation,36.0,specifying object attributes and relations in interactive scene generation,5.0,201.0,1.0,201.0,1.0,2.2,151.5,80,http://openaccess.thecvf.com/content_ICCV_2019/papers/Ashual_Specifying_Object_Attributes_and_Relations_in_Interactive_Scene_Generation_ICCV_2019_paper.pdf,"We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space, by selecting an appearance archetype. Our code is publicly available at https://www.github.com/ashual/scene_generation",Ispobatanreininscge,80.0,31.0,18.0
5142,Image generation,37.0,virtual unenhanced images of the abdomen with second-generation dual-source dual-energy computed tomography: image quality and liver lesion detection,5.0,201.0,1.0,201.0,1.0,2.2,151.8,81,http://arxiv.org/pdf/1710.01766v2,"Extracting, harvesting and building large-scale annotated radiological image datasets is a greatly important yet challenging problem. It is also the bottleneck to designing more effective data-hungry computing paradigms (e.g., deep learning) for medical image analysis. Yet, vast amounts of clinical annotations (usually associated with disease image findings and marked using arrows, lines, lesion diameters, segmentation, etc.) have been collected over several decades and stored in hospitals' Picture Archiving and Communication Systems. In this paper, we mine and harvest one major type of clinical annotation data - lesion diameters annotated on bookmarked images - to learn an effective multi-class lesion detector via unsupervised and supervised deep Convolutional Neural Networks (CNN). Our dataset is composed of 33,688 bookmarked radiology images from 10,825 studies of 4,477 unique patients. For every bookmarked image, a bounding box is created to cover the target lesion based on its measured diameters. We categorize the collection of lesions using an unsupervised deep mining scheme to generate clustered pseudo lesion labels. Next, we adopt a regional-CNN method to detect lesions of multiple categories, regardless of missing annotations (normally only one lesion is annotated, despite the presence of multiple co-existing findings). Our integrated mining, categorization and detection framework is validated with promising empirical results, as a scalable, universal or multi-purpose CAD paradigm built upon abundant retrospective medical data. Furthermore, we demonstrate that detection accuracy can be significantly improved by incorporating pseudo lesion labels (e.g., Liver lesion/tumor, Lung nodule/tumor, Abdomen lesions, Chest lymph node and others). This dataset will be made publicly available (under the open science initiative).",Iviunimofthabwiseduducotoimquanlilede,71.0,21.0,1.0
5143,Image generation,39.0,dose and image quality at ct pulmonary angiography—comparison of first and second generation dual-energy ct and 64-slice ct,5.0,201.0,1.0,201.0,1.0,2.2,152.4,82,http://arxiv.org/pdf/2102.09615v1,"We propose a Noise Entangled GAN (NE-GAN) for simulating low-dose computed tomography (CT) images from a higher dose CT image. First, we present two schemes to generate a clean CT image and a noise image from the high-dose CT image. Then, given these generated images, an NE-GAN is proposed to simulate different levels of low-dose CT images, where the level of generated noise can be continuously controlled by a noise factor. NE-GAN consists of a generator and a set of discriminators, and the number of discriminators is determined by the number of noise levels during training. Compared with the traditional methods based on the projection data that are usually unavailable in real applications, NE-GAN can directly learn from the real and/or simulated CT images and may create low-dose CT images quickly without the need of raw data or other proprietary CT scanner information. The experimental results show that the proposed method has the potential to simulate realistic low-dose CT images.",Idoanimquatctpuanoffiansegeductan64ct,65.0,39.0,2.0
5144,Image generation,105.0,image smoothing via l0 gradient minimization,3.0,201.0,1.0,181.0,3.0,2.2,166.2,83,https://core.ac.uk/download/pdf/193602336.pdf,"We present a new image editing method, particularly effective for sharpening major edges by increasing the steepness of transition while eliminating a manageable degree of low-amplitude structures. The seemingly contradictive effect is achieved in an optimization framework making use of L0 gradient minimization, which can globally control how many non-zero gradients are resulted in to approximate prominent structure in a sparsity-control manner. Unlike other edge-preserving smoothing approaches, our method does not depend on local features, but instead globally locates important edges. It, as a fundamental tool, finds many applications and is particularly beneficial to edge extraction, clip-art JPEG artifact removal, and non-photorealistic effect generation.",Iimsmvil0grmi,891.0,48.0,140.0
5145,Image generation,401.0,stargan v2: diverse image synthesis for multiple domains,1.0,42.0,4.0,201.0,1.0,2.2,197.4,84,http://arxiv.org/pdf/1912.01865v2,"A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.",Istv2diimsyfomudo,303.0,61.0,71.0
5146,Image generation,401.0,gradient penalty from a maximum margin perspective,1.0,43.0,4.0,201.0,1.0,2.2,197.8,85,http://arxiv.org/pdf/1910.06922v2,"A popular heuristic for improved performance in Generative adversarial networks (GANs) is to use some form of gradient penalty on the discriminator. This gradient penalty was originally motivated by a Wasserstein distance formulation. However, the use of gradient penalty in other GAN formulations is not well motivated. We present a unifying framework of expected margin maximization and show that a wide range of gradient-penalized GANs (e.g., Wasserstein, Standard, Least-Squares, and Hinge GANs) can be derived from this framework. Our results imply that employing gradient penalties induces a large-margin classifier (thus, a large-margin discriminator in GANs). We describe how expected margin maximization helps reduce vanishing gradients at fake (generated) samples, a known problem in GANs. From this framework, we derive a new $L^\infty$ gradient norm penalty with Hinge loss which generally produces equally good (or better) generated output in GANs than $L^2$-norm penalties (based on the Fr\'echet Inception Distance).",Igrpeframamape,5.0,24.0,0.0
5147,Image generation,401.0,generating images with perceptual similarity metrics based on deep networks,1.0,44.0,4.0,201.0,1.0,2.2,198.2,86,http://arxiv.org/pdf/1602.02644v2,"Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.",Igeimwipesimebaondene,764.0,45.0,63.0
5148,Image generation,401.0,large scale gan training for high fidelity natural image synthesis,1.0,45.0,4.0,201.0,1.0,2.2,198.6,87,http://arxiv.org/pdf/1809.11096v2,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",Ilascgatrfohifinaimsy,2160.0,56.0,329.0
5149,Image generation,401.0,beyond self-attention: external attention using two linear layers for visual tasks,1.0,46.0,4.0,201.0,1.0,2.2,199.0,88,http://arxiv.org/pdf/2105.02358v2,"Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.",Ibeseexatustwlilafovita,17.0,104.0,2.0
5150,Image generation,401.0,training generative adversarial networks with limited data,1.0,49.0,4.0,201.0,1.0,2.2,200.2,89,http://arxiv.org/pdf/2101.06309v1,"Adversarial training is among the most effective techniques to improve the robustness of models against adversarial perturbations. However, the full effect of this approach on models is not well understood. For example, while adversarial training can reduce the adversarial risk (prediction error against an adversary), it sometimes increase standard risk (generalization error when there is no adversary). Even more, such behavior is impacted by various elements of the learning problem, including the size and quality of training data, specific forms of adversarial perturbations in the input, model overparameterization, and adversary's power, among others. In this paper, we focus on \emph{distribution perturbing} adversary framework wherein the adversary can change the test distribution within a neighborhood of the training data distribution. The neighborhood is defined via Wasserstein distance between distributions and the radius of the neighborhood is a measure of adversary's manipulative power. We study the tradeoff between standard risk and adversarial risk and derive the Pareto-optimal tradeoff, achievable over specific classes of models, in the infinite data limit with features dimension kept fixed. We consider three learning settings: 1) Regression with the class of linear models; 2) Binary classification under the Gaussian mixtures data model, with the class of linear classifiers; 3) Regression with the class of random features model (which can be equivalently represented as two-layer neural network with random first-layer weights). We show that a tradeoff between standard and adversarial risk is manifested in all three settings. We further characterize the Pareto-optimal tradeoff curves and discuss how a variety of factors, such as features correlation, adversary's power or the width of two-layer neural network would affect this tradeoff.",Itrgeadnewilida,226.0,60.0,62.0
5151,Image generation,401.0,taming transformers for high-resolution image synthesis,1.0,50.0,4.0,201.0,1.0,2.2,200.6,90,http://arxiv.org/pdf/2012.09841v3,"Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .",Itatrfohiimsy,68.0,102.0,13.0
5152,Image generation,401.0,liquid warping gan with attention: a unified framework for human image synthesis,1.0,51.0,4.0,201.0,1.0,2.2,201.0,91,http://arxiv.org/pdf/2011.09055v2,"We tackle human image synthesis, including human motion imitation, appearance transfer, and novel view synthesis, within a unified framework. It means that the model, once being trained, can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints to estimate the human body structure. However, they only express the position information with no abilities to characterize the personalized shape of the person and model the limb rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape. It can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block (AttLWB) that propagates the source information in both image and feature spaces to the synthesized reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method can support a more flexible warping from multiple sources. To further improve the generalization ability of the unseen source images, a one/few-shot adversarial learning is applied. In detail, it firstly trains a model in an extensive training set. Then, it finetunes the model by one/few-shot unseen image(s) in a self-supervised way to generate high-resolution (512 x 512 and 1024 x 1024) results. Also, we build a new dataset, namely iPER dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our methods in terms of preserving face identity, shape consistency, and clothes details. All codes and dataset are available on https://impersonator.org/work/impersonator-plus-plus.html.",Iliwagawiataunfrfohuimsy,1.0,69.0,0.0
5153,Image generation,401.0,neural photo editing with introspective adversarial networks,1.0,52.0,4.0,201.0,1.0,2.2,201.4,92,http://arxiv.org/pdf/1609.07093v3,"The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.",Inephedwiinadne,317.0,42.0,22.0
5154,Image generation,401.0,deep domain-adversarial image generation for domain generalisation,1.0,201.0,1.0,4.0,5.0,2.2,201.9,93,http://arxiv.org/pdf/2003.06054v1,"Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on Deep Domain-Adversarial Image Generation (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach.",Idedoimgefodoge,44.0,54.0,10.0
5155,Image generation,401.0,rebooting acgan: auxiliary classifier gans with stable training,1.0,54.0,4.0,201.0,1.0,2.2,202.2,94,http://arxiv.org/pdf/2111.01118v1,"Conditional Generative Adversarial Networks (cGAN) generate realistic images by incorporating class information into GAN. While one of the most popular cGANs is an auxiliary classifier GAN with softmax cross-entropy loss (ACGAN), it is widely known that training ACGAN is challenging as the number of classes in the dataset increases. ACGAN also tends to generate easily classifiable samples with a lack of diversity. In this paper, we introduce two cures for ACGAN. First, we identify that gradient exploding in the classifier can cause an undesirable collapse in early training, and projecting input vectors onto a unit hypersphere can resolve the problem. Second, we propose the Data-to-Data Cross-Entropy loss (D2D-CE) to exploit relational information in the class-labeled dataset. On this foundation, we propose the Rebooted Auxiliary Classifier Generative Adversarial Network (ReACGAN). The experimental results show that ReACGAN achieves state-of-the-art generation results on CIFAR10, Tiny-ImageNet, CUB200, and ImageNet datasets. We also verify that ReACGAN benefits from differentiable augmentations and that D2D-CE harmonizes with StyleGAN2 architecture. Model weights and a software package that provides implementations of representative cGANs and all experiments in our paper are available at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.",Ireacauclgawisttr,0.0,74.0,0.0
5156,Image generation,401.0,differentiable augmentation for data-efficient gan training,1.0,56.0,4.0,201.0,1.0,2.2,203.0,95,http://arxiv.org/pdf/2006.10738v4,"The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128 and 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans.",Idiaufodagatr,108.0,57.0,19.0
5157,Image generation,401.0,bipartite graph reasoning gans for person image generation,1.0,201.0,1.0,10.0,5.0,2.2,203.7,96,http://arxiv.org/pdf/2008.04381v2,"We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the challenging person image generation task. The proposed graph generator mainly consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed Bipartite Graph Reasoning (BGR) block aims to reason the crossing long-range relations between the source pose and the target pose in a bipartite graph, which mitigates some challenges caused by pose deformation. Moreover, we propose a new Interaction-and-Aggregation (IA) block to effectively update and enhance the feature representation capability of both person's shape and appearance in an interactive way. Experiments on two challenging and public datasets, i.e., Market-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at this https URL.",Ibigrregafopeimge,20.0,46.0,4.0
5158,Image generation,401.0,self-supervised gans via auxiliary rotation loss,1.0,59.0,4.0,201.0,1.0,2.2,204.2,97,http://arxiv.org/pdf/1811.11212v2,"Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.",Isegaviaurolo,154.0,47.0,25.0
5159,Image generation,401.0,learning canonical representations for scene graph to image generation,1.0,201.0,1.0,13.0,5.0,2.2,204.6,98,http://arxiv.org/pdf/1912.07414v5,"Generating realistic images of complex visual scenes becomes challenging when one wishes to control the structure of the generated images. Previous approaches showed that scenes with few entities can be controlled using scene graphs, but this approach struggles as the complexity of the graph (the number of objects and edges) increases. In this work, we show that one limitation of current methods is their inability to capture semantic equivalence in graphs. We present a novel model that addresses these issues by learning canonical graph representations from the data, resulting in improved image generation for complex visual scenes. Our model demonstrates improved empirical performance on large scene graphs, robustness to noise in the input scene graph, and generalization on semantically equivalent graphs. Finally, we show improved performance of the model on three different benchmarks: Visual Genome, COCO, and CLEVR.",Ilecarefoscgrtoimge,25.0,81.0,2.0
5160,Image generation,401.0,semi-supervised cross-modal image generation with generative adversarial networks,1.0,201.0,1.0,15.0,5.0,2.2,205.2,99,http://arxiv.org/pdf/1810.10352v1,"Abstract Cross-modal image generation is an important aspect of the multi-modal learning. Existing methods usually use the semantic feature to reduce the modality gap. Although these methods have achieved notable progress, there are still some limitations: (1) they usually use single modality information to learn the semantic feature; (2) they require the training data to be paired. To overcome these problems, we propose a novel semi-supervised cross-modal image generation method, which consists of two semantic networks and one image generation network. Specifically, in the semantic networks, we use image modality to assist non-image modality for semantic feature learning by using a deep mutual learning strategy. In the image generation network, we introduce an additional discriminator to reduce the image reconstruction loss. By leveraging large amounts of unpaired data, our method can be trained in a semi-supervised manner. Extensive experiments demonstrate the effectiveness of the proposed method.",Isecrimgewigeadne,16.0,51.0,1.0
5161,Image generation,401.0,semantic pyramid for image generation,1.0,201.0,1.0,16.0,5.0,2.2,205.5,100,http://arxiv.org/pdf/2003.06221v2,"We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid -- a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled inpainting and compositing; all achieved with the same model, with no further training.",Isepyfoimge,20.0,47.0,1.0
5512,Object Recognition,20.0,multiple object recognition with visual attention,5.0,126.0,3.0,26.0,5.0,4.2,64.2,1,https://arxiv.org/pdf/1412.7755.pdf%EF%BC%89,We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.,Omuobrewiviat,801.0,20.0,41.0
5513,Object Recognition,401.0,imagenet large scale visual recognition challenge,1.0,30.0,5.0,5.0,5.0,3.8,133.8,2,http://arxiv.org/pdf/1409.0575v3,"The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.",Oimlascvirech,22402.0,137.0,3089.0
5514,Object Recognition,401.0,domain generalization for object recognition with multi-task autoencoders,1.0,29.0,5.0,18.0,5.0,3.8,137.3,3,http://arxiv.org/pdf/1508.07680v1,"The problem of domain generalization is to take knowledge acquired from a number of related domains, where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition. The algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.",Odogefoobrewimuau,350.0,50.0,51.0
5515,Object Recognition,401.0,spatial pyramid pooling in deep convolutional networks for visual recognition,1.0,37.0,5.0,40.0,5.0,3.8,147.1,4,http://arxiv.org/abs/1406.4729v4,"Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\times$ </tex-math><alternatives><inline-graphic xlink:type=""simple"" xlink:href=""he-ieq1-2389824.gif""/></alternatives></inline-formula>224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\times$</tex-math><alternatives><inline-graphic xlink:type=""simple"" xlink:href=""he-ieq2-2389824.gif""/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.",Osppypoindeconefovire,4199.0,49.0,416.0
5516,Object Recognition,7.0,object recognition,5.0,201.0,1.0,9.0,5.0,3.4,85.2,5,http://arxiv.org/pdf/1701.05349v2,"We propose an end-to-end learning framework for generating foreground object segmentations. Given a single novel image, our approach produces pixel-level masks for all ""object-like"" regions---even for object categories never seen during training. We formulate the task as a structured prediction problem of assigning foreground/background labels to all pixels, implemented using a deep fully convolutional network. Key to our idea is training with a mix of image-level object category examples together with relatively few images with boundary-level annotations. Our method substantially improves the state-of-the-art on foreground segmentation for ImageNet and MIT Object Discovery datasets. Furthermore, on over 1 million images, we show that it generalizes well to segment object categories unseen in the foreground maps used for training. Finally, we demonstrate how our approach benefits image retrieval and image retargeting, both of which flourish when given our high-quality foreground maps.",Oobre,4402.0,55.0,395.0
5517,Object Recognition,15.0,selective search for object recognition,5.0,201.0,1.0,1.0,5.0,3.4,85.2,6,https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf,"This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 % recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html).",Osesefoobre,4402.0,55.0,395.0
5518,Object Recognition,17.0,shape matching and object recognition using shape contexts,5.0,201.0,1.0,2.0,5.0,3.4,86.1,7,https://apps.dtic.mil/sti/pdfs/ADA640016.pdf,"This paper presents my work on computing shape models that are computationally fast and invariant basic transformations like translation, scaling and rotation. In this paper, I propose shape detection using a feature called shape context. Shape context describes all boundary points of a shape with respect to any single boundary point. Thus it is descriptive of the shape of the object. Object recognition can be achieved by matching this feature with a priori knowledge of the shape context of the boundary points of the object. Experimental results are promising on handwritten digits, trademark images.",Oshmaanobreusshco,5747.0,95.0,491.0
5519,Object Recognition,8.0,recurrent convolutional neural network for object recognition,5.0,201.0,1.0,13.0,5.0,3.4,86.70000000000002,8,https://openaccess.thecvf.com/content_cvpr_2015/papers/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf,"In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.",Oreconenefoobre,694.0,78.0,57.0
5520,Object Recognition,19.0,voxnet: a 3d convolutional neural network for real-time object recognition,5.0,201.0,1.0,4.0,5.0,3.4,87.30000000000001,9,http://graphics.stanford.edu/courses/cs233-21-spring/ReferencedPapers/voxnet_07353481.pdf,"Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.",Ovoa3dconeneforeobre,1835.0,41.0,198.0
5521,Object Recognition,2.0,what is the best multi-stage architecture for object recognition?,5.0,201.0,1.0,25.0,5.0,3.4,88.5,10,https://koray.kavukcuoglu.org/publis/jarrett-iccv-09.pdf,"In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (≫ 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).",Owhisthbemuarfoobre,1992.0,55.0,125.0
5522,Object Recognition,10.0,how does the brain solve visual object recognition?,5.0,201.0,1.0,20.0,5.0,3.4,89.4,11,http://arxiv.org/pdf/1407.5822v1,"Mounting evidence suggests that 'core object recognition,' the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal.",Ohodothbrsoviobre,1201.0,218.0,50.0
5523,Object Recognition,29.0,domain adaptation for object recognition: an unsupervised approach,5.0,201.0,1.0,35.0,5.0,3.4,99.6,12,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.300.3323&rep=rep1&type=pdf,"Adapting the classifier trained on a source domain to recognize instances from a new target domain is an important problem that is receiving recent attention. In this paper, we present one of the first studies on unsupervised domain adaptation in the context of object recognition, where we have labeled data only from the source domain (and therefore do not have correspondences between object categories across domains). Motivated by incremental learning, we create intermediate representations of data between the two domains by viewing the generative subspaces (of same dimension) created from these domains as points on the Grassmann manifold, and sampling points along the geodesic between them to obtain subspaces that provide a meaningful description of the underlying domain shift. We then obtain the projections of labeled source domain data onto these subspaces, from which a discriminative classifier is learnt to classify projected data from the target domain. We discuss extensions of our approach for semi-supervised adaptation, and for cases with multiple source and target domains, and report competitive results on standard datasets.",Odoadfoobreanunap,940.0,47.0,114.0
5524,Object Recognition,401.0,relation networks for object detection,1.0,22.0,5.0,108.0,3.0,3.2,161.5,13,http://arxiv.org/pdf/1711.11575v2,"Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances individually, without exploiting their relations during learning. This work proposes an object relation module. It processes a set of objects simultaneously through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the first fully end-to-end object detector.",Orenefoobde,569.0,62.0,46.0
5525,Object Recognition,401.0,learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling,1.0,23.0,5.0,118.0,3.0,3.2,164.9,14,http://papers.nips.cc/paper/6096-learning-a-probabilisticlatent-space-of-object-shapes-via-3d-generative-adversarial-modeling.pdf,"We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",Oleaprlaspofobshvi3dgemo,1164.0,68.0,112.0
5526,Object Recognition,401.0,scalable object detection using deep neural networks,1.0,15.0,5.0,161.0,3.0,3.2,174.6,15,http://arxiv.org/pdf/1312.2249v1,"Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.",Oscobdeusdenene,906.0,20.0,52.0
5527,Object Recognition,3.0,pictorial structures for object recognition,5.0,201.0,1.0,43.0,4.0,3.1,94.20000000000002,16,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.216.7719&rep=rep1&type=pdf,"In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.",Opistfoobre,2418.0,90.0,257.0
5528,Object Recognition,11.0,robust object recognition with cortex-like mechanisms,5.0,201.0,1.0,48.0,4.0,3.1,98.1,17,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.9137&rep=rep1&type=pdf,"We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex",Oroobrewicome,1625.0,82.0,194.0
5529,Object Recognition,16.0,evaluation of pooling operations in convolutional architectures for object recognition,5.0,201.0,1.0,59.0,4.0,3.1,102.9,18,https://ais.uni-bonn.de/papers/icann2010_maxpool.pdf,"A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over nonoverlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.",Oevofpoopincoarfoobre,1183.0,27.0,31.0
5530,Object Recognition,53.0,"the novel object recognition memory: neurobiology, test procedure, and its modifications",4.0,201.0,1.0,31.0,5.0,3.1,105.6,19,http://arxiv.org/pdf/1612.04838v1,"Animal models of memory have been considered as the subject of many scientific publications at least since the beginning of the twentieth century. In humans, memory is often accessed through spoken or written language, while in animals, cognitive functions must be accessed through different kind of behaviors in many specific, experimental models of memory and learning. Among them, the novel object recognition test can be evaluated by the differences in the exploration time of novel and familiar objects. Its application is not limited to a field of research and enables that various issues can be studied, such as the memory and learning, the preference for novelty, the influence of different brain regions in the process of recognition, and even the study of different drugs and their effects. This paper describes the novel object recognition paradigms in animals, as a valuable measure of cognition. The purpose of this work was to review the neurobiology and methodological modifications of the test commonly used in behavioral pharmacology.",Othnoobremenetepranitmo,1216.0,52.0,37.0
5531,Object Recognition,30.0,segmentation as selective search for object recognition,5.0,201.0,1.0,56.0,4.0,3.1,106.2,20,https://www.koen.me/research/pub/vandesande-iccv2011.pdf,"For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.",Oseassesefoobre,681.0,41.0,72.0
5532,Object Recognition,85.0,multi-view harmonized bilinear network for 3d object recognition,4.0,201.0,1.0,11.0,5.0,3.1,109.2,21,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Multi-View_Harmonized_Bilinear_CVPR_2018_paper.pdf,"View-based methods have achieved considerable success in 3D object recognition tasks. Different from existing view-based methods pooling the view-wise features, we tackle this problem from the perspective of patches-to-patches similarity measurement. By exploiting the relationship between polynomial kernel and bilinear pooling, we obtain an effective 3D object representation by aggregating local convolutional features through bilinear pooling. Meanwhile, we harmonize different components inherited in the bilinear feature to obtain a more discriminative representation. To achieve an end-to-end trainable framework, we incorporate the harmonized bilinear pooling as a layer of a network, constituting the proposed Multi-view Harmonized Bilinear Network (MHBN). Systematic experiments conducted on two public benchmark datasets demonstrate the efficacy of the proposed methods in 3D object recognition.",Omuhabinefo3dobre,102.0,36.0,13.0
5533,Object Recognition,32.0,recurrent processing during object recognition,5.0,201.0,1.0,81.0,4.0,3.1,114.3,22,http://arxiv.org/abs/1706.02240v2,"How does the brain learn to recognize objects visually, and perform this difficult feat robustly in the face of many sources of ambiguity and variability? We present a computational model based on the biology of the relevant visual pathways that learns to reliably recognize 100 different object categories in the face of naturally occurring variability in location, rotation, size, and lighting. The model exhibits robustness to highly ambiguous, partially occluded inputs. Both the unified, biologically plausible learning mechanism and the robustness to occlusion derive from the role that recurrent connectivity and recurrent processing mechanisms play in the model. Furthermore, this interaction of recurrent connectivity and learning predicts that high-level visual representations should be shaped by error signals from nearby, associated brain areas over the course of visual learning. Consistent with this prediction, we show how semantic knowledge about object categories changes the nature of their learned visual representations, as well as how this representational shift supports the mapping between perceptual and conceptual knowledge. Altogether, these findings support the potential importance of ongoing recurrent processing throughout the brain’s visual system and suggest ways in which object recognition can be understood in terms of interactions within and between processes over time.",Oreprduobre,118.0,159.0,7.0
5534,Object Recognition,42.0,object recognition as machine translation: learning a lexicon for a fixed image vocabulary,4.0,201.0,1.0,82.0,4.0,2.8,117.6,23,https://link.springer.com/content/pdf/10.1007/3-540-47979-1_7.pdf,"We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well -- for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach.",Oobreasmatrlealefoafiimvo,1778.0,17.0,195.0
5535,Object Recognition,45.0,"textonboost: joint appearance, shape and context modeling for multi-class object recognition and segmentation",4.0,201.0,1.0,91.0,4.0,2.8,121.2,24,https://link.springer.com/content/pdf/10.1007/11744023_1.pdf,"This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods. 
 
High classification and segmentation accuracy are demonstrated on three different databases: i) our own 21-object class database of photographs of real objects viewed under general lighting conditions, poses and viewpoints, ii) the 7-class Corel subset and iii) the 7-class Sowerby database used in [1]. The proposed algorithm gives competitive results both for highly textured (e.g. grass, trees), highly structured (e.g. cars, faces, bikes, aeroplanes) and articulated objects (e.g. body, cow).",Otejoapshancomofomuobreanse,1332.0,27.0,212.0
5536,Object Recognition,6.0,"what, where and who? classifying events by scene and object recognition",5.0,201.0,1.0,145.0,3.0,2.8,125.7,25,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.1786&rep=rep1&type=pdf,"We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4% accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance.",Owhwhanwhclevbyscanobre,815.0,29.0,88.0
5537,Object Recognition,37.0,crowding—an essential bottleneck for object recognition: a mini-review,5.0,201.0,1.0,119.0,3.0,2.8,127.2,26,http://arxiv.org/abs/1510.01257v1,"Crowding, generally defined as the deleterious influence of nearby contours on visual discrimination, is ubiquitous in spatial vision. Crowding impairs the ability to recognize objects in clutter. It has been extensively studied over the last 80 years or so, and much of the renewed interest is the hope that studying crowding may lead to a better understanding of the processes involved in object recognition. Crowding also has important clinical implications for patients with macular degeneration, amblyopia and dyslexia. There is no shortage of theories for crowding-from low-level receptive field models to high-level attention. The current picture is that crowding represents an essential bottleneck for object perception, impairing object perception in peripheral, amblyopic and possibly developing vision. Crowding is neither masking nor surround suppression. We can localize crowding to the cortex, perhaps as early as V1; however, there is a growing consensus for a two-stage model of crowding in which the first stage involves the detection of simple features (perhaps in V1), and a second stage is required for the integration or interpretation of the features as an object beyond V1. There is evidence for top-down effects in crowding, but the role of attention in this process remains unclear. The strong effect of learning in shrinking the spatial extent of crowding places strong constraints on possible models for crowding and for object recognition. The goal of this review is to try to provide a broad, balanced and succinct review that organizes and summarizes the diverse and scattered studies of crowding, and also helps to explain it to the non-specialist. A full understanding of crowding may allow us to understand this bottleneck to object recognition and the rules that govern the integration of features into objects.",Ocresbofoobreami,826.0,203.0,113.0
5538,Object Recognition,4.0,the role of context in object recognition,5.0,201.0,1.0,156.0,3.0,2.8,128.4,27,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.420.992&rep=rep1&type=pdf,"In the real world, objects never occur in isolation; they co-vary with other objects and particular environments, providing a rich source of contextual associations to be exploited by the visual system. A natural way of representing the context of an object is in terms of its relationship to other objects. Alternately, recent work has shown that a statistical summary of the scene provides a complementary and effective source of information for contextual inference, which enables humans to quickly guide their attention and eyes to regions of interest in natural scenes. A better understanding of how humans build such scene representations, and of the mechanisms of contextual analysis, will lead to a new generation of computer vision systems.",Othroofcoinobre,849.0,92.0,30.0
5539,Object Recognition,25.0,unsupervised learning of invariant feature hierarchies with applications to object recognition,5.0,201.0,1.0,142.0,3.0,2.8,130.5,28,http://ailab.chonbuk.ac.kr/seminar_board/pds1_files/unsupervised_invariant%20feature%20hierarchy.pdf,"We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.",Ounleofinfehiwiaptoobre,1065.0,25.0,60.0
5540,Object Recognition,34.0,learning methods for generic object recognition with invariance to pose and lighting,5.0,201.0,1.0,135.0,3.0,2.8,131.10000000000002,29,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.7765&rep=rep1&type=pdf,"We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.",Olemefogeobrewiintopoanli,1249.0,41.0,120.0
5541,Object Recognition,5.0,untangling invariant object recognition,5.0,201.0,1.0,171.0,3.0,2.8,133.2,30,http://www.rowland.harvard.edu/rjf/cox/pdfs/TICS_DiCarloCox_2007.pdf,"Despite tremendous variation in the appearance of visual objects, primates can recognize a multitude of objects, each in a fraction of a second, with no apparent effort. However, the brain mechanisms that enable this fundamental ability are not understood. Drawing on ideas from neurophysiology and computation, we present a graphical perspective on the key computational challenges of object recognition, and argue that the format of neuronal population representation and a property that we term 'object tangling' are central. We use this perspective to show that the primate ventral visual processing stream achieves a particularly effective solution in which single-neuron invariance is not the goal. Finally, we speculate on the key neuronal mechanisms that could enable this solution, which, if understood, would have far-reaching implications for cognitive neuroscience.",Ouninobre,756.0,71.0,24.0
5542,Object Recognition,173.0,brain-like object recognition with high-performing shallow recurrent anns,3.0,53.0,4.0,201.0,1.0,2.8,133.39999999999998,31,https://arxiv.org/pdf/1909.06161,"Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-1 performance. Finally, we report that the temporal evolution of the CORnet-S ""IT"" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream.",Obrobrewihishrean,70.0,51.0,6.0
5543,Object Recognition,18.0,object recognition with features inspired by visual cortex,5.0,201.0,1.0,159.0,3.0,2.8,133.5,32,https://apps.dtic.mil/sti/pdfs/ADA454604.pdf,We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.,Oobrewifeinbyvico,971.0,27.0,114.0
5544,Object Recognition,24.0,the lateral occipital complex and its role in object recognition,5.0,201.0,1.0,160.0,3.0,2.8,135.60000000000002,33,http://arxiv.org/abs/1004.3153v1,"Here we review recent findings that reveal the functional properties of extra-striate regions in the human visual cortex that are involved in the representation and perception of objects. We characterize both the invariant and non-invariant properties of these regions and we discuss the correlation between activation of these regions and recognition. Overall, these results indicate that the lateral occipital complex plays an important role in human object recognition.",Othlaoccoanitroinobre,1160.0,136.0,38.0
5545,Object Recognition,13.0,context-based vision system for place and object recognition,5.0,201.0,1.0,180.0,3.0,2.8,138.3,34,https://dspace.mit.edu/bitstream/handle/1721.1/6711/AIM-2003-005.pdf?sequence=2,"While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.",Ocovisyfoplanobre,976.0,23.0,46.0
5546,Object Recognition,40.0,shape matching and object recognition using low distortion correspondences,5.0,201.0,1.0,177.0,3.0,2.8,145.5,35,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.8044&rep=rep1&type=pdf,"We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48% correct classification rate, compared to Fei-Fei et al 's 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces.",Oshmaanobreuslodico,961.0,42.0,49.0
5547,Object Recognition,39.0,"the pharmacology, neuroanatomy and neurogenetics of one-trial object recognition in rodents",5.0,201.0,1.0,192.0,3.0,2.8,149.7,36,http://arxiv.org/pdf/1505.06434v1,"Rats and mice are attracted by novel objects. They readily approach novel objects and explore them with their vibrissae, nose and forepaws. It is assumed that such a single explorative episode leaves a lasting and complex memory trace, which includes information about the features of the object explored, as well as where and even when the object was encountered. Indeed, it has been shown that rodents are able to discriminate a novel from a familiar object (one-trial object recognition), can detect a mismatch between the past and present location of a familiar object (one-trial object-place recognition), and can discriminate different objects in terms of their relative recency (temporal order memory), i.e., which one of two objects has been encountered earlier. Since the novelty-preference paradigm is very versatile and has some advantages compared to several other memory tasks, such as the water maze, it has become a powerful tool in current neurophamacological, neuroanatomical and neurogenetical memory research using both rats and mice. This review is intended to provide a comprehensive summary on key findings delineating the brain structures, neurotransmitters, molecular mechanisms and genes involved in encoding, consolidation, storage and retrieval of different forms of one-trial object memory in rats and mice.",Othphneanneofonobreinro,617.0,232.0,39.0
5548,Object Recognition,401.0,deep compositional captioning: describing novel object categories without paired training data,1.0,84.0,4.0,189.0,3.0,2.8,210.6,37,http://arxiv.org/pdf/1511.05284v2,"While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired imagesentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-sentence data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.",Odecocadenoobcawipatrda,217.0,48.0,36.0
5549,Object Recognition,401.0,looking fast and slow: memory-guided mobile video object detection,1.0,1.0,5.0,201.0,1.0,2.6,181.0,38,http://arxiv.org/pdf/1903.10172v1,"With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the ""gist"" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.",Olofaanslmemoviobde,35.0,42.0,3.0
5550,Object Recognition,401.0,going deeper with convolutions,1.0,2.0,5.0,201.0,1.0,2.6,181.4,39,http://arxiv.org/pdf/1409.4842v1,"We propose a deep convolutional neural network architecture codenamed ""Inception"", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",Ogodewico,26591.0,277.0,2983.0
5551,Object Recognition,401.0,feature pyramid grids,1.0,3.0,5.0,201.0,1.0,2.6,181.8,40,http://arxiv.org/pdf/2004.03580v1,"Feature pyramid networks have been widely adopted in the object detection literature to improve feature representations for better handling of variations in scale. In this paper, we present Feature Pyramid Grids (FPG), a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral connections. FPG can improve single-pathway feature pyramid networks by significantly increasing its performance at similar computation cost, highlighting importance of deep pyramid representations. In addition to its general and uniform structure, over complicated structures that have been found with neural architecture search, it also compares favorably against such approaches without relying on search. We hope that FPG with its uniform and effective nature can serve as a strong component for future work in object recognition.",Ofepygr,11.0,41.0,1.0
5552,Object Recognition,401.0,densely connected convolutional networks,1.0,5.0,5.0,201.0,1.0,2.6,182.6,41,http://arxiv.org/pdf/1608.06993v5,"Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .",Odecocone,15839.0,59.0,2458.0
5553,Object Recognition,401.0,distinctive image features from scale-invariant keypoints,1.0,6.0,5.0,201.0,1.0,2.6,183.0,42,http://arxiv.org/pdf/1704.04235v1,"Domain adaptation is transfer learning which aims to generalize a learning model across training and testing data with different distributions. Most previous research tackle this problem in seeking a shared feature representation between source and target domains while reducing the mismatch of their data distributions. In this paper, we propose a close yet discriminative domain adaptation method, namely CDDA, which generates a latent feature representation with two interesting properties. First, the discrepancy between the source and target domain, measured in terms of both marginal and conditional probability distribution via Maximum Mean Discrepancy is minimized so as to attract two domains close to each other. More importantly, we also design a repulsive force term, which maximizes the distances between each label dependent sub-domain to all others so as to drag different class dependent sub-domains far away from each other and thereby increase the discriminative power of the adapted domain. Moreover, given the fact that the underlying data manifold could have complex geometric structure, we further propose the constraints of label smoothness and geometric structure consistency for label propagation. Extensive experiments are conducted on 36 cross-domain image classification tasks over four public datasets. The comprehensive results show that the proposed method consistently outperforms the state-of-the-art methods with significant margins.",Odiimfefrscke,24111.0,75.0,3849.0
5554,Object Recognition,401.0,sparse r-cnn: end-to-end object detection with learnable proposals,1.0,7.0,5.0,201.0,1.0,2.6,183.4,43,http://arxiv.org/pdf/2011.12450v2,"We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as $k$ anchor boxes pre-defined on all grids of image feature map of size $H\times W$. In our method, however, a fixed sparse set of learned object proposals, total length of $N$, are provided to object recognition head to perform classification and location. By eliminating $HWk$ (up to hundreds of thousands) hand-designed object candidates to $N$ (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard $3\times$ training schedule and running at 22 fps using ResNet-50 FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN.",Ospr-enobdewilepr,49.0,55.0,11.0
5555,Object Recognition,401.0,microsoft coco: common objects in context,1.0,8.0,5.0,201.0,1.0,2.6,183.8,44,http://arxiv.org/pdf/1405.0312v3,"We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",Omicocoobinco,16256.0,60.0,2813.0
5556,Object Recognition,401.0,cutting the error by half: investigation of very deep cnn and advanced training strategies for document image classification,1.0,9.0,5.0,201.0,1.0,2.6,184.2,45,http://arxiv.org/abs/1704.03557v1,"We present an exhaustive investigation of recent Deep Learning architectures, algorithms, and strategies for the task of document image classification to finally reduce the error by more than half. Existing approaches, such as the DeepDocClassifier, apply standard Convolutional Network architectures with transfer learning from the object recognition domain. The contribution of the paper is threefold: First, it investigates recently introduced very deep neural network architectures (GoogLeNet, VGG, ResNet) using transfer learning (from real images). Second, it proposes transfer learning from a huge set of document images, i.e. 400,000 documents. Third, it analyzes the impact of the amount of training data (document images) and other parameters to the classification abilities. We use two datasets, the Tobacco-3482 and the large-scale RVL-CDIP dataset. We achieve an accuracy of 91.13% for the Tobacco-3482 dataset while earlier approaches reach only 77.6%. Thus, a relative error reduction of more than 60% is achieved. For the large dataset RVL-CDIP, an accuracy of 90.97% is achieved, corresponding to a relative error reduction of 11.5%.",Ocutherbyhainofvedecnanadtrstfodoimcl,33.0,30.0,5.0
5557,Object Recognition,401.0,improved deep metric learning with multi-class n-pair loss objective,1.0,11.0,5.0,201.0,1.0,2.6,185.0,46,http://arxiv.org/pdf/1905.10675v1,"Metric learning has become an attractive field for research on the latest years. Loss functions like contrastive loss, triplet loss or multi-class N-pair loss have made possible generating models capable of tackling complex scenarios with the presence of many classes and scarcity on the number of images per class not only work to build classifiers, but to many other applications where measuring similarity is the key. Deep Neural Networks trained via metric learning also offer the possibility to solve few-shot learning problems. Currently used state of the art loss functions such as triplet and contrastive loss functions, still suffer from slow convergence due to the selection of effective training samples that has been partially solved by the multi-class N-pair loss by simultaneously adding additional samples from the different classes. In this work, we extend triplet and multiclass-N-pair loss function by proposing the constellation loss metric where the distances among all class combinations are simultaneously learned. We have compared our constellation loss for visual class embedding showing that our loss function over-performs the other methods by obtaining more compact clusters while achieving better classification results.",Oimdemelewimun-loob,827.0,35.0,122.0
5558,Object Recognition,401.0,efficient attention: attention with linear complexities,1.0,14.0,5.0,201.0,1.0,2.6,186.2,47,http://arxiv.org/abs/math/0506577v3,"It is known since 1954 that every 3-manifold bounds a 4-manifold. Thus, for instance, every 3-manifold has a surgery diagram. There are several proofs of this fact, including constructive proofs, but there has been little attention to the complexity of the 4-manifold produced. Given a 3-manifold M of complexity n, we show how to construct a 4-manifold bounded by M of complexity O(n^2). Here we measure ``complexity'' of a piecewise-linear manifold by the minimum number of n-simplices in a triangulation. It is an open question whether this quadratic bound can be replaced by a linear bound.   The proof goes through the notion of ""shadow complexity"" of a 3-manifold M. A shadow of M is a well-behaved 2-dimensional spine of a 4-manifold bounded by M. We prove that, for a manifold M satisfying the Geometrization Conjecture with Gromov norm G and shadow complexity S, c_1 G <= S <= c_2 G^2 for suitable constants c_1, c_2. In particular, the manifolds with shadow complexity 0 are the graph manifolds.",Oefatatwilico,47.0,49.0,5.0
5559,Object Recognition,401.0,some improvements on deep convolutional neural network based image classification,1.0,16.0,5.0,201.0,1.0,2.6,187.0,48,http://arxiv.org/pdf/1312.5402v1,"We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner.",Osoimondeconenebaimcl,331.0,9.0,11.0
5560,Object Recognition,401.0,temporal roi align for video object recognition,1.0,17.0,5.0,201.0,1.0,2.6,187.4,49,http://arxiv.org/pdf/2109.03495v2,"Video object detection is challenging in the presence of appearance deterioration in certain video frames. Therefore, it is a natural choice to aggregate temporal information from other frames of the same video into the current frame. However, RoI Align, as one of the most core procedures of video detectors, still remains extracting features from a single-frame feature map for proposals, making the extracted RoI features lack temporal information from videos. In this work, considering the features of the same object instance are highly similar among frames in a video, a novel Temporal RoI Align operator is proposed to extract features from other frames feature maps for current frame proposals by utilizing feature similarity. The proposed Temporal RoI Align operator can extract temporal information from the entire video for proposals. We integrate it into single-frame video detectors and other state-of-the-art video detectors, and conduct quantitative experiments to demonstrate that the proposed Temporal RoI Align operator can consistently and significantly boost the performance. Besides, the proposed Temporal RoI Align can also be applied into video instance segmentation. Codes are available at https://github.com/open-mmlab/mmtracking",Oteroalfoviobre,0.0,42.0,0.0
5561,Object Recognition,401.0,sparse 3d convolutional neural networks,1.0,18.0,5.0,201.0,1.0,2.6,187.8,50,http://arxiv.org/pdf/1706.01307v1,"Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include pen-strokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard ""dense"" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than ""dilating"" the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation.",Osp3dconene,99.0,18.0,9.0
5562,Object Recognition,401.0,texture synthesis using convolutional neural networks,1.0,20.0,5.0,201.0,1.0,2.6,188.6,51,http://arxiv.org/pdf/1712.03111v2,"Deep neural networks have been successfully applied to problems such as image segmentation, image super-resolution, coloration and image inpainting. In this work we propose the use of convolutional neural networks (CNN) for image inpainting of large regions in high-resolution textures. Due to limited computational resources processing high-resolution images with neural networks is still an open problem. Existing methods separate inpainting of global structure and the transfer of details, which leads to blurry results and loss of global coherence in the detail transfer step. Based on advances in texture synthesis using CNNs we propose patch-based image inpainting by a CNN that is able to optimize for global as well as detail texture statistics. Our method is capable of filling large inpainting regions, oftentimes exceeding the quality of comparable methods for high-resolution images. For reference patch look-up we propose to use the same summary statistics that are used in the inpainting process.",Otesyusconene,824.0,32.0,83.0
5563,Object Recognition,401.0,finding tiny faces,1.0,21.0,5.0,201.0,1.0,2.6,189.0,52,http://arxiv.org/pdf/1612.04402v2,"Though tremendous strides have been made in object recognition, one of the remaining open challenges is detecting small objects. We explore three aspects of the problem in the context of finding small faces: the role of scale invariance, image resolution, and contextual reasoning. While most recognition approaches aim to be scale-invariant, the cues for recognizing a 3px tall face are fundamentally different than those for recognizing a 300px tall face. We take a different approach and train separate detectors for different scales. To maintain efficiency, detectors are trained in a multi-task fashion: they make use of features extracted from multiple layers of single (deep) feature hierarchy. While training detectors for large objects is straightforward, the crucial challenge remains training detectors for small objects. We show that context is crucial, and define templates that make use of massively-large receptive fields (where 99% of the template extends beyond the object of interest). Finally, we explore the role of scale in pre-trained deep networks, providing ways to extrapolate networks tuned for limited scales to rather extreme ranges. We demonstrate state-of-the-art results on massively-benchmarked face datasets (FDDB and WIDER FACE). In particular, when compared to prior art on WIDER FACE, our results reduce error by a factor of 2 (our models produce an AP of 82% while prior art ranges from 29-64%).",Ofitifa,532.0,28.0,79.0
5564,Object Recognition,401.0,attribution in scale and space,1.0,24.0,5.0,201.0,1.0,2.6,190.2,53,http://arxiv.org/pdf/1611.03021v2,"In this paper we describe an algorithm for estimating the provenance of hacks on websites. That is, given properties of sites and the temporal occurrence of attacks, we are able to attribute individual attacks to joint causes and vulnerabilities, as well as estimating the evolution of these vulnerabilities over time. Specifically, we use hazard regression with a time-varying additive hazard function parameterized in a generalized linear form. The activation coefficients on each feature are continuous-time functions over time. We formulate the problem of learning these functions as a constrained variational maximum likelihood estimation problem with total variation penalty and show that the optimal solution is a 0th order spline (a piecewise constant function) with a finite number of known knots. This allows the inference problem to be solved efficiently and at scale by solving a finite dimensional optimization problem. Extensive experiments on real data sets show that our method significantly outperforms Cox's proportional hazard model. We also conduct a case study and verify that the fitted functions are indeed recovering vulnerable features and real-life events such as the release of code to exploit these features in hacker blogs.",Oatinscansp,15.0,36.0,1.0
5565,Object Recognition,401.0,deep predictive coding networks for video prediction and unsupervised learning,1.0,25.0,5.0,201.0,1.0,2.6,190.6,54,http://arxiv.org/pdf/1605.08104v5,"While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (""PredNet"") architecture that is inspired by the concept of ""predictive coding"" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.",Odeprconefovipranunle,598.0,83.0,84.0
5566,Object Recognition,401.0,striving for simplicity: the all convolutional net,1.0,26.0,5.0,201.0,1.0,2.6,191.0,55,http://arxiv.org/pdf/1412.6806v3,"Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the ""deconvolution approach"" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.",Ostfosithalcone,2915.0,31.0,307.0
5567,Object Recognition,401.0,generalisation in humans and deep neural networks,1.0,27.0,5.0,201.0,1.0,2.6,191.4,56,http://arxiv.org/pdf/1808.08750v3,"We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.",Ogeinhuandenene,247.0,81.0,22.0
5568,Object Recognition,401.0,residual attention network for image classification,1.0,28.0,5.0,201.0,1.0,2.6,191.8,57,http://arxiv.org/pdf/1608.02201v1,"Convolutional Neural networks nowadays are of tremendous importance for any image classification system. One of the most investigated methods to increase the accuracy of CNN is by increasing the depth of CNN. Increasing the depth by stacking more layers also increases the difficulty of training besides making it computationally expensive. Some research found that adding auxiliary forks after intermediate layers increases the accuracy. Specifying which intermediate layer shoud have the fork just addressed recently. Where a simple rule were used to detect the position of intermediate layers that needs the auxiliary supervision fork. This technique known as convolutional neural networks with deep supervision (CNDS). This technique enhanced the accuracy of classification over the straight forward CNN used on the MIT places dataset and ImageNet. In the other side, Residual Learning is another technique emerged recently to ease the training of very deep CNN. Residual Learning framwork changed the learning of layers from unreferenced functions to learning residual function with regard to the layer's input. Residual Learning achieved state of arts results on ImageNet 2015 and COCO competitions. In this paper, we study the effect of adding residual connections to CNDS network. Our experiments results show increasing of accuracy over using CNDS only.",Oreatnefoimcl,1669.0,41.0,123.0
5569,Object Recognition,401.0,deep subdomain adaptation network for image classification,1.0,33.0,5.0,201.0,1.0,2.6,193.8,58,http://arxiv.org/abs/2106.09388v1,"For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. Previous deep domain adaptation methods mainly learn a global domain shift, i.e., align the global source and target distributions without considering the relationships between two subdomains within the same category of different domains, leading to unsatisfying transfer learning performance without capturing the fine-grained information. Recently, more and more researchers pay attention to Subdomain Adaptation which focuses on accurately aligning the distributions of the relevant subdomains. However, most of them are adversarial methods which contain several loss functions and converge slowly. Based on this, we present Deep Subdomain Adaptation Network (DSAN) which learns a transfer network by aligning the relevant subdomain distributions of domain-specific layer activations across different domains based on a local maximum mean discrepancy (LMMD). Our DSAN is very simple but effective which does not need adversarial training and converges fast. The adaptation can be achieved easily with most feed-forward network models by extending them with LMMD loss, which can be trained efficiently via back-propagation. Experiments demonstrate that DSAN can achieve remarkable results on both object recognition tasks and digit classification tasks. Our code will be available at: https://github.com/easezyc/deep-transfer-learning",Odesuadnefoimcl,31.0,57.0,5.0
5570,Object Recognition,401.0,mixstyle neural networks for domain generalization and adaptation,1.0,34.0,5.0,201.0,1.0,2.6,194.2,59,http://arxiv.org/pdf/2104.02008v1,"Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.",Ominenefodogeanad,1.0,83.0,0.0
5571,Object Recognition,401.0,dynamic few-shot visual learning without forgetting,1.0,35.0,5.0,201.0,1.0,2.6,194.6,60,http://arxiv.org/pdf/1804.09458v1,"The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on ""unseen"" categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results. The code and models of our paper will be published on: https://github.com/gidariss/FewShotWithoutForgetting",Odyfevilewifo,545.0,39.0,79.0
5572,Object Recognition,401.0,unsupervised vision-and-language pre-training without parallel images and captions,1.0,38.0,5.0,201.0,1.0,2.6,195.8,61,http://arxiv.org/pdf/2010.01288v2,"Research in image captioning has mostly focused on English because of the availability of image-caption paired datasets in this language. However, building vision-language systems only for English deprives a large part of the world population of AI technologies' benefit. On the other hand, creating image-caption paired datasets for every target language is expensive. In this work, we present a novel unsupervised cross-lingual method to generate image captions in a target language without using any image-caption corpus in the source or target languages. Our method relies on (i) a cross-lingual scene graph to sentence translation process, which learns to decode sentences in the target language from a cross-lingual encoding space of scene graphs using a sentence parallel (bitext) corpus, and (ii) an unsupervised cross-modal feature mapping which seeks to map an encoded scene graph features from image modality to language modality. We verify the effectiveness of our proposed method on the Chinese image caption generation task. The comparisons against several existing methods demonstrate the effectiveness of our approach.",Ounviprwipaimanca,2.0,66.0,0.0
5573,Object Recognition,401.0,a survey on bayesian deep learning,1.0,40.0,5.0,201.0,1.0,2.6,196.6,62,http://arxiv.org/pdf/1604.01662v4,"A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to https://github.com/js05212/BayesianDeepLearning-Survey.",Oasuonbadele,27.0,232.0,0.0
5574,Object Recognition,64.0,"textonboost for image understanding: multi-class object recognition and segmentation by jointly modeling texture, layout, and context",4.0,201.0,1.0,128.0,3.0,2.5,138.0,63,http://www.patternrecognition.asia/perception/understanding2009a.pdf,"Abstract
This paper details a new approach for learning a discriminative model of object classes, incorporating texture, layout, and context information efficiently. The learned model is used for automatic visual understanding and semantic segmentation of photographs. Our discriminative model exploits texture-layout filters, novel features based on textons, which jointly model patterns of texture and their spatial layout. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating the unary classifier in a conditional random field, which (i) captures the spatial interactions between class labels of neighboring pixels, and (ii) improves the segmentation of specific object instances. Efficient training of the model on large datasets is achieved by exploiting both random feature selection and piecewise training methods.

High classification and segmentation accuracy is demonstrated on four varied databases: (i) the MSRC 21-class database containing photographs of real objects viewed under general lighting conditions, poses and viewpoints, (ii) the 7-class Corel subset and (iii) the 7-class Sowerby database used in He et al. (Proceeding of IEEE Conference on Computer Vision and Pattern Recognition, vol. 2, pp. 695–702, June 2004), and (iv) a set of video sequences of television shows. The proposed algorithm gives competitive and visually pleasing results for objects that are highly textured (grass, trees, etc.), highly structured (cars, faces, bicycles, airplanes, etc.), and even articulated (body, cow, etc.).
",Otefoimunmuobreansebyjomotelaanco,1072.0,69.0,123.0
5575,Object Recognition,56.0,dorsal–ventral integration in object recognition,4.0,201.0,1.0,167.0,3.0,2.5,147.3,64,http://arxiv.org/pdf/1811.02910v2,"The idea of two parallel hierarchical pathways in vision has fueled a great deal of research and enhanced our understanding of visual processing in the brain. However, after 25 years, it has become clear that the earlier distinctions in terms of neuroanatomy and functional dissociation are less pure than originally considered. Dorsal visual areas may exhibit object-selective responses and many 3-D cues of shape, particularly structure-from-motion, appear to be computed exclusively by dorsal areas. These findings imply a more important role for dorsal visual areas in object recognition than previously considered and also place restrictions on the nature of ventral object representations. These representations will need to include information about the objects in 3-D, making them more viewpoint-invariant. They will also need to be invariant to the 3-D cue used to describe them. Through the discussion of relevant findings in psychophysics, single-unit electrophysiology, neuroanatomy and functional imaging, I suggest that these qualities are indeed present in ventral stream representations. Thus dorsal visual areas that extract 3-D structure of shapes from certain cues, can relate these representations to cue-invariant and view-invariant representations in the ventral stream.",Odoininobre,99.0,83.0,7.0
5576,Object Recognition,1.0,models of object recognition,5.0,201.0,1.0,201.0,1.0,2.2,141.0,65,https://www.cs.utexas.edu/~grauman/courses/spring2007/395T/395T/papers/poggio_Nature2000.pdf,"Scene recognition is a fundamental task in robotic perception. For human beings, scene recognition is reasonable because they have abundant object knowledge of the real world. The idea of transferring prior object knowledge from humans to scene recognition is significant but still less exploited. In this paper, we propose to utilize meaningful object representations for indoor scene representation. First, we utilize an improved object model (IOM) as a baseline that enriches the object knowledge by introducing a scene parsing algorithm pretrained on the ADE20K dataset with rich object categories related to the indoor scene. To analyze the object co-occurrences and pairwise object relations, we formulate the IOM from a Bayesian perspective as the Bayesian object relation model (BORM). Meanwhile, we incorporate the proposed BORM with the PlacesCNN model as the combined Bayesian object relation model (CBORM) for scene recognition and significantly outperforms the state-of-the-art methods on the reduced Places365 dataset, and SUN RGB-D dataset without retraining, showing the excellent generalization ability of the proposed method. Code can be found at https://github.com/hszhoushen/borm.",Omoofobre,3298.0,51.0,289.0
5577,Object Recognition,9.0,revisiting snodgrass and vanderwart's object pictorial set: the role of surface detail in basic-level object recognition,5.0,201.0,1.0,201.0,1.0,2.2,143.4,66,http://files.face-categorization-lab.webnode.com/200000674-aef01b2005/RossionPourtois_2004_Perception.pdf,"We address the generalization ability of recent learning-based point cloud registration methods. Despite their success, these approaches tend to have poor performance when applied to mismatched conditions that are not well-represented in the training set, such as unseen object categories, different complex scenes, or unknown depth sensors. In these circumstances, it has often been better to rely on classical non-learning methods (e.g., Iterative Closest Point), which have better generalization ability. Hybrid learning methods, that use learning for predicting point correspondences and then a deterministic step for alignment, have offered some respite, but are still limited in their generalization abilities. We revisit a recent innovation -- PointNetLK -- and show that the inclusion of an analytical Jacobian can exhibit remarkable generalization properties while reaping the inherent fidelity benefits of a learning framework. Our approach not only outperforms the state-of-the-art in mismatched conditions but also produces results competitive with current learning methods when operating on real-world test data close to the training set.",Oresnanvaobpisethroofsudeinbaobre,823.0,91.0,55.0
5578,Object Recognition,14.0,high-level vision: object recognition and visual cognition,5.0,201.0,1.0,201.0,1.0,2.2,144.9,67,http://arxiv.org/pdf/0808.0056v1,"The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.",Ohiviobreanvico,369.0,0.0,15.0
5579,Object Recognition,21.0,the role of color diagnosticity in object recognition and representation,5.0,201.0,1.0,201.0,1.0,2.2,147.0,68,https://education.ufl.edu/dtherriault/files/2013/03/Color-diagnosticity_2009.pdf,"Despite the recent success of state-of-the-art 3D object recognition approaches, service robots are frequently failed to recognize many objects in real human-centric environments. For these robots, object recognition is a challenging task due to the high demand for accurate and real-time response under changing and unpredictable environmental conditions. Most of the recent approaches use either the shape information only and ignore the role of color information or vice versa. Furthermore, they mainly utilize the $L_n$ Minkowski family functions to measure the similarity of two object views, while there are various distance measures that are applicable to compare two object views. In this paper, we explore the importance of shape information, color constancy, color spaces, and various similarity measures in open-ended 3D object recognition. Towards this goal, we extensively evaluate the performance of object recognition approaches in three different configurations, including \textit{color-only}, \textit{shape-only}, and \textit{ combinations of color and shape}, in both offline and online settings. Experimental results concerning scalability, memory usage, and object recognition performance show that all of the \textit{combinations of color and shape} yields significant improvements over the \textit{shape-only} and \textit{color-only} approaches. The underlying reason is that color information is an important feature to distinguish objects that have very similar geometric properties with different colors and vice versa. Moreover, by combining color and shape information, we demonstrate that the robot can learn new object categories from very few training examples in a real-world setting.",Othroofcodiinobreanre,71.0,38.0,8.0
5580,Object Recognition,23.0,learning models for object recognition,5.0,201.0,1.0,201.0,1.0,2.2,147.60000000000002,69,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.2137&rep=rep1&type=pdf,"Learning object models from views in 3D visual object recognition is usually formulated either as a function approximation problem of a function describing the view-manifold of an object, or as that of learning a class-conditional density. This paper describes an alternative framework for learning in visual object recognition, that of learning the view-generalization function. Using the view-generalization function, an observer can perform Bayes-optimal 3D object recognition given one or more 2D training views directly, without the need for a separate model acquisition step. The paper shows that view generalization functions can be computationally practical by restating two widely-used methods, the eigenspace and linear combination of views approaches, in a view generalization framework. The paper relates the approach to recent methods for object recognition based on non-uniform blurring. The paper presents results both on simulated 3D ``paperclip'' objects and real-world images from the COIL-100 database showing that useful view-generalization functions can be realistically be learned from a comparatively small number of training examples.",Olemofoobre,91.0,19.0,6.0
5581,Object Recognition,26.0,after the viewpoint debate: where next in object recognition?,5.0,201.0,1.0,201.0,1.0,2.2,148.5,70,http://arxiv.org/pdf/astro-ph/9612229v1,"The outstanding task of gamma-ray burst astronomy is to test the hypothesis that they are at cosmological distances. This can be done by determining the coordinates of at least a few bursts to ~ 15'' or better. If they are in distant galaxies, the next task is to determine from which component of the galaxies the bursts originate; sub-arcsecond positions would answer this question. I outline ground-based systems which can accomplish these tasks, given BATSE/BACODINE coordinates and conservative assumptions about visible counterparts; these systems would be a ground-based upgrade to GRO which would accomplish the chief objectives of HETE.",Oafthvidewhneinobre,81.0,26.0,4.0
5582,Object Recognition,27.0,learning image components for object recognition.,5.0,201.0,1.0,201.0,1.0,2.2,148.8,71,https://www.jmlr.org/papers/volume7/spratling06a/spratling06a.pdf,"Compressive Learning is an emerging topic that combines signal acquisition via compressive sensing and machine learning to perform inference tasks directly on a small number of measurements. Many data modalities naturally have a multi-dimensional or tensorial format, with each dimension or tensor mode representing different features such as the spatial and temporal information in video sequences or the spatial and spectral information in hyperspectral images. However, in existing compressive learning frameworks, the compressive sensing component utilizes either random or learned linear projection on the vectorized signal to perform signal acquisition, thus discarding the multi-dimensional structure of the signals. In this paper, we propose Multilinear Compressive Learning, a framework that takes into account the tensorial nature of multi-dimensional signals in the acquisition step and builds the subsequent inference model on the structurally sensed measurements. Our theoretical complexity analysis shows that the proposed framework is more efficient compared to its vector-based counterpart in both memory and computation requirement. With extensive experiments, we also empirically show that our Multilinear Compressive Learning framework outperforms the vector-based framework in object classification and face recognition tasks, and scales favorably when the dimensionalities of the original signals increase, making it highly efficient for high-dimensional multi-dimensional signals.",Oleimcofoobre,87.0,49.0,10.0
5583,Object Recognition,28.0,on the usefulness of attention for object recognition,5.0,201.0,1.0,201.0,1.0,2.2,149.10000000000002,72,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.89.7783&rep=rep1&type=pdf,"Object detection and counting are related but challenging problems, especially for drone based scenes with small objects and cluttered background. In this paper, we propose a new Guided Attention Network (GANet) to deal with both object detection and counting tasks based on the feature pyramid. Different from the previous methods relying on unsupervised attention modules, we fuse different scales of feature maps by using the proposed weakly-supervised Background Attention (BA) between the background and objects for more semantic feature representation. Then, the Foreground Attention (FA) module is developed to consider both global and local appearance of the object to facilitate accurate localization. Moreover, the new data argumentation strategy is designed to train a robust model in various complex scenes. Extensive experiments on three challenging benchmarks (i.e., UAVDT, CARPK and PUCPR+) show the state-of-the-art detection and counting performance of the proposed method compared with existing methods.",Oonthusofatfoobre,93.0,39.0,3.0
5584,Object Recognition,31.0,an introduction to object recognition: selected algorithms for a wide variety of applications,5.0,201.0,1.0,201.0,1.0,2.2,150.0,73,http://arxiv.org/pdf/1711.09455v2,"The proximal point algorithm is a widely used tool for solving a variety of convex optimization problems such as finding zeros of maximally monotone operators, fixed points of nonexpansive mappings, as well as minimizing convex functions. The algorithm works by applying successively so-called ""resolvent"" mappings associated to the original object that one aims to optimize. In this paper we abstract from the corresponding resolvents employed in these problems the natural notion of jointly firmly nonexpansive families of mappings. This leads to a streamlined method of proving weak convergence of this class of algorithms in the context of complete CAT(0) spaces (and hence also in Hilbert spaces). In addition, we consider the notion of uniform firm nonexpansivity in order to similarly provide a unified presentation of a case where the algorithm converges strongly. Methods which stem from proof mining, an applied subfield of logic, yield in this situation computable and low-complexity rates of convergence.",Oanintoobresealfoawivaofap,36.0,0.0,4.0
5585,Object Recognition,33.0,coordinate transformations in object recognition.,5.0,201.0,1.0,201.0,1.0,2.2,150.60000000000002,74,https://www.academia.edu/download/41448269/psych-bull-2006-920_2284_5B0_5D.pdf,"A collaborative object represents a data type (such as a text document) designed to be shared by a group of dispersed users. The Operational Transformation (OT) is a coordination approach used for supporting optimistic replication for these objects. It allows the users to concurrently update the shared data and exchange their updates in any order since the convergence of all replicas, i.e. the fact that all users view the same data, is ensured in all cases. However, designing algorithms for achieving convergence with the OT approach is a critical and challenging issue. In this paper, we propose a formal compositional method for specifying complex collaborative objects. The most important feature of our method is that designing an OT algorithm for the composed collaborative object can be done by reusing the OT algorithms of component collaborative objects. By using our method, we can start from correct small collaborative objects which are relatively easy to handle and incrementally combine them to build more complex collaborative objects.",Ocotrinobre,82.0,322.0,5.0
5586,Object Recognition,35.0,neural evidence for intermediate representations in object recognition,5.0,201.0,1.0,201.0,1.0,2.2,151.2,75,http://arxiv.org/pdf/2104.03110v2,"We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated objects learned from images. While recent advances in 3D implicit representation have made it possible to learn models of complex objects, learning pose-controllable representations of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representation of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can generalize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF",Oneevfoinreinobre,105.0,17.0,3.0
5587,Object Recognition,38.0,an automatic algorithm for object recognition and detection based on asift keypoints,5.0,201.0,1.0,201.0,1.0,2.2,152.10000000000002,76,https://arxiv.org/pdf/1211.5829,"Object recognition is an important task in image processing and computer vision. This paper presents a perfect method for object recognition with full boundary detection by combining affine scale invariant feature transform (ASIFT) and a region merging algorithm. ASIFT is a fully affine invariant algorithm that means features are invariant to six affine parameters namely translation (2 parameters), zoom, rotation and two camera axis orientations. The features are very reliable and give us strong keypoints that can be used for matching between different images of an object. We trained an object in several images with different aspects for finding best keypoints of it. Then, a robust region merging algorithm is used to recognize and detect the object with full boundary in the other images based on ASIFT keypoints and a similarity measure for merging regions in the image. Experimental results show that the presented method is very efficient and powerful to recognize the object and detect it with high accuracy.",Oanaualfoobreandebaonaske,40.0,15.0,2.0
5588,Object Recognition,119.0,a distributed cortical representation underlies crossmodal object recognition in rats,3.0,201.0,1.0,129.0,3.0,2.2,154.79999999999998,77,https://www.jneurosci.org/content/jneuro/30/18/6253.full.pdf,"The mechanisms by which the brain integrates the unimodal sensory features of an object into a comprehensive multimodal object representation are poorly understood. We have recently developed a procedure for assessing crossmodal object recognition (CMOR) and object feature binding in rats using a modification of the spontaneous object recognition (SOR) paradigm. Here we show for the first time that rats are capable of spontaneous crossmodal object recognition when they are asked to recognize a visually presented object having previously only explored the tactile features of that object. Moreover, rats with bilateral perirhinal cortex (PRh) lesions were impaired on the CMOR task and a visual-only, but not a tactile-only, version of SOR. Conversely, rats with bilateral posterior parietal cortex (PPC) lesions were impaired on the CMOR and tactile-only tasks but not the visual-only task. Finally, crossmodal object recognition ability was severely and selectively impaired in rats with unilateral lesions made to PRh and PPC in opposite hemispheres. Thus, spontaneous tactile-to-visual crossmodal object recognition in rats relies on an object representation that requires functional interaction between PRh and PPC, which appear to mediate the visual and tactile information-processing demands of the task, respectively. These results imply that, at least under certain conditions, the separate sensory features of an object are represented in a distributed manner in the cortex. The novel paradigm introduced here should be a valuable tool for further study of the neurobiological bases of crossmodal cognition and object feature binding.",Oadicoreuncrobreinra,105.0,51.0,11.0
5589,Object Recognition,137.0,scale-hierarchical 3d object recognition in cluttered scenes,3.0,201.0,1.0,131.0,3.0,2.2,160.8,78,https://www.researchgate.net/profile/Ko-Nishino/publication/224164222_Scale-Hierarchical_3D_Object_Recognition_in_Cluttered_Scenes/links/5591f49e08ae15962d8e3bdb/Scale-Hierarchical-3D-Object-Recognition-in-Cluttered-Scenes.pdf,"3D object recognition in scenes with occlusion and clutter is a difficult task. In this paper, we introduce a method that exploits the geometric scale-variability to aid in this task. Our key insight is to leverage the rich discriminative information provided by the scale variation of local geometric structures to constrain the massive search space of potential correspondences between model and scene points. In particular, we exploit the geometric scale variability in the form of the intrinsic geometric scale of each computed feature, the hierarchy induced within the set of these intrinsic geometric scales, and the discriminative power of the local scale-dependent/invariant 3D shape descriptors. The method exploits the added information in a hierarchical coarse-to-fine manner that lets it cull the space of all potential correspondences effectively. We experimentally evaluate the accuracy of our method on an extensive set of real scenes with varying amounts of partial occlusion and achieve recognition rates higher than the state-of-the-art. Furthermore, for the first time we systematically demonstrate the method's ability to accurately localize objects despite changes in their global scales.",Osc3dobreinclsc,69.0,14.0,8.0
5590,Object Recognition,401.0,overcoming classifier imbalance for long-tail object detection with balanced group softmax,1.0,41.0,4.0,201.0,1.0,2.2,197.0,79,http://arxiv.org/pdf/2006.10408v1,"Solving long-tail large vocabulary object detection with deep learning based models is a challenging and demanding task, which is however under-explored.In this work, we provide the first systematic analysis on the underperformance of state-of-the-art models in front of long-tail distribution. We find existing detection methods are unable to model few-shot classes when the dataset is extremely skewed, which can result in classifier imbalance in terms of parameter magnitude. Directly adapting long-tail classification models to detection frameworks can not solve this problem due to the intrinsic difference between detection and classification.In this work, we propose a novel balanced group softmax (BAGS) module for balancing the classifiers within the detection frameworks through group-wise training. It implicitly modulates the training process for the head and tail classes and ensures they are both sufficiently trained, without requiring any extra sampling for the instances from the tail classes.Extensive experiments on the very recent long-tail large vocabulary object recognition benchmark LVIS show that our proposed BAGS significantly improves the performance of detectors with various backbones and frameworks on both object detection and instance segmentation. It beats all state-of-the-art methods transferred from long-tail image classification and establishes new state-of-the-art.Code is available at https://github.com/FishYuLi/BalancedGroupSoftmax.",Oovclimfoloobdewibagrso,62.0,45.0,10.0
5591,Object Recognition,401.0,hypergraph neural networks,1.0,43.0,4.0,201.0,1.0,2.2,197.8,80,http://arxiv.org/pdf/1901.08150v2,"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",Ohynene,188.0,46.0,27.0
5592,Object Recognition,401.0,compact generalized non-local network,1.0,44.0,4.0,201.0,1.0,2.2,198.2,81,http://arxiv.org/pdf/2104.00269v1,"Neural networks are popular and useful in many fields, but they have the problem of giving high confidence responses for examples that are away from the training data. This makes the neural networks very confident in their prediction while making gross mistakes, thus limiting their reliability for safety-critical applications such as autonomous driving, space exploration, etc. In this paper, we present a neuron generalization that has the standard dot-product-based neuron and the RBF neuron as two extreme cases of a shape parameter. Using ReLU as the activation function we obtain a novel neuron that has compact support, which means its output is zero outside a bounded domain. We show how to avoid difficulties in training a neural network with such neurons, by starting with a trained standard neural network and gradually increasing the shape parameter to the desired value. Through experiments on standard benchmark datasets, we show the promise of the proposed approach, in that it can have good prediction accuracy on in-distribution samples while being able to consistently detect and have low confidence on out-of-distribution samples.",Ocogenone,88.0,35.0,21.0
5593,Object Recognition,401.0,improving neural networks by preventing co-adaptation of feature detectors,1.0,45.0,4.0,201.0,1.0,2.2,198.6,82,http://arxiv.org/pdf/1207.0580v1,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",Oimnenebyprcooffede,5816.0,30.0,466.0
5594,Object Recognition,401.0,theano-based large-scale visual recognition with multiple gpus,1.0,46.0,4.0,201.0,1.0,2.2,199.0,83,http://arxiv.org/pdf/1412.2302v4,"In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.",Othlavirewimugp,42.0,16.0,5.0
5595,Object Recognition,401.0,multiple instance detection network with online instance classifier refinement,1.0,48.0,4.0,201.0,1.0,2.2,199.8,84,http://arxiv.org/pdf/1704.00138v1,"Of late, weakly supervised object detection is with great importance in object recognition. Based on deep learning, weakly supervised detectors have achieved many promising results. However, compared with fully supervised detection, it is more challenging to train deep network based detectors in a weakly supervised manner. Here we formulate weakly supervised detection as a Multiple Instance Learning (MIL) problem, where instance classifiers (object detectors) are put into the network as hidden nodes. We propose a novel online instance classifier refinement algorithm to integrate MIL and the instance classifier refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information. More precisely, instance labels inferred from weak supervision are propagated to their spatially overlapped instances to refine instance classifier online. The iterative instance classifier refinement procedure is implemented using multiple streams in deep network, where each stream supervises its latter stream. Weakly supervised object detection experiments are carried out on the challenging PASCAL VOC 2007 and 2012 benchmarks. We obtain 47% mAP on VOC 2007 that significantly outperforms the previous state-of-the-art.",Omuindenewioninclre,230.0,37.0,77.0
5596,Object Recognition,401.0,domain generalization by solving jigsaw puzzles,1.0,49.0,4.0,201.0,1.0,2.2,200.2,85,http://arxiv.org/pdf/1903.06864v2,"Human adaptability relies crucially on the ability to learn and merge knowledge both from supervised and unsupervised learning: the parents point out few important concepts, but then the children fill in the gaps on their own. This is particularly effective, because supervised learning can never be exhaustive and thus learning autonomously allows to discover invariances and regularities that help to generalize. In this paper we propose to apply a similar approach to the task of object recognition across domains: our model learns the semantic labels in a supervised fashion, and broadens its understanding of the data by learning from self-supervised signals how to solve a jigsaw puzzle on the same images. This secondary task helps the network to learn the concepts of spatial correlation while acting as a regularizer for the classification task. Multiple experiments on the PACS, VLCS, Office-Home and digits datasets confirm our intuition and show that this simple method outperforms previous domain generalization and adaptation solutions. An ablation study further illustrates the inner workings of our approach.",Odogebysojipu,276.0,54.0,62.0
5597,Object Recognition,401.0,self-supervised pretraining of 3d features on any point-cloud,1.0,51.0,4.0,201.0,1.0,2.2,201.0,86,http://arxiv.org/pdf/2104.08027v2,"Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent years. However, previous work has indicated that off-the-shelf MLMs are not effective as universal lexical or sentence encoders without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective universal lexical and sentence encoders even without any additional data and without any supervision. We propose an extremely simple, fast and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds without any additional external knowledge. Mirror-BERT relies on fully identical or slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during identity fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in both lexical-level and sentence-level tasks, across different domains and different languages. Notably, in the standard sentence semantic similarity (STS) tasks, our self-supervised Mirror-BERT model even matches the performance of the task-tuned Sentence-BERT models from prior work. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple approach can yield effective universal lexical and sentence encoders.",Oseprof3dfeonanpo,12.0,115.0,1.0
5598,Object Recognition,401.0,chessboard and chess piece recognition with the support of neural networks,1.0,52.0,4.0,201.0,1.0,2.2,201.4,87,http://arxiv.org/pdf/1708.03898v3,"Chessboard and chess piece recognition is a computer vision problem that has not yet been efficiently solved. However, its solution is crucial for many experienced players who wish to compete against AI bots, but also prefer to make decisions based on the analysis of a physical chessboard. It is also important for organizers of chess tournaments who wish to digitize play for online broadcasting or ordinary players who wish to share their gameplay with friends. Typically, such digitization tasks are performed by humans or with the aid of specialized chessboards and pieces. However, neither solution is easy or convenient. To solve this problem, we propose a novel algorithm for digitizing chessboard configurations.   We designed a method that is resistant to lighting conditions and the angle at which images are captured, and works correctly with numerous chessboard styles. The proposed algorithm processes pictures iteratively. During each iteration, it executes three major sub-processes: detecting straight lines, finding lattice points, and positioning the chessboard. Finally, we identify all chess pieces and generate a description of the board utilizing standard notation. For each of these steps, we designed our own algorithm that surpasses existing solutions. We support our algorithms by utilizing machine learning techniques whenever possible.   The described method performs extraordinarily well and achieves an accuracy over $99.5\%$ for detecting chessboard lattice points (compared to the $74\%$ for the best alternative), $95\%$ (compared to $60\%$ for the best alternative) for positioning the chessboard in an image, and almost $95\%$ for chess piece recognition.",Ochanchpirewithsuofnene,4.0,64.0,1.0
5599,Object Recognition,401.0,object recognition from local scale-invariant features,1.0,201.0,1.0,3.0,5.0,2.2,201.6,88,http://arxiv.org/pdf/1511.04164v3,"An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds.",Oobrefrloscfe,15860.0,25.0,1933.0
5600,Object Recognition,401.0,two-stream convolutional networks for dynamic texture synthesis,1.0,54.0,4.0,201.0,1.0,2.2,202.2,89,http://arxiv.org/pdf/1706.06982v4,"We introduce a two-stream model for dynamic texture synthesis. Our model is based on pre-trained convolutional networks (ConvNets) that target two independent tasks: (i) object recognition, and (ii) optical flow prediction. Given an input dynamic texture, statistics of filter responses from the object recognition ConvNet encapsulate the per-frame appearance of the input texture, while statistics of filter responses from the optical flow ConvNet model its dynamics. To generate a novel texture, a randomly initialized input sequence is optimized to match the feature statistics from each stream of an example texture. Inspired by recent work on image style transfer and enabled by the two-stream model, we also apply the synthesis approach to combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures. We show that our approach generates novel, high quality samples that match both the framewise appearance and temporal evolution of input texture. Finally, we quantitatively evaluate our texture synthesis approach with a thorough user study.",Otwconefodytesy,34.0,71.0,11.0
5602,Object Recognition,401.0,hierarchical models of object recognition in cortex,1.0,201.0,1.0,7.0,5.0,2.2,202.8,90,http://arxiv.org/pdf/1706.09262v2,"Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.",Ohimoofobreinco,3297.0,51.0,289.0
5603,Object Recognition,401.0,multi-branch and multi-scale attention learning for fine-grained visual categorization,1.0,56.0,4.0,201.0,1.0,2.2,203.0,91,http://arxiv.org/abs/1709.00340v4,"Fine-grained visual categorization is to recognize hundreds of subcategories belonging to the same basic-level category, which is a highly challenging task due to the quite subtle and local visual distinctions among similar subcategories. Most existing methods generally learn part detectors to discover discriminative regions for better categorization performance. However, not all parts are beneficial and indispensable for visual categorization, and the setting of part detector number heavily relies on prior knowledge as well as experimental validation. As is known to all, when we describe the object of an image via textual descriptions, we mainly focus on the pivotal characteristics, and rarely pay attention to common characteristics as well as the background areas. This is an involuntary transfer from human visual attention to textual attention, which leads to the fact that textual attention tells us how many and which parts are discriminative and significant to categorization. So textual attention could help us to discover visual attention in image. Inspired by this, we propose a fine-grained visual-textual representation learning (VTRL) approach, and its main contributions are: (1) Fine-grained visual-textual pattern mining devotes to discovering discriminative visual-textual pairwise information for boosting categorization performance through jointly modeling vision and text with generative adversarial networks (GANs), which automatically and adaptively discovers discriminative parts. (2) Visual-textual representation learning jointly combines visual and textual information, which preserves the intra-modality and inter-modality information to generate complementary fine-grained representation, as well as further improves categorization performance.",Omuanmuatlefofivica,18.0,29.0,1.0
5604,Object Recognition,401.0,equalization loss for long-tailed object recognition,1.0,57.0,4.0,201.0,1.0,2.2,203.4,92,http://arxiv.org/pdf/2003.05176v2,"Object recognition techniques using convolutional neural networks (CNN) have achieved great success. However, state-of-the-art object detection methods still perform poorly on large vocabulary and long-tailed datasets, e.g. LVIS. In this work, we analyze this problem from a novel perspective: each positive sample of one category can be seen as a negative sample for other categories, making the tail categories receive more discouraging gradients. Based on it, we propose a simple but effective loss, named equalization loss, to tackle the problem of long-tailed rare categories by simply ignoring those gradients for rare categories. The equalization loss protects the learning of rare categories from being at a disadvantage during the network parameter updating. Thus the model is capable of learning better discriminative features for objects of rare classes. Without any bells and whistles, our method achieves AP gains of 4.1% and 4.8% for the rare and common categories on the challenging LVIS benchmark, compared to the Mask R-CNN baseline. With the utilization of the effective equalization loss, we finally won the 1st place in the LVIS Challenge 2019. Code has been made available at: https: //github.com/tztztztztz/eql.detectron2",Oeqlofoloobre,71.0,47.0,20.0
5605,Object Recognition,401.0,comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence,1.0,201.0,1.0,10.0,5.0,2.2,203.7,93,http://arxiv.org/pdf/1601.02970v1,"The complex multi-stage architecture of cortical visual pathways provides the neural basis for efficient visual object recognition in humans. However, the stage-wise computations therein remain poorly understood. Here, we compared temporal (magnetoencephalography) and spatial (functional MRI) visual brain representations with representations in an artificial deep neural network (DNN) tuned to the statistics of real-world visual recognition. We showed that the DNN captured the stages of human visual processing in both time and space from early visual areas towards the dorsal and ventral streams. Further investigation of crucial DNN parameters revealed that while model architecture was important, training on real-world categorization was necessary to enforce spatio-temporal hierarchical relationships with the brain. Together our results provide an algorithmically informed view on the spatio-temporal dynamics of visual object recognition in the human visual brain.",Ocoofdenenetospcodyofhuviobrerehico,410.0,61.0,25.0
5606,Object Recognition,401.0,learning compact binary descriptors with unsupervised deep neural networks,1.0,58.0,4.0,201.0,1.0,2.2,203.8,94,http://arxiv.org/pdf/1304.7948v2,A standard deep convolutional neural network paired with a suitable loss function learns compact local image descriptors that perform comparably to state-of-the art approaches.,Olecobidewiundenene,245.0,55.0,45.0
5607,Object Recognition,401.0,video to events: recycling video datasets for event cameras,1.0,59.0,4.0,201.0,1.0,2.2,204.2,95,http://arxiv.org/pdf/1912.03095v2,"Event cameras are novel sensors that output brightness changes in the form of a stream of asynchronous ""events"" instead of intensity frames. They offer significant advantages with respect to conventional cameras: high dynamic range (HDR), high temporal resolution, and no motion blur. Recently, novel learning approaches operating on event data have achieved impressive results. Yet, these methods require a large amount of event data for training, which is hardly available due the novelty of event sensors in computer vision research. In this paper, we present a method that addresses these needs by converting any existing video dataset recorded with conventional cameras to synthetic event data. This unlocks the use of a virtually unlimited number of existing video datasets for training networks designed for real event data. We evaluate our method on two relevant vision tasks, i.e., object recognition and semantic segmentation, and show that models trained on synthetic events have several benefits: (i) they generalize well to real event data, even in scenarios where standard-camera images are blurry or overexposed, by inheriting the outstanding properties of event cameras; (ii) they can be used for fine-tuning on real data to improve over state-of-the-art for both classification and semantic segmentation.",Ovitoevrevidafoevca,33.0,46.0,7.0
5608,Object Recognition,401.0,orientation-boosted voxel nets for 3d object recognition,1.0,201.0,1.0,12.0,5.0,2.2,204.3,96,http://arxiv.org/pdf/2006.04043v1,"Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.",Oorvonefo3dobre,164.0,44.0,12.0
5609,Object Recognition,401.0,domain generalization in vision: a survey,1.0,60.0,4.0,201.0,1.0,2.2,204.6,97,http://arxiv.org/pdf/1812.03713v1,"In a valuation domain $(V,M)$ every nonzero finitely generated ideal $J$ is principal and so, in particular, $J=J^t$, hence the maximal ideal $M$ is a $t$-ideal. Therefore, the $t$-local domains (i.e., the local domains, with maximal ideal being a $t$-ideal) are ""cousins"" of valuation domains, but, as we will see in detail, not so close. Indeed, for instance, a localization of a $t$-local domain is not necessarily $t$-local, but of course a localization of a valuation domain is a valuation domain.   So it is natural to ask under what conditions is a $t$-local domain a valuation domain? The main purpose of the present paper is to address this question, surveying in part previous work by various authors containing useful properties for applying them to our goal.",Odogeinviasu,3.0,179.0,0.0
5610,Object Recognition,401.0,novel object recognition test for the investigation of learning and memory in mice.,1.0,201.0,1.0,14.0,5.0,2.2,204.9,98,http://arxiv.org/abs/1804.03803v2,"The object recognition test (ORT) is a commonly used behavioral assay for the investigation of various aspects of learning and memory in mice. The ORT is fairly simple and can be completed over 3 days: habituation day, training day, and testing day. During training, the mouse is allowed to explore 2 identical objects. On test day, one of the training objects is replaced with a novel object. Because mice have an innate preference for novelty, if the mouse recognizes the familiar object, it will spend most of its time at the novel object. Due to this innate preference, there is no need for positive or negative reinforcement or long training schedules. Additionally, the ORT can also be modified for numerous applications. The retention interval can be shortened to examine short-term memory, or lengthened to probe long-term memory. Pharmacological intervention can be used at various times prior to training, after training, or prior to recall to investigate different phases of learning (i.e., acquisition, early or late consolidation, or recall). Overall, the ORT is a relatively low-stress, efficient test for memory in mice, and is appropriate for the detection of neuropsychological changes following pharmacological, biological, or genetic manipulations.",Onoobretefothinofleanmeinmi,196.0,0.0,4.0
5611,Object Recognition,401.0,pointwise convolutional neural networks,1.0,61.0,4.0,201.0,1.0,2.2,205.0,99,http://arxiv.org/pdf/1712.05245v2,"Deep learning with 3D data such as reconstructed point clouds and CAD models has received great research interests recently. However, the capability of using point clouds with convolutional neural network has been so far not fully explored. In this paper, we present a convolutional neural network for semantic segmentation and object recognition with 3D point clouds. At the core of our network is pointwise convolution, a new convolution operator that can be applied at each point of a point cloud. Our fully convolutional network design, while being surprisingly simple to implement, can yield competitive accuracy in both semantic segmentation and object recognition task.",Opoconene,229.0,47.0,18.0
5612,Object Recognition,401.0,multimodal deep learning for robust rgb-d object recognition,1.0,201.0,1.0,15.0,5.0,2.2,205.2,100,http://arxiv.org/pdf/1507.06821v2,"Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset [15] and show recognition in challenging RGB-D real-world noisy settings.",Omudelefororgobre,500.0,29.0,45.0
5996,Object Tracking,7.0,fast online object tracking and segmentation: a unifying approach,5.0,10.0,5.0,8.0,5.0,5.0,8.5,1,http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf,"In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 55 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017.",Ofaonobtranseaunap,466.0,76.0,83.0
5997,Object Tracking,9.0,mot16: a benchmark for multi-object tracking,5.0,9.0,5.0,21.0,5.0,5.0,12.6,2,https://arxiv.org/pdf/1603.00831,"Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for reseach. 
Recently, a new benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal of collecting existing and new data and creating a framework for the standardized evaluation of multiple object tracking methods. The first release of the benchmark focuses on multiple people tracking, since pedestrians are by far the most studied object in the tracking community. This paper accompanies a new release of the MOTChallenge benchmark. Unlike the initial release, all videos of MOT16 have been carefully annotated following a consistent protocol. Moreover, it not only offers a significant increase in the number of labeled boxes, but also provides multiple object classes beside pedestrians and the level of visibility for every single object of interest.",Omoabefomutr,793.0,60.0,217.0
5998,Object Tracking,27.0,fully-convolutional siamese networks for object tracking,5.0,39.0,5.0,1.0,5.0,5.0,24.000000000000004,3,http://arxiv.org/pdf/2105.03049v1,"The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object’s appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.",Ofusinefoobtr,2008.0,52.0,447.0
5999,Object Tracking,71.0,distractor-aware siamese networks for visual object tracking,4.0,19.0,5.0,19.0,5.0,4.7,34.6,4,https://openaccess.thecvf.com/content_ECCV_2018/papers/Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper.pdf,"Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks.",Odisinefoviobtr,509.0,40.0,80.0
6000,Object Tracking,36.0,learning a neural solver for multiple object tracking,5.0,43.0,4.0,10.0,5.0,4.6,31.0,5,http://openaccess.thecvf.com/content_CVPR_2020/papers/Braso_Learning_a_Neural_Solver_for_Multiple_Object_Tracking_CVPR_2020_paper.pdf,"Graphs offer a natural way to formulate Multiple Object Tracking (MOT) within the tracking-by-detection paradigm. However, they also introduce a major challenge for learning methods, as defining a model that can operate on such structured domain is not trivial. As a consequence, most learning-based work has been devoted to learning better features for MOT and then using these with well-established optimization frameworks. In this work, we exploit the classical network flow formulation of MOT to define a fully differentiable framework based on Message Passing Networks (MPNs). By operating directly on the graph domain, our method can reason globally over an entire set of detections and predict final solutions. Hence, we show that learning in MOT does not need to be restricted to feature extraction, but it can also be applied to the data association step. We show a significant improvement in both MOTA and IDF1 on three publicly available benchmarks. Our code is available at https://bit.ly/motsolv.",Oleanesofomuobtr,89.0,83.0,20.0
6001,Object Tracking,44.0,deep affinity network for multiple object tracking,4.0,35.0,5.0,86.0,4.0,4.4,53.0,6,https://arxiv.org/pdf/1810.11780,"Multiple Object Tracking (MOT) plays an important role in solving many fundamental problems in video analysis and computer vision. Most MOT methods employ two steps: Object Detection and Data Association. The first step detects objects of interest in every frame of a video, and the second establishes correspondence between the detected objects in different frames to obtain their tracks. Object detection has made tremendous progress in the last few years due to deep learning. However, data association for tracking still relies on hand crafted constraints such as appearance, motion, spatial proximity, grouping etc. to compute affinities between the objects in different frames. In this paper, we harness the power of deep learning for data association in tracking by jointly modeling object appearances and their affinities between different frames in an end-to-end fashion. The proposed Deep Affinity Network (DAN) learns compact, yet comprehensive features of pre-detected objects at several levels of abstraction, and performs exhaustive pairing permutations of those features in any two frames to infer object affinities. DAN also accounts for multiple objects appearing and disappearing between video frames. We exploit the resulting efficient affinity computations to associate objects in the current frame deep into the previous frames for reliable on-line tracking. Our technique is evaluated on popular multiple object tracking challenges MOT15, MOT17 and UA-DETRAC. Comprehensive benchmarking under twelve evaluation metrics demonstrates that our approach is among the best performing techniques on the leader board for these challenges. The open source implementation of our work is available at https://github.com/shijieS/SST.git.",Odeafnefomuobtr,116.0,117.0,9.0
6002,Object Tracking,34.0,learning dynamic memory networks for object tracking,5.0,98.0,4.0,54.0,4.0,4.3,65.60000000000001,7,https://openaccess.thecvf.com/content_ECCV_2018/papers/Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper.pdf,"Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block. As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, unlike other tracking methods where the model capacity is fixed after offline training --- the capacity of our tracker can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.",Oledymenefoobtr,130.0,38.0,25.0
6003,Object Tracking,15.0,a twofold siamese network for real-time object tracking,5.0,150.0,3.0,28.0,5.0,4.2,72.9,8,https://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf,"Observing that Semantic features learned in an image classification task and Appearance features learned in a similarity matching task complement each other, we build a twofold Siamese network, named SA-Siam, for real-time object tracking. SA-Siam is composed of a semantic branch and an appearance branch. Each branch is a similaritylearning Siamese network. An important design choice in SA-Siam is to separately train the two branches to keep the heterogeneity of the two types of features. In addition, we propose a channel attention mechanism for the semantic branch. Channel-wise weights are computed according to the channel activations around the target position. While the inherited architecture from SiamFC [3] allows our tracker to operate beyond real-time, the twofold design and the attention mechanism significantly improve the tracking performance. The proposed SA-Siam outperforms all other real-time trackers by a large margin on OTB-2013/50/100 benchmarks.",Oatwsineforeobtr,303.0,42.0,29.0
6004,Object Tracking,40.0,know your surroundings: exploiting scene information for object tracking,5.0,13.0,5.0,201.0,1.0,3.8,77.5,9,https://arxiv.org/pdf/2003.11014,"Current state-of-the-art trackers only rely on a target appearance model in order to localize the object in each frame. Such approaches are however prone to fail in case of e.g. fast appearance changes or presence of distractor objects, where a target appearance model alone is insufficient for robust tracking. Having the knowledge about the presence and locations of other objects in the surrounding scene can be highly beneficial in such cases. This scene information can be propagated through the sequence and used to, for instance, explicitly avoid distractor objects and eliminate target candidate regions.   In this work, we propose a novel tracking architecture which can utilize scene information for tracking. Our tracker represents such information as dense localized state vectors, which can encode, for example, if the local region is target, background, or distractor. These state vectors are propagated through the sequence and combined with the appearance model output to localize the target. Our network is learned to effectively utilize the scene information by directly maximizing tracking performance on video segments. The proposed approach sets a new state-of-the-art on 3 tracking benchmarks, achieving an AO score of 63.6% on the recent GOT-10k dataset.",Oknyosuexscinfoobtr,42.0,58.0,12.0
6005,Object Tracking,401.0,towards real-time multi-object tracking,1.0,7.0,5.0,7.0,5.0,3.8,125.2,10,http://arxiv.org/pdf/1707.07410v1,"Modern multiple object tracking (MOT) systems usually follow the tracking-by-detection paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. As such, the system is formulated as a multi-task learning problem: there are multiple objectives, i.e., anchor classification, bounding box regression, and embedding learning; and the individual losses are automatically weighted. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 18.8 to 24.1 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning (64.4% MOTA v.s. 66.1% MOTA on MOT-16 challenge). The code and models are available at this https URL.",Otoremutr,115.0,67.0,22.0
6006,Object Tracking,401.0,spatially supervised recurrent convolutional neural networks for visual object tracking,1.0,22.0,5.0,65.0,4.0,3.5,148.6,11,http://arxiv.org/pdf/1607.05781v1,"In this paper, we develop a new approach of spatially supervised recurrent convolutional neural networks for visual object tracking. Our recurrent convolutional network exploits the history of locations as well as the distinctive visual features learned by the deep neural networks. Inspired by recent bounding box regression methods for object detection, we study the regression capability of Long Short-Term Memory (LSTM) in the temporal domain, and propose to concatenate high-level visual features produced by convolutional networks with region information. In contrast to existing deep learning based trackers that use binary classification for region candidates, we use regression for direct prediction of the tracking locations both at the convolutional layer and at the recurrent unit. Our experimental results on challenging benchmark video tracking datasets show that our tracker is competitive with state-of-the-art approaches while maintaining low computational cost.",Ospsureconenefoviobtr,188.0,29.0,19.0
6007,Object Tracking,401.0,robust multi-modality multi-object tracking,1.0,50.0,4.0,31.0,5.0,3.4000000000000004,149.60000000000002,12,http://arxiv.org/abs/1711.09492v4,"Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and could further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT.",Oromumutr,66.0,62.0,11.0
6008,Object Tracking,3.0,online object tracking: a benchmark,5.0,201.0,1.0,2.0,5.0,3.4,81.9,13,https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Wu_Online_Object_Tracking_2013_CVPR_paper.pdf,"Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.",Oonobtrabe,3049.0,72.0,751.0
6009,Object Tracking,1.0,object tracking: a survey,5.0,201.0,1.0,23.0,5.0,3.4,87.60000000000001,14,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.8588&rep=rep1&type=pdf,"The goal of this article is to review the state-of-the-art tracking methods, classify them into different categories, and identify new trends. Object tracking, in general, is a challenging problem. Difficulties in tracking objects can arise due to abrupt object motion, changing appearance patterns of both the object and the scene, nonrigid object structures, object-to-object and object-to-scene occlusions, and camera motion. Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Typically, assumptions are made to constrain the tracking problem in the context of a particular application. In this survey, we categorize the tracking methods on the basis of the object and motion representations used, provide detailed descriptions of representative methods in each category, and examine their pros and cons. Moreover, we discuss the important issues related to tracking including the use of appropriate image features, selection of motion models, and detection of objects.",Oobtrasu,5222.0,193.0,219.0
6010,Object Tracking,4.0,kernel-based object tracking,5.0,201.0,1.0,22.0,5.0,3.4,88.2,15,http://www.techfak.uni-bielefeld.de/techfak/persons/iluetkeb/2006/surveillance/paper/tracking/kerneltracking.pdf,"A new approach toward target representation and localization, the central component in visual tracking of nonrigid objects, is proposed. The feature histogram-based target representations are regularized by spatial masking with an isotropic kernel. The masking induces spatially-smooth similarity functions suitable for gradient-based optimization, hence, the target localization problem can be formulated using the basin of attraction of the local maxima. We employ a metric derived from the Bhattacharyya coefficient as similarity measure, and use the mean shift procedure to perform the optimization. In the presented tracking examples, the new method successfully coped with camera motion, partial occlusions, clutter, and target scale variations. Integration with motion filters and data association techniques is also discussed. We describe only a few of the potential applications: exploitation of background information, Kalman tracking using motion models, and face tracking.",Okeobtr,4952.0,177.0,449.0
6011,Object Tracking,8.0,the sixth visual object tracking vot2018 challenge results,5.0,201.0,1.0,18.0,5.0,3.4,88.20000000000002,16,http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.pdf,"The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a “real-time” experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new long-term tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).",Othsiviobtrvochre,365.0,110.0,84.0
6012,Object Tracking,6.0,the visual object tracking vot2017 challenge results,5.0,201.0,1.0,24.0,5.0,3.4,89.4,17,http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Kristan_The_Visual_Object_ICCV_2017_paper.pdf,"The Visual Object Tracking challenge VOT2017 is the fifth annual tracker benchmarking activity organized by the VOT initiative. Results of 51 trackers are presented; many are state-of-the-art published at major computer vision conferences or journals in recent years. The evaluation included the standard VOT and other popular methodologies and a new ""real-time"" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The VOT2017 goes beyond its predecessors by (i) improving the VOT public dataset and introducing a separate VOT2017 sequestered dataset, (ii) introducing a realtime tracking experiment and (iii) releasing a redesigned toolkit that supports complex experiments. The dataset, the evaluation kit and the results are publicly available at the challenge website1.",Othviobtrvochre,336.0,137.0,62.0
6013,Object Tracking,25.0,a simple baseline for multi-object tracking,5.0,201.0,1.0,12.0,5.0,3.4,91.5,18,https://www.chunyuwang.org/img/oneshottracker.pdf,"There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problem. It remarkably outperforms the state-of-the-arts on the public datasets at $30$ fps. We hope this baseline could inspire and help evaluate new ideas in this field. The code and the pre-trained models are available at \url{this https URL}.",Oasibafomutr,52.0,65.0,13.0
6014,Object Tracking,33.0,overview and methods of correlation filter algorithms in object tracking,5.0,201.0,1.0,16.0,5.0,3.4,95.1,19,http://arxiv.org/pdf/1604.00970v3,"An important area of computer vision is real-time object tracking, which is now widely used in intelligent transportation and smart industry technologies. Although the correlation filter object tracking methods have a good real-time tracking effect, it still faces many challenges such as scale variation, occlusion, and boundary effects. Many scholars have continuously improved existing methods for better efficiency and tracking performance in some aspects. To provide a comprehensive understanding of the background, key technologies and algorithms of single object tracking, this article focuses on the correlation filter-based object tracking algorithms. Specifically, the background and current advancement of the object tracking methodologies, as well as the presentation of the main datasets are introduced. All kinds of methods are summarized to present tracking results in various vision problems, and a visual tracking method based on reliability is observed.",Oovanmeofcofialinobtr,59.0,37.0,0.0
6015,Object Tracking,26.0,robust object tracking with online multiple instance learning,5.0,201.0,1.0,34.0,5.0,3.4,98.4,20,https://www.researchgate.net/file.PostFileLoader.html?id=55d80f3f5cd9e323458b45ab&assetKey=AS:273982077505536@1442333825137,"In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. Recently, a class of tracking techniques called “tracking by detection” has been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking that achieves superior results with real-time performance. We present thorough experimental results (both qualitative and quantitative) on a number of challenging video clips.",Oroobtrwionmuinle,1961.0,53.0,263.0
6016,Object Tracking,38.0,learning dynamic siamese network for visual object tracking,5.0,201.0,1.0,26.0,5.0,3.4,99.6,21,http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf,"How to effectively learn temporal variation of target appearance, to exclude the interference of cluttered background, while maintaining real-time response, is an essential problem of visual object tracking. Recently, Siamese networks have shown great potentials of matching based trackers in achieving balanced accuracy and beyond realtime speed. However, they still have a big gap to classification & updating based trackers in tolerating the temporal changes of objects and imaging conditions. In this paper, we propose dynamic Siamese network, via a fast transformation learning model that enables effective online learning of target appearance variation and background suppression from previous frames. We then present elementwise multi-layer fusion to adaptively integrate the network outputs using multi-level deep features. Unlike state-of-theart trackers, our approach allows the usage of any feasible generally- or particularly-trained features, such as SiamFC and VGG. More importantly, the proposed dynamic Siamese network can be jointly trained as a whole directly on the labeled video sequences, thus can take full advantage of the rich spatial temporal information of moving objects. As a result, our approach achieves state-of-the-art performance on OTB-2013 and VOT-2015 benchmarks, while exhibits superiorly balanced accuracy and real-time response over state-of-the-art competitors.",Oledysinefoviobtr,409.0,40.0,40.0
6017,Object Tracking,192.0,robust estimation of similarity transformation for visual object tracking,3.0,37.0,5.0,201.0,1.0,3.2,132.7,22,https://ojs.aaai.org/index.php/AAAI/article/download/4888/4761,"Most of existing correlation filter-based tracking approaches only estimate simple axis-aligned bounding boxes, and very few of them is capable of recovering the underlying similarity transformation. To tackle this challenging problem, in this paper, we propose a new correlation filter-based tracker with a novel robust estimation of similarity transformation on the large displacements. In order to efficiently search in such a large 4-DoF space in real-time, we formulate the problem into two 2-DoF sub-problems and apply an efficient Block Coordinates Descent solver to optimize the estimation result. Specifically, we employ an efficient phase correlation scheme to deal with both scale and rotation changes simultaneously in log-polar coordinates. Moreover, a variant of correlation filter is used to predict the translational motion individually. Our experimental results demonstrate that the proposed tracker achieves very promising prediction performance compared with the state-of-the-art visual object tracking methods while still retaining the advantages of high efficiency and simplicity in conventional correlation filter-based tracking methods.",Oroesofsitrfoviobtr,47.0,53.0,4.0
6018,Object Tracking,401.0,tracking without bells and whistles,1.0,23.0,5.0,164.0,3.0,3.2,178.7,23,http://arxiv.org/pdf/1904.01828v1,"The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.",Otrwibeanwh,266.0,78.0,69.0
6019,Object Tracking,401.0,high performance visual tracking with siamese region proposal network,1.0,20.0,5.0,197.0,3.0,3.2,187.4,24,http://arxiv.org/pdf/1905.02857v1,"Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.",Ohipevitrwisireprne,835.0,41.0,204.0
6020,Object Tracking,401.0,how to train your deep multi-object tracker,1.0,29.0,5.0,186.0,3.0,3.2,187.7,25,http://arxiv.org/pdf/1906.06618v3,"The recent trend in vision-based multi-object tracking (MOT) is heading towards leveraging the representational power of deep learning to jointly learn to detect and track objects. However, existing methods train only certain sub-modules using loss functions that often do not correlate with established tracking evaluation measures such as Multi-Object Tracking Accuracy (MOTA) and Precision (MOTP). As these measures are not differentiable, the choice of appropriate loss functions for end-to-end training of multi-object tracking methods is still an open research problem. In this paper, we bridge this gap by proposing a differentiable proxy of MOTA and MOTP, which we combine in a loss function suitable for end-to-end training of deep multi-object trackers. As a key ingredient, we propose a Deep Hungarian Net (DHN) module that approximates the Hungarian matching algorithm. DHN allows estimating the correspondence between object tracks and ground truth objects to compute differentiable proxies of MOTA and MOTP, which are in turn used to optimize deep trackers directly. We experimentally demonstrate that the proposed differentiable framework improves the performance of existing multi-object trackers, and we establish a new state of the art on the MOTChallenge benchmark. Our code is publicly available from https://github.com/yihongXU/deepMOT.",Ohototryodemutr,63.0,85.0,10.0
6021,Object Tracking,5.0,the visual object tracking vot2015 challenge results,5.0,201.0,1.0,47.0,4.0,3.1,96.0,26,https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w14/papers/Kristan_The_Visual_Object_ICCV_2015_paper.pdf,"The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 predecessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website.",Othviobtrvochre,554.0,90.0,134.0
6022,Object Tracking,52.0,the visual object tracking algorithm research based on adaptive combination kernel,4.0,201.0,1.0,36.0,5.0,3.1,106.8,27,http://arxiv.org/pdf/2008.02834v3,"In order to enhance the robustness to complicated changes of multiple objects and complex background scene, the visual object tracking algorithm based on Adaptive Combination Kernel has been proposed in the paper. The object tracking procedure has been decomposed into two subtasks: Translation Filter and Scale Filter to estimate the object’s details. Firstly, the Translation Kernel Tracker has used the adaptive combination of Linear Kernel Filter and Gaussian Kernel Filter. The objective function has been developed to obtain the weight coefficients for Linear Kernel filter and the Gaussian Kernel filter, which incorporates not only empirical risk but also maximum value of response output for each kernel. The Adaptive Combination Kernel has the advantages of both local kernel and global kernel. Secondly, the tracking position has been calculated according to the response output of adaptive combination kernel correlation filter. Thirdly, according to the maximum response value, the scene-adaptive learning rate has been designed in the translation filter. The translation filter can be updated with the adaptive learning rate. Finally, one-dimensional scale filter has been used to estimate the object scale. The extensive experimental results have shown that the proposed algorithm is optimal on OTB-50 dataset in success rate and distance precision parameters, which is 6.8 percentage points and 4.1% points than those of KCF and is 2.0 percentage points and 3.2% points than those of BSET. The proposed algorithm has better robustness to the deformation and occlusion than others.",Othviobtralrebaonadcoke,90.0,46.0,0.0
6023,Object Tracking,10.0,multiple object tracking: a literature review,5.0,201.0,1.0,83.0,4.0,3.1,108.3,28,https://arxiv.org/pdf/1409.7618,"Multiple Object Tracking (MOT) is an important computer vision problem which has gained increasing attention due to its academic and commercial potential. Although different kinds of approaches have been proposed to tackle this problem, it still remains challenging due to factors like abrupt appearance changes and severe object occlusions. In this work, we contribute the first comprehensive and most recent review on this problem. We inspect the recent advances in various aspects and propose some interesting directions for future research. To the best of our knowledge, there has not been any extensive review on this topic in the community. We endeavor to provide a thorough review on the development of this problem in recent decades. The main contributions of this review are fourfold: 1) Key aspects in a multiple object tracking system, including formulation, categorization, key principles, evaluation of an MOT are discussed. 2) Instead of enumerating individual works, we discuss existing approaches according to various aspects, in each of which methods are divided into different groups and each group is discussed in detail for the principles, advances and drawbacks. 3) We examine experiments of existing publications and summarize results on popular datasets to provide quantitative comparisons. We also point to some interesting discoveries by analyzing these results. 4) We provide a discussion about issues of MOT research, as well as some interesting directions which could possibly become potential research effort in the future.",Omuobtralire,226.0,269.0,14.0
6024,Object Tracking,29.0,learning to track: online multi-object tracking by decision making,5.0,201.0,1.0,68.0,4.0,3.1,109.5,29,https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xiang_Learning_to_Track_ICCV_2015_paper.pdf,"Online Multi-Object Tracking (MOT) has wide applications in time-critical video analysis scenarios, such as robot navigation and autonomous driving. In tracking-by-detection, a major challenge of online MOT is how to robustly associate noisy object detections on a new video frame with previously tracked objects. In this work, we formulate the online MOT problem as decision making in Markov Decision Processes (MDPs), where the lifetime of an object is modeled with a MDP. Learning a similarity function for data association is equivalent to learning a policy for the MDP, and the policy learning is approached in a reinforcement learning fashion which benefits from both advantages of offline-learning and online-learning for data association. Moreover, our framework can naturally handle the birth/death and appearance/disappearance of targets by treating them as state transitions in the MDP while leveraging existing online single object tracking methods. We conduct experiments on the MOT Benchmark [24] to verify the effectiveness of our method.",Oletotronmutrbydema,504.0,48.0,62.0
6025,Object Tracking,77.0,rgb-t object tracking: benchmark and baseline,4.0,201.0,1.0,27.0,5.0,3.1,111.6,30,https://arxiv.org/pdf/1805.08982,"RGB-Thermal (RGB-T) object tracking receives more and more attention due to the strongly complementary benefits of thermal information to visible data. However, RGB-T research is limited by lacking a comprehensive evaluation platform. In this paper, we propose a large-scale video benchmark dataset for RGB-T this http URL has three major advantages over existing ones: 1) Its size is sufficiently large for large-scale performance evaluation (total frame number: 234K, maximum frame per sequence: 8K). 2) The alignment between RGB-T sequence pairs is highly accurate, which does not need pre- or post-processing. 3) The occlusion levels are annotated for occlusion-sensitive performance analysis of different tracking algorithms.Moreover, we propose a novel graph-based approach to learn a robust object representation for RGB-T tracking. In particular, the tracked object is represented with a graph with image patches as nodes. This graph including graph structure, node weights and edge weights is dynamically learned in a unified ADMM (alternating direction method of multipliers)-based optimization framework, in which the modality weights are also incorporated for adaptive fusion of multiple source data.Extensive experiments on the large-scale dataset are executed to demonstrate the effectiveness of the proposed tracker against other state-of-the-art tracking methods. We also provide new insights and potential research directions to the field of RGB-T object tracking.",Orgobtrbeanba,108.0,75.0,11.0
6026,Object Tracking,80.0,multi-object tracking with multiple cues and switcher-aware classification,4.0,201.0,1.0,30.0,5.0,3.1,113.4,31,https://arxiv.org/pdf/1901.06129,"In this paper, we propose a unified Multi-Object Tracking (MOT) framework learning to make full use of long term and short term cues for handling complex cases in MOT scenes. Besides, for better association, we propose switcher-aware classification (SAC), which takes the potential identity-switch causer (switcher) into consideration. Specifically, the proposed framework includes a Single Object Tracking (SOT) sub-net to capture short term cues, a re-identification (ReID) sub-net to extract long term cues and a switcher-aware classifier to make matching decisions using extracted features from the main target and the switcher. Short term cues help to find false negatives, while long term cues avoid critical mistakes when occlusion happens, and the SAC learns to combine multiple cues in an effective way and improves robustness. The method is evaluated on the challenging MOT benchmarks and achieves the state-of-the-art results.",Omutrwimucuanswcl,75.0,42.0,8.0
6027,Object Tracking,87.0,large margin object tracking with circulant feature maps,4.0,201.0,1.0,25.0,5.0,3.1,114.0,32,http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Large_Margin_Object_CVPR_2017_paper.pdf,"Structured output support vector machine (SVM) based tracking algorithms have shown favorable performance recently. Nonetheless, the time-consuming candidate sampling and complex optimization limit their real-time applications. In this paper, we propose a novel large margin object tracking method which absorbs the strong discriminative ability from structured output SVM and speeds up by the correlation filter algorithm significantly. Secondly, a multimodal target detection technique is proposed to improve the target localization precision and prevent model drift introduced by similar objects or background noise. Thirdly, we exploit the feedback from high-confidence tracking results to avoid the model corruption problem. We implement two versions of the proposed tracker with the representations from both conventional hand-crafted and deep convolution neural networks (CNNs) based features to validate the strong compatibility of the algorithm. The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms on the challenging benchmark sequences while runs at speed in excess of 80 frames per second.",Olamaobtrwicifema,354.0,43.0,52.0
6028,Object Tracking,100.0,spm-tracker: series-parallel matching for real-time visual object tracking,4.0,201.0,1.0,29.0,5.0,3.1,119.1,33,https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_SPM-Tracker_Series-Parallel_Matching_for_Real-Time_Visual_Object_Tracking_CVPR_2019_paper.pdf,"The greatest challenge facing visual object tracking is the simultaneous requirements on robustness and discrimination power. In this paper, we propose a SiamFC-based tracker, named SPM-Tracker, to tackle this challenge. The basic idea is to address the two requirements in two separate matching stages. Robustness is strengthened in the coarse matching (CM) stage through generalized training while discrimination power is enhanced in the fine matching (FM) stage through a distance learning network. The two stages are connected in series as the input proposals of the FM stage are generated by the CM stage. They are also connected in parallel as the matching scores and box location refinements are fused to generate the final results. This innovative series-parallel structure takes advantage of both stages and results in superior performance. The proposed SPM-Tracker, running at 120fps on GPU, achieves an AUC of 0.687 on OTB-100 and an EAO of 0.434 on VOT-16, exceeding other real-time trackers by a notable margin.",Ospsemaforeviobtr,77.0,62.0,9.0
6029,Object Tracking,86.0,a unified object motion and affinity model for online multi-object tracking,4.0,95.0,4.0,201.0,1.0,3.1,124.1,34,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yin_A_Unified_Object_Motion_and_Affinity_Model_for_Online_Multi-Object_CVPR_2020_paper.pdf,"Current popular online multi-object tracking (MOT) solutions apply single object trackers (SOTs) to capture object motions, while often requiring an extra affinity network to associate objects, especially for the occluded ones. This brings extra computational overhead due to repetitive feature extraction for SOT and affinity computation. Meanwhile, the model size of the sophisticated affinity network is usually non-trivial. In this paper, we propose a novel MOT framework that unifies object motion and affinity model into a single network, named UMA, in order to learn a compact feature that is discriminative for both object motion and affinity measure. In particular, UMA integrates single object tracking and metric learning into a unified triplet network by means of multi-task learning. Such design brings advantages of improved computation efficiency, low memory requirement and simplified training procedure. In addition, we equip our model with a task-specific attention module, which is used to boost task-aware feature learning. The proposed UMA can be easily trained end-to-end, and is elegant - requiring only one training stage. Experimental results show that it achieves promising performance on several MOT Challenge benchmarks.",Oaunobmoanafmofoonmutr,18.0,71.0,2.0
6030,Object Tracking,401.0,beyond pixels: leveraging geometry and shape cues for online multi-object tracking,1.0,69.0,4.0,56.0,4.0,3.1,164.70000000000002,35,http://arxiv.org/pdf/1802.09298v2,"This paper introduces geometry and object shape and pose costs for multi-object tracking in urban driving scenarios. Using images from a monocular camera alone, we devise pairwise costs for object tracks, based on several 3D cues such as object pose, shape, and motion. The proposed costs are agnostic to the data association method and can be incorporated into any optimization framework to output the pairwise data associations. These costs are easy to implement, can be computed in real-time, and complement each other to account for possible errors in a tracking-by-detection framework. We perform an extensive analysis of the designed costs and empirically demonstrate consistent improvement over the state-of-the-art under varying conditions that employ a range of object detectors, exhibit a variety in camera and object motions, and, more importantly, are not reliant on the choice of the association framework. We also show that, by using the simplest of associations frameworks (two-frame Hungarian assignment), we surpass the state-of-the-art in multi-object-tracking on road scenes. More qualitative and quantitative results can be found at https://junaidcs032.github.io/Geometry_ObjectShape_MOT/.",Obepilegeanshcufoonmutr,94.0,26.0,16.0
6031,Object Tracking,401.0,adaptive correlation filters with long-term and short-term memory for object tracking,1.0,75.0,4.0,55.0,4.0,3.1,166.8,36,http://arxiv.org/abs/1707.02309v2,"Object tracking is challenging as target objects often undergo drastic appearance changes over time. Recently, adaptive correlation filters have been successfully applied to object tracking. However, tracking algorithms relying on highly adaptive correlation filters are prone to drift due to noisy updates. Moreover, as these algorithms do not maintain long-term memory of target appearance, they cannot recover from tracking failures caused by heavy occlusion or target disappearance in the camera view. In this paper, we propose to learn multiple adaptive correlation filters with both long-term and short-term memory of target appearance for robust object tracking. First, we learn a kernelized correlation filter with an aggressive learning rate for locating target objects precisely. We take into account the appropriate size of surrounding context and the feature representations. Second, we learn a correlation filter over a feature pyramid centered at the estimated target position for predicting scale changes. Third, we learn a complementary correlation filter with a conservative learning rate to maintain long-term memory of target appearance. We use the output responses of this long-term filter to determine if tracking failure occurs. In the case of tracking failures, we apply an incrementally learned detector to recover the target position in a sliding window fashion. Extensive experimental results on large-scale benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness.",Oadcofiwiloanshmefoobtr,102.0,69.0,16.0
6032,Object Tracking,401.0,online multi-object tracking with dual matching attention networks,1.0,96.0,4.0,44.0,4.0,3.1,171.89999999999998,37,http://arxiv.org/pdf/1902.00749v1,"In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits of single object tracking and data association methods in a unified framework to handle noisy detections and frequent interactions between targets. Specifically, for applying single object tracking in MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, which encourages the model to focus on hard negative distractors during online learning. For data association, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention mechanisms. The spatial attention module generates dual attention maps which enable the network to focus on the matching patterns of the input image pair, while the temporal attention module adaptively allocates different levels of attention to different samples in the tracklet to suppress noisy observations. Experimental results on the MOT benchmark datasets show that the proposed algorithm performs favorably against both online and offline trackers in terms of identity-preserving metrics.",Oonmutrwidumaatne,200.0,59.0,20.0
6033,Object Tracking,401.0,exploit the connectivity: multi-object tracking with trackletnet,1.0,106.0,3.0,37.0,5.0,3.0,173.79999999999998,38,http://arxiv.org/pdf/1811.07258v1,"Multi-object tracking (MOT) is an important topic and critical task related to both static and moving camera applications, such as traffic flow analysis, autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works exploit spatial and temporal information for MOT, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by associating detection results frame by frame with the help of the appearance similarity and the spatial consistency. To compensate camera movement, epipolar constraints are taken into consideration in the association. Then, for every pair of two tracklets, the similarity, called the connectivity in the paper, is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups and each group represents a unique object ID. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieves promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods.",Oexthcomutrwitr,79.0,59.0,1.0
6034,Object Tracking,45.0,robust object tracking via sparsity-based collaborative model,4.0,201.0,1.0,52.0,4.0,2.8,109.5,39,http://faculty.ucmerced.edu/mhyang/project/cvpr12_scm.files/scm_files/cvpr12_wei.pdf,"In this paper we propose a robust object tracking algorithm using a collaborative model. As the main challenge for object tracking is to account for drastic appearance change, we propose a robust appearance model that exploits both holistic templates and local representations. We develop a sparsity-based discriminative classifier (SD-C) and a sparsity-based generative model (SGM). In the S-DC module, we introduce an effective method to compute the confidence value that assigns more weights to the foreground than the background. In the SGM module, we propose a novel histogram-based method that takes the spatial information of each patch into consideration with an occlusion handing scheme. Furthermore, the update scheme considers both the latest observations and the original template, thereby enabling the tracker to deal with appearance change effectively and alleviate the drift problem. Numerous experiments on various challenging videos demonstrate that the proposed tracker performs favorably against several state-of-the-art algorithms.",Oroobtrvispcomo,974.0,38.0,189.0
6035,Object Tracking,42.0,extending iou based multi-object tracking by visual information,4.0,201.0,1.0,60.0,4.0,2.8,111.0,40,http://elvera.nue.tu-berlin.de/typo3/files/1547Bochinski2018.pdf,"Today’s multi-object tracking approaches benefit greatly from nearly perfect object detections when following the popular tracking-by-detection scheme. This allows for extremely simple but accurate tracking methods which completely rely on the input detections as the high-speed IOU tracker. For real world applications, few missing detections cause a high number of ID switches and fragmentations which degrades the quality of the tracks significantly. We show that this problem can be efficiently overcome if the tracker falls back to visual single-object tracking in cases where no object detection is available. In several experiments we show for different visual trackers that the number of ID switches and fragmentations can be reduced by a large amount while maintaining high tracking speeds and outperforming the state-of-the art for the UA-DETRAC and VisDrone datasets.",Oexiobamutrbyviin,75.0,32.0,8.0
6036,Object Tracking,63.0,multi-task correlation particle filter for robust object tracking,4.0,201.0,1.0,43.0,4.0,2.8,112.20000000000002,41,https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhang_Multi-Task_Correlation_Particle_CVPR_2017_paper.pdf,"In this paper, we propose a multi-task correlation particle filter (MCPF) for robust visual tracking. We first present the multi-task correlation filter (MCF) that takes the interdependencies among different features into account to learn correlation filters jointly. The proposed MCPF is designed to exploit and complement the strength of a MCF and a particle filter. Compared with existing tracking methods based on correlation filters and particle filters, the proposed tracker has several advantages. First, it can shepherd the sampled particles toward the modes of the target state distribution via the MCF, thereby resulting in robust tracking performance. Second, it can effectively handle large-scale variation via a particle sampling strategy. Third, it can effectively maintain multiple modes in the posterior density using fewer particles than conventional particle filters, thereby lowering the computational cost. Extensive experimental results on three benchmark datasets demonstrate that the proposed MCPF performs favorably against the state-of-the-art methods.",Omucopafiforoobtr,302.0,53.0,36.0
6037,Object Tracking,47.0,multi-store tracker (muster): a cognitive psychology inspired approach to object tracking,4.0,201.0,1.0,62.0,4.0,2.8,113.1,42,http://openaccess.thecvf.com/content_cvpr_2015/papers/Hong_MUlti-Store_Tracker_MUSTer_2015_CVPR_paper.pdf,"Variations in the appearance of a tracked object, such as changes in geometry/photometry, camera viewpoint, illumination, or partial occlusion, pose a major challenge to object tracking. Here, we adopt cognitive psychology principles to design a flexible representation that can adapt to changes in object appearance during tracking. Inspired by the well-known Atkinson-Shiffrin Memory Model, we propose MUlti-Store Tracker (MUSTer), a dual-component approach consisting of short- and long-term memory stores to process target appearance memories. A powerful and efficient Integrated Correlation Filter (ICF) is employed in the short-term store for short-term tracking. The integrated long-term component, which is based on keypoint matching-tracking and RANSAC estimation, can interact with the long-term memory and provide additional information for output control. MUSTer was extensively evaluated on the CVPR2013 Online Object Tracking Benchmark (OOTB) and ALOV++ datasets. The experimental results demonstrated the superior performance of MUSTer in comparison with other state-of-art trackers.",Omutr(macopsinaptoobtr,543.0,63.0,92.0
6038,Object Tracking,41.0,visual object tracking using adaptive correlation filters,4.0,201.0,1.0,70.0,4.0,2.8,113.7,43,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.4992&rep=rep1&type=pdf,"Although not commonly used, correlation filters can track complex objects through rotations, occlusions and other distractions at over 20 times the rate of current state-of-the-art techniques. The oldest and simplest correlation filters use simple templates and generally fail when applied to tracking. More modern approaches such as ASEF and UMACE perform better, but their training needs are poorly suited to tracking. Visual tracking requires robust filters to be trained from a single frame and dynamically adapted as the appearance of the target object changes. This paper presents a new type of correlation filter, a Minimum Output Sum of Squared Error (MOSSE) filter, which produces stable correlation filters when initialized using a single frame. A tracker based upon MOSSE filters is robust to variations in lighting, scale, pose, and nonrigid deformations while operating at 669 frames per second. Occlusion is detected based upon the peak-to-sidelobe ratio, which enables the tracker to pause and resume where it left off when the object reappears.",Oviobtrusadcofi,2046.0,20.0,266.0
6039,Object Tracking,12.0,online object tracking with proposal selection,5.0,201.0,1.0,115.0,3.0,2.8,118.5,44,http://openaccess.thecvf.com/content_iccv_2015/papers/Hua_Online_Object_Tracking_ICCV_2015_paper.pdf,"Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object, and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i.e., detection score and edgeness score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance.",Oonobtrwiprse,99.0,62.0,1.0
6041,Object Tracking,11.0,online object tracking with sparse prototypes,5.0,201.0,1.0,125.0,3.0,2.8,121.2,45,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.309.5615&rep=rep1&type=pdf,"Online object tracking is a challenging problem as it entails learning an effective model to account for appearance change caused by intrinsic and extrinsic factors. In this paper, we propose a novel online object tracking algorithm with sparse prototypes, which exploits both classic principal component analysis (PCA) algorithms with recent sparse representation schemes for learning effective appearance models. We introduce l1 regularization into the PCA reconstruction, and develop a novel algorithm to represent an object by sparse prototypes that account explicitly for data and noise. For tracking, objects are represented by the sparse prototypes learned online with update. In order to reduce tracking drift, we present a method that takes occlusion and motion blur into account rather than simply includes image observations for model update. Both qualitative and quantitative evaluations on challenging image sequences demonstrate that the proposed tracking algorithm performs favorably against several state-of-the-art methods.",Oonobtrwisppr,411.0,53.0,32.0
6042,Object Tracking,20.0,a thermal object tracking benchmark,5.0,201.0,1.0,119.0,3.0,2.8,122.1,46,https://www.diva-portal.org/smash/get/diva2:850688/FULLTEXT02,"Short-term single-object (STSO) tracking in thermal images is a challenging problem relevant in a growing number of applications. In order to evaluate STSO tracking algorithms on visual imagery, there are de facto standard benchmarks. However, we argue that tracking in thermal imagery is different than in visual imagery, and that a separate benchmark is needed. The available thermal infrared datasets are few and the existing ones are not challenging for modern tracking algorithms. Therefore, we hereby propose a thermal infrared benchmark according to the Visual Visual Object Tracking (VOT) protocol for evaluation of STSO tracking methods. The benchmark includes the new LTIR dataset containing 20 thermal image sequences which have been collected from multiple sources and annotated in the format used in the VOT Challenge. In addition, we show that the ranking of different tracking principles differ between the visual and thermal benchmarks, confirming the need for the new benchmark.",Oathobtrbe,70.0,23.0,7.0
6043,Object Tracking,17.0,evaluating multiple object tracking performance: the clear mot metrics,5.0,201.0,1.0,124.0,3.0,2.8,122.7,47,https://link.springer.com/content/pdf/10.1155/2008/246309.pdf,"Simultaneous tracking of multiple persons in real-world environments is an active research field and several approaches have been proposed, based on a variety of features and algorithms. Recently, there has been a growing interest in organizing systematic evaluations to compare the various techniques. Unfortunately, the lack of common metrics for measuring the performance of multiple object trackers still makes it hard to compare their results. In this work, we introduce two intuitive and general metrics to allow for objective comparison of tracker characteristics, focusing on their precision in estimating object locations, their accuracy in recognizing object configurations and their ability to consistently label objects over time. These metrics have been extensively used in two large-scale international evaluations, the 2006 and 2007 CLEAR evaluations, to measure and compare the performance of multiple object trackers for a wide variety of tracking tasks. Selected performance results are presented and the advantages and drawbacks of the presented metrics are discussed based on the experience gained during the evaluations.",Oevmuobtrpethclmome,1549.0,22.0,200.0
6044,Object Tracking,62.0,tracking the trackers: an analysis of the state of the art in multiple object tracking,4.0,201.0,1.0,79.0,4.0,2.8,122.7,48,https://arxiv.org/pdf/1704.02781,"Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking tables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important guides for research. We present a benchmark for Multiple Object Tracking launched in the late 2014, with the goal of creating a framework for the standardized evaluation of multiple object tracking methods. This paper collects the two releases of the benchmark made so far, and provides an in-depth analysis of almost 50 state-of-the-art trackers that were tested on over 11000 frames. We show the current trends and weaknesses of multiple people tracking methods, and provide pointers of what researchers should be focusing on to push the field forward.",Otrthtrananofthstoftharinmuobtr,91.0,68.0,5.0
6045,Object Tracking,39.0,a survey of appearance models in visual object tracking,5.0,201.0,1.0,106.0,3.0,2.8,123.9,49,https://arxiv.org/pdf/1303.4803,"Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models. To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic. The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey.",Oasuofapmoinviobtr,661.0,267.0,20.0
6046,Object Tracking,73.0,deep convolutional neural networks for thermal infrared object tracking,4.0,201.0,1.0,77.0,4.0,2.8,125.4,50,http://www.hezhenyu.cn/UpLoadFiles/Papers/16Liu.pdf,"Abstract Unlike the visual object tracking, thermal infrared object tracking can track a target object in total darkness. Therefore, it has broad applications, such as in rescue and video surveillance at night. However, there are few studies in this field mainly because thermal infrared images have several unwanted attributes, which make it difficult to obtain the discriminative features of the target. Considering the powerful representational ability of convolutional neural networks and their successful application in visual tracking, we transfer the pre-trained convolutional neural networks based on visible images to thermal infrared tracking. We observe that the features from the fully-connected layer are not suitable for thermal infrared tracking due to the lack of spatial information of the target, while the features from the convolution layers are. Besides, the features from a single convolution layer are not robust to various challenges. Based on this observation, we propose a correlation filter based ensemble tracker with multi-layer convolutional features for thermal infrared tracking (MCFTS). Firstly, we use pre-trained convolutional neural networks to extract the features of the multiple convolution layers of the thermal infrared target. Then, a correlation filter is used to construct multiple weak trackers with the corresponding convolution layer features. These weak trackers give the response maps of the target’s location. Finally, we propose an ensemble method that coalesces these response maps to get a stronger one. Furthermore, a simple but effective scale estimation strategy is exploited to boost the tracking accuracy. To evaluate the performance of the proposed tracker, we carry out experiments on two thermal infrared tracking benchmarks: VOT-TIR 2015 and VOT-TIR 2016. The experimental results demonstrate that our tracker is effective and achieves promising performance.",Odeconenefothinobtr,103.0,91.0,3.0
6047,Object Tracking,54.0,robust object tracking via sparse collaborative appearance model,4.0,201.0,1.0,97.0,4.0,2.8,125.7,51,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.646.1467&rep=rep1&type=pdf,"In this paper, we propose a robust object tracking algorithm based on a sparse collaborative model that exploits both holistic templates and local representations to account for drastic appearance changes. Within the proposed collaborative appearance model, we develop a sparse discriminative classifier (SDC) and sparse generative model (SGM) for object tracking. In the SDC module, we present a classifier that separates the foreground object from the background based on holistic templates. In the SGM module, we propose a histogram-based method that takes the spatial information of each local patch into consideration. The update scheme considers both the most recent observations and original templates, thereby enabling the proposed algorithm to deal with appearance changes effectively and alleviate the tracking drift problem. Numerous experiments on various challenging videos demonstrate that the proposed tracker performs favorably against several state-of-the-art algorithms.",Oroobtrvispcoapmo,309.0,38.0,47.0
6048,Object Tracking,139.0,modality-correlation-aware sparse representation for rgb-infrared object tracking,3.0,201.0,1.0,14.0,5.0,2.8,126.3,52,https://leicester.figshare.com/articles/journal_contribution/Modality-Correlation-Aware_Sparse_Representation_for_RGB-Infrared_Object_Tracking/10243511/files/18490700.pdf,"Abstract To intelligently analyze and understand video content, a key step is to accurately perceive the motion of the interested objects in videos. To this end, the task of object tracking, which aims to determine the position and status of the interested object in consecutive video frames, is very important, and has received great research interest in the last decade. Although numerous algorithms have been proposed for object tracking in RGB videos, most of them may fail to track the object when the information from the RGB video is not reliable (e.g. in dim environment or large illumination change). To address this issue, with the popularity of dual-camera systems for capturing RGB and infrared videos, this paper presents a feature representation and fusion model to combine the feature representation of the object in RGB and infrared modalities for object tracking. Specifically, this proposed model is able to (1) perform feature representation of objects in different modalities by employing the robustness of sparse representation, and (2) combine the representation by exploiting the modality correlation. Extensive experiments demonstrate the effectiveness of the proposed method.",Omospreforgobtr,62.0,89.0,0.0
6049,Object Tracking,68.0,robust online multi-object tracking based on tracklet confidence and online discriminative appearance learning,4.0,201.0,1.0,95.0,4.0,2.8,129.3,53,https://openaccess.thecvf.com/content_cvpr_2014/papers/Bae_Robust_Online_Multi-Object_2014_CVPR_paper.pdf,"Online multi-object tracking aims at producing complete tracks of multiple objects using the information accumulated up to the present moment. It still remains a difficult problem in complex scenes, because of frequent occlusion by clutter or other objects, similar appearances of different objects, and other factors. In this paper, we propose a robust online multi-object tracking method that can handle these difficulties effectively. We first propose the tracklet confidence using the detectability and continuity of a tracklet, and formulate a multi-object tracking problem based on the tracklet confidence. The multi-object tracking problem is then solved by associating tracklets in different ways according to their confidence values. Based on this strategy, tracklets sequentially grow with online-provided detections, and fragmented tracklets are linked up with others without any iterative and expensive associations. Here, for reliable association between tracklets and detections, we also propose a novel online learning method using an incremental linear discriminant analysis for discriminating the appearances of objects. By exploiting the proposed learning method, tracklet association can be successfully achieved even under severe occlusion. Experiments with challenging public datasets show distinct performance improvement over other batch and online tracking methods.",Oroonmutrbaontrcoanondiaple,364.0,22.0,53.0
6050,Object Tracking,32.0,l2-rls-based object tracking,5.0,201.0,1.0,132.0,3.0,2.8,129.6,54,http://arxiv.org/pdf/2103.08808v1,"In this paper, we present a robust and fast tracking algorithm in which object tracking is achieved by solving ℓ2-regularized least square (ℓ2-RLS) problems in a Bayesian inference framework. First, the changing appearance of the tracked target is modeled with PCA basis vectors and square templates, which makes the tracker not only exploit the strength of subspace representation but also explicitly take partial occlusion into consideration. They can together represent both the intact and corrupted objects well. Second, we adopt the ℓ2-regularized least square method to solve the proposed representation model. Compared with the complex ℓ1-based algorithm, it provides a very fast performance without the loss of accuracy in handling the tracking problem. In addition, a novel likelihood function and a refined update scheme further help to improve the robustness of our tracker. Both qualitative and quantitative evaluations on several challenging image sequences demonstrate that the proposed method performs favorably against several state-of-the-art tracking algorithms.",Ol2obtr,61.0,31.0,5.0
6051,Object Tracking,23.0,transferring visual prior for online object tracking,5.0,201.0,1.0,158.0,3.0,2.8,134.70000000000002,55,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.422.742&rep=rep1&type=pdf,"Visual prior from generic real-world images can be learned and transferred for representing objects in a scene. Motivated by this, we propose an algorithm that transfers visual prior learned offline for online object tracking. From a collection of real-world images, we learn an overcomplete dictionary to represent visual prior. The prior knowledge of objects is generic, and the training image set does not necessarily contain any observation of the target object. During the tracking process, the learned visual prior is transferred to construct an object representation by sparse coding and multiscale max pooling. With this representation, a linear classifier is learned online to distinguish the target from the background and to account for the target and background appearance variations over time. Tracking is then carried out within a Bayesian inference framework, in which the learned classifier is used to construct the observation model and a particle filter is used to estimate the tracking result sequentially. Experiments on a variety of challenging sequences with comparisons to several state-of-the-art methods demonstrate that more robust object tracking can be achieved by transferring visual prior.",Otrviprfoonobtr,93.0,35.0,2.0
6052,Object Tracking,151.0,online multi-object tracking with instance-aware tracker and dynamic model refreshment,3.0,201.0,1.0,33.0,5.0,2.8,135.6,56,https://arxiv.org/pdf/1902.08231,"Recent progresses in model-free single object tracking (SOT) algorithms have largely inspired applying SOT to multi-object tracking (MOT) to improve the robustness as well as relieving dependency on external detector. However, SOT algorithms are generally designed for distinguishing a target from its environment, and hence meet problems when a target is spatially mixed with similar objects as observed frequently in MOT. To address this issue, in this paper we propose an instance-aware tracker to integrate SOT techniques for MOT by encoding awareness both within and between target models. In particular, we construct each target model by fusing information for distinguishing target both from background and other instances (tracking targets). To conserve uniqueness of all target models, our instance-aware tracker considers response maps from all target models and assigns spatial locations exclusively to optimize the overall accuracy. Another contribution we make is a dynamic model refreshing strategy learned by a convolutional neural network. This strategy helps to eliminate initialization noise as well as to adapt to variation of target size and appearance. To show the effectiveness of the proposed approach, it is evaluated on the popular MOT15 and MOT16 challenge benchmarks. On both benchmarks, our approach achieves the best overall performances in comparison with published results.",Oonmutrwiintrandymore,63.0,56.0,6.0
6053,Object Tracking,2.0,fundamentals of object tracking,5.0,201.0,1.0,184.0,3.0,2.8,136.2,57,http://arxiv.org/abs/1810.07032v2,"Preface 1. Introduction to object tracking 2. Filtering theory and non-maneuvering object tracking 3. Maneuvering object tracking 4. Single-object tracking in clutter 5. Single- and multiple-object tracking in clutter: object-existence-based approach 6. Multiple-object tracking in clutter: random-set-based approach 7. Bayesian smoothing algorithms for object tracking 8. Object tracking with time-delayed, out-of-sequence measurements 9. Practical object tracking A. Mathematical and statistical preliminaries B. Finite set statistics (FISST) C. Pseudo-functions in object tracking References Index.",Ofuofobtr,206.0,132.0,19.0
6054,Object Tracking,37.0,the state-of-the-art in visual object tracking,5.0,201.0,1.0,173.0,3.0,2.8,143.4,58,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.296.8445&rep=rep1&type=pdf#page=3,"There is a broad range of applications of visual object tracking that motivate the interests of researchers worldwide. These include video surveillance to know the suspicious activity, sport video analysis to extract highlights, traffic monitoring to analyse traffic flow and human computer interface to assist visually challenged people. In general, the processing framework of object tracking in dynamic scenes includes the following stages: segmentation and modelling of interesting moving object, predicting possible location of candidate object in each frame, localization of object in each frame, generally through a similarity measure in feature space. However, tracking an object in a complex environment is a challenging task. This survey discusses some of the core concepts used in object tracking and present a comprehensive survey of efforts in the past to address this problem. We have also explored wavelet domain and found that it has great potential in object tracking as it provides a rich and robust representation of an object. Povzetek: Podan je pregled metod vizualnega sledenja objektov .",Othstinviobtr,51.0,344.0,3.0
6055,Object Tracking,28.0,an experimental comparison of online object-tracking algorithms,5.0,201.0,1.0,196.0,3.0,2.8,147.60000000000002,59,https://www.academia.edu/download/31225288/spie11a.pdf,"This paper reviews and evaluates several state-of-the-art online object tracking algorithms. Notwithstanding decades of efforts, object tracking remains a challenging problem due to factors such as illumination, pose, scale, deformation, motion blur, noise, and occlusion. To account for appearance change, most recent tracking algorithms focus on robust object representations and effective state prediction. In this paper, we analyze the components of each tracking method and identify their key roles in dealing with specific challenges, thereby shedding light on how to choose and design algorithms for different situations. We compare state-of-the-art online tracking methods including the IVT,1 VRT,2 FragT,3 BoostT,4 SemiT,5 BeSemiT,6 L1T,7 MILT,8 VTD9 and TLD10 algorithms on numerous challenging sequences, and evaluate them with different performance metrics. The qualitative and quantitative comparative results demonstrate the strength and weakness of these algorithms.",Oanexcoofonobal,78.0,35.0,4.0
6056,Object Tracking,401.0,siam r-cnn: visual tracking by re-detection,1.0,65.0,4.0,178.0,3.0,2.8,199.7,60,http://arxiv.org/pdf/1911.12836v2,"We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www.vision.rwth-aachen.de/page/siamrcnn.",Osir-vitrbyre,75.0,131.0,22.0
6057,Object Tracking,401.0,triplet loss in siamese network for object tracking,1.0,105.0,3.0,42.0,4.0,2.7,174.9,61,http://arxiv.org/pdf/2012.07403v2,"Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy. In this paper, a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, our approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, we propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method. In experiments, we apply the proposed triplet loss for three real-time trackers based on Siamese network. And the results on several popular tracking benchmarks show our variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.",Otrloinsinefoobtr,263.0,38.0,21.0
6058,Object Tracking,401.0,unsupervised learning of object structure and dynamics from videos,1.0,1.0,5.0,201.0,1.0,2.6,181.0,62,http://arxiv.org/pdf/1903.05136v1,"Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, first, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.",Ounleofobstandyfrvi,60.0,38.0,7.0
6059,Object Tracking,401.0,real time pear fruit detection and counting using yolov4 models and deep sort,1.0,2.0,5.0,201.0,1.0,2.6,181.4,63,http://arxiv.org/pdf/2011.04244v2,"The ""You only look once v4""(YOLOv4) is one type of object detection methods in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on YOLOv4-tiny. It firstly uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved YOLOv4-tiny. Simulation results show that the proposed method has faster object detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of average precision as the YOLOv4-tiny. It is more suitable for real-time object detection.",Oretipefrdeancousyomoandeso,1.0,58.0,0.0
6060,Object Tracking,401.0,simple online and realtime tracking with a deep association metric,1.0,8.0,5.0,201.0,1.0,2.6,183.8,64,http://arxiv.org/pdf/1703.07402v1,"Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates.",Osionanretrwiadeasme,871.0,24.0,167.0
6061,Object Tracking,401.0,simple online and realtime tracking,1.0,11.0,5.0,201.0,1.0,2.6,185.0,65,http://arxiv.org/abs/1602.00763v2,"This paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects efficiently for online and realtime applications. To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9%. Despite only using a rudimentary combination of familiar techniques such as the Kalman Filter and Hungarian algorithm for the tracking components, this approach achieves an accuracy comparable to state-of-the-art online trackers. Furthermore, due to the simplicity of our tracking method, the tracker updates at a rate of 260 Hz which is over 20x faster than other state-of-the-art trackers.",Osionanretr,864.0,29.0,148.0
6062,Object Tracking,401.0,rethinking the competition between detection and reid in multi-object tracking,1.0,12.0,5.0,201.0,1.0,2.6,185.4,66,http://arxiv.org/pdf/2010.12138v2,"Due to balanced accuracy and speed, joint learning detection and ReID-based one-shot models have drawn great attention in multi-object tracking(MOT). However, the differences between the above two tasks in the one-shot tracking paradigm are unconsciously overlooked, leading to inferior performance than the two-stage methods. In this paper, we dissect the reasoning process of the aforementioned two tasks. Our analysis reveals that the competition of them inevitably hurts the learning of task-dependent representations, which further impedes the tracking performance. To remedy this issue, we propose a novel cross-correlation network that can effectively impel the separate branches to learn task-dependent representations. Furthermore, we introduce a scale-aware attention network that learns discriminative embeddings to improve the ReID capability. We integrate the delicately designed networks into a one-shot online MOT system, dubbed CSTrack. Without bells and whistles, our model achieves new state-of-the-art performances on MOT16 and MOT17. Our code is released at https://github.com/JudasDie/SOTS.",Orethcobedeanreinmutr,14.0,30.0,2.0
6063,Object Tracking,401.0,center-based 3d object detection and tracking,1.0,14.0,5.0,201.0,1.0,2.6,186.2,67,http://arxiv.org/pdf/2006.15506v1,"This technical report presents the online and real-time 2D and 3D multi-object tracking (MOT) algorithms that reached the 1st places on both Waymo Open Dataset 2D tracking and 3D tracking challenges. An efficient and pragmatic online tracking-by-detection framework named HorizonMOT is proposed for camera-based 2D tracking in the image space and LiDAR-based 3D tracking in the 3D world space. Within the tracking-by-detection paradigm, our trackers leverage our high-performing detectors used in the 2D/3D detection challenges and achieved 45.13% 2D MOTA/L2 and 63.45% 3D MOTA/L2 in the 2D/3D tracking challenges.",Oce3dobdeantr,43.0,80.0,12.0
6064,Object Tracking,401.0,rethinking self-supervised correspondence learning: a video frame-level similarity perspective,1.0,17.0,5.0,201.0,1.0,2.6,187.4,68,http://arxiv.org/pdf/2103.17263v5,"Learning a good representation for space-time correspondence is the key for various computer vision tasks, including tracking object bounding boxes and performing video object pixel segmentation. To learn generalizable representation for correspondence in large-scale, a variety of self-supervised pretext tasks are proposed to explicitly perform object-level or patch-level similarity learning. Instead of following the previous literature, we propose to learn correspondence using Video Frame-level Similarity (VFS) learning, i.e, simply learning from comparing video frames. Our work is inspired by the recent success in image-level contrastive learning and similarity learning for visual recognition. Our hypothesis is that if the representation is good for recognition, it requires the convolutional features to find correspondence between similar objects or parts. Our experiments show surprising results that VFS surpasses state-of-the-art self-supervised approaches for both OTB visual object tracking and DAVIS video object segmentation. We perform detailed analysis on what matters in VFS and reveals new properties on image and frame level similarity learning. Project page with code is available at https://jerryxu.net/VFS",Oresecoleavifrsipe,2.0,86.0,0.0
6065,Object Tracking,401.0,3d multi-object tracking: a baseline and new evaluation metrics,1.0,18.0,5.0,201.0,1.0,2.6,187.8,69,http://arxiv.org/pdf/1907.03961v5,"3D multi-object tracking (MOT) is an essential component for many applications such as autonomous driving and assistive robotics. Recent work on 3D MOT focuses on developing accurate systems giving less attention to practical considerations such as computational cost and system complexity. In contrast, this work proposes a simple real-time 3D MOT system. Our system first obtains 3D detections from a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter and the Hungarian algorithm is used for state estimation and data association. Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in the 2D space and standardized 3D MOT evaluation tools are missing for a fair comparison of 3D MOT methods. Therefore, we propose a new 3D MOT evaluation tool along with three new metrics to comprehensively evaluate 3D MOT methods. We show that, although our system employs a combination of classical MOT modules, we achieve state-of-the-art 3D MOT performance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly, although our system does not use any 2D data as inputs, we achieve competitive performance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate of $207.4$ FPS on the KITTI dataset, achieving the fastest speed among all modern MOT systems. To encourage standardized 3D MOT evaluation, our system and evaluation code are made publicly available at https://github.com/xinshuoweng/AB3DMOT.",O3dmutrabaanneevme,60.0,59.0,12.0
6066,Object Tracking,401.0,joint monocular 3d vehicle detection and tracking,1.0,25.0,5.0,201.0,1.0,2.6,190.6,70,http://arxiv.org/pdf/1811.10742v3,"Vehicle 3D extents and trajectories are critical cues for predicting the future location of vehicles and planning future agent ego-motion based on those predictions. In this paper, we propose a novel online framework for 3D vehicle detection and tracking from monocular videos. The framework can not only associate detections of vehicles in motion over time, but also estimate their complete 3D bounding box information from a sequence of 2D images captured on a moving platform. Our method leverages 3D box depth-ordering matching for robust instance association and utilizes 3D trajectory prediction for re-identification of occluded vehicles. We also design a motion learning module based on an LSTM for more accurate long-term motion extrapolation. Our experiments on simulation, KITTI, and Argoverse datasets show that our 3D tracking pipeline offers robust data association and tracking. On Argoverse, our image-based method is significantly better for tracking 3D vehicles within 30 meters than the LiDAR-centric baseline methods.",Ojomo3dvedeantr,70.0,61.0,6.0
6067,Object Tracking,401.0,towards accurate pixel-wise object tracking by attention retrieval,1.0,26.0,5.0,201.0,1.0,2.6,191.0,71,http://arxiv.org/abs/2008.02745v3,"The encoding of the target in object tracking moves from the coarse bounding-box to fine-grained segmentation map recently. Revisiting de facto real-time approaches that are capable of predicting mask during tracking, we observed that they usually fork a light branch from the backbone network for segmentation. Although efficient, directly fusing backbone features without considering the negative influence of background clutter tends to introduce false-negative predictions, lagging the segmentation accuracy. To mitigate this problem, we propose an attention retrieval network (ARN) to perform soft spatial constraints on backbone features. We first build a look-up-table (LUT) with the ground-truth mask in the starting frame, and then retrieves the LUT to obtain an attention map for spatial constraints. Moreover, we introduce a multi-resolution multi-stage segmentation network (MMS) to further weaken the influence of background clutter by reusing the predicted mask to filter backbone features. Our approach set a new state-of-the-art on recent pixel-wise object tracking benchmark VOT2020 while running at 40 fps. Notably, the proposed model surpasses SiamMask by 11.7/4.2/5.5 points on VOT2020, DAVIS2016, and DAVIS2017, respectively. We will release our code at https://github.com/researchmm/TracKit.",Otoacpiobtrbyatre,3.0,56.0,0.0
6068,Object Tracking,401.0,real-time multiple people tracking with deeply learned candidate selection and person re-identification,1.0,27.0,5.0,201.0,1.0,2.6,191.4,72,http://arxiv.org/abs/1809.04427v1,"Online multi-object tracking is a fundamental problem in time-critical video analysis applications. A major challenge in the popular tracking-by-detection framework is how to associate unreliable detection results with existing tracks. In this paper, we propose to handle unreliable detection by collecting candidates from outputs of both detection and tracking. The intuition behind generating redundant candidates is that detection and tracks can complement each other in different scenarios. Detection results of high confidence prevent tracking drifts in the long term, and predictions of tracks can handle noisy detection caused by occlusion. In order to apply optimal selection from a considerable amount of candidates in real-time, we present a novel scoring function based on a fully convolutional neural network, that shares most computations on the entire image. Moreover, we adopt a deeply learned appearance representation, which is trained on large-scale person re-identification datasets, to improve the identification ability of our tracker. Extensive experiments show that our tracker achieves real-time and state-of-the-art performance on a widely used people tracking benchmark.",Oremupetrwidelecaseanpere,163.0,25.0,25.0
6069,Object Tracking,401.0,joint object detection and multi-object tracking with graph neural networks,1.0,32.0,5.0,201.0,1.0,2.6,193.4,73,http://arxiv.org/pdf/2006.13164v3,"Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior works often design detection and data association modules separately which are trained with separate objectives. As a result, one cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent works simultaneously optimize detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variable-sized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show state-of-the-art performance for both detection and MOT tasks. Our code is available at: https://github.com/yongxinw/GSDT",Ojoobdeanmutrwigrnene,13.0,90.0,2.0
6070,Object Tracking,401.0,track to detect and segment: an online multi-object tracker,1.0,33.0,5.0,201.0,1.0,2.6,193.8,74,http://arxiv.org/pdf/1605.05863v1,"In this paper we present a tracker, which is radically different from state-of-the-art trackers: we apply no model updating, no occlusion detection, no combination of trackers, no geometric matching, and still deliver state-of-the-art tracking performance, as demonstrated on the popular online tracking benchmark (OTB) and six very challenging YouTube videos. The presented tracker simply matches the initial patch of the target in the first frame with candidates in a new frame and returns the most similar patch by a learned matching function. The strength of the matching function comes from being extensively trained generically, i.e., without any data of the target, using a Siamese deep neural network, which we design for tracking. Once learned, the matching function is used as is, without any adapting, to track previously unseen targets. It turns out that the learned matching function is so powerful that a simple tracker built upon it, coined Siamese INstance search Tracker, SINT, which only uses the original observation of the target from the first frame, suffices to reach state-of-the-art performance. Further, we show the proposed tracker even allows for target re-identification after the target was absent for a complete video shot.",Otrtodeanseanonmutr,11.0,72.0,2.0
6071,Object Tracking,401.0,detection and tracking meet drones challenge,1.0,34.0,5.0,201.0,1.0,2.6,194.2,75,http://arxiv.org/pdf/2001.06303v3,"Drones, or general UAVs, equipped with cameras have been fast deployed with a wide range of applications, including agriculture, aerial photography, and surveillance. Consequently, automatic understanding of visual data collected from drones becomes highly demanding, bringing computer vision and drones more and more closely. To promote and track the developments of object detection and tracking algorithms, we have organized three challenge workshops in conjunction with ECCV 2018, ICCV 2019 and ECCV 2020, attracting more than 100 teams around the world. We provide a large-scale drone captured dataset, VisDrone, which includes four tracks, i.e., (1) image object detection, (2) video object detection, (3) single object tracking, and (4) multi-object tracking. In this paper, we first present a thorough review of object detection and tracking datasets and benchmarks, and discuss the challenges of collecting large-scale drone-based object detection and tracking datasets with fully manual annotations. After that, we describe our VisDrone dataset, which is captured over various urban/suburban areas of 14 different cities across China from North to South. Being the largest such dataset ever published, VisDrone enables extensive evaluation and investigation of visual analysis algorithms for the drone platform. We provide a detailed analysis of the current state of the field of large-scale object detection and tracking on drones, and conclude the challenge as well as propose future directions. We expect the benchmark largely boost the research and development in video analysis on drone platforms. All the datasets and experimental results can be downloaded from https://github.com/VisDrone/VisDrone-Dataset.",Odeantrmedrch,0.0,163.0,0.0
6072,Object Tracking,401.0,monocular quasi-dense 3d object tracking,1.0,40.0,5.0,201.0,1.0,2.6,196.6,76,http://arxiv.org/pdf/2103.07351v1,"A reliable and accurate 3D tracking framework is essential for predicting future locations of surrounding objects and planning the observer's actions in numerous applications such as autonomous driving. We propose a framework that can effectively associate moving objects over time and estimate their full 3D bounding box information from a sequence of 2D images captured on a moving platform. The object association leverages quasi-dense similarity learning to identify objects in various poses and viewpoints with appearance cues only. After initial 2D association, we further utilize 3D bounding boxes depth-ordering heuristics for robust instance association and motion-based 3D trajectory prediction for re-identification of occluded vehicles. In the end, an LSTM-based object velocity learning module aggregates the long-term trajectory information for more accurate motion extrapolation. Experiments on our proposed simulation data and real-world benchmarks, including KITTI, nuScenes, and Waymo datasets, show that our tracking framework offers robust object association and tracking on urban-driving scenarios. On the Waymo Open benchmark, we establish the first camera-only baseline in the 3D tracking and 3D detection challenges. Our quasi-dense 3D tracking pipeline achieves impressive improvements on the nuScenes 3D tracking benchmark with near five times tracking accuracy of the best vision-only submission among all published methods. Our code, data and trained models are available at https://github.com/SysCV/qd-3dt.",Omoqu3dobtr,2.0,102.0,0.0
6073,Object Tracking,55.0,multiple object tracking using k-shortest paths optimization,4.0,201.0,1.0,102.0,3.0,2.5,127.5,77,https://infoscience.epfl.ch/record/164041/files/top.pdf,"Multi-object tracking can be achieved by detecting objects in individual frames and then linking detections across frames. Such an approach can be made very robust to the occasional detection failure: If an object is not detected in a frame but is in previous and following ones, a correct trajectory will nevertheless be produced. By contrast, a false-positive detection in a few frames will be ignored. However, when dealing with a multiple target problem, the linking step results in a difficult optimization problem in the space of all possible families of trajectories. This is usually dealt with by sampling or greedy search based on variants of Dynamic Programming which can easily miss the global optimum. In this paper, we show that reformulating that step as a constrained flow optimization results in a convex problem. We take advantage of its particular structure to solve it using the k-shortest paths algorithm, which is very fast. This new approach is far simpler formally and algorithmically than existing techniques and lets us demonstrate excellent performance in two very different contexts.",Omuobtrusk-paop,929.0,50.0,108.0
6074,Object Tracking,108.0,collaborative deep reinforcement learning for multi-object tracking,3.0,201.0,1.0,69.0,4.0,2.5,133.5,78,http://openaccess.thecvf.com/content_ECCV_2018/papers/Liangliang_Ren_Collaborative_Deep_Reinforcement_ECCV_2018_paper.pdf,"In this paper, we propose a collaborative deep reinforcement learning (C-DRL) method for multi-object tracking. Most existing multi-object tracking methods employ the tracking-by-detection strategy which first detects objects in each frame and then associates them across different frames. However, the performance of these methods rely heavily on the detection results, which are usually unsatisfied in many real applications, especially in crowded scenes. To address this, we develop a deep prediction-decision network in our C-DRL, which simultaneously detects and predicts objects under a unified network via deep reinforcement learning. Specifically, we consider each object as an agent and track it via the prediction network, and seek the optimal tracked results by exploiting the collaborative interactions of different agents and environments via the decision network. Experimental results on the challenging MOT15 and MOT16 benchmarks are presented to show the effectiveness of our approach.",Ocoderelefomutr,42.0,54.0,1.0
6075,Object Tracking,104.0,studying visual attention using the multiple object tracking paradigm: a tutorial review,3.0,201.0,1.0,91.0,4.0,2.5,138.9,79,http://arxiv.org/pdf/2008.00106v1,"Human observers are capable of tracking multiple objects among identical distractors based only on their spatiotemporal information. Since the first report of this ability in the seminal work of Pylyshyn and Storm (1988, Spatial Vision, 3, 179–197), multiple object tracking has attracted many researchers. A reason for this is that it is commonly argued that the attentional processes studied with the multiple object paradigm apparently match the attentional processing during real-world tasks such as driving or team sports. We argue that multiple object tracking provides a good mean to study the broader topic of continuous and dynamic visual attention. Indeed, several (partially contradicting) theories of attentive tracking have been proposed within the almost 30 years since its first report, and a large body of research has been conducted to test these theories. With regard to the richness and diversity of this literature, the aim of this tutorial review is to provide researchers who are new in the field of multiple object tracking with an overview over the multiple object tracking paradigm, its basic manipulations, as well as links to other paradigms investigating visual attention and working memory. Further, we aim at reviewing current theories of tracking as well as their empirical evidence. Finally, we review the state of the art in the most prominent research fields of multiple object tracking and how this research has helped to understand visual attention in dynamic settings.",Ostviatusthmuobtrpaature,50.0,235.0,3.0
6076,Object Tracking,112.0,deep reinforcement learning for visual object tracking in videos,3.0,201.0,1.0,88.0,4.0,2.5,140.4,80,https://arxiv.org/pdf/1701.08936,"In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.",Oderelefoviobtrinvi,87.0,69.0,3.0
6077,Object Tracking,115.0,multi-class multi-object tracking using changing point detection,3.0,201.0,1.0,87.0,4.0,2.5,141.0,81,http://arxiv.org/pdf/1809.05929v7,This paper presents a robust multi-class multi-object tracking (MCMOT) formulated by a Bayesian filtering framework. Multi-object tracking for unlimited object classes is conducted by combining detection responses and changing point detection (CPD) algorithm. The CPD model is used to observe abrupt or abnormal changes due to a drift and an occlusion based spatiotemporal characteristics of track states. The ensemble of convolutional neural network (CNN) based object detector and Lucas-Kanede Tracker (KLT) based motion detector is employed to compute the likelihoods of foreground regions as the detection responses of different object classes. Extensive experiments are performed using lately introduced challenging benchmark videos; ImageNet VID and MOT benchmark dataset. The comparison to state-of-the-art video tracking techniques shows very encouraging results.,Omumutruschpode,112.0,44.0,7.0
6078,Object Tracking,148.0,mono-camera 3d multi-object tracking using deep learning detections and pmbm filtering,3.0,201.0,1.0,63.0,4.0,2.5,143.70000000000002,82,https://arxiv.org/pdf/1802.09975,"Monocular cameras are one of the most commonly used sensors in the automotive industry for autonomous vehicles. One major drawback using a monocular camera is that it only makes observations in the two dimensional image plane and can not directly measure the distance to objects. In this paper, we aim at filling this gap by developing a multi-object tracking algorithm that takes an image as input and produces trajectories of detected objects in a world coordinate system. We solve this by using a deep neural network trained to detect and estimate the distance to objects from a single input image. The detections from a sequence of images are fed in to a state-of-the art Poisson multi-Bernoulli mixture tracking filter. The combination of the learned detector and the PMBM filter results in an algorithm that achieves 3D tracking using only mono-camera images as input. The performance of the algorithm is evaluated both in 3D world coordinates, and 2D image coordinates, using the publicly available KITTI object tracking dataset. The algorithm shows the ability to accurately track objects, correctly handle data associations, even when there is a big overlap of the objects in the image, and is one of the top performing algorithms on the KITTI object tracking benchmark. Furthermore, the algorithm is efficient, running on average close to 20 frames per second.",Omo3dmutrusdeledeanpmfi,74.0,35.0,5.0
6079,Object Tracking,70.0,probabilistic object tracking using a range camera,4.0,201.0,1.0,155.0,3.0,2.5,147.9,83,https://arxiv.org/pdf/1505.00241,"We address the problem of tracking the 6-DoF pose of an object while it is being manipulated by a human or a robot. We use a dynamic Bayesian network to perform inference and compute a posterior distribution over the current object pose. Depending on whether a robot or a human manipulates the object, we employ a process model with or without knowledge of control inputs. Observations are obtained from a range camera. As opposed to previous object tracking methods, we explicitly model self-occlusions and occlusions from the environment, e.g, the human or robotic hand. This leads to a strongly non-linear observation model and additional dependencies in the Bayesian network. We employ a Rao-Blackwellised particle filter to compute an estimate of the object pose at every time step. In a set of experiments, we demonstrate the ability of our method to accurately and robustly track the object pose in real-time while it is being manipulated by a human or a robot.",Oprobtrusaraca,53.0,22.0,7.0
6080,Object Tracking,165.0,fusion of head and full-body detectors for multi-object tracking,3.0,201.0,1.0,61.0,4.0,2.5,148.20000000000002,84,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w29/Henschel_Fusion_of_Head_CVPR_2018_paper.pdf,"In order to track all persons in a scene, the tracking-by-detection paradigm has proven to be a very effective approach. Yet, relying solely on a single detector is also a major limitation, as useful image information might be ignored. Consequently, this work demonstrates how to fuse two detectors into a tracking system. To obtain the trajectories, we propose to formulate tracking as a weighted graph labeling problem, resulting in a binary quadratic program. As such problems are NP-hard, the solution can only be approximated. Based on the Frank-Wolfe algorithm, we present a new solver that is crucial to handle such difficult problems. Evaluation on pedestrian tracking is provided for multiple scenarios, showing superior results over single detector tracking and standard QP-solvers. Finally, our tracker ranks 2nd on the MOT16 benchmark and 1st on the new MOT17 benchmark, outperforming over 90 trackers.",Ofuofheanfudefomutr,113.0,77.0,7.0
6081,Object Tracking,137.0,an equalized global graph model-based approach for multicamera object tracking,3.0,201.0,1.0,90.0,4.0,2.5,148.5,85,https://arxiv.org/pdf/1502.03532,"Nonoverlapping multicamera visual object tracking typically consists of two steps: single-camera object tracking (SCT) and inter-camera object tracking (ICT). Most of tracking methods focus on SCT, which happens in the same scene, while for real surveillance scenes, ICT is needed and single-camera tracking methods cannot work effectively. In this paper, we try to improve the overall multicamera object tracking performance by a global graph model with an improved similarity metric. Our method treats the similarities of single-camera tracking and inter-camera tracking differently and obtains the optimization in a global graph model. The results show that our method can work better even in the condition of poor SCT.",Oaneqglgrmoapfomuobtr,42.0,69.0,6.0
6082,Object Tracking,155.0,non-markovian globally consistent multi-object tracking,3.0,201.0,1.0,84.0,4.0,2.5,152.1,86,https://openaccess.thecvf.com/content_ICCV_2017/papers/Maksai_Non-Markovian_Globally_Consistent_ICCV_2017_paper.pdf,"Many state-of-the-art approaches to multi-object tracking rely on detecting them in each frame independently, grouping detections into short but reliable trajectory segments, and then further grouping them into full trajectories. This grouping typically relies on imposing local smoothness constraints but almost never on enforcing more global ones on the trajectories.,,In this paper, we propose a non-Markovian approach to imposing global consistency by using behavioral patterns to guide the tracking algorithm. When used in conjunction with state-of-the-art tracking algorithms, this further increases their already good performance on multiple challenging datasets. We show significant improvements both in supervised settings where ground truth is available and behavioral patterns can be learned from it, and in completely unsupervised settings.",Onoglcomutr,70.0,95.0,1.0
6083,Object Tracking,182.0,hybridizing sine cosine algorithm with differential evolution for global optimization and object tracking,3.0,201.0,1.0,57.0,4.0,2.5,152.1,87,http://arxiv.org/abs/1805.00873v1,"Abstract A new optimization algorithm called Hybrid Sine-Cosine Algorithm with Differential Evolution algorithm (Hybrid SCA-DE) is proposed in this paper for solving optimization problems and object tracking. The proposed hybrid algorithm has better capability to escape from local optima with faster convergence than the standard SCA and DE. The effectiveness of this algorithm is evaluated using 23 benchmark functions, which are divided into three groups: unimodal, multimodal, and fixed dimension multimodal functions. Statistical parameters have been employed to observe the efficiency of the Hybrid SCA-DE qualitatively and results prove that the proposed algorithm is very competitive compared to the state-of-the-art metaheuristic algorithms. The proposed algorithm is applied for object tracking as a real thought-provoking case study. To demonstrate the tracking ability of a Hybrid SCA-DE-based tracker, a comparative study of tracking accuracy and speed of the Hybrid SCA-DE-based tracker with four other trackers, namely, Particle Filter, Scale-invariant feature transform, Particle swarm optimization and Bat algorithm are presented. Comparative results show that the Hybrid SCA-DE-based tracker can robustly track an arbitrary target in various challenging conditions than the other trackers.",Ohysicoalwidievfoglopanobtr,99.0,55.0,3.0
6084,Object Tracking,83.0,object tracking using high resolution satellite imagery,4.0,201.0,1.0,172.0,3.0,2.5,156.9,88,https://www.cis.rit.edu/people/faculty/kerekes/pdfs/Meng_Final.pdf,"High resolution multispectral satellite images with multi-angular look capability have tremendous potential applications. We present an object tracking algorithm that includes moving object estimation, target modeling, and target matching three-step processing. Potentially moving objects are first identified on the time-series images. The target is then modeled by extracting both spectral and spatial features. In the target matching procedure, the Bhattacharyya distance, histogram intersection, and pixel count similarity are combined in a novel regional operator design. Our algorithm has been tested using a set of multi-angular sequence images acquired by the WorldView-2 satellite. The tracking performance is analyzed by the calculation of recall, precision, and F1 score of the test. In this study, we have demonstrated the capability of object tracking in a complex environment with the help of high resolution multispectral satellite imagery.",Oobtrushiresaim,56.0,25.0,1.0
6085,Object Tracking,89.0,robust object tracking based on tracking-learning-detection,4.0,201.0,1.0,176.0,3.0,2.5,159.9,89,https://repositum.tuwien.at/bitstream/20.500.12708/12875/2/Nebehay%20Georg%20-%202012%20-%20Robust%20object%20tracking%20based%20on...pdf,"Current state-of-the-art methods for object tracking perform adaptive tracking-by-detection, meaning that a detector predicts the position of an object and adapts its parameters to the object’s appearance at the same time. While suitable for cases when the object does not disappear from the scene, these methods tend to fail on occlusions. In this work, we build on a novel approach called Tracking-Learning-Detection (TLD) that overcomes this problem. In methods based on TLD, a detector is trained with examples found on the trajectory of a tracker that itself does not depend on the object detector. By decoupling object tracking and object detection we achieve high robustness and outperform existing adaptive tracking-by-detection methods. We show that by using simple features for object detection and by employing a cascaded approach a considerable reduction of computing time is achieved. We evaluate our approach both on existing standard single-camera datasets as well as on newly recorded sequences in multi-camera scenarios.",Oroobtrbaontr,42.0,51.0,4.0
6086,Object Tracking,135.0,lifted disjoint paths with application in multiple object tracking,3.0,126.0,3.0,201.0,1.0,2.4,151.2,90,http://proceedings.mlr.press/v119/hornakova20a/hornakova20a.pdf,"We present an extension to the disjoint paths problem in which additional \emph{lifted} edges are introduced to provide path connectivity priors. We call the resulting optimization problem the lifted disjoint paths problem. We show that this problem is NP-hard by reduction from integer multicommodity flow and 3-SAT. To enable practical global optimization, we propose several classes of linear inequalities that produce a high-quality LP-relaxation. Additionally, we propose efficient cutting plane algorithms for separating the proposed linear inequalities. The lifted disjoint path problem is a natural model for multiple object tracking and allows an elegant mathematical formulation for long range temporal interactions. Lifted edges help to prevent id switches and to re-identify persons. Our lifted disjoint paths tracker achieves nearly optimal assignments with respect to input detections. As a consequence, it leads on all three main benchmarks of the MOT challenge, improving significantly over state-of-the-art.",Olidipawiapinmuobtr,40.0,65.0,2.0
6087,Object Tracking,132.0,exploiting spatial invariance for scalable unsupervised object tracking,3.0,185.0,3.0,201.0,1.0,2.4,173.89999999999998,91,https://ojs.aaai.org/index.php/AAAI/article/download/5777/5633,"The ability to detect and track objects in the visual world is a crucial skill for any intelligent agent, as it is a necessary precursor to any object-level reasoning process. Moreover, it is important that agents learn to track objects without supervision (i.e. without access to annotated training videos) since this will allow agents to begin operating in new environments with minimal human assistance. The task of learning to discover and track objects in videos, which we call \textit{unsupervised object tracking}, has grown in prominence in recent years; however, most architectures that address it still struggle to deal with large scenes containing many objects. In the current work, we propose an architecture that scales well to the large-scene, many-object setting by employing spatially invariant computations (convolutions and spatial attention) and representations (a spatially local object specification scheme). In a series of experiments, we demonstrate a number of attractive features of our architecture; most notably, that it outperforms competing methods at tracking objects in cluttered scenes with many objects, and that it can generalize well to videos that are larger and/or contain more objects than videos encountered during training.",Oexspinfoscunobtr,28.0,46.0,5.0
6088,Object Tracking,167.0,"online object tracking, learning and parsing with and-or graphs",3.0,165.0,3.0,201.0,1.0,2.4,176.39999999999998,92,http://openaccess.thecvf.com/content_cvpr_2014/papers/Lu_Online_Object_Tracking_2014_CVPR_paper.pdf,"This paper presents a method, called AOGTracker, for simultaneously tracking, learning and parsing (TLP) of unknown objects in video sequences with a hierarchical and compositional And-Or graph (AOG) representation. %The AOG captures both structural and appearance variations of a target object in a principled way. The TLP method is formulated in the Bayesian framework with a spatial and a temporal dynamic programming (DP) algorithms inferring object bounding boxes on-the-fly. During online learning, the AOG is discriminatively learned using latent SVM to account for appearance (e.g., lighting and partial occlusion) and structural (e.g., different poses and viewpoints) variations of a tracked object, as well as distractors (e.g., similar objects) in background. Three key issues in online inference and learning are addressed: (i) maintaining purity of positive and negative examples collected online, (ii) controling model complexity in latent structure learning, and (iii) identifying critical moments to re-learn the structure of AOG based on its intrackability. The intrackability measures uncertainty of an AOG based on its score maps in a frame. In experiments, our AOGTracker is tested on two popular tracking benchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks, and the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery tracking). In the former, our AOGTracker outperforms state-of-the-art tracking algorithms including two trackers based on deep convolutional network. In the latter, our AOGTracker outperforms all other trackers in VOT2013 and is comparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.",Oonobtrleanpawiangr,61.0,129.0,2.0
6089,Object Tracking,156.0,deformable siamese attention networks for visual object tracking,3.0,177.0,3.0,201.0,1.0,2.4,177.89999999999998,93,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Deformable_Siamese_Attention_Networks_for_Visual_Object_Tracking_CVPR_2020_paper.pdf,"Siamese-based trackers have achieved excellent performance on visual object tracking. However, the target template is not updated online, and the features of the target template and search image are computed independently in a Siamese architecture. In this paper, we propose Deformable Siamese Attention Networks, referred to as SiamAttn, by introducing a new Siamese attention mechanism that computes deformable self-attention and cross-attention. The self attention learns strong context information via spatial attention, and selectively emphasizes interdependent channel-wise features with channel attention. The cross-attention is capable of aggregating rich contextual inter-dependencies between the target template and the search image, providing an implicit manner to adaptively update the target template. In addition, we design a region refinement module that computes depth-wise cross correlations between the attentional features for more accurate tracking. We conduct experiments on six benchmarks, where our method achieves new state of-the-art results, outperforming the strong baseline, SiamRPN++ [24], by 0.464->0.537 and 0.415->0.470 EAO on VOT 2016 and 2018. Our code is available at: https://github.com/msight-tech/research-siamattn.",Odesiatnefoviobtr,40.0,48.0,9.0
6090,Object Tracking,13.0,infants' knowledge of objects: beyond object files and object tracking,5.0,201.0,1.0,201.0,1.0,2.2,144.60000000000002,94,https://www.academia.edu/download/48458807/Infants_Knowledge_of_Objects_Beyond_Obje20160831-3758-1fss2n1.pdf,"Objective: Preterm infants' limb monitoring in neonatal intensive care units (NICUs) is of primary importance for assessing infants' health status and motor/cognitive development. Herein, we propose a new approach to preterm infants' limb pose estimation that features spatio-temporal information to detect and track limb joints from depth videos with high reliability. Methods: Limb-pose estimation is performed using a deep-learning framework consisting of a detection and a regression convolutional neural network (CNN) for rough and precise joint localization, respectively. The CNNs are implemented to encode connectivity in the temporal direction through 3D convolution. Assessment of the proposed framework is performed through a comprehensive study with sixteen depth videos acquired in the actual clinical practice from sixteen preterm infants (the babyPose dataset). Results: When applied to pose estimation, the median root mean squared distance, computed among all limbs, between the estimated and the ground-truth pose was 9.06 pixels, overcoming approaches based on spatial features only (11.27pixels). Conclusion: Results showed that the spatio-temporal features had a significant influence on the pose-estimation performance, especially in challenging cases (e.g., homogeneous image intensity). Significance: This paper significantly enhances the state of art in automatic assessment of preterm infants' health status by introducing the use of spatio-temporal features for limb detection and tracking, and by being the first study to use depth videos acquired in the actual clinical practice for limb-pose estimation. The babyPose dataset has been released as the first annotated dataset for infants' pose estimation.",Oinknofobbeobfianobtr,266.0,107.0,8.0
6091,Object Tracking,14.0,"simultaneous localization, mapping and moving object tracking",5.0,201.0,1.0,201.0,1.0,2.2,144.9,95,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.1229&rep=rep1&type=pdf,"The development of automatic nutrition diaries, which would allow to keep track objectively of everything we eat, could enable a whole new world of possibilities for people concerned about their nutrition patterns. With this purpose, in this paper we propose the first method for simultaneous food localization and recognition. Our method is based on two main steps, which consist in, first, produce a food activation map on the input image (i.e. heat map of probabilities) for generating bounding boxes proposals and, second, recognize each of the food types or food-related objects present in each bounding box. We demonstrate that our proposal, compared to the most similar problem nowadays - object localization, is able to obtain high precision and reasonable recall levels with only a few bounding boxes. Furthermore, we show that it is applicable to both conventional and egocentric images.",Osilomaanmoobtr,558.0,118.0,39.0
6092,Object Tracking,16.0,object tracking with bayesian estimation of dynamic layer representations,5.0,201.0,1.0,201.0,1.0,2.2,145.5,96,https://www.researchgate.net/profile/Rakesh-Kumar-174/publication/3193341_Object_tracking_with_Bayesian_estimation_of_dynamic_layerrepresentations/links/53fd53570cf2364ccc08aa1a/Object-tracking-with-Bayesian-estimation-of-dynamic-layerrepresentations.pdf,"This paper introduces a novel deep learning based approach for vision based single target tracking. We address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples. This is achieved by combining a deep convolutional neural network with a Bayesian loss layer in a unified framework. In order to deal with the limited number of positive training examples, the network is pre-trained offline for a generic image feature representation and then is fine-tuned in multiple steps. An online fine-tuning step is carried out at every frame to learn the appearance of the target. We adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a probability density for target/non-target regions. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results.",Oobtrwibaesofdylare,357.0,17.0,7.0
6093,Object Tracking,18.0,robust object tracking by hierarchical association of detection responses,5.0,201.0,1.0,201.0,1.0,2.2,146.10000000000002,97,https://link.springer.com/content/pdf/10.1007/978-3-540-88688-4_58.pdf,"In this paper, we propose to exploit the rich hierarchical features of deep convolutional neural networks to improve the accuracy and robustness of visual tracking. Deep neural networks trained on object recognition datasets consist of multiple convolutional layers. These layers encode target appearance with different levels of abstraction. For example, the outputs of the last convolutional layers encode the semantic information of targets and such representations are invariant to significant appearance variations. However, their spatial resolutions are too coarse to precisely localize the target. In contrast, features from earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchical features of convolutional layers as a nonlinear counterpart of an image pyramid representation and explicitly exploit these multiple levels of abstraction to represent target objects. Specifically, we learn adaptive correlation filters on the outputs from each convolutional layer to encode the target appearance. We infer the maximum response of each layer to locate targets in a coarse-to-fine manner. To further handle the issues with scale estimation and re-detecting target objects from tracking failures caused by heavy occlusion or out-of-the-view movement, we conservatively learn another correlation filter, that maintains a long-term memory of target appearance, as a discriminative classifier. We apply the classifier to two types of object proposals: (1) proposals with a small step size and tightly around the estimated location for scale estimation; and (2) proposals with large step size and across the whole image for target re-detection. Extensive experimental results on large-scale benchmark datasets show that the proposed algorithm performs favorably against state-of-the-art tracking methods.",Oroobtrbyhiasofdere,533.0,20.0,51.0
6094,Object Tracking,19.0,moving object tracking using kalman filter,5.0,201.0,1.0,201.0,1.0,2.2,146.4,98,https://www.academia.edu/download/31207660/V2I4201376.pdf,"It is an important task to reliably detect and track multiple moving objects for video surveillance and monitoring. However, when occlusion occurs in nonlinear motion scenarios, many existing methods often fail to continuously track multiple moving objects of interest. In this paper we propose an effective approach for detection and tracking of multiple moving objects with occlusion. Moving targets are initially detected using a simple yet efficient block matching technique, providing rough location information for multiple object tracking. More accurate location information is then estimated for each moving object by a nonlinear tracking algorithm. Considering the ambiguity caused by the occlusion among multiple moving objects, we apply an unscented Kalman filtering (UKF) technique for reliable object detection and tracking. Different from conventional Kalman filtering (KF), which cannot achieve the optimal estimation in nonlinear tracking scenarios, UKF can be used to track both linear and nonlinear motions due to the unscented transform. Further, it estimates the velocity information for each object to assist to the object detection algorithm, effectively delineating multiple moving objects of occlusion. The experimental results demonstrate that the proposed method can correctly detect and track multiple moving objects with nonlinear motion patterns and occlusions.",Omoobtruskafi,26.0,14.0,0.0
6096,Object Tracking,22.0,object tracking using sift features and mean shift,5.0,201.0,1.0,201.0,1.0,2.2,147.3,99,https://www.academia.edu/download/54853691/j.cviu.2008.08.00620171029-9832-1cs9xkb.pdf,"In this paper, we are presenting a rotation variant Oriented Texture Curve (OTC) descriptor based mean shift algorithm for tracking an object in an unstructured crowd scene. The proposed algorithm works by first obtaining the OTC features for a manually selected object target, then a visual vocabulary is created by using all the OTC features of the target. The target histogram is obtained using codebook encoding method which is then used in mean shift framework to perform similarity search. Results are obtained on different videos of challenging scenes and the comparison of the proposed approach with several state-of-the-art approaches are provided. The analysis shows the advantages and limitations of the proposed approach for tracking an object in unstructured crowd scenes.",Oobtrussifeanmesh,568.0,33.0,11.0
6097,Object Tracking,24.0,prediction-based strategies for energy saving in object tracking sensor networks,5.0,201.0,1.0,201.0,1.0,2.2,147.9,100,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.7613&rep=rep1&type=pdf,"Vehicle tracking has become one of the key applications of wireless sensor networks (WSNs) in the fields of rescue, surveillance, traffic monitoring, etc. However, the increased tracking accuracy requires more energy consumption. In this letter, a decentralized vehicle tracking strategy is conceived for improving both tracking accuracy and energy saving, which is based on adjusting the intersection area between the fixed sensing area and the dynamic activation area. Then, two deep reinforcement learning (DRL) aided solutions are proposed relying on the dynamic selection of the activation area radius. Finally, simulation results show the superiority of our DRL aided design.",Oprstfoensainobtrsene,329.0,28.0,35.0
6401,Object detection,5.0,efficientdet: scalable and efficient object detection,5.0,5.0,5.0,11.0,5.0,5.0,6.8,1,https://openaccess.thecvf.com/content_CVPR_2020/papers/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.pdf,"Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x – 9x smaller and using 13x – 42x fewer FLOPs than previous detector.",Oefscanefobde,719.0,51.0,111.0
6402,Object detection,20.0,focal loss for dense object detection,5.0,17.0,5.0,1.0,5.0,5.0,13.1,2,https://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf,"The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.",Ofolofodeobde,5949.0,45.0,1091.0
6403,Object detection,23.0,feature pyramid networks for object detection,5.0,24.0,5.0,4.0,5.0,5.0,17.7,3,https://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf,"Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.",Ofepynefoobde,7390.0,49.0,1458.0
6404,Object detection,10.0,"you only look once: unified, real-time object detection",5.0,38.0,5.0,2.0,5.0,5.0,18.800000000000004,4,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf,"We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",Oyoonloonunreobde,13838.0,49.0,1777.0
6405,Object detection,27.0,faster r-cnn: towards real-time object detection with region proposal networks,5.0,26.0,5.0,3.0,5.0,5.0,19.4,5,https://proceedings.neurips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf,"State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available",Ofar-toreobdewireprne,27925.0,55.0,5280.0
6406,Object detection,54.0,yolov4: optimal speed and accuracy of object detection,4.0,2.0,5.0,7.0,5.0,4.7,19.1,6,https://arxiv.org/pdf/2004.10934,"There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of ~65 FPS on Tesla V100. Source code is at this https URL",Oyoopspanacofobde,1324.0,111.0,166.0
6407,Object detection,48.0,r-fcn: object detection via region-based fully convolutional networks,4.0,25.0,5.0,6.0,5.0,4.7,26.2,7,https://papers.nips.cc/paper/6464-r-fcn-object-detection-via-region-based-fully-convolutional-networks.pdf,"We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.",Or-obdevirefucone,3502.0,34.0,516.0
6408,Object detection,51.0,cascade r-cnn: delving into high quality object detection,4.0,74.0,4.0,13.0,5.0,4.3,48.8,8,http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf,"In object detection, an intersection over union (IoU) threshold is required to define positives and negatives. An object detector, trained with low IoU threshold, e.g. 0.5, usually produces noisy detections. However, detection performance tends to degrade with increasing the IoU thresholds. Two main factors are responsible for this: 1) overfitting during training, due to exponentially vanishing positive samples, and 2) inference-time mismatch between the IoUs for which the detector is optimal and those of the input hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, is proposed to address these problems. It consists of a sequence of detectors trained with increasing IoU thresholds, to be sequentially more selective against close false positives. The detectors are trained stage by stage, leveraging the observation that the output of a detector is a good distribution for training the next higher quality detector. The resampling of progressively improved hypotheses guarantees that all detectors have a positive set of examples of equivalent size, reducing the overfitting problem. The same cascade procedure is applied at inference, enabling a closer match between the hypotheses and the detector quality of each stage. A simple implementation of the Cascade R-CNN is shown to surpass all single-model object detectors on the challenging COCO dataset. Experiments also show that the Cascade R-CNN is widely applicable across detector architectures, achieving consistent gains independently of the baseline detector strength. The code is available at https://github.com/zhaoweicai/cascade-rcnn.",Ocar-deinhiquobde,1377.0,40.0,357.0
6409,Object detection,42.0,fcos: fully convolutional one-stage object detection,4.0,120.0,3.0,15.0,5.0,3.9,65.1,9,https://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf,"We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the pre-defined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at: https://tinyurl.com/FCOSv1",Ofcfucoonobde,980.0,39.0,231.0
6410,Object detection,53.0,centernet: keypoint triplets for object detection,4.0,186.0,3.0,28.0,5.0,3.9,98.70000000000002,10,https://openaccess.thecvf.com/content_ICCV_2019/papers/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.pdf,"In object detection, keypoint-based approaches often experience the drawback of a large number of incorrect object bounding boxes, arguably due to the lack of an additional assessment inside cropped regions. This paper presents an efficient solution that explores the visual patterns within individual cropped regions with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules, cascade corner pooling, and center pooling, that enrich information collected by both the top-left and bottom-right corners and provide more recognizable information from the central regions. On the MS-COCO dataset, CenterNet achieves an AP of 47.0 %, outperforming all existing one-stage detectors by at least 4.9%. Furthermore, with a faster inference speed than the top-ranked two-stage detectors, CenterNet demonstrates a comparable performance to these detectors. Code is available at https://github.com/Duankaiwen/CenterNet.",Oceketrfoobde,596.0,56.0,57.0
6411,Object detection,74.0,pointpillars: fast encoders for object detection from point clouds,4.0,196.0,3.0,20.0,5.0,3.9,106.6,11,https://openaccess.thecvf.com/content_CVPR_2019/papers/Lang_PointPillars_Fast_Encoders_for_Object_Detection_From_Point_Clouds_CVPR_2019_paper.pdf,"Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird’s eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.",Opofaenfoobdefrpocl,592.0,37.0,155.0
6412,Object detection,401.0,end-to-end object detection with transformers,1.0,20.0,5.0,8.0,5.0,3.8,130.70000000000002,12,http://arxiv.org/pdf/2106.03146v1,"We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.",Oenobdewitr,962.0,55.0,174.0
6413,Object detection,105.0,voxelnet: end-to-end learning for point cloud based 3d object detection,3.0,143.0,3.0,19.0,5.0,3.6,94.4,13,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_VoxelNet_End-to-End_Learning_CVPR_2018_paper.pdf,"Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.",Ovoenlefopoclba3dobde,1228.0,51.0,196.0
6414,Object detection,113.0,rich feature hierarchies for accurate object detection and semantic segmentation,3.0,174.0,3.0,5.0,5.0,3.6,105.0,14,https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf,"Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",Orifehifoacobdeansese,15395.0,64.0,2056.0
6415,Object detection,401.0,scale-aware trident networks for object detection,1.0,31.0,5.0,63.0,4.0,3.5,151.6,15,http://arxiv.org/pdf/2110.04004v1,"Scale variation is one of the key challenges in object detection. In this work, we ﬁrst present a controlled experiment to investigate the effect of receptive ﬁelds for scale variation in object detection. Based on the ﬁndings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-speciﬁc feature maps with a uniform representational power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive ﬁelds. Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve signiﬁcant improvements without any additional parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP. Codes are available at https://git.io/fj5vR.",Osctrnefoobde,318.0,48.0,19.0
6416,Object detection,401.0,libra r-cnn: towards balanced learning for object detection,1.0,66.0,4.0,25.0,5.0,3.4000000000000004,154.2,16,http://arxiv.org/pdf/1904.02701v1,"Compared with model architectures, the training process, which is also crucial to the success of detectors, has received relatively less attention in object detection. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels - sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple but effective framework towards balanced learning for object detection. It integrates three novel components: IoU-balanced sampling, balanced feature pyramid, and balanced L1 loss, respectively for reducing the imbalance at sample, feature, and objective level. Benefitted from the overall balanced design, Libra R-CNN significantly improves the detection performance. Without bells and whistles, it achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN Faster R-CNN and RetinaNet respectively on MSCOCO.",Olir-tobalefoobde,369.0,39.0,67.0
6417,Object detection,7.0,object detection with deep learning: a review,5.0,201.0,1.0,12.0,5.0,3.4,86.1,17,https://arxiv.org/pdf/1807.05511.pdf&usg=ALkJrhhpApwNJOmg83O8p2Ua76PNh6tR8A,"Due to object detection’s close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.",Oobdewideleare,1229.0,245.0,54.0
6418,Object detection,116.0,dota: a large-scale dataset for object detection in aerial images,3.0,125.0,3.0,49.0,4.0,3.3,99.5,18,http://openaccess.thecvf.com/content_cvpr_2018/papers/Xia_DOTA_A_Large-Scale_CVPR_2018_paper.pdf,"Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000 Ã— 4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188, 282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.",Odoaladafoobdeinaeim,566.0,46.0,141.0
6419,Object detection,137.0,cornernet-lite: efficient keypoint based object detection,3.0,118.0,3.0,41.0,4.0,3.3,100.6,19,https://arxiv.org/pdf/1904.08900,"Keypoint-based methods are a relatively new paradigm in object detection, eliminating the need for anchor boxes and offering a simplified detection framework. Keypoint-based CornerNet achieves state of the art accuracy among single-stage detectors. However, this accuracy comes at high processing cost. In this work, we tackle the problem of efficient keypoint-based object detection and introduce CornerNet-Lite. CornerNet-Lite is a combination of two efficient variants of CornerNet: CornerNet-Saccade, which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image, and CornerNet-Squeeze, which introduces a new compact backbone architecture. Together these two variants address the two critical use cases in efficient object detection: improving efficiency without sacrificing accuracy, and improving accuracy at real-time efficiency. CornerNet-Saccade is suitable for offline processing, improving the efficiency of CornerNet by 6.0x and the AP by 1.0% on COCO. CornerNet-Squeeze is suitable for real-time detection, improving both the efficiency and accuracy of the popular real-time detector YOLOv3 (34.4% AP at 30ms for CornerNet-Squeeze compared to 33.0% AP at 39ms for YOLOv3 on COCO). Together these contributions for the first time reveal the potential of keypoint-based detection to be useful for applications requiring processing efficiency.",Ocoefkebaobde,86.0,73.0,9.0
6420,Object detection,125.0,frustum pointnets for 3d object detection from rgb-d data,3.0,142.0,3.0,45.0,4.0,3.3,107.8,20,https://openaccess.thecvf.com/content_cvpr_2018/papers/Qi_Frustum_PointNets_for_CVPR_2018_paper.pdf,"In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.",Ofrpofo3dobdefrrgda,1050.0,44.0,186.0
6421,Object detection,11.0,deep learning for generic object detection: a survey,5.0,201.0,1.0,46.0,4.0,3.1,97.5,21,http://arxiv.org/pdf/1908.03673v1,"Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.",Odelefogeobdeasu,808.0,435.0,28.0
6422,Object detection,2.0,relation networks for object detection,5.0,201.0,1.0,59.0,4.0,3.1,98.7,22,https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Relation_Networks_for_CVPR_2018_paper.pdf,"Although it is well believed for years that modeling relations between objects would help object recognition, there has not been evidence that the idea is working in the deep learning era. All state-of-the-art object detection systems still rely on recognizing object instances individually, without exploiting their relations during learning. This work proposes an object relation module. It processes a set of objects simultaneously through interaction between their appearance feature and geometry, thus allowing modeling of their relations. It is lightweight and in-place. It does not require additional supervision and is easy to embed in existing networks. It is shown effective on improving object recognition and duplicate removal steps in the modern object detection pipeline. It verifies the efficacy of modeling object relations in CNN based detection. It gives rise to the first fully end-to-end object detector.",Orenefoobde,569.0,62.0,46.0
6423,Object detection,19.0,salient object detection: a survey,5.0,201.0,1.0,47.0,4.0,3.1,100.2,23,http://arxiv.org/pdf/2105.03053v1,"Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.",Osaobdeasu,428.0,474.0,30.0
6424,Object detection,49.0,rethinking classification and localization for object detection,4.0,63.0,4.0,201.0,1.0,3.1,100.2,24,http://openaccess.thecvf.com/content_CVPR_2020/papers/Wu_Rethinking_Classification_and_Localization_for_Object_Detection_CVPR_2020_paper.pdf,"Recently, it was found that many real-world examples without intentional modifications can fool machine learning models, and such examples are called ""natural adversarial examples"". ImageNet-A is a famous dataset of natural adversarial examples. By analyzing this dataset, we hypothesized that large, cluttered and/or unusual background is an important reason why the images in this dataset are difficult to be classified. We validated the hypothesis by reducing the background influence in ImageNet-A examples with object detection techniques. Experiments showed that the object detection models with various classification models as backbones obtained much higher accuracy than their corresponding classification models. A detection model based on the classification model EfficientNet-B7 achieved a top-1 accuracy of 53.95%, surpassing previous state-of-the-art classification models trained on ImageNet, suggesting that accurate localization information can significantly boost the performance of classification models on ImageNet-A. We then manually cropped the objects in images from ImageNet-A and created a new dataset, named ImageNet-A-Plus. A human test on the new dataset showed that the deep learning-based classifiers still performed quite poorly compared with humans. Therefore, the new dataset can be used to study the robustness of classification models to the internal variance of objects without considering the background disturbance.",Oreclanlofoobde,71.0,50.0,7.0
6425,Object detection,67.0,rapid object detection using a boosted cascade of simple features,4.0,201.0,1.0,14.0,5.0,3.1,104.7,25,https://merl.com/publications/docs/TR2004-043.pdf,"This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the ""integral image"" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a ""cascade"" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.",Oraobdeusabocaofsife,17457.0,25.0,1674.0
6426,Object detection,34.0,joint 3d proposal generation and object detection from view aggregation,5.0,201.0,1.0,50.0,4.0,3.1,105.6,26,https://arxiv.org/pdf/1712.02294,"We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at",Ojo3dprgeanobdefrviag,633.0,32.0,118.0
6427,Object detection,4.0,robust real-time object detection,5.0,201.0,1.0,86.0,4.0,3.1,107.4,27,https://www.hpl.hp.com/techreports/Compaq-DEC/CRL-2001-1.pdf,"This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [4]. The third contribution is a method for combining classifiers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performance comparable to the best previous systems [16, 11, 14, 10, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second. Author email: fPaul.Viola,Mike.J.Jonesg@compaq.com c Compaq Computer Corporation, 2001 This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of the Cambridge Research Laboratory of Compaq Computer Corporation in Cambridge, Massachusetts; an acknowledgment of the authors and individual contributors to the work; and all applicable portions of the copyright notice. Copying, reproducing, or republishing for any other purpose shall require a license with payment of fee to the Cambridge Research Laboratory. All rights reserved. CRL Technical reports are available on the CRL’s web page at http://crl.research.compaq.com. Compaq Computer Corporation Cambridge Research Laboratory One Cambridge Center Cambridge, Massachusetts 02142 USA",Ororeobde,3922.0,22.0,470.0
6428,Object detection,16.0,fast feature pyramids for object detection,5.0,201.0,1.0,77.0,4.0,3.1,108.3,28,https://authors.library.caltech.edu/49239/7/DollarPAMI14pyramids_0.pdf,"Multi-resolution image features may be approximated via extrapolation from nearby scales, rather than being computed explicitly. This fundamental insight allows us to design object detection algorithms that are as accurate, and considerably faster, than the state-of-the-art. The computational bottleneck of many modern detectors is the computation of features at every scale of a finely-sampled image pyramid. Our key insight is that one may compute finely sampled feature pyramids at a fraction of the cost, without sacrificing performance: for a broad family of features we find that features computed at octave-spaced scale intervals are sufficient to approximate features on a finely-sampled pyramid. Extrapolation is inexpensive as compared to direct feature computation. As a result, our approximation yields considerable speedups with negligible loss in detection accuracy. We modify three diverse visual recognition systems to use fast feature pyramids and show results on both pedestrian detection (measured on the Caltech, INRIA, TUD-Brussels and ETH data sets) and general object detection (measured on the PASCAL VOC). The approach is general and is widely applicable to vision algorithms requiring fine-grained multi-scale analysis. Our approximation is valid for images with broad spectra (most natural images) and fails for images with narrow band-pass spectra (e.g., periodic textures).",Ofafepyfoobde,1638.0,80.0,264.0
6429,Object detection,71.0,strong-weak distribution alignment for adaptive object detection,4.0,201.0,1.0,34.0,5.0,3.1,111.9,29,https://openaccess.thecvf.com/content_CVPR_2019/papers/Saito_Strong-Weak_Distribution_Alignment_for_Adaptive_Object_Detection_CVPR_2019_paper.pdf,"We propose an approach for unsupervised adaptation of object detectors from label-rich to label-poor domains which can significantly reduce annotation costs associated with detection. Recently, approaches that align distributions of source and target images using an adversarial loss have been proven effective for adapting object classifiers. However, for object detection, fully matching the entire distributions of source and target images to each other at the global image level may fail, as domains could have distinct scene layouts and different combinations of objects. On the other hand, strong matching of local features such as texture and color makes sense, as it does not change category level semantics. This motivates us to propose a novel method for detector adaptation based on strong local alignment and weak global alignment. Our key contribution is the weak alignment model, which focuses the adversarial alignment loss on images that are globally similar and puts less emphasis on aligning images that are globally dissimilar. Additionally, we design the strong domain alignment model to only look at local receptive fields of the feature map. We empirically verify the effectiveness of our method on four datasets comprising both large and small domain shifts. Our code is available at https://github.com/VisionLearningGroup/DA_Detection.",Ostdialfoadobde,156.0,45.0,70.0
6430,Object detection,21.0,salient object detection: a benchmark,5.0,201.0,1.0,87.0,4.0,3.1,112.8,30,https://link.springer.com/content/pdf/10.1007/978-3-642-33709-3_30.pdf,"We extensively compare, qualitatively and quantitatively, 41 state-of-the-art models (29 salient object detection, 10 fixation prediction, 1 objectness, and 1 baseline) over seven challenging data sets for the purpose of benchmarking salient object detection and segmentation methods. From the results obtained so far, our evaluation shows a consistent rapid progress over the last few years in terms of both accuracy and running time. The top contenders in this benchmark significantly outperform the models identified as the best in the previous benchmark conducted three years ago. We find that the models designed specifically for salient object detection generally work better than models in closely related areas, which in turn provides a precise definition and suggests an appropriate treatment of this problem that distinguishes it from other problems. In particular, we analyze the influences of center bias and scene complexity in model performance, which, along with the hard cases for the state-of-the-art models, provide useful hints toward constructing more challenging large-scale data sets and better saliency models. Finally, we propose probable solutions for tackling several open problems, such as evaluation scores and data set bias, which also suggest future research directions in the rapidly growing field of salient object detection.",Osaobdeabe,1282.0,186.0,123.0
6431,Object detection,26.0,detnas: backbone search for object detection,5.0,201.0,1.0,92.0,4.0,3.1,115.8,31,http://papers.nips.cc/paper/8890-detnas-backbone-search-for-object-detection.pdf,"Object detectors are usually equipped with backbone networks designed for image classification. It might be sub-optimal because of the gap between the tasks of image classification and object detection. In this work, we present DetNAS to use Neural Architecture Search (NAS) for the design of better backbones for object detection. It is non-trivial because detection training typically needs ImageNet pre-training while NAS systems require accuracies on the target detection task as supervisory signals. Based on the technique of one-shot supernet, which contains all possible networks in the search space, we propose a framework for backbone search on object detection. We train the supernet under the typical detector training schedule: ImageNet pre-training and detection fine-tuning. Then, the architecture search is performed on the trained supernet, using the detection task as the guidance. This framework makes NAS on backbones very efficient. In experiments, we show the effectiveness of DetNAS on various detectors, for instance, one-stage RetinaNet and the two-stage FPN. We empirically find that networks searched on object detection shows consistent superiority compared to those searched on ImageNet classification. The resulting architecture achieves superior performance than hand-crafted networks on COCO with much less FLOPs complexity.",Odebasefoobde,109.0,38.0,4.0
6432,Object detection,401.0,feature selective anchor-free module for single-shot object detection,1.0,67.0,4.0,79.0,4.0,3.1,170.79999999999998,32,http://arxiv.org/pdf/1903.00621v1,"We motivate and present feature selective anchor-free (FSAF) module, a simple and effective building block for single-shot object detectors. It can be plugged into single-shot detectors with feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. The general concept of the FSAF module is online feature selection applied to the training of multi-level anchor-free branches. Specifically, an anchor-free branch is attached to each level of the feature pyramid, allowing box encoding and decoding in the anchor-free manner at an arbitrary level. During training, we dynamically assign each instance to the most suitable feature level. At the time of inference, the FSAF module can work independently or jointly with anchor-based branches. We instantiate this concept with simple implementations of anchor-free branches and online feature selection strategy. Experimental results on the COCO detection track show that our FSAF module performs better than anchor-based counterparts while being faster. When working jointly with anchor-based branches, the FSAF module robustly improves the baseline RetinaNet by a large margin under various settings, while introducing nearly free inference overhead. And the resulting best model can achieve a state-of-the-art 44.6% mAP, outperforming all existing single-shot detectors on COCO.",Ofeseanmofosiobde,288.0,48.0,24.0
6433,Object detection,29.0,a survey on performance metrics for object-detection algorithms,5.0,136.0,3.0,201.0,1.0,3.0,123.4,33,https://www.researchgate.net/profile/Rafael-Padilla/publication/343194514_A_Survey_on_Performance_Metrics_for_Object-Detection_Algorithms/links/5f1b5a5e45851515ef478268/A-Survey-on-Performance-Metrics-for-Object-Detection-Algorithms.pdf,"Airspace sectorisation provides a partition of a given airspace into sectors, subject to geometric constraints and workload constraints, so that some cost metric is minimised. We survey the algorithmic aspects of methods for automatic airspace sectorisation, for an intended readership of experts on air traffic management.",Oasuonpemefoobal,78.0,48.0,7.0
6434,Object detection,24.0,scalable object detection using deep neural networks,5.0,173.0,3.0,201.0,1.0,3.0,136.7,34,http://openaccess.thecvf.com/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf,"Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations.",Oscobdeusdenene,906.0,20.0,52.0
6435,Object detection,150.0,spatial memory for context reasoning in object detection,3.0,138.0,3.0,188.0,3.0,3.0,156.6,35,https://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Spatial_Memory_for_ICCV_2017_paper.pdf,"Modeling instance-level context and object-object relationships is extremely challenging. It requires reasoning about bounding boxes of different classes, locations etc. Above all, instance-level spatial reasoning inherently requires modeling conditional distributions on previous detections. Unfortunately, our current object detection systems do not have any memory to remember what to condition on! The state-of-the-art object detectors still detect all object in parallel followed by non-maximal suppression (NMS). While memory has been used for tasks such as captioning, they mostly use image-level memory cells without capturing the spatial layout. On the other hand, modeling object-object relationships requires spatial reasoning – not only do we need a memory to store the spatial layout, but also a effective reasoning module to extract spatial patterns. This paper presents a conceptually simple yet powerful solution – Spatial Memory Network (SMN), to model the instance-level context efficiently and effectively. Our spatial memory essentially assembles object instances back into a pseudo “image” representation that is easy to be fed into another ConvNet for object-object context reasoning. This leads to a new sequential reasoning architecture where image and memory are processed in parallel to obtain detections which update the memory again. We show our SMN direction is promising as it provides 2.2% improvement over baseline Faster RCNN on the COCO dataset with VGG161.",Ospmefocoreinobde,109.0,105.0,10.0
6436,Object detection,401.0,learning data augmentation strategies for object detection,1.0,131.0,3.0,22.0,5.0,3.0,179.29999999999998,36,http://arxiv.org/pdf/1906.11172v1,"Data augmentation is a critical component of training deep learning models. Although data augmentation has been shown to significantly improve image classification, its potential has not been thoroughly investigated for object detection. Given the additional cost for annotating images for object detection, data augmentation may be of even greater importance for this computer vision task. In this work, we study the impact of data augmentation on object detection. We first demonstrate that data augmentation operations borrowed from image classification may be helpful for training detection models, but the improvement is limited. Thus, we investigate how learned, specialized data augmentation policies improve generalization performance for detection models. Importantly, these augmentation policies only affect training and leave a trained model unchanged during evaluation. Experiments on the COCO dataset indicate that an optimized data augmentation policy improves detection accuracy by more than +2.3 mAP, and allow a single inference model to achieve a state-of-the-art accuracy of 50.7 mAP. Importantly, the best policy found on COCO may be transferred unchanged to other detection datasets and models to improve predictive accuracy. For example, the best augmentation policy identified with COCO improves a strong baseline on PASCAL-VOC by +2.7 mAP. Our results also reveal that a learned augmentation policy is superior to state-of-the-art architecture regularization methods for object detection, even when considering strong baselines. Code for training with the learned policy is available online at this https URL",Oledaaustfoobde,156.0,67.0,8.0
6437,Object detection,401.0,deep hough voting for 3d object detection in point clouds,1.0,193.0,3.0,29.0,5.0,3.0,206.2,37,http://arxiv.org/pdf/1904.09664v2,"Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.",Odehovofo3dobdeinpocl,305.0,63.0,63.0
6438,Object detection,43.0,single-shot refinement neural network for object detection,4.0,201.0,1.0,54.0,4.0,2.8,109.5,38,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.pdf,"For object detection, the two-stage approach (e.g., Faster R-CNN) has been achieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has the advantage of high efficiency. To inherit the merits of both while overcoming their disadvantages, in this paper, we propose a novel single-shot based detector, called RefineDet, that achieves better accuracy than two-stage methods and maintains comparable efficiency of one-stage methods. RefineDet consists of two inter-connected modules, namely, the anchor refinement module and the object detection module. Specifically, the former aims to (1) filter out negative anchors to reduce search space for the classifier, and (2) coarsely adjust the locations and sizes of anchors to provide better initialization for the subsequent regressor. The latter module takes the refined anchors as the input from the former to further improve the regression accuracy and predict multi-class label. Meanwhile, we design a transfer connection block to transfer the features in the anchor refinement module to predict locations, sizes and class labels of objects in the object detection module. The multitask loss function enables us to train the whole network in an end-to-end way. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO demonstrate that RefineDet achieves state-of-the-art detection accuracy with high efficiency. Code is available at https://github.com/sfzhang15/RefineDet.",Osirenenefoobde,703.0,65.0,100.0
6439,Object detection,45.0,a unified multi-scale deep convolutional neural network for fast object detection,4.0,201.0,1.0,78.0,4.0,2.8,117.3,39,http://arxiv.org/pdf/1607.07155v1,"A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.",Oaunmudeconenefofaobde,1049.0,43.0,125.0
6440,Object detection,90.0,region-based convolutional networks for accurate object detection and segmentation,4.0,201.0,1.0,48.0,4.0,2.8,121.8,40,https://ieeexplore.ieee.org/ielaam/34/7346524/7112511-aam.pdf,"Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",Oreconefoacobdeanse,1414.0,85.0,115.0
6441,Object detection,9.0,hypernet: towards accurate region proposal generation and joint object detection,5.0,201.0,1.0,131.0,3.0,2.8,122.4,41,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Kong_HyperNet_Towards_Accurate_CVPR_2016_paper.pdf,"Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.",Ohytoacreprgeanjoobde,564.0,40.0,36.0
6442,Object detection,12.0,detnet: design backbone for object detection,5.0,201.0,1.0,153.0,3.0,2.8,129.9,42,https://openaccess.thecvf.com/content_ECCV_2018/papers/Zeming_Li_DetNet_Design_Backbone_ECCV_2018_paper.pdf,"Recent CNN based object detectors, either one-stage methods like YOLO, SSD, and RetinaNet, or two-stage detectors like Faster R-CNN, R-FCN and FPN, are usually trying to directly finetune from ImageNet pre-trained models designed for the task of image classification. However, there has been little work discussing the backbone feature extractor specifically designed for the task of object detection. More importantly, there are several differences between the tasks of image classification and object detection. (i) Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. (ii) Object detection not only needs to recognize the category of the object instances but also spatially locate them. Large downsampling factors bring large valid receptive field, which is good for image classification, but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet (4.8G FLOPs) backbone. Codes will be released (https://github.com/zengarden/DetNet).",Odedebafoobde,116.0,42.0,9.0
6443,Object detection,140.0,multi-view 3d object detection network for autonomous driving,3.0,201.0,1.0,27.0,5.0,2.8,130.5,43,https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf,"This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the birds eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.",Omu3dobdenefoaudr,1211.0,43.0,178.0
6444,Object detection,18.0,application of deep learning for object detection,5.0,201.0,1.0,170.0,3.0,2.8,136.8,44,https://www.sciencedirect.com/science/article/pii/S1877050918308767/pdf?md5=d31a961271e33c7dc31f12bb04bb66ae&pid=1-s2.0-S1877050918308767-main.pdf,"Abstract The ubiquitous and wide applications like scene understanding, video surveillance, robotics, and self-driving systems triggered vast research in the domain of computer vision in the most recent decade. Being the core of all these applications, visual recognition systems which encompasses image classification, localization and detection have achieved great research momentum. Due to significant development in neural networks especially deep learning, these visual recognition systems have attained remarkable performance. Object detection is one of these domains witnessing great success in computer vision. This paper demystifies the role of deep learning techniques based on convolutional neural network for object detection. Deep learning frameworks and services available for object detection are also enunciated. Deep learning techniques for state-of-the-art object detection systems are assessed in this paper.",Oapofdelefoobde,66.0,50.0,0.0
6445,Object detection,168.0,few-shot object detection with attention-rpn and multi-relation detector,3.0,201.0,1.0,24.0,5.0,2.8,138.0,45,https://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Few-Shot_Object_Detection_With_Attention-RPN_and_Multi-Relation_Detector_CVPR_2020_paper.pdf,"Conventional methods for object detection typically require a substantial amount of training data and preparing such high-quality training data is very labor-intensive. In this paper, we propose a novel few-shot object detection network that aims at detecting objects of unseen categories with only a few annotated examples. Central to our method are our Attention-RPN, Multi-Relation Detector and Contrastive Training strategy, which exploit the similarity between the few shot support set and query set to detect novel objects while suppressing false detection in the background. To train our network, we contribute a new dataset that contains 1000 categories of various objects with high-quality annotations. To the best of our knowledge, this is one of the first datasets specifically designed for few-shot object detection. Once our few-shot network is trained, it can detect objects of unseen categories without further training or fine-tuning. Our method is general and has a wide range of potential applications. We produce a new state-of-the-art performance on different datasets in the few-shot setting. The dataset link is https://github.com/fanq15/Few-Shot-Object-Detection-Dataset.",Ofeobdewiatanmude,111.0,68.0,21.0
6446,Object detection,31.0,deep regionlets for object detection,5.0,201.0,1.0,169.0,3.0,2.8,140.4,46,http://openaccess.thecvf.com/content_ECCV_2018/papers/Hongyu_Xu_Deep_Regionlets_for_ECCV_2018_paper.pdf,"A key challenge in generic object detection is being to handle large variations in object scale, poses, viewpoints, especially part deformations when determining the location for specified object categories. Recent advances in deep neural networks have achieved promising results for object detection by extending the traditional detection methodologies using the convolutional neural network architectures. In this paper, we make an attempt to incorporate another traditional detection schema, Regionlet into an end-to-end trained deep learning framework, and perform ablation studies on its behavior on multiple object detection datasets. More specifically, we propose a ""region selection network"" and a ""gating network"". The region selection network serves as a guidance on where to select regions to learn the features from. Additionally, the gating network serves as a local feature selection module to select and transform feature maps to be suitable for detection task. It acts as soft Regionlet selection and pooling. The proposed network is trained end-to-end without additional efforts. Extensive experiments and analysis on the PASCAL VOC dataset and Microsoft COCO dataset show that the proposed framework achieves comparable state-of-the-art results.",Oderefoobde,57.0,63.0,3.0
6447,Object detection,39.0,context refinement for object detection,5.0,201.0,1.0,178.0,3.0,2.8,145.5,47,http://openaccess.thecvf.com/content_ECCV_2018/papers/Zhe_Chen_Context_Refinement_for_ECCV_2018_paper.pdf,"Current two-stage object detectors, which consists of a region proposal stage and a refinement stage, may produce unreliable results due to ill-localized proposed regions. To address this problem, we propose a context refinement algorithm that explores rich contextual information to better refine each proposed region. In particular, we first identify neighboring regions that may contain useful contexts and then perform refinement based on the extracted and unified contextual information. In practice, our method effectively improves the quality of the final detection results as well as region proposals. Empirical studies show that context refinement yields substantial and consistent improvements over different baseline detectors. Moreover, the proposed algorithm brings around 3% performance gain on PASCAL VOC benchmark and around 6% gain on MS COCO benchmark respectively.",Ocorefoobde,58.0,39.0,2.0
6448,Object detection,37.0,imbalance problems in object detection: a review,5.0,201.0,1.0,197.0,3.0,2.8,150.6,48,https://arxiv.org/pdf/1909.00169,"In this paper, we present a comprehensive review of the imbalance problems in object detection. To analyze the problems in a systematic manner, we introduce a problem-based taxonomy. Following this taxonomy, we discuss each problem in depth and present a unifying yet critical perspective on the solutions in the literature. In addition, we identify major open issues regarding the existing imbalance problems as well as imbalance problems that have not been discussed before. Moreover, in order to keep our review up to date, we provide an accompanying webpage which catalogs papers addressing imbalance problems, according to our problem-based taxonomy. Researchers can track newer studies on this webpage available at: https://github.com/kemaloksuz/ObjectDetectionImbalance.",Oimprinobdeare,107.0,187.0,4.0
6449,Object detection,401.0,deformable detr: deformable transformers for end-to-end object detection,1.0,43.0,4.0,113.0,3.0,2.8,171.4,49,https://arxiv.org/pdf/2010.04159,"DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released.",Odededetrfoenobde,268.0,51.0,55.0
6450,Object detection,78.0,bag of freebies for training object detection neural networks,4.0,103.0,3.0,201.0,1.0,2.7,124.9,50,https://arxiv.org/pdf/1902.04103,"Training heuristics greatly improve various image classification model accuracies~\cite{he2018bag}. Object detection models, however, have more complex neural network structures and optimization targets. The training strategies and pipelines dramatically vary among different models. In this works, we explore training tweaks that apply to various models including Faster R-CNN and YOLOv3. These tweaks do not change the model architectures, therefore, the inference costs remain the same. Our empirical results demonstrate that, however, these freebies can improve up to 5% absolute precision compared to state-of-the-art baselines.",Obaoffrfotrobdenene,80.0,27.0,4.0
6451,Object detection,401.0,flow-guided feature aggregation for video object detection,1.0,199.0,3.0,82.0,4.0,2.7,224.5,51,http://arxiv.org/pdf/1703.10025v2,"Extending state-of-the-art object detectors from image to video is challenging. The accuracy of detection suffers from degenerated object appearances in videos, e.g., motion blur, video defocus, rare poses, etc. Existing work attempts to exploit temporal information on box level, but such methods are not trained end-to-end. We present flow-guided feature aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead. It improves the per-frame features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy. Our method significantly improves upon strong singleframe baselines in ImageNet VID [33], especially for more challenging fast moving objects. Our framework is principled, and on par with the best engineered systems winning the ImageNet VID challenges 2016, without additional bells-and-whistles. The code would be released.",Oflfeagfoviobde,323.0,58.0,70.0
6452,Object detection,401.0,context r-cnn: long term temporal context for per-camera object detection,1.0,3.0,5.0,201.0,1.0,2.6,181.8,52,http://arxiv.org/pdf/1807.02842v1,"Region-based convolutional neural networks (R-CNN)~\cite{fast_rcnn,faster_rcnn,mask_rcnn} have largely dominated object detection. Operators defined on RoIs (Region of Interests) play an important role in R-CNNs such as RoIPooling~\cite{fast_rcnn} and RoIAlign~\cite{mask_rcnn}. They all only utilize information inside RoIs for RoI prediction, even with their recent deformable extensions~\cite{deformable_cnn}. Although surrounding context is well-known for its importance in object detection, it has yet been integrated in R-CNNs in a flexible and effective way. Inspired by the auto-context work~\cite{auto_context} and the multi-class object layout work~\cite{nms_context}, this paper presents a generic context-mining RoI operator (i.e., \textit{RoICtxMining}) seamlessly integrated in R-CNNs, and the resulting object detection system is termed \textbf{Auto-Context R-CNN} which is trained end-to-end. The proposed RoICtxMining operator is a simple yet effective two-layer extension of the RoIPooling or RoIAlign operator. Centered at an object-RoI, it creates a $3\times 3$ layout to mine contextual information adaptively in the $8$ surrounding context regions on-the-fly. Within each of the $8$ context regions, a context-RoI is mined in term of discriminative power and its RoIPooling / RoIAlign features are concatenated with the object-RoI for final prediction. \textit{The proposed Auto-Context R-CNN is robust to occlusion and small objects, and shows promising vulnerability for adversarial attacks without being adversarially-trained.} In experiments, it is evaluated using RoIPooling as the backbone and shows competitive results on Pascal VOC, Microsoft COCO, and KITTI datasets (including $6.9\%$ mAP improvements over the R-FCN~\cite{rfcn} method on COCO \textit{test-dev} dataset and the first place on both KITTI pedestrian and cyclist detection as of this submission).",Ocor-lotetecofopeobde,31.0,63.0,3.0
6453,Object detection,401.0,searching for mobilenetv3,1.0,6.0,5.0,201.0,1.0,2.6,183.0,53,http://arxiv.org/pdf/1905.02244v5,"We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",Osefomo,1040.0,61.0,208.0
6454,Object detection,401.0,objects as points,1.0,7.0,5.0,201.0,1.0,2.6,183.4,54,http://arxiv.org/pdf/2104.06041v1,"Image-only and pseudo-LiDAR representations are commonly used for monocular 3D object detection. However, methods based on them have shortcomings of either not well capturing the spatial relationships in neighbored image pixels or being hard to handle the noisy nature of the monocular pseudo-LiDAR point cloud. To overcome these issues, in this paper we propose a novel object-centric voxel representation tailored for monocular 3D object detection. Specifically, voxels are built on each object proposal, and their sizes are adaptively determined by the 3D spatial distribution of the points, allowing the noisy point cloud to be organized effectively within a voxel grid. This representation is proved to be able to locate the object in 3D space accurately. Furthermore, prior works would like to estimate the orientation via deep features extracted from an entire image or a noisy point cloud. By contrast, we argue that the local RoI information from the object image patch alone with a proper resizing scheme is a better input as it provides complete semantic clues meanwhile excludes irrelevant interferences. Besides, we decompose the confidence mechanism in monocular 3D object detection by considering the relationship between 3D objects and the associated 2D boxes. Evaluated on KITTI, our method outperforms state-of-the-art methods by a large margin. The code will be made publicly available soon.",Oobaspo,746.0,65.0,177.0
6455,Object detection,401.0,looking fast and slow: memory-guided mobile video object detection,1.0,8.0,5.0,201.0,1.0,2.6,183.8,55,http://arxiv.org/pdf/1903.10172v1,"With a single eye fixation lasting a fraction of a second, the human visual system is capable of forming a rich representation of a complex environment, reaching a holistic understanding which facilitates object recognition and detection. This phenomenon is known as recognizing the ""gist"" of the scene and is accomplished by relying on relevant prior knowledge. This paper addresses the analogous question of whether using memory in computer vision systems can not only improve the accuracy of object detection in video streams, but also reduce the computation time. By interleaving conventional feature extractors with extremely lightweight ones which only need to recognize the gist of the scene, we show that minimal computation is required to produce accurate detections when temporal memory is present. In addition, we show that the memory contains enough information for deploying reinforcement learning algorithms to learn an adaptive inference policy. Our model achieves state-of-the-art performance among mobile methods on the Imagenet VID 2015 dataset, while running at speeds of up to 70+ FPS on a Pixel 3 phone.",Olofaanslmemoviobde,35.0,42.0,3.0
6456,Object detection,401.0,speed/accuracy trade-offs for modern convolutional object detectors,1.0,11.0,5.0,201.0,1.0,2.6,185.0,56,http://arxiv.org/pdf/1611.10012v3,"The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016] and SSD [Liu et al., 2015] systems, which we view as ""meta-architectures"" and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.",Osptrfomocoobde,1839.0,59.0,202.0
6457,Object detection,401.0,deep residual learning for image recognition,1.0,12.0,5.0,201.0,1.0,2.6,185.4,57,http://arxiv.org/pdf/1612.05400v1,"Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks.   Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.",Oderelefoimre,79331.0,61.0,15629.0
6458,Object detection,401.0,going deeper with convolutions,1.0,13.0,5.0,201.0,1.0,2.6,185.8,58,http://arxiv.org/pdf/1409.4842v1,"We propose a deep convolutional neural network architecture codenamed ""Inception"", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",Ogodewico,26591.0,277.0,2983.0
6459,Object detection,401.0,pooling pyramid network for object detection,1.0,15.0,5.0,201.0,1.0,2.6,186.6,59,http://arxiv.org/pdf/2106.12011v3,"This paper jointly resolves two problems in vision transformer: i) the computation of Multi-Head Self-Attention (MHSA) has high computational/space complexity; ii) recent vision transformer networks are overly tuned for image classification, ignoring the difference between image classification (simple scenarios, more similar to NLP) and downstream scene understanding tasks (complicated scenarios, rich structural and contextual information). To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction, and its natural property of spatial invariance is also suitable to address the loss of structural information (problem ii)). Hence, we propose to adapt pyramid pooling to MHSA for alleviating its high requirement on computational resources (problem i)). In this way, this pooling-based MHSA can well address the above two problems and is thus flexible and powerful for downstream scene understanding tasks. Plugged with our pooling-based MHSA, we build a downstream-task-oriented transformer network, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as the backbone network, it shows substantial superiority in various downstream scene understanding tasks such as semantic segmentation, object detection, instance segmentation, and visual saliency detection, compared to previous CNN- and transformer-based networks. The code will be released at https://github.com/yuhuan-wu/P2T.",Opopynefoobde,10.0,5.0,2.0
6460,Object detection,401.0,mobile video object detection with temporally-aware feature maps,1.0,16.0,5.0,201.0,1.0,2.6,187.0,60,http://arxiv.org/pdf/1711.06368v2,"This paper introduces an online model for object detection in videos designed to run in real-time on low-powered mobile and embedded devices. Our approach combines fast single-image object detection with convolutional long short term memory (LSTM) layers to create an interweaved recurrent-convolutional architecture. Additionally, we propose an efficient Bottleneck-LSTM layer that significantly reduces computational cost compared to regular LSTMs. Our network achieves temporal awareness by using Bottleneck-LSTMs to refine and propagate feature maps across frames. This approach is substantially faster than existing detection methods in video, outperforming the fastest single-frame models in model size and computational cost while attaining accuracy comparable to much more expensive single-frame models on the Imagenet VID 2015 dataset. Our model reaches a real-time inference speed of up to 15 FPS on a mobile CPU.",Omoviobdewitefema,108.0,44.0,18.0
6461,Object detection,401.0,mask r-cnn,1.0,18.0,5.0,201.0,1.0,2.6,187.8,61,http://arxiv.org/pdf/1703.06870v3,"We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron",Omar-,7336.0,52.0,1571.0
6462,Object detection,401.0,group normalization,1.0,21.0,5.0,201.0,1.0,2.6,189.0,62,http://arxiv.org/pdf/1803.08494v3,"Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.",Ogrno,1274.0,75.0,83.0
6463,Object detection,401.0,data distillation: towards omni-supervised learning,1.0,22.0,5.0,201.0,1.0,2.6,189.4,63,http://arxiv.org/abs/1806.03971v1,"Technology is generating a huge and growing availability of observa tions of diverse nature. This big data is placing data learning as a central scientific discipline. It includes collection, storage, preprocessing, visualization and, essentially, statistical analysis of enormous batches of data. In this paper, we discuss the role of statistics regarding some of the issues raised by big data in this new paradigm and also propose the name of data learning to describe all the activities that allow to obtain relevant knowledge from this new source of information.",Odaditoomle,259.0,58.0,30.0
6464,Object detection,401.0,non-local neural networks,1.0,23.0,5.0,201.0,1.0,2.6,189.8,64,http://arxiv.org/pdf/cs/0504056v1,The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.,Ononene,3202.0,57.0,500.0
6465,Object detection,401.0,fast r-cnn,1.0,27.0,5.0,201.0,1.0,2.6,191.4,65,http://arxiv.org/pdf/1504.08083v2,"This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.",Ofar-,12443.0,35.0,2350.0
6466,Object detection,401.0,realtime multi-person 2d pose estimation using part affinity fields,1.0,28.0,5.0,201.0,1.0,2.6,191.8,66,http://arxiv.org/pdf/1611.08050v2,"We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",Oremu2dpoesuspaaffi,3396.0,81.0,579.0
6467,Object detection,401.0,stand-alone self-attention in vision models,1.0,29.0,5.0,201.0,1.0,2.6,192.2,67,http://arxiv.org/pdf/2107.05637v2,"Self-Attention has become prevalent in computer vision models. Inspired by fully connected Conditional Random Fields (CRFs), we decompose it into local and context terms. They correspond to the unary and binary terms in CRF and are implemented by attention mechanisms with projection matrices. We observe that the unary terms only make small contributions to the outputs, and meanwhile standard CNNs that rely solely on the unary terms achieve great performances on a variety of tasks. Therefore, we propose Locally Enhanced Self-Attention (LESA), which enhances the unary term by incorporating it with convolutions, and utilizes a fusion module to dynamically couple the unary and binary operations. In our experiments, we replace the self-attention modules with LESA. The results on ImageNet and COCO show the superiority of LESA over convolution and self-attention baselines for the tasks of image recognition, object detection, and instance segmentation. The code is made publicly available.",Ostseinvimo,306.0,70.0,27.0
6468,Object detection,401.0,path aggregation network for instance segmentation,1.0,32.0,5.0,201.0,1.0,2.6,193.4,68,http://arxiv.org/pdf/1803.01534v4,"The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet",Opaagnefoinse,1042.0,72.0,139.0
6469,Object detection,401.0,real time pear fruit detection and counting using yolov4 models and deep sort,1.0,33.0,5.0,201.0,1.0,2.6,193.8,69,http://arxiv.org/pdf/2011.04244v2,"The ""You only look once v4""(YOLOv4) is one type of object detection methods in deep learning. YOLOv4-tiny is proposed based on YOLOv4 to simple the network structure and reduce parameters, which makes it be suitable for developing on the mobile and embedded devices. To improve the real-time of object detection, a fast object detection method is proposed based on YOLOv4-tiny. It firstly uses two ResBlock-D modules in ResNet-D network instead of two CSPBlock modules in Yolov4-tiny, which reduces the computation complexity. Secondly, it designs an auxiliary residual network block to extract more feature information of object to reduce detection error. In the design of auxiliary network, two consecutive 3x3 convolutions are used to obtain 5x5 receptive fields to extract global features, and channel attention and spatial attention are also used to extract more effective information. In the end, it merges the auxiliary network and backbone network to construct the whole network structure of improved YOLOv4-tiny. Simulation results show that the proposed method has faster object detection than YOLOv4-tiny and YOLOv3-tiny, and almost the same mean value of average precision as the YOLOv4-tiny. It is more suitable for real-time object detection.",Oretipefrdeancousyomoandeso,1.0,58.0,0.0
6470,Object detection,401.0,an aerial weed detection system for green onion crops using the you only look once (yolov3) deep learning algorithm,1.0,35.0,5.0,201.0,1.0,2.6,194.6,70,http://arxiv.org/pdf/2011.03651v1,"Over the last few years, the number of precision farming projects has increased specifically in harvesting robots and many of which have made continued progress from identifying crops to grasping the desired fruit or vegetable. One of the most common issues found in precision farming projects is that successful application is heavily dependent not just on identifying the fruit but also on ensuring that localisation allows for accurate navigation. These issues become significant factors when the robot is not operating in a prearranged environment, or when vegetation becomes too thick, thus covering crop. Moreover, running a state-of-the-art deep learning algorithm on an embedded platform is also very challenging, resulting most of the times in low frame rates. This paper proposes using the You Only Look Once version 3 (YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image processing techniques for the application of precision farming robots targeting strawberry detection, accelerated on a heterogeneous multiprocessor platform. The results show a performance acceleration by five times when implemented on a Field-Programmable Gate Array (FPGA) when compared with the same algorithm running on the processor side with an accuracy of 78.3\% over the test set comprised of 146 images.",Oanaewedesyfogroncrusthyoonloon(ydeleal,,,
6471,Object detection,401.0,a novel region of interest extraction layer for instance segmentation,1.0,39.0,5.0,201.0,1.0,2.6,196.2,71,http://arxiv.org/pdf/2004.13665v2,"Given the wide diffusion of deep neural network architectures for computer vision tasks, several new applications are nowadays more and more feasible. Among them, a particular attention has been recently given to instance segmentation, by exploiting the results achievable by two-stage networks (such as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex architectures, a crucial role is played by the Region of Interest (RoI) extraction layer, devoted to extracting a coherent subset of features from a single Feature Pyramid Network (FPN) layer attached on top of a backbone.   This paper is motivated by the need to overcome the limitations of existing RoI extractors which select only one (the best) layer from FPN. Our intuition is that all the layers of FPN retain useful information. Therefore, the proposed layer (called Generic RoI Extractor - GRoIE) introduces non-local building blocks and attention mechanisms to boost the performance.   A comprehensive ablation study at component level is conducted to find the best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can be integrated seamlessly with every two-stage architecture for both object detection and instance segmentation tasks. Therefore, the improvements brought about by the use of GRoIE in different state-of-the-art architectures are also evaluated. The proposed layer leads up to gain a 1.1% AP improvement on bounding box detection and 1.7% AP improvement on instance segmentation.   The code is publicly available on GitHub repository at https://github.com/IMPLabUniPr/mmdetection/tree/groie_dev",Oanoreofinexlafoinse,9.0,28.0,0.0
6472,Object detection,401.0,feature pyramid grids,1.0,40.0,5.0,201.0,1.0,2.6,196.6,72,http://arxiv.org/pdf/2004.03580v1,"Feature pyramid networks have been widely adopted in the object detection literature to improve feature representations for better handling of variations in scale. In this paper, we present Feature Pyramid Grids (FPG), a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral connections. FPG can improve single-pathway feature pyramid networks by significantly increasing its performance at similar computation cost, highlighting importance of deep pyramid representations. In addition to its general and uniform structure, over complicated structures that have been found with neural architecture search, it also compares favorably against such approaches without relying on search. We hope that FPG with its uniform and effective nature can serve as a strong component for future work in object recognition.",Ofepygr,11.0,41.0,1.0
6473,Object detection,58.0,consistency-based semi-supervised learning for object detection,4.0,201.0,1.0,105.0,3.0,2.5,129.3,73,http://papers.nips.cc/paper/9259-consistency-based-semi-supervised-learning-for-object-detection.pdf,"Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage and two-stage detectors and the results show the effectiveness of our method.",Ocoselefoobde,67.0,25.0,18.0
6474,Object detection,109.0,deeply supervised salient object detection with short connections,3.0,201.0,1.0,65.0,4.0,2.5,132.6,74,http://openaccess.thecvf.com/content_cvpr_2017/papers/Hou_Deeply_Supervised_Salient_CVPR_2017_paper.pdf,"Recent progress on saliency detection is substantial, benefiting mostly from the explosive development of Convolutional Neural Networks (CNNs). Semantic segmentation and saliency detection algorithms developed lately have been mostly based on Fully Convolutional Neural Networks (FCNs). There is still a large room for improvement over the generic FCN models that do not explicitly deal with the scale-space problem. Holisitcally-Nested Edge Detector (HED) provides a skip-layer structure with deep supervision for edge and boundary detection, but the performance gain of HED on saliency detection is not obvious. In this paper, we propose a new saliency method by introducing short connections to the skip-layer structures within the HED architecture. Our framework provides rich multi-scale feature maps at each layer, a property that is critically needed to perform segment detection. Our method produces state-of-the-art results on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.08 seconds per image), effectiveness, and simplicity over the existing algorithms.",Odesusaobdewishco,751.0,84.0,149.0
6475,Object detection,44.0,a survey on object detection in optical remote sensing images,4.0,201.0,1.0,130.0,3.0,2.5,132.60000000000002,75,https://www.sciencedirect.com/science/article/am/pii/S0924271616300144,"Object detection in optical remote sensing images, being a fundamental but challenging problem in the field of aerial and satellite image analysis, plays an important role for a wide range of applications and is receiving significant attention in recent years. While enormous methods exist, a deep review of the literature concerning generic object detection is still lacking. This paper aims to provide a review of the recent progress in this field. Different from several previously published surveys that focus on a specific object class such as building and road, we concentrate on more generic object categories including, but are not limited to, road, building, tree, vehicle, ship, airport, urban-area. Covering about 270 publications we survey (1) template matching-based object detection methods, (2) knowledge-based object detection methods, (3) object-based image analysis (OBIA)-based object detection methods, (4) machine learning-based object detection methods, and (5) five publicly available datasets and three standard evaluation metrics. We also discuss the challenges of current studies and propose two promising research directions, namely deep learning-based feature representation and weakly supervised learning-based geospatial object detection. It is our hope that this survey will be beneficial for the researchers to have better understanding of this research field.",Oasuonobdeinopreseim,646.0,342.0,28.0
6476,Object detection,72.0,predictive inequity in object detection,4.0,201.0,1.0,103.0,3.0,2.5,132.9,76,https://arxiv.org/pdf/1902.11097.pdf?fbclid=IwAR0XylkVZ-dRJZ_II6Me5200IWjvFYylQ1zInyZrlpwXFP6Ns7S7JZ7jeOc,"In this work, we investigate whether state-of-the-art object detection systems have equitable predictive performance on pedestrians with different skin tones. This work is motivated by many recent examples of ML and vision systems displaying higher error rates for certain demographic groups than others. We annotate an existing large scale dataset which contains pedestrians, BDD100K, with Fitzpatrick skin tones in ranges [1-3] or [4-6]. We then provide an in-depth comparative analysis of performance between these two skin tone groupings, finding that neither time of day nor occlusion explain this behavior, suggesting this disparity is not merely the result of pedestrians in the 4-6 range appearing in more difficult scenes for detection. We investigate to what extent time of day, occlusion, and reweighting the supervised loss during training affect this predictive bias.",Oprininobde,84.0,33.0,5.0
6477,Object detection,91.0,deep contrast learning for salient object detection,4.0,201.0,1.0,111.0,3.0,2.5,141.0,77,http://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Deep_Contrast_Learning_CVPR_2016_paper.pdf,"Salient object detection has recently witnessed substantial progress due to powerful features extracted using deep convolutional neural networks (CNNs). However, existing CNN-based methods operate at the patch level instead of the pixel level. Resulting saliency maps are typically blurry, especially near the boundary of salient objects. Furthermore, image patches are treated as independent samples even when they are overlapping, giving rise to significant redundancy in computation and storage. In this paper, we propose an end-to-end deep contrast network to overcome the aforementioned limitations. Our deep network consists of two complementary components, a pixel-level fully convolutional stream and a segment-wise spatial pooling stream. The first stream directly produces a saliency map with pixel-level accuracy from an input image. The second stream extracts segment-wise features very efficiently, and better models saliency discontinuities along object boundaries. Finally, a fully connected CRF model can be optionally incorporated to improve spatial coherence and contour localization in the fused result from these two streams. Experimental results demonstrate that our deep model significantly improves the state of the art.",Odecolefosaobde,593.0,58.0,107.0
6478,Object detection,60.0,beyond pascal: a benchmark for 3d object detection in the wild,4.0,201.0,1.0,162.0,3.0,2.5,147.0,78,https://cvgl.stanford.edu/papers/xiang_wacv14.pdf,"3D object detection and pose estimation methods have become popular in recent years since they can handle ambiguities in 2D images and also provide a richer description for objects compared to 2D object detectors. However, most of the datasets for 3D recognition are limited to a small amount of images per category or are captured in controlled environments. In this paper, we contribute PASCAL3D+ dataset, which is a novel and challenging dataset for 3D object detection and pose estimation. PASCAL3D+ augments 12 rigid categories of the PASCAL VOC 2012 [4] with 3D annotations. Furthermore, more images are added for each category from ImageNet [3]. PASCAL3D+ images exhibit much more variability compared to the existing 3D datasets, and on average there are more than 3,000 object instances per category. We believe this dataset will provide a rich testbed to study 3D detection and pose estimation and will help to significantly push forward research in this area. We provide the results of variations of DPM [6] on our new dataset for object detection and viewpoint estimation in different scenarios, which can be used as baselines for the community. Our benchmark is available online at http://cvgl.stanford.edu/projects/pascal3d.",Obepaabefo3dobdeinthwi,570.0,32.0,142.0
6479,Object detection,100.0,weakly supervised region proposal network and object detection,4.0,201.0,1.0,145.0,3.0,2.5,153.9,79,http://openaccess.thecvf.com/content_ECCV_2018/papers/Peng_Tang_Weakly_Supervised_Region_ECCV_2018_paper.pdf,"The Convolutional Neural Network (CNN) based region proposal generation method (i.e. region proposal network), trained using bounding box annotations, is an essential component in modern fully supervised object detectors. However, Weakly Supervised Object Detection (WSOD) has not benefited from CNN-based proposal generation due to the absence of bounding box annotations, and is relying on standard proposal generation methods such as selective search. In this paper, we propose a weakly supervised region proposal network which is trained using only image-level annotations. The weakly supervised region proposal network consists of two stages. The first stage evaluates the objectness scores of sliding window boxes by exploiting the low-level information in CNN and the second stage refines the proposals from the first stage using a region-based CNN classifier. Our proposed region proposal network is suitable for WSOD, can be plugged into a WSOD network easily, and can share its convolutional computations with the WSOD network. Experiments on the PASCAL VOC and ImageNet detection datasets show that our method achieves the state-of-the-art performance for WSOD with performance gain of about \(3\%\) on average.",Owesureprneanobde,125.0,47.0,20.0
6480,Object detection,155.0,edge assisted real-time object detection for mobile augmented reality,3.0,201.0,1.0,93.0,4.0,2.5,154.8,80,https://dl.acm.org/doi/pdf/10.1145/3300061.3300116,"Most existing Augmented Reality (AR) and Mixed Reality (MR) systems are able to understand the 3D geometry of the surroundings but lack the ability to detect and classify complex objects in the real world. Such capabilities can be enabled with deep Convolutional Neural Networks (CNN), but it remains difficult to execute large networks on mobile devices. Offloading object detection to the edge or cloud is also very challenging due to the stringent requirements on high detection accuracy and low end-to-end latency. The long latency of existing offloading techniques can significantly reduce the detection accuracy due to changes in the user's view. To address the problem, we design a system that enables high accuracy object detection for commodity AR/MR system running at 60fps. The system employs low latency offloading techniques, decouples the rendering pipeline from the offloading pipeline, and uses a fast object tracking method to maintain detection accuracy. The result shows that the system can improve the detection accuracy by 20.2%-34.8% for the object detection and human keypoint detection tasks, and only requires 2.24ms latency for object tracking on the AR device. Thus, the system leaves more time and computational resources to render virtual elements for the next frame and enables higher quality AR/MR experiences.",Oedasreobdefomoaure,114.0,51.0,12.0
6481,Object detection,75.0,mimicking very efficient network for object detection,4.0,201.0,1.0,180.0,3.0,2.5,156.9,81,https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf,"Current CNN based object detectors need initialization from pre-trained ImageNet classification models, which are usually time-consuming. In this paper, we present a fully convolutional feature mimic framework to train very efficient CNN based detectors, which do not need ImageNet pre-training and achieve competitive performance as the large and slow models. We add supervision from high-level features of the large networks in training to help the small network better learn object representation. More specifically, we conduct a mimic method for the features sampled from the entire feature map and use a transform layer to map features from the small network onto the same dimension of the large network. In training the small network, we optimize the similarity between features sampled from the same region on the feature maps of both networks. Extensive experiments are conducted on pedestrian and common object detection tasks using VGG, Inception and ResNet. On both Caltech and Pascal VOC, we show that the modified 2.5&#xd7; accelerated Inception network achieves competitive performance as the full Inception Network. Our faster model runs at 80 FPS for a 1000&#xd7;1500 large input with only a minor degradation of performance on Caltech.",Omiveefnefoobde,128.0,39.0,13.0
6482,Object detection,84.0,salient object detection: a discriminative regional feature integration approach,4.0,201.0,1.0,173.0,3.0,2.5,157.5,82,https://openaccess.thecvf.com/content_cvpr_2013/papers/Jiang_Salient_Object_Detection_2013_CVPR_paper.pdf,"Feature integration provides a computational framework for saliency detection, and a lot of hand-crafted integration rules have been developed. In this paper, we present a principled extension, supervised feature integration, which learns a random forest regressor to discriminatively integrate the saliency features for saliency computation. In addition to contrast features, we introduce regional object-sensitive descriptors: the objectness descriptor characterizing the common spatial and appearance property of the salient object, and the image-specific backgroundness descriptor characterizing the appearance of the background of a specific image, which are shown more important for estimating the saliency. To the best of our knowledge, our supervised feature integration framework is the first successful approach to perform the integration over the saliency features for salient object detection, and outperforms the integration approach over the saliency maps. Together with fusing the multi-level regional saliency maps to impose the spatial saliency consistency, our approach significantly outperforms state-of-the-art methods on seven benchmark datasets. We also discuss several followup works which jointly learn the representation and the saliency map using deep learning.",Osaobdeadirefeinap,871.0,120.0,159.0
6483,Object detection,99.0,the role of context for object detection and semantic segmentation in the wild,4.0,201.0,1.0,160.0,3.0,2.5,158.10000000000002,83,https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Mottaghi_The_Role_of_2014_CVPR_paper.pdf,"In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.",Othroofcofoobdeanseseinthwi,801.0,47.0,139.0
6484,Object detection,85.0,application of deep learning in object detection,4.0,201.0,1.0,186.0,3.0,2.5,161.7,84,https://www.researchgate.net/profile/Wenlong-Fu-4/publication/318035834_Application_of_deep_learning_in_object_detection/links/60b0b6f392851c168e48d02e/Application-of-deep-learning-in-object-detection.pdf,"This paper deals with the field of computer vision, mainly for the application of deep learning in object detection task. On the one hand, there is a simple summary of the datasets and deep learning algorithms commonly used in computer vision. On the other hand, a new dataset is built according to those commonly used datasets, and choose one of the network called faster r-cnn to work on this new dataset. Through the experiment to strengthen the understanding of these networks, and through the analysis of the results learn the importance of deep learning technology, and the importance of the dataset for deep learning.",Oapofdeleinobde,117.0,14.0,6.0
6485,Object detection,401.0,from points to parts: 3d object detection from point cloud with part-aware and part-aggregation network,1.0,180.0,3.0,198.0,3.0,2.4000000000000004,251.7,85,https://arxiv.org/pdf/1907.03670,"3D object detection from LiDAR point cloud is a challenging problem in 3D scene understanding and has many practical applications. In this paper, we extend our preliminary work PointRCNN to a novel and strong point-cloud-based 3D object detection framework, the part-aware and aggregation neural network (Part-<inline-formula><tex-math notation=""LaTeX"">$A^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""shi-ieq1-2977026.gif""/></alternatives></inline-formula> net). The whole framework consists of the part-aware stage and the part-aggregation stage. First, the part-aware stage for the first time fully utilizes free-of-charge part supervisions derived from 3D ground-truth boxes to simultaneously predict high quality 3D proposals and accurate intra-object part locations. The predicted intra-object part locations within the same proposal are grouped by our new-designed RoI-aware point cloud pooling module, which results in an effective representation to encode the geometry-specific features of each 3D proposal. Then the part-aggregation stage learns to re-score the box and refine the box location by exploring the spatial relationship of the pooled intra-object part locations. Extensive experiments are conducted to demonstrate the performance improvements from each component of our proposed framework. Our Part-<inline-formula><tex-math notation=""LaTeX"">$A^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""shi-ieq2-2977026.gif""/></alternatives></inline-formula> net outperforms all existing 3D detection methods and achieves new state-of-the-art on KITTI 3D object detection dataset by utilizing only the LiDAR point cloud data.",Ofrpotopa3dobdefrpoclwipaanpane,139.0,63.0,12.0
6486,Object detection,170.0,sparse r-cnn: end-to-end object detection with learnable proposals,3.0,108.0,3.0,201.0,1.0,2.4,154.5,86,https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Sparse_R-CNN_End-to-End_Object_Detection_With_Learnable_Proposals_CVPR_2021_paper.pdf,"We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as $k$ anchor boxes pre-defined on all grids of image feature map of size $H\times W$. In our method, however, a fixed sparse set of learned object proposals, total length of $N$, are provided to object recognition head to perform classification and location. By eliminating $HWk$ (up to hundreds of thousands) hand-designed object candidates to $N$ (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard $3\times$ training schedule and running at 22 fps using ResNet-50 FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors. The code is available at: https://github.com/PeizeSun/SparseR-CNN.",Ospr-enobdewilepr,49.0,55.0,11.0
6487,Object detection,184.0,object detection at 200 frames per second,3.0,124.0,3.0,201.0,1.0,2.4,165.1,87,http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Mehta_Object_detection_at_200_Frames_Per_Second_ECCVW_2018_paper.pdf,"In this paper, we propose an efficient and fast object detector which can process hundreds of frames per second. To achieve this goal we investigate three main aspects of the object detection framework: network architecture, loss function and training data (labeled and unlabeled). In order to obtain compact network architecture, we introduce various improvements, based on recent work, to develop an architecture which is computationally light-weight and achieves a reasonable performance. To further improve the performance, while keeping the complexity same, we utilize distillation loss function. Using distillation loss we transfer the knowledge of a more accurate teacher network to proposed light-weight student network. We propose various innovations to make distillation efficient for the proposed one stage detector pipeline: objectness scaled distillation loss, feature map non-maximal suppression and a single unified distillation loss function for detection. Finally, building upon the distillation loss, we explore how much can we push the performance by utilizing the unlabeled data. We train our model with unlabeled data using the soft labels of the teacher network. Our final network consists of 10x fewer parameters than the VGG based object detection network and it achieves a speed of more than 200 FPS and proposed changes improve the detection accuracy by 14 mAP over the baseline on Pascal dataset.",Oobdeat20frpese,19.0,49.0,5.0
6488,Object detection,194.0,sequence level semantics aggregation for video object detection,3.0,198.0,3.0,201.0,1.0,2.4,197.7,88,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Sequence_Level_Semantics_Aggregation_for_Video_Object_Detection_ICCV_2019_paper.pdf,"Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.",Oseleseagfoviobde,51.0,46.0,17.0
6489,Object detection,1.0,a trainable system for object detection,5.0,201.0,1.0,201.0,1.0,2.2,141.0,89,https://dspace.mit.edu/bitstream/handle/1721.1/86481/46874896-MIT.pdf?sequence=2&isAllowed=y,"Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles which combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy and optimization function, etc. In this paper, we provide a review on deep learning based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely Convolutional Neural Network (CNN). Then we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network based learning systems.",Oatrsyfoobde,1391.0,39.0,81.0
6490,Object detection,3.0,an empirical study of context in object detection,5.0,201.0,1.0,201.0,1.0,2.2,141.60000000000002,90,https://kilthub.cmu.edu/articles/An_Empirical_Study_of_Context_in_Object_Detection/6551903/files/12033113.pdf,"Context: The dire consequences of the COVID-19 pandemic has influenced development of COVID-19 software i.e., software used for analysis and mitigation of COVID-19. Bugs in COVID-19 software can be consequential, as COVID-19 software projects can impact public health policy and user data privacy. Objective: The goal of this paper is to help practitioners and researchers improve the quality of COVID-19 software through an empirical study of open source software projects related to COVID-19. Methodology: We use 129 open source COVID-19 software projects hosted on GitHub to conduct our empirical study. Next, we apply qualitative analysis on 550 bug reports from the collected projects to identify bug categories. Findings: We identify 8 bug categories, which include data bugs i.e., bugs that occur during mining and storage of COVID-19 data. The identified bug categories appear for 7 categories of software projects including (i) projects that use statistical modeling to perform predictions related to COVID-19, and (ii) medical equipment software that are used to design and implement medical equipment, such as ventilators. Conclusion: Based on our findings, we advocate for robust statistical model construction through better synergies between data science practitioners and public health experts. Existence of security bugs in user tracking software necessitates development of tools that will detect data privacy violations and security weaknesses.",Oanemstofcoinobde,442.0,57.0,15.0
6491,Object detection,6.0,object detection using the statistics of parts,5.0,201.0,1.0,201.0,1.0,2.2,142.5,91,http://www.cs.cmu.edu/~efros/courses/LBMV09/Papers/schneiderman-ijcv-04.pdf,"Object detection is a critical part of visual scene understanding. The representation of the object in the detection task has important implications on the efficiency and feasibility of annotation, robustness to occlusion, pose, lighting, and other visual sources of semantic uncertainty, and effectiveness in real-world applications (e.g., autonomous driving). Popular object representations include 2D and 3D bounding boxes, polygons, splines, pixels, and voxels. Each have their strengths and weakness. In this work, we propose a new representation of objects based on the bivariate normal distribution. This distribution-based representation has the benefit of robust detection of highly-overlapping objects and the potential for improved downstream tracking and instance segmentation tasks due to the statistical representation of object edges. We provide qualitative evaluation of this representation for the object detection task and quantitative evaluation of its use in a baseline algorithm for the instance segmentation task.",Oobdeusthstofpa,413.0,41.0,17.0
6492,Object detection,8.0,deep neural networks for object detection,5.0,201.0,1.0,201.0,1.0,2.2,143.10000000000002,92,https://research.google/pubs/pub41457.pdf,"Computer vision is developing rapidly with the support of deep learning techniques. This thesis proposes an advanced vehicle-detection model based on an improvement to classical convolutional neural networks. The advanced model was applied against a vehicle detection benchmark and was built to detect on-road objects. First, we propose a high-level architecture for our advanced model, which utilizes different state-of-the-art deep learning techniques. Then, we utilize the residual neural networks and region proposal network to achieve competitive performance according to the vehicle detection benchmark. Lastly, we describe the developing trend of vehicle detection techniques and the future direction of research.",Odenenefoobde,1094.0,21.0,35.0
6493,Object detection,107.0,iou loss for 2d/3d object detection,3.0,201.0,1.0,104.0,3.0,2.2,143.7,93,https://arxiv.org/pdf/1908.03851,"In the 2D/3D object detection task, Intersection-over-Union (IoU) has been widely employed as an evaluation metric to evaluate the performance of different detectors in the testing stage. However, during the training stage, the common distance loss (e.g, L_1 or L_2) is often adopted as the loss function to minimize the discrepancy between the predicted and ground truth Bounding Box (Bbox). To eliminate the performance gap between training and testing, the IoU loss has been introduced for 2D object detection in [1] and [2]. Unfortunately, all these approaches only work for axis-aligned 2D Boxes, which cannot be applied for more general object detection task with rotated Boxes. To resolve this issue, we investigate the IoU computation for two rotated Boxes first and then implement a unified framework, IoU loss layer for both 2D and 3D object detection tasks. By integrating the implemented IoU loss into several state-of-the-art 3D object detectors, consistent improvements have been achieved for both bird-eye-view 2D detection and point cloud 3D detection on the public KITTI [3] benchmark.",Oiolofo2dobde,70.0,31.0,6.0
6494,Object detection,13.0,object detection with discriminatively trained part-based models,5.0,201.0,1.0,201.0,1.0,2.2,144.60000000000002,94,http://lear.inrialpes.fr/~oneata/reading_group/dpm.pdf,"Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.",Oobdewiditrpamo,9130.0,54.0,1368.0
6495,Object detection,14.0,towards adversarially robust object detection,5.0,201.0,1.0,201.0,1.0,2.2,144.9,95,https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Towards_Adversarially_Robust_Object_Detection_ICCV_2019_paper.pdf,"Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.",Otoadroobde,39.0,73.0,8.0
6496,Object detection,17.0,object detection in equirectangular panorama,5.0,201.0,1.0,201.0,1.0,2.2,145.8,96,https://arxiv.org/pdf/1805.08009,"We introduced a high-resolution equirectangular panorama (360-degree, virtual reality) dataset for object detection and propose a multi-projection variant of YOLO detector. The main challenge with equirectangular panorama image are i) the lack of annotated training data, ii) high-resolution imagery and iii) severe geometric distortions of objects near the panorama projection poles. In this work, we solve the challenges by i) using training examples available in the ""conventional datasets"" (ImageNet and COCO), ii) employing only low-resolution images that require only moderate GPU computing power and memory, and iii) our multi-projection YOLO handles projection distortions by making multiple stereographic sub-projections. In our experiments, YOLO outperforms the other state-of-art detector, Faster RCNN and our multi-projection YOLO achieves the best accuracy with low-resolution input.",Oobdeineqpa,31.0,19.0,5.0
6497,Object detection,22.0,interactive object detection,5.0,201.0,1.0,201.0,1.0,2.2,147.3,97,https://homes.esat.kuleuven.be/~konijn/publications/2012/CVPR-Yao.pdf,"Given a C*-algebra B, a closed *-subalgebra A contained in B, and a partial isometry S in B which ""interacts"" with A in the sense that S*aS = H(a)S*S and SaS* = V(a)SS*, where V and H are positive linear operators on A, we derive a few properties which V and H are forced to satisfy. Removing B and S from the picture we define an ""interaction"" as being a pair of maps (V,H) satisfying the derived properties. Starting with an abstract interaction (V,H) over a C*-algebra A we construct a C*-algebra B containing A and a partial isometry S whose ""interaction"" with A follows the above rules. We then discuss the possibility of constructing a ""covariance algebra"" from an interaction. This turns out to require a generalization of the notion of correspondences (also known as Pimsner bimodules) which we call a ""generalized correspondence"". Such an object should be seen as an usual correspondence, except that the inner-products need not lie in the coefficient algebra. The covariance algebra is then defined using a natural generalization of Pimsner's construction of the celebrated Cuntz-Pimsner algebras.",Oinobde,0.0,0.0,0.0
6498,Object detection,25.0,learning a sparse representation for object detection,5.0,201.0,1.0,201.0,1.0,2.2,148.2,98,https://link.springer.com/content/pdf/10.1007/3-540-47979-1_8.pdf,"A variety of representation learning approaches have been investigated for reinforcement learning; much less attention, however, has been given to investigating the utility of sparse coding. Outside of reinforcement learning, sparse coding representations have been widely used, with non-convex objectives that result in discriminative representations. In this work, we develop a supervised sparse coding objective for policy evaluation. Despite the non-convexity of this objective, we prove that all local minima are global minima, making the approach amenable to simple optimization strategies. We empirically show that it is key to use a supervised objective, rather than the more straightforward unsupervised sparse coding approach. We compare the learned representations to a canonical fixed sparse representation, called tile-coding, demonstrating that the sparse coding representation outperforms a wide variety of tilecoding representations.",Oleasprefoobde,646.0,27.0,69.0
6499,Object detection,28.0,a boundary-fragment-model for object detection,5.0,201.0,1.0,201.0,1.0,2.2,149.10000000000002,99,https://link.springer.com/content/pdf/10.1007/11744047_44.pdf,"In contrast to object recognition models, humans do not blindly trust their perception when building representations of the world, instead recruiting metacognition to detect percepts that are unreliable or false, such as when we realize that we mistook one object for another. We propose METAGEN, an unsupervised model that enhances object recognition models through a metacognition. Given noisy output from an object-detection model, METAGEN learns a meta-representation of how its perceptual system works and uses it to infer the objects in the world responsible for the detections. METAGEN achieves this by conditioning its inference on basic principles of objects that even human infants understand (known as Spelke principles: object permanence, cohesion, and spatiotemporal continuity). We test METAGEN on a variety of state-of-the-art object detection neural networks. We find that METAGEN quickly learns an accurate metacognitive representation of the neural network, and that this improves detection accuracy by filling in objects that the detection model missed and removing hallucinated objects. This approach enables generalization to out-of-sample data and outperforms comparison models that lack a metacognition.",Oabofoobde,409.0,29.0,27.0
6500,Object detection,32.0,contextual priming for object detection,5.0,201.0,1.0,201.0,1.0,2.2,150.3,100,https://dspace.mit.edu/bitstream/handle/1721.1/7239/AIM-2001-020.pdf?sequence=2&isAllowed=y,"We propose contextual convolution (CoConv) for visual recognition. CoConv is a direct replacement of the standard convolution, which is the core component of convolutional neural networks. CoConv is implicitly equipped with the capability of incorporating contextual information while maintaining a similar number of parameters and computational cost compared to the standard convolution. CoConv is inspired by neuroscience studies indicating that (i) neurons, even from the primary visual cortex (V1 area), are involved in detection of contextual cues and that (ii) the activity of a visual neuron can be influenced by the stimuli placed entirely outside of its theoretical receptive field. On the one hand, we integrate CoConv in the widely-used residual networks and show improved recognition performance over baselines on the core tasks and benchmarks for visual recognition, namely image classification on the ImageNet data set and object detection on the MS COCO data set. On the other hand, we introduce CoConv in the generator of a state-of-the-art Generative Adversarial Network, showing improved generative results on CIFAR-10 and CelebA. Our code is available at https://github.com/iduta/coconv.",Ocoprfoobde,894.0,72.0,58.0
6782,Optical Character Recognition,16.0,handwritten optical character recognition (ocr): a comprehensive systematic literature review (slr),5.0,201.0,1.0,2.0,5.0,3.4,85.8,1,https://ieeexplore.ieee.org/iel7/6287639/8948470/09151144.pdf,"Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.",Ohaopchre(oacosylire(s,40.0,237.0,2.0
6783,Optical Character Recognition,1.0,optical character recognition,5.0,201.0,1.0,26.0,5.0,3.4,88.5,2,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.673.8061&rep=rep1&type=pdf,"This paper describes two implementations in optical character recognition using template matching method and feature extraction method followed by support vector machine classification. With proper image preprocessing, the texts are segmented into isolated characters and the correlations between a single character and a given set of templates are computed to find the similarities and then identify the input character. In the second method, features extracted from the segmented characters are used to train the SVM classifiers, which are later, tested by a test set of handwritten digits. Keywords—Optical character recognition; template matching; feature extraction; support vector machine.",Oopchre,221.0,2.0,10.0
6784,Optical Character Recognition,19.0,urdu optical character recognition systems: present contributions and future directions,5.0,201.0,1.0,9.0,5.0,3.4,88.80000000000001,3,https://ieeexplore.ieee.org/iel7/6287639/6514899/08438450.pdf,"This paper gives an across-the-board comprehensive review and survey of the most prominent studies in the field of Urdu optical character recognition (OCR). This paper introduces the OCR technology and presents a historical review of the OCR systems, providing comparisons between the English, Arabic, and Urdu systems. Detailed background and literature have also been provided for Urdu script, discussing the script’s past, OCR categories, and phases. This paper further reports all state-of-the-art studies for different phases, namely, image acquisition, pre-processing, segmentation, feature extraction, classification/recognition, and post-processing for an Urdu OCR system. In the segmentation section, the analytical and holistic approaches for Urdu text have been emphasized. In the feature extraction section, a comparison has been provided between the feature learning and feature engineering approaches. Deep learning and traditional machine learning approaches have been discussed. The Urdu numeral recognition systems have also been deliberated concisely. The research paper concludes by identifying some open problems and suggesting some future directions.",Ouropchresyprcoanfudi,19.0,104.0,2.0
6785,Optical Character Recognition,18.0,optical character recognition for sanskrit using convolution neural networks,5.0,201.0,1.0,11.0,5.0,3.4,89.10000000000001,4,http://arxiv.org/abs/1211.4385v1,"Ancient Sanskrit manuscripts are a rich source of knowledge about Science, Mathematics, Hindu mythology, Indian civilization, and culture. It therefore becomes critical that access to these manuscripts is made easy, to share this knowledge with the world and to facilitate further research on this Ancient literature. In this paper, we propose a Convolutional Neural Network (CNN) based Optical Character Recognition system (OCR) which accurately digitizes Ancient Sanskrit manuscripts (Devanagari Script) that are not necessarily in good condition. We use an image segmentation algorithm for calculating pixel intensities to identify letters in the image. The OCR considers typical compound characters (half letter combinations) as separate classes in order to improve the segmentation accuracy. The novelty of the OCR is its robustness to image quality, image contrast, font style and font size, which makes it an ideal choice for digitizing soiled and poorly maintained Sanskrit manuscripts.",Oopchrefosausconene,19.0,23.0,1.0
6786,Optical Character Recognition,17.0,comparison between neural network and support vector machine in optical character recognition,5.0,201.0,1.0,20.0,5.0,3.4,91.5,5,https://www.sciencedirect.com/science/article/pii/S1877050917321099/pdf?md5=22c277f9e6208442a558c8d870445866&pid=1-s2.0-S1877050917321099-main.pdf&_valck=1,"Abstract Optical Character Recognition is one of the popular area in artificial intelligence and pattern recognition area. Generally, this technique converts the input image into an editable format in computer. This paper uses several techniques as a comparison for some extracted features, such as: zoning algorithm, projection profile, Histogram of Oriented Gradients (HOG) and combination of those feature extractions ( zoning + projection, projection + HOG, zoning + HOG, zoning + projection + HOG). For the evaluation of the proposed system, this paper compare the most commonly classifiers: Support Vector Machine (SVM) and Artificial Neural Network (ANN). This experiment achieves the highest accuracy of 94.43% using Support Vector Machine (SVM) classifier with the feature extraction algorithms are:projection profile and the combination of zoning + projection profile",Ocobeneneansuvemainopchre,24.0,10.0,0.0
6787,Optical Character Recognition,4.0,a detailed analysis of optical character recognition technology,5.0,201.0,1.0,37.0,5.0,3.4,92.7,6,https://dergipark.org.tr/en/download/article-file/236939,"In many different fields, there is a high demand for storing information to a computer storage disk from the data available in printed or handwritten documents or images to later re-utilize this information by means of computers. One simple way to store information to a computer system from these printed documents could be first to scan the documents and then store them as image files. But to re-utilize this information, it would very difficult to read or query text or other information from these image files. Therefore a technique to automatically retrieve and store information, in particular text, from image files is needed. Optical character recognition is an active research area that attempts to develop a computer system with the ability to extract and process text from images automatically. The objective of OCR is to achieve modification or conversion of any form of text or text-containing documents such as handwritten text, printed or scanned text images, into an editable digital format for deeper and further processing. Therefore, OCR enables a machine to automatically recognize text in such documents. Some major challenges need to be recognized and handled in order to achieve a successful automation. The font characteristics of the characters in paper documents and quality of images are only some of the recent challenges. Due to these challenges, characters sometimes may not be recognized correctly by computer system. In this paper we investigate OCR in four different ways. First we give a detailed overview of the challenges that might emerge in OCR stages. Second, we review the general phases of an OCR system such as pre-processing, segmentation, normalization, feature extraction, classification and post-processing. Then, we highlight developments and main applications and uses of OCR and finally, a brief OCR history are discussed. Therefore, this discussion provides a very comprehensive review of the state-of-the-art of the field.",Oadeanofopchrete,34.0,36.0,2.0
6788,Optical Character Recognition,26.0,hardware architecture of bidirectional long short-term memory neural network for optical character recognition,5.0,201.0,1.0,15.0,5.0,3.4,92.7,7,https://www.date-conference.com/proceedings-archive/2017/pdf/0529.pdf,"Optical Character Recognition is conversion of printed or handwritten text images into machine-encoded text. It is a building block of many processes such as machine translation, text-to-speech conversion and text mining. Bidirectional Long Short-Term Memory Neural Networks have shown a superior performance in character recognition with respect to other types of neural networks. In this paper, to the best of our knowledge, we propose the first hardware architecture of Bidirectional Long Short-Term Memory Neural Network with Connectionist Temporal Classification for Optical Character Recognition. Based on the new architecture, we present an FPGA hardware accelerator that achieves 459 times higher throughput than state-of-the-art. Visual recognition is a typical task on mobile platforms that usually use two scenarios either the task runs locally on embedded processor or offloaded to a cloud to be run on high performance machine. We show that computationally intensive visual recognition task benefits from being migrated to our dedicated hardware accelerator and outperforms high-performance CPU in terms of runtime, while consuming less energy than low power systems with negligible loss of recognition accuracy.",Ohaarofbiloshmenenefoopchre,37.0,14.0,4.0
6789,Optical Character Recognition,38.0,implementation of optical character recognition using tesseract with the javanese script target in android application,5.0,201.0,1.0,7.0,5.0,3.4,93.9,8,https://www.sciencedirect.com/science/article/pii/S1877050919311640/pdf?md5=9216e8aa23a3761eae772a84e6f16299&pid=1-s2.0-S1877050919311640-main.pdf,"Abstract Recognising characters from text have been a popular topic in the computer vision area. The application can benefit to many problems in the world. For example: recognising text in documents, classifying the text or scripts of documents, plate recognition, etc. Many researchers have been developed the methods for recognising characters in by using Optical Character Recognition methods. Although text recognition problem using Optical Character Recognition has been more or less solved, most of the Optical Character Recognition problem explored is belong to Latin alphabet texts. Meanwhile, there are several languages have non-Latin scripts as the written text. Recognising a non-Latin script is quite challenging as the contour and shape of the text are relatively different with a Latin script text. This research aims to collect datasets for OCR in Javanese characters. A total of 5880 characters were collected and trained with several methods with Tesseract OCR tools. The models then be implemented to a mobile phone (Android based). The highest accuracy (97,50%) achieved by the model was achieved by combining single boundary box for the whole parts of the character and the separate boundary boxes in main body and sandangan parts.",Oimofopchreustewithjasctainanap,10.0,23.0,0.0
6790,Optical Character Recognition,13.0,the optical character recognition of urdu-like cursive scripts,5.0,201.0,1.0,33.0,5.0,3.4,94.20000000000002,9,https://www.academia.edu/download/35278719/N_K_2013_PR.pdf,"We survey the optical character recognition (OCR) literature with reference to the Urdu-like cursive scripts. In particular, the Urdu, Pushto, and Sindhi languages are discussed, with the emphasis being on the Nasta'liq and Naskh scripts. Before detaining the OCR works, the peculiarities of the Urdu-like scripts are outlined, which are followed by the presentation of the available text image databases. For the sake of clarity, the various attempts are grouped into three parts, namely: (a) printed, (b) handwritten, and (c) online character recognition. Within each part, the works are analyzed par rapport a typical OCR pipeline with an emphasis on the preprocessing, segmentation, feature extraction, classification, and recognition. HighlightsA literature review of the Nasta'liq and Naskh cursive script OCR.The peculiarities and challenges are described a priori.Printed, handwritten and online OCR efforts are being explored.Analyses based on the stages of a typical OCR pipeline.",Othopchreofurcusc,109.0,135.0,6.0
6791,Optical Character Recognition,34.0,segmentation-free optical character recognition for printed urdu text,5.0,201.0,1.0,19.0,5.0,3.4,96.3,10,http://arxiv.org/pdf/2103.05105v1,"This paper presents a segmentation-free optical character recognition system for printed Urdu Nastaliq font using ligatures as units of recognition. The proposed technique relies on statistical features and employs Hidden Markov Models for classification. A total of 1525 unique high-frequency Urdu ligatures from the standard Urdu Printed Text Images (UPTI) database are considered in our study. Ligatures extracted from text lines are first split into primary (main body) and secondary (dots and diacritics) ligatures and multiple instances of the same ligature are grouped into clusters using a sequential clustering algorithm. Hidden Markov Models are trained separately for each ligature using the examples in the respective cluster by sliding right-to-left the overlapped windows and extracting a set of statistical features. Given the query text, the primary and secondary ligatures are separately recognized and later associated together using a set of heuristics to recognize the complete ligature. The system evaluated on the standard UPTI Urdu database reported a ligature recognition rate of 92% on more than 6000 query ligatures.",Oseopchrefoprurte,24.0,56.0,0.0
6792,Optical Character Recognition,23.0,a study on optical character recognition techniques,5.0,201.0,1.0,34.0,5.0,3.4,97.5,11,https://www.academia.edu/download/54439477/4117ijcsitce01.pdf,"Optical Character Recognition (OCR) is the process which enables a system to without human intervention identifies the scripts or alphabets written into the users’ verbal communication. Optical Character identification has grown to be individual of the mainly flourishing applications of knowledge in the field of pattern detection and artificial intelligence. In our survey we study on the various OCR techniques. In this paper we resolve and examine the hypothetical and numerical models of Optical Character Identification. The Optical character identification or classification (OCR) and Magnetic Character Recognition (MCR) techniques are generally utilized for the recognition of patterns or alphabets. In general the alphabets are in the variety of pixel pictures and it could be either handwritten or stamped, of any series, shape or direction etc. Alternatively in MCR the alphabets are stamped with magnetic ink and the studying machine categorize the alphabet on the basis of the exclusive magnetic field that is shaped by every alphabet. Both MCR and OCR discover utilization in banking and different trade appliances. Earlier exploration going on Optical Character detection or recognition has shown that the In Handwritten text there is no limitation lying on the script technique. Hand written correspondence is complicated to be familiar through due to diverse human handwriting style, disparity in angle, size and shape of calligraphy. An assortment of approaches of Optical Character Identification is discussed here all along through their achievement.",Oastonopchrete,7.0,10.0,0.0
6793,Optical Character Recognition,32.0,improving optical character recognition performance for low quality images,5.0,201.0,1.0,28.0,5.0,3.4,98.4,12,http://arxiv.org/pdf/2005.03780v1,"Efficient Optical Character Recognition (OCR) in images grabbed from Set-Top Boxes (STBs) plays an important role in STB testing. However, running OCR software on such images usually ends with low OCR performance since images can have low resolution, low image quality or colorful background. In order to improve OCR performance, four different image preprocessing methods are proposed. In this paper OCR is performed with Tesseract 3.5 and the relatively new Tesseract 4.0 on the images grabbed from different STBs. On the original images Tesseract 3.5 provides a 35.7% accuracy while Tesseract 4.0 attains a 70.2% accuracy. The proposed preprocessing methods improve OCR performance by 33.3% for Tesseract 3.5 and 22.6% for Tesseract 4.0 on the available images.",Oimopchrepefoloquim,12.0,4.0,0.0
6794,Optical Character Recognition,40.0,optical character recognition for brahmi script using geometric method,5.0,201.0,1.0,32.0,5.0,3.4,102.0,13,https://journal.utem.edu.my/index.php/jtec/article/viewFile/3197/2298,"Optical character recognition (OCR) system has been widely used for conversion of images of typed, handwritten or printed text into machine-encoded text (digital character). Previous researches on character recognition of South Asian scripts focus on modern scripts such as Sanskrit, Hindi, Tamil, Malayalam, and Sinhala etc. but little work is traceable to Brahmi script which is referred to as the origin of many scripts in south Asian. This study proposes a method for recognition of both handwritten and printed Brahmi characters which involve preprocessing, segmentation, feature extraction, and classification of Brahmi script characters. The geometric method was used for feature extraction into six different entities, followed by a newly developed classification rules to recognize the Brahmi characters based on the features. The method obtains accuracy of 91.69% and 89.55% for handwritten vowels and consonants character respectively and 93.30% and 94.90% for printed vowel and consonants character respectively.",Oopchrefobrscusgeme,9.0,30.0,0.0
6795,Optical Character Recognition,35.0,state of the art optical character recognition of 19th century fraktur scripts using open source engines,5.0,96.0,4.0,201.0,1.0,3.4,109.2,14,https://arxiv.org/pdf/1810.03436,"In this paper we evaluate Optical Character Recognition (OCR) of 19th century Fraktur scripts without book-specific training using mixed models, i.e. models trained to recognize a variety of fonts and typesets from previously unseen sources. We describe the training process leading to strong mixed OCR models and compare them to freely available models of the popular open source engines OCRopus and Tesseract as well as the commercial state of the art system ABBYY. For evaluation, we use a varied collection of unseen data from books, journals, and a dictionary from the 19th century. The experiments show that training mixed models with real data is superior to training with synthetic data and that the novel OCR engine Calamari outperforms the other engines considerably, on average reducing ABBYYs character error rate (CER) by over 70%, resulting in an average CER below 1%.",Ostoftharopchreof19cefrscusopsoen,15.0,7.0,1.0
6796,Optical Character Recognition,11.0,optical character recognition by open source ocr tool tesseract: a case study,5.0,201.0,1.0,49.0,4.0,3.1,98.4,15,https://www.researchgate.net/profile/Chirag-Patel-12/publication/235956427_Optical_Character_Recognition_by_Open_source_OCR_Tool_Tesseract_A_Case_Study/links/00463516fa43a64739000000/Optical-Character-Recognition-by-Open-source-OCR-Tool-Tesseract-A-Case-Study.pdf,"ABSTRACT Optical character recognition (OCR) method has been used in converting printed text into editable text. OCR is very useful and popular method in various applications. Accuracy of OCR can be dependent on text preprocessing and segmentation algorithms. Sometimes it is difficult to retrieve text from the image because of different size, style, orientation, complex background of image etc. We begin this paper with an introduction of Optical Character Recognition (OCR) method, History of Open Source OCR tool Tesseract, architecture of it and experiment result of OCR performed by Tesseract on different kinds images are discussed. We conclude this paper by comparative study of this tool with other commercial OCR tool Transym OCR by considering vehicle number plate as input. From vehicle number plate we tried to extract vehicle number by using Tesseract and Transym and compared these tools based on various parameters. explained.Keywords like: Desktop OCR, Server OCR, Web OCR etc.",Oopchrebyopsooctoteacast,176.0,33.0,11.0
6797,Optical Character Recognition,6.0,optical character recognition: an overview and an insight,5.0,201.0,1.0,56.0,4.0,3.1,99.0,16,http://arxiv.org/pdf/1710.05703v1,Many researches are going on in the field of optical character recognition (OCR) for the last few decades and a lot of articles have been published. Also a large number of OCR is available commercially. In this literature a review of the OCR history and the various techniques used for OCR development in the chronological order is being done.,Oopchreanovananin,26.0,39.0,2.0
6798,Optical Character Recognition,43.0,improving optical character recognition of finnish historical newspapers with a combination of fraktur & antiqua models and image preprocessing,4.0,201.0,1.0,25.0,5.0,3.1,100.8,17,https://helda.helsinki.fi/bitstream/handle/10138/310105/ecp17131038.pdf?sequence=1,In this paper we describe a method for improving the optical character recognition (OCR) toolkit Tesseract for Finnish historical documents. First we create a model for Finnish Fraktur fonts. Second we test Tesseract with the created Fraktur model and Antiqua model on single images and combinations of images with different image preprocessing methods. Against commercial ABBYY FineReader toolkit our method achieves 27.48% (FineReader 7 or 8) and 9.16% (FineReader 11) improvement on word level.,Oimopchreoffihinewiacooffr&anmoanimpr,15.0,32.0,0.0
6799,Optical Character Recognition,53.0,a survey on arabic optical character recognition and an isolated handwritten arabic character recognition algorithm using encoded freeman chain code,4.0,201.0,1.0,21.0,5.0,3.1,102.6,18,http://arxiv.org/abs/2009.13450v1,"Optical Character Recognition (OCR) is the process of identifying text in an image and convert it into a digital form. Several approaches have been attempted to accurately recognize characters in printed Arabic language. This survey focuses on OCR in handwritten Arabic language. We will describe the general difficulties in Arabic language text, the main process of a typical OCR system and some enhancements to Arabic OCR systems. We will also describe a novel approach for identifying handwritten isolated Arabic characters using encoded Freeman chain code. Several handwritten Arabic characters were trained and tested, and the preliminary experimental results are promising.",Oasuonaropchreananishaarchrealusenfrchco,16.0,21.0,1.0
6800,Optical Character Recognition,29.0,active contour based optical character recognition for automated scene understanding,5.0,201.0,1.0,48.0,4.0,3.1,103.5,19,http://arxiv.org/pdf/2109.03976v1,"In this paper, we present a new optical character recognition (OCR) approach which allows real-time, automatic extraction and recognition of digits in images and videos. Our method relies on active contours in order to robustly extract optical characters from real-world visual scenes. The detected character recognition is based on template matching. Our developed system has shown excellent results when applied to the automated identification of team playersÂ? numbers in sport datasets and has outperformed state-of-the-art methods.",Oaccobaopchrefoauscun,36.0,29.0,0.0
6801,Optical Character Recognition,20.0,optical character recognition for hindi language using a neural-network approach,5.0,201.0,1.0,77.0,4.0,3.1,109.5,20,https://www.koreascience.or.kr/article/JAKO201316349187067.pdf,"Hindi is the most widely spoken language in India, with more than 300 million speakers. As there is no separation between the characters of texts written in Hindi as there is in English, the Optical Character Recognition (OCR) systems developed for the Hindi language carry a very poor recognition rate. In this paper we propose an OCR for printed Hindi text in Devanagari script, using Artificial Neural Network (ANN), which improves its efficiency. One of the major reasons for the poor recognition rate is error in character segmentation. The presence of touching characters in the scanned documents further complicates the segmentation process, creating a major problem when designing an effective character segmentation technique. Preprocessing, character segmentation, feature extraction, and finally, classification and recognition are the major steps which are followed by a general OCR. The preprocessing tasks considered in the paper are conversion of gray scaled images to binary images, image rectification, and segmentation of the documents textual contents into paragraphs, lines, words, and then at the level of basic symbols. The basic symbols, obtained as the fundamental unit from the segmentation process, are recognized by the neural classifier. In this work, three feature extraction techniques-: histogram of projection based on mean distance, histogram of projection based on pixel value, and vertical zero crossing, have been used to improve the rate of recognition. These feature extraction techniques are powerful enough to extract features of even distorted characters/symbols. For development of the neural classifier, a back-propagation neural network with two hidden layers is used. The classifier is trained and tested for printed Hindi texts. A performance of approximately 90% correct recognition rate is achieved.",Oopchrefohilausaneap,32.0,47.0,1.0
6802,Optical Character Recognition,59.0,optical character recognition using template matching and back propagation algorithm,4.0,201.0,1.0,40.0,5.0,3.1,110.1,21,http://arxiv.org/abs/quant-ph/0512057v3,"Building an effective methodology to detect characters from images with less error rate is the great task. Our aim is to furnish such an algorithm that will be able to generate error free recognition of text from the given input image which will help in document digitizing and prevention to the hand written text recognition. OCR has been in the intensive research topic for more than 4 decades, it is probably the most time consuming and labor intensive work of inputting the data through keyboard. This paper discuss about mechanical or electronic conversion of scanned images, text which contain graphics, image captured by camera, scanned images and the recognition of images where characters may be broken or smeared. The optical character recognition is the desktop based application developed using Java IDE and mysql as a database. We have gain 91.82% accuracy when applied on different data sets, in pre-processing we used different techniques to remove noise from the image in post processing we used dictionary for the characters which are not recognized during classification, in classification we have used the back propagation algorithm for the training of neural network, feature extraction has been performed by template matching and hamming distance. All the algorithms have been developed in java technology.",Oopchreustemaanbapral,13.0,24.0,0.0
6803,Optical Character Recognition,81.0,an efficient fpga implementation of optical character recognition for license plate recognition,4.0,201.0,1.0,22.0,5.0,3.1,111.3,22,http://arxiv.org/pdf/1003.1072v2,"Optical Character Recognition system (OCR) can be used in intelligent transportation systems for license plate detection. However, most times the systems are unable to work with noisy and imperfect images. In this work, a robust FPGA-based OCR system has been designed and tested with imperfect and noisy license plate images. The OCR system is based on a feedforward neural networks, which uses an efficient and precise neuron. The neuron transfer function is based on an approximation of the Hyperbolic Tangent Activation Function. The neuron is utilized in a 189 − 160 − 36 feed forward neural network configuration. The network parameters were optimized and then tested with noisy images of license plates numbers. The network was able to maintain a 98.2% accuracy in recognizing the characters despite the image imperfections.",Oaneffpimofopchrefoliplre,14.0,15.0,1.0
6804,Optical Character Recognition,24.0,an overview and applications of optical character recognition,5.0,201.0,1.0,79.0,4.0,3.1,111.3,23,https://www.academia.edu/download/64225273/An%20Overview%20and%20Applications%20of%20Optical%20Character%20Recognition.pdf,"Optical character recognition, usually abbreviated to OCR, is the mechanical or electronic conversion of scanned or photographed images of typewritten or printed text into machine-encoded/computer-readable text. It is widely used as a form of data entry from some sort of original paper data source, whether passport documents, invoices, bank statement, receipts, business card, mail, or any number of printed records. It is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as machine translation, text-to-speech, key data extraction and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision. Optical Character Recognition or OCR is the electronic translation of handwritten, typewritten or printed text into machine translated images. It is widely used to recognize and search text from electronic documents or to publish the text on a website [1]. A large number of research papers and reports have already been published on this topic. The paper presents introduction, major research work and applications of Optical Character Recognition in various fields. At the first introduction of OCR will be discussed and then some points will be stressed on the major research works that have made a great impact in character recognition. And finally the most important applications of OCR will be covered and then conclusion.",Oanovanapofopchre,8.0,121.0,0.0
6805,Optical Character Recognition,30.0,optical character recognition in real environments using neural networks and k-nearest neighbor,5.0,201.0,1.0,85.0,4.0,3.1,114.9,24,https://www.researchgate.net/profile/Oliviu-Matei/publication/257518097_Optical_character_recognition_in_real_environments_using_neural_networks_and_k-nearest_neighbor/links/5746e80b08ae9ace84262a90/Optical-character-recognition-in-real-environments-using-neural-networks-and-k-nearest-neighbor.pdf,"In this paper, we propose a novel process to optical character recognition (OCR) used in real environments, such as gas-meters and electricity-meters, where the quantity of noise is sometimes as large as the quantity of good signal. Our method combines two algorithms an artificial neural network on one hand, and the k-nearest neighbor as the confirmation algorithm. Our approach, unlike other OCR systems, it is based on the angles of the digits rather than on pixels. Some of the advantages of the proposed system are: insensitivity to the possible rotations of the digits, the possibility to work in different light and exposure conditions, the ability to deduct and use heuristics for character recognition. The experimental results point out that our method with moderate level of training epochs can produce a high accuracy of 99.3 % in recognizing the digits, proving that our system is very successful.",Oopchreinreenusneneank-ne,24.0,24.0,0.0
6806,Optical Character Recognition,401.0,calamari - a high-performance tensorflow-based deep learning package for optical character recognition,1.0,125.0,3.0,1.0,5.0,3.0,170.60000000000002,25,http://arxiv.org/pdf/1309.0261v1,"Optical Character Recognition (OCR) on contemporary and historical data is still in the focus of many researchers. Especially historical prints require book specific trained OCR models to achieve applicable results (Springmann and L\""udeling, 2016, Reul et al., 2017a). To reduce the human effort for manually annotating ground truth (GT) various techniques such as voting and pretraining have shown to be very efficient (Reul et al., 2018a, Reul et al., 2018b). Calamari is a new open source OCR line recognition software that both uses state-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and giving native support for techniques such as pretraining and voting. The customizable network architectures constructed of Convolutional Neural Networks (CNNS) and Long-ShortTerm-Memory (LSTM) layers are trained by the so-called Connectionist Temporal Classification (CTC) algorithm of Graves et al. (2006). Optional usage of a GPU drastically reduces the computation times for both training and prediction. We use two different datasets to compare the performance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches a Character Error Rate (CER) of 0.11% on the UW3 dataset written in modern English and 0.18% on the DTA19 dataset written in German Fraktur, which considerably outperforms the results of the existing softwares.",Oca-ahitedelepafoopchre,35.0,10.0,4.0
6807,Optical Character Recognition,46.0,optical character recognition using artificial neural network,4.0,201.0,1.0,44.0,4.0,2.8,107.4,26,https://www.academia.edu/download/64730221/IRJET_V7I8593.pdf,"The increasing use of computers for documentations have lead to a large amount of data in the form of various unstructured documents which are not arranged in a uniform, understandable and integrated way. The processing required for extracting information is still only in its preliminary stage and the hardly predictable document structure make it very hard to extract information automatically. This project focuses on extracting structured data from unstructured data using OCR (Optical Character Recognition) and Neural Network. OCR is a technology which is required to deal with common facts as well as complex designed fonts. It focuses on recognizing characters of a document, that is it does script identification from a variety of unstructured printed or handwritten documents. Discrete Cosine transform is used to obtain data sets for classification and recognition. The DCT components with maximum variances will be used for this purpose as trained data, that is, the system is already trained for recognizing characters of the input unstructured document. The techniques of feedforwarding and backpropagation will be used in Neural networks which will match the patterns and add new patterns on recognition. The system will be implemented and simulated using Java with Neural Network as the backend for the optical character recognition process. Such an OCR system with Neural Network at it’s back focuses towards increases accuracy by eliminating human errors that would occur if the work had be done manually. It also focuses on extracting data irrespective of the noise and the image processing Keywords— Optical Character Recognition (OCR), Artificial Neural Network, Discrete Cosine Transform, Back propagation, Image Processing, Synaptic weights.",Oopchreusarnene,7.0,7.0,2.0
6808,Optical Character Recognition,42.0,boosting optical character recognition: a super-resolution approach,4.0,201.0,1.0,50.0,4.0,2.8,108.0,27,https://arxiv.org/pdf/1506.02211,"Text image super-resolution is a challenging yet open research problem in the computer vision community. In particular, low-resolution images hamper the performance of typical optical character recognition (OCR) systems. In this article, we summarize our entry to the ICDAR2015 Competition on Text Image Super-Resolution. Experiments are based on the provided ICDAR2015 TextSR dataset (3) and the released Tesseract-OCR 3.02 system (1). We report that our winning entry of text image super-resolution framework has largely improved the OCR performance with low-resolution images used as input, reaching an OCR accuracy score of 77.19%, which is comparable with that of using the original high-resolution images (78.80%). Index Terms—super resolution; optical character recogni- tion.",Oboopchreasuap,22.0,13.0,4.0
6809,Optical Character Recognition,57.0,optical character recognition (ocr) system for roman script & english language using artificial neural network (ann) classifier,4.0,201.0,1.0,43.0,4.0,2.8,110.4,28,http://arxiv.org/pdf/1703.09550v1,"Character recognition from scanned images is a very complex task. But as for record keeping we require all the data in digital format to perform various manipulation operations. The main issue in case of character recognition is the different styles and fonts in which the text is written. We proposed a new approach by using the concept of Artificial Neural Network and Nearest Neighbour approach for character recognition from scanned images. Three layers are used for classification purpose. First is the input layer consist the input given by the segmented characters, then hidden layer consist the neurons trained by the training network and the output layer consist output neurons to generate unicode.",Oopchre(osyforosc&enlausarnene(acl,15.0,22.0,1.0
6810,Optical Character Recognition,63.0,optical character recognition system for urdu words in nastaliq font,4.0,201.0,1.0,41.0,4.0,2.8,111.6,29,https://pdfs.semanticscholar.org/fdc0/e84f8fba1ff863310d21058b2b7f9f935ca0.pdf,"Optical Character Recognition (OCR) has been an attractive research area for the last three decades and mature OCR systems reporting near to 100% recognition rates are available for many scripts/languages today. Despite these develop-ments, research on recognition of text in many languages is still in its early days, Urdu being one of them. The limited existing literature on Urdu OCR is either limited to isolated characters or considers limited vocabularies in fixed font sizes. This research presents a segmentation free and size invariant technique for recognition of Urdu words in Nastaliq font using ligatures as units of recognition. Ligatures, separated into primary ligatures and diacritics, are recognized using right-to-left HMMs. Diacritics are then associated with the main body using position information and the resulting ligatures are validated using a dictionary. The system evaluated on Urdu words realized promising recognition rates at ligature and word levels.",Oopchresyfourwoinnafo,14.0,47.0,0.0
6811,Optical Character Recognition,9.0,design of an optical character recognition system for camera-based handheld devices,5.0,201.0,1.0,103.0,3.0,2.8,114.0,30,https://arxiv.org/pdf/1109.3317,"This paper presents a complete Optical Character Recognition (OCR) system for camera captured image/graphics embedded textual documents for handheld devices. At first, text regions are extracted and skew corrected. Then, these regions are binarized and segmented into lines and characters. Characters are passed into the recognition module. Experimenting with a set of 100 business card images, captured by cell phone camera, we have achieved a maximum recognition accuracy of 92.74%. Compared to Tesseract, an open source desktop-based powerful OCR engine, present recognition accuracy is worth contributing. Moreover, the developed technique is computationally efficient and consumes low memory so as to be applicable on handheld devices.",Odeofanopchresyfocahade,75.0,17.0,2.0
6812,Optical Character Recognition,51.0,development of an optical character recognition pipeline for handwritten form fields from an electronic health record,4.0,201.0,1.0,81.0,4.0,2.8,120.0,31,http://arxiv.org/pdf/1003.5897v1,"BACKGROUND
Although the penetration of electronic health records is increasing rapidly, much of the historical medical record is only available in handwritten notes and forms, which require labor-intensive, human chart abstraction for some clinical research. The few previous studies on automated extraction of data from these handwritten notes have focused on monolithic, custom-developed recognition systems or third-party systems that require proprietary forms.


METHODS
We present an optical character recognition processing pipeline, which leverages the capabilities of existing third-party optical character recognition engines, and provides the flexibility offered by a modular custom-developed system. The system was configured and run on a selected set of form fields extracted from a corpus of handwritten ophthalmology forms.


OBSERVATIONS
The processing pipeline allowed multiple configurations to be run, with the optimal configuration consisting of the Nuance and LEADTOOLS engines running in parallel with a positive predictive value of 94.6% and a sensitivity of 13.5%.


DISCUSSION
While limitations exist, preliminary experience from this project yielded insights on the generalizability and applicability of integrating multiple, inexpensive general-purpose third-party optical character recognition engines in a modular pipeline.",Odeofanopchrepifohafofifranelhere,36.0,12.0,1.0
6813,Optical Character Recognition,7.0,automatic number plate recognition system for vehicle identification using optical character recognition,5.0,201.0,1.0,128.0,3.0,2.8,120.9,32,https://www.academia.edu/download/53797425/IRJET-V4I6488.pdf,"Automatic Number Plate Recognition (ANPR) is an image processing technology which uses number (license) plate to identify the vehicle. The objective is to design an efficient automatic authorized vehicle identification system by using the vehicle number plate. The system is implemented on the entrance for security control of a highly restricted area like military zones or area around top government offices e.g. Parliament, Supreme Court etc. The developed system first detects the vehicle and then captures the vehicle image. Vehicle number plate region is extracted using the image segmentation in an image. Optical character recognition technique is used for the character recognition. The resulting data is then used to compare with the records on a database so as to come up with the specific information like the vehicle’s owner, place of registration, address, etc. The system is implemented and simulated in Matlab, and it performance is tested on real image. It is observed from the experiment that the developed system successfully detects and recognize the vehicle number plate on real images.",Oaunuplresyfoveidusopchre,175.0,6.0,4.0
6814,Optical Character Recognition,82.0,a neural network based handwritten meitei mayek alphabet optical character recognition system,4.0,201.0,1.0,53.0,4.0,2.8,120.9,33,http://arxiv.org/abs/1103.0365v1,Handwritten character recognition is a part of optical character (OCR) system. OCR can be applied to both printed text and handwritten documents. In this paper we discussed the handwritten character recognition of Meitei Mayek (Manipuri script). Although OCR has been studied and developed for many Indian script very few works have been reported so far for Meitei-Mayek. This paper describes the handwritten Meitei Mayek (Manipuri script) alphabets recognition (HMMAR) using a neural network approach. The alphabet database is pre-processed and the extracted feature is sent to a neural network system for training. The trained neural network is further tested and performance analysis is observed. The emphasis is given on the process of character segmentation from a whole document i.e. isolating a single character image from a complete scanned document.,Oanenebahamemaalopchresy,16.0,19.0,0.0
6815,Optical Character Recognition,80.0,the use of optical character recognition (ocr) in the digitisation of herbarium specimen labels,4.0,201.0,1.0,60.0,4.0,2.8,122.4,34,http://arxiv.org/pdf/2105.13808v1,"Abstract At the Royal Botanic Garden Edinburgh (RBGE) the use of Optical Character Recognition (OCR) to aid the digitisation process has been investigated. This was tested using a herbarium specimen digitisation process with two stages of data entry. Records were initially batch-processed to add data extracted from the OCR text prior to being sorted based on Collector and/or Country. Using images of the specimens, a team of six digitisers then added data to the specimen records. To investigate whether the data from OCR aid the digitisation process, they completed a series of trials which compared the efficiency of data entry between sorted and unsorted batches of specimens. A survey was carried out to explore the opinion of the digitisation staff to the different sorting options. In total 7,200 specimens were processed. When compared to an unsorted, random set of specimens, those which were sorted based on data added from the OCR were quicker to digitise. Of the methods tested here, the most successful in terms of efficiency used a protocol which required entering data into a limited set of fields and where the records were filtered by Collector and Country. The survey and subsequent discussions with the digitisation staff highlighted their preference for working with sorted specimens, in which label layout, locations and handwriting are likely to be similar, and so a familiarity with the Collector or Country is rapidly established.",Othusofopchre(ointhdiofhespla,16.0,33.0,1.0
6816,Optical Character Recognition,89.0,optical character recognition menggunakan algoritma template matching correlation,4.0,201.0,1.0,51.0,4.0,2.8,122.4,35,https://ejournal3.undip.ac.id/index.php/joint/article/viewFile/333/335,"OCR (Optical Character Recognition) is an effective solution to the process of converting printed documents into digital documents. The problems that arise in the process of computer letters recognition is how a recognition techniques to identify different types of characters with different sizes and shapes. Recognition method used in this final project is the template matching correlation method. Prior to the recognition process, the input image with a format *.bmp or *.jpg processed first at the preprocessing process, which includes the binerisasi, segmentation, and normalization of images. Average recognition success rate of 92,90% is generated by this system. The final results showed that the use of the template matching correlation method is effective enough to build an OCR system with good accuracy.",Oopchremealtemaco,15.0,18.0,0.0
6817,Optical Character Recognition,70.0,embedded optical character recognition on tamil text image using raspberry pi,4.0,201.0,1.0,80.0,4.0,2.8,125.4,36,https://www.academia.edu/download/49122439/Embedded_Optical_Character_Recognition.pdf,"Optical Character recognition is used to digitize and reproduce texts that have been produced with non-computerized system. Digitizing texts also helps reduce storage space. Editing and Reprinting of Text document that were printed on paper are time consuming and labour intensive. Optical Character recognition is also useful for visually impaired people who cannot read Text document, but need to access the content of the Text documents. This paper is on Methodology of a camera based assistive device that can be used by people to read Tamil Text document. The framework is on implementing image capturing technique in an embedded system based on Raspberry Pi board",Oemopchreontateimusrapi,8.0,14.0,0.0
6818,Optical Character Recognition,60.0,optical character recognition (ocr) performance in server-based mobile environment,4.0,201.0,1.0,92.0,4.0,2.8,126.0,37,http://arxiv.org/pdf/2105.07983v1,"There are several Optical Character Recognition (OCR) mobile applications on the market running on mobile devices, both android and iOS (iPhone, iPad, iPod) platforms. The limitations of mobile device processor hinder the possible execution of computationally intensive applications that need less time of process. This paper proposes a framework of Optical Character Recognition (OCR) on mobile device using server-based processing. Comparison methods proposed by this paper by conducting a series of tests using standalone and server-based OCR on mobile devices, and compare the results of the accuracy and time required for the entire OCR processing. Server-based mobile OCR obtains 5% higher character recognition accuracy than the standalone OCR and its format recognition accuracy is 99.8%. The framework tries to overcome the limitation of mobile device capability process, so the devices can do the computationally intensive application more quickly.",Oopchre(opeinsemoen,15.0,14.0,0.0
6819,Optical Character Recognition,66.0,genetic algorithm and neural network for optical character recognition,4.0,201.0,1.0,96.0,4.0,2.8,129.0,38,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.678.6351&rep=rep1&type=pdf,"Computer system has been able to recognize writing as human brain does. The method mostly used for character recognition is the backpropagation network. Backpropagation network has been known for its accuracy because it allows itself to learn and improving itself thus it can achieve higher accuracy. On the other hand, backpropagation was less to be used because of its time length needed to train the network to achieve the best result possible. In this study, backpropagation network algorithm is combined with genetic algorithm to achieve both accuracy and training swiftness for recognizing alphabets. Genetic algorithm is used to define the best initial values for the network’s architecture and synapses’ weight thus within a shorter period of time, the network could achieve the best accuracy. The optimized backpropagation network has better accuracy and less training time than the standard backpropagation network. The accuracy in recognizing character differ by 10, 77%, with a success rate of 90, 77% for the optimized backpropagation and 80% accuracy for the standard backpropagation network. The training time needed for backpropagation learning phase improved significantly from 03 h, 14 min and 40 sec, a standard backpropagation training time, to 02 h 18 min and 1 sec for the optimized backpropagation network.",Ogealannenefoopchre,14.0,16.0,0.0
6820,Optical Character Recognition,67.0,extraction of text on tv screen using optical character recognition,4.0,201.0,1.0,97.0,4.0,2.8,129.6,39,https://www.researchgate.net/profile/Ivan-Kastelan/publication/261351265_Extraction_of_text_on_TV_screen_using_optical_character_recognition/links/579f24b008ae6a2882f56626/Extraction-of-text-on-TV-screen-using-optical-character-recognition.pdf,"Optical character recognition (OCR) is a very active area of research and has become very successful in pattern recognition. It is based on algorithms for machine vision and artificial intelligence and used in developing algorithms for reading text on images, e.g. reading registration plates, scanned books and documents, etc. This paper presents the system for text extraction on the image taken by grabbing the content of the TV screen. The preparation steps for OCR are developed which detect the text regions in an image. An open-source algorithm for OCR is then run to read the text regions. The system is used as a part of Black Box Testing system in order to test and functionally verify TV set operation. After reading the text regions, comparison with the expected text is performed to make a final pass/fail decision for the test case. The system successfully read the text from the TV screen and can be used in a functional verification system.",Oexofteontvscusopchre,14.0,11.0,0.0
6821,Optical Character Recognition,22.0,a comparative study of optical character recognition for tamil script,5.0,201.0,1.0,145.0,3.0,2.8,130.5,40,https://www.researchgate.net/profile/Riya-Prabhakar/publication/254686762_A_Comparative_Study_of_Optical_Character_Recognition_for_Tamil_Script/links/5dcc42584585156b35101fa4/A-Comparative-Study-of-Optical-Character-Recognition-for-Tamil-Script.pdf,"A position input device that makes it possible to control the position of a cursor by directly utilizing the movement of a finger of a person with good operability and convenience. This device includes an electrode support formed to define a space into which a position indicator is inserted. A plurality of electrode pairs are fixed on the support. Each of the plurality of electrode pairs generates the capacitance. A plurality of capacitance sensors are provided. Each of the plurality of capacitance sensors serves to sense the capacitance of a corresponding one of the plurality of electrode pairs to output a capacitance signal, respectively. A position calculator calculates the position of the position indicator based on the capacitance signals from the plurality of capacitance sensors. A cursor control signal generator generates a cursor control signal. The cursor control signal generated by the cursor control signal generator is sent to a computer to be used to locate a cursor on a screen.",Oacostofopchrefotasc,24.0,54.0,3.0
6822,Optical Character Recognition,37.0,optical character recognition system for urdu (naskh font) using pattern matching technique,5.0,201.0,1.0,131.0,3.0,2.8,130.8,41,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.177.6460&rep=rep1&type=pdf#page=4,"The offline optical character recognition (OCR) for different languages has been developed over the recent years. Since 1965, the US postal service has been using this system for automating their services. The range of the applications under this area is increasing day by day, due to its utility in almost major areas of government as well as private sector. This technique has been very useful in making paper free environment in many major organizations as far as the backup of their previous file record is concerned. Our this system has been proposed for the Offline Character Recognition for Isolated Characters of Urdu language, as Urdu language forms words by combining Isolated Characters. Urdu is a cursive language, having connected characters making words. The major area of utility for Urdu OCR will be digitizing of a lot of literature related material already stocked in libraries. Urdu language is famous and spoken in more than 3 big countries including Pakistan, India and Bangladesh. A lot of work has been done in Urdu poetry and literature up to the recent century. Creation of OCR for Urdu language will make an important role in converting all those work from physical libraries to electronic libraries. Most of the stuff already placed on internet is in the form of images having text, which took a lot of space to transfer and even read online. So the need of an Urdu OCR is a must. The system is of training system type. It consists of the image preprocessing, line and character segmentation, creation of xml file for training purpose. While Recognition system includes taking xml file, the image to be recognized, segment it and creation of chain codes for character images and matching with already stored in xml file. Tabassam Nawaz, Syed Ammar Hassan Shah Naqvi, Habib ur Rehman & Anoshia Faiz International Journal of Image Processing, (IJIP)Volume (3) : Issue (3) 93 The system has been implemented and it has 89% recognition accuracy with a 15 char/sec recognition rate.",Oopchresyfour(nfouspamate,32.0,55.0,2.0
6823,Optical Character Recognition,12.0,a complete optical character recognition methodology for historical documents,5.0,201.0,1.0,157.0,3.0,2.8,131.1,42,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.454.8600&rep=rep1&type=pdf,"In this paper a complete OCR methodology for recognizing historical documents, either printed or handwritten without any knowledge of the font, is presented. This methodology consists of three steps: The first two steps refer to creating a database for training using a set of documents, while the third one refers to recognition of new document images. First, a pre-processing step that includes image binarization and enhancement takes place. At a second step a top-down segmentation approach is used in order to detect text lines, words and characters. A clustering scheme is then adopted in order to group characters of similar shape. This is a semi-automatic procedure since the user is able to interact at any time in order to correct possible errors of clustering and assign an ASCII label. After this step, a database is created in order to be used for recognition. Finally, in the third step, for every new document image the above segmentation approach takes place while the recognition is based on the character database that has been produced at the previous step.",Oacoopchremefohido,81.0,27.0,0.0
6824,Optical Character Recognition,138.0,an end-to-end deep neural architecture for optical character verification and recognition in retail food packaging,3.0,201.0,1.0,36.0,5.0,2.8,132.60000000000002,43,http://eprints.lincoln.ac.uk/id/eprint/31978/1/ICIP2018_Leontidis_preprint.pdf,"There exist various types of information in retail food packages, including food product name, ingredients list and use by date. The correct recognition and coding of use by dates is especially critical in ensuring proper distribution of the product to the market and eliminating potential health risks caused by erroneous mislabelling. The latter can have a major negative effect on the health of consumers and consequently raise legal issues for suppliers. In this work, an end-to-end architecture, composed of a dual deep neural network based system is proposed for automatic recognition of use by dates in food package photos. The system includes: a Global level convolutional neural network (CNN) for high-level food package image quality evaluation (blurry/clear/missing use by date statistics); a Local level fully convolutional network (FCN) for use by date ROI localisation. Post ROI extraction, the date characters are then segmented and recognised. The proposed framework is the first to employ deep neural networks for end-to-end automatic use by date recognition in retail packaging photos. It is capable of achieving very good levels of performance on all the aforementioned tasks, despite the varied textual/pictorial content complexity found in food packaging design.",Oanendenearfoopchveanreinrefopa,23.0,20.0,2.0
6825,Optical Character Recognition,15.0,urdu nastaleeq optical character recognition,5.0,201.0,1.0,165.0,3.0,2.8,134.4,44,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.193.3286&rep=rep1&type=pdf,"This paper discusses the Urdu script characteristics, Urdu Nastaleeq and a simple but a novel and robust technique to recognize the printed Urdu script without a lexicon. Urdu being a family of Arabic script is cursive and complex script in its nature, the main complexity of Urdu compound/connected text is not its connections but the forms/shapes the characters change when it is placed at initial, middle or at the end of a word. The characters recognition technique presented here is using the inherited complexity of Urdu script to solve the problem. A word is scanned and analyzed for the level of its complexity, the point where the level of complexity changes is marked for a character, segmented and feeded to Neural Networks. A prototype of the system has been tested on Urdu text and currently achieves 93.4% accuracy on the average.",Ournaopchre,48.0,9.0,3.0
6826,Optical Character Recognition,85.0,geometric feature points based optical character recognition,4.0,201.0,1.0,95.0,4.0,2.8,134.4,45,http://arxiv.org/pdf/1904.08760v1,"Optical character recognition is an application of pattern recognition which automatically detects and recognizes the optical characters with out human intervention. All the characters are basically made up from three geometric entities, i.e. corners, endings and bifurcations which can be used to identify different characters. In this paper, we present a method for optical character recognition based on basic geometric features. The method uses a crossing number method to extract features from thinned character. The feature vector for each character consists of number of corners, endings and bifurcations. The classification stage recognizes a character by using a simple rule based method. The proposed system is tested using different samples for each character and the results show the validity of the proposed algorithm.",Ogefepobaopchre,10.0,13.0,3.0
6827,Optical Character Recognition,31.0,optical character recognition of amharic documents,5.0,201.0,1.0,159.0,3.0,2.8,137.4,46,https://epress.lib.uts.edu.au/journals/index.php/ajict/article/view/E1I3122007006/429,"In Africa around 2,500 languages are spoken. Some of these languages have their own indigenous scripts. Accordingly, there is a bulk of printed documents available in libraries, information centers, museums and offices. Digitization of these documents enables to harness already available information technologies to local information needs and developments. This paper presents an Optical Character Recognition (OCR) system for converting digitized documents in local languages. An extensive literature survey reveals that this is the first attempt that report the challenges towards the recognition of indigenous African scripts and a possible solution for Amharic script. Research in the recognition of African indigenous scripts faces major challenges due to (i) the use of large number characters in the writing and (ii) existence of large set of visually similar characters. In this paper, we propose a novel feature extraction scheme using principal component and linear discriminant analysis, followed by a decision directed acyclic graph based support vector machine classifier. Recognition results are presented on real-life degraded documents such as books, magazines and newspapers to demonstrate the performance of the recognizer.",Oopchreofamdo,24.0,21.0,6.0
6829,Optical Character Recognition,5.0,an arabic optical character recognition system using recognition-based segmentation,5.0,201.0,1.0,189.0,3.0,2.8,138.6,47,http://arxiv.org/pdf/2009.09115v1,"Abstract Optical character recognition (OCR) systems improve human–machine interaction and are widely used in many areas. The recognition of cursive scripts is a difficult task as their segmentation suffers from serious problems. This paper proposes an Arabic OCR system, which uses a recognition-based segmentation technique to overcome the classical segmentation problems. A newly developed Arabic word segmentation algorithm is also introduced to separate horizontally overlapping Arabic words/subwords. There is also a feedback loop to control the combination of character fragments for recognition. The system was implemented and the results show a 90% recognition accuracy with a 20 chars/s recognition rate.",Oanaropchresyusrese,133.0,30.0,6.0
6830,Optical Character Recognition,21.0,optical character recognition for printed tamil text using unicode,5.0,201.0,1.0,176.0,3.0,2.8,139.5,48,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.484.9302&rep=rep1&type=pdf,"Optical Character Recognition (OCR) refers to the process of converting printed Tamil text documents into software translated Unicode Tamil Text. The printed documents available in the form of books, papers, magazines, etc. are scanned using standard scanners which produce an image of the scanned document. As part of the preprocessing phase the image file is checked for skewing. If the image is skewed, it is corrected by a simple rotation technique in the appropriate direction. Then the image is passed through a noise elimination phase and is binarized. The preprocessed image is segmented using an algorithm which decomposes the scanned text into paragraphs using special space detection technique and then the paragraphs into lines using vertical histograms, and lines into words using horizontal histograms, and words into character image glyphs using horizontal histograms. Each image glyph is comprised of 32×32 pixels. Thus a database of character image glyphs is created out of the segmentation phase. Then all the image glyphs are considered for recognition using Unicode mapping. Each image glyph is passed through various routines which extract the features of the glyph. The various features that are considered for classification are the character height, character width, the number of horizontal lines (long and short), the number of vertical lines (long and short), the horizontally oriented curves, the vertically oriented curves, the number of circles, number of slope lines, image centroid and special dots. The glyphs are now set ready for classification based on these features. The extracted features are passed to a Support Vector Machine (SVM) where the characters are classified by Supervised Learning Algorithm. These classes are mapped onto Unicode for recognition. Then the text is reconstructed using Unicode fonts.",Oopchrefoprtateusun,80.0,3.0,5.0
6831,Optical Character Recognition,8.0,a complete tamil optical character recognition system,5.0,201.0,1.0,198.0,3.0,2.8,142.20000000000002,49,https://link.springer.com/content/pdf/10.1007/3-540-45869-7_6.pdf,"Document Image processing and Optical Character Recognition (OCR) have been a frontline research area in the field of human-machine interface for the last few decades. Recognition of Indian language characters has been a topic of interest for quite some time. The earlier contributions were reported in [1] and [2]. A more recent work is reported in [3] and [9]. The need for efficient and robust algorithms and systems for recognition is being felt in India, especially in the post and telegraph department where OCR can assist the staff in sorting mail. Character recognition can also form a part in applications like intelligent scanning machines, text to speech converters, and automatic language-to-language translators.",Oacotaopchresy,55.0,9.0,2.0
6832,Optical Character Recognition,36.0,an optical character recognition system for printed telugu text,5.0,201.0,1.0,180.0,3.0,2.8,145.2,50,https://www.academia.edu/download/47911598/s10044-004-0217-220160809-29365-l5q3ko.pdf,"Telugu is one of the oldest and popular languages of India, spoken by more than 66 million people, especially in South India. Not much work has been reported on the development of optical character recognition (OCR) systems for Telugu text. Therefore, it is an area of current research. Some characters in Telugu are made up of more than one connected symbol. Compound characters are written by associating modifiers with consonants, resulting in a huge number of possible combinations, running into hundreds of thousands. A compound character may contain one or more connected symbols. Therefore, systems developed for documents of other scripts, like Roman, cannot be used directly for the Telugu language.The individual connected portions of a character or a compound character are defined as basic symbols in this paper and treated as a unit of recognition. The algorithms designed exploit special characteristics of Telugu script for processing the document images efficiently. The algorithms have been implemented to create a Telugu OCR system for printed text (TOSP). The output of TOSP is in phonetic English that can be transliterated to generate editable Telugu text. A special feature of TOSP is that it is designed to handle a large variety of sizes and multiple fonts, and still provides raw OCR accuracy of nearly 98%. The phonetic English representation can be also used to develop a Telugu text-to-speech system; work is in progress in this regard.",Oanopchresyfoprtete,34.0,29.0,3.0
6833,Optical Character Recognition,176.0,unknown-box approximation to improve optical character recognition performance,3.0,91.0,4.0,201.0,1.0,2.8,149.5,51,https://arxiv.org/pdf/2105.07983,"Optical character recognition (OCR) is a widely used pattern recognition application in numerous domains. There are several feature-rich, general-purpose OCR solutions available for consumers, which can provide moderate to excellent accuracy levels. However, accuracy can diminish with difficult and uncommon document domains. Preprocessing of document images can be used to minimize the effect of domain shift. In this paper, a novel approach is presented for creating a customized preprocessor for a given OCR engine. Unlike the previous OCR agnostic preprocessing techniques, the proposed approach approximates the gradient of a particular OCR engine to train a preprocessor module. Experiments with two datasets and two OCR engines show that the presented preprocessor is able to improve the accuracy of the OCR up to 46% from the baseline by applying pixel-level manipulations to the document image. The implementation of the proposed method and the enhanced public datasets are available for download.",Ounaptoimopchrepe,0.0,39.0,0.0
6834,Optical Character Recognition,401.0,attention-based extraction of structured information from street view imagery,1.0,1.0,5.0,201.0,1.0,2.6,181.0,52,http://arxiv.org/pdf/2108.06302v1,Localization of street objects from images has gained a lot of attention in recent years. We propose an approach to improve asset geolocation from street view imagery by enhancing the quality of the metadata associated with the images using Structure from Motion. The predicted object geolocation is further refined by imposing contextual geographic information extracted from OpenStreetMap. Our pipeline is validated experimentally against the state of the art approaches for geotagging traffic lights.,Oatexofstinfrstviim,100.0,32.0,15.0
6835,Optical Character Recognition,401.0,adapting the tesseract open source ocr engine for multilingual ocr,1.0,3.0,5.0,201.0,1.0,2.6,181.8,53,http://arxiv.org/pdf/1003.5891v1,"In the present work, we have used Tesseract 2.01 open source Optical Character Recognition (OCR) Engine under Apache License 2.0 for recognition of handwriting samples of lower case Roman script. Handwritten isolated and free-flow text samples were collected from multiple users. Tesseract is trained to recognize user-specific handwriting samples of both the categories of document pages. On a single user model, the system is trained with 1844 isolated handwritten characters and the performance is tested on 1133 characters, taken form the test set. The overall character-level accuracy of the system is observed as 83.5%. The system fails to segment 5.56% characters and erroneously classifies 10.94% characters.",Oadthteopsoocenfomuoc,111.0,17.0,8.0
6836,Optical Character Recognition,401.0,chinese text in the wild,1.0,7.0,5.0,201.0,1.0,2.6,183.4,54,http://arxiv.org/pdf/1803.00085v1,"We introduce Chinese Text in the Wild, a very large dataset of Chinese text in street view images. While optical character recognition (OCR) in document images is well studied and many commercial tools are available, detection and recognition of text in natural images is still a challenging problem, especially for more complicated character sets such as Chinese text. Lack of training data has always been a problem, especially for deep learning methods which require massive training data.   In this paper we provide details of a newly created dataset of Chinese text with about 1 million Chinese characters annotated by experts in over 30 thousand street view images. This is a challenging dataset with good diversity. It contains planar text, raised text, text in cities, text in rural areas, text under poor illumination, distant text, partially occluded text, etc. For each character in the dataset, the annotation includes its underlying character, its bounding box, and 6 attributes. The attributes indicate whether it has complex background, whether it is raised, whether it is handwritten or printed, etc. The large size and diversity of this dataset make it suitable for training robust neural networks for various tasks, particularly detection and recognition. We give baseline results using several state-of-the-art networks, including AlexNet, OverFeat, Google Inception and ResNet for character recognition, and YOLOv2 for character detection in images. Overall Google Inception has the best performance on recognition with 80.5% top-1 accuracy, while YOLOv2 achieves an mAP of 71.0% on detection. Dataset, source code and trained models will all be publicly available on the website.",Ochteinthwi,16.0,35.0,6.0
6837,Optical Character Recognition,401.0,end-to-end interpretation of the french street name signs dataset,1.0,8.0,5.0,201.0,1.0,2.6,183.8,55,http://arxiv.org/pdf/1702.03970v1,"We introduce the French Street Name Signs (FSNS) Dataset consisting of more than a million images of street name signs cropped from Google Street View images of France. Each image contains several views of the same street name sign. Every image has normalized, title case folded ground-truth text as it would appear on a map. We believe that the FSNS dataset is large and complex enough to train a deep network of significant complexity to solve the street name extraction problem ""end-to-end"" or to explore the design trade-offs between a single complex engineered network and multiple sub-networks designed and trained to solve sub-problems. We present such an ""end-to-end"" network/graph for Tensor Flow and its results on the FSNS dataset.",Oeninofthfrstnasida,33.0,19.0,3.0
6838,Optical Character Recognition,401.0,license plate detection and recognition in unconstrained scenarios,1.0,11.0,5.0,201.0,1.0,2.6,185.0,56,http://arxiv.org/pdf/1906.04376v3,"License plate detection and recognition (LPDR) is of growing importance for enabling intelligent transportation and ensuring the security and safety of the cities. However, LPDR faces a big challenge in a practical environment. The license plates can have extremely diverse sizes, fonts and colors, and the plate images are usually of poor quality caused by skewed capturing angles, uneven lighting, occlusion, and blurring. In applications such as surveillance, it often requires fast processing. To enable real-time and accurate license plate recognition, in this work, we propose a set of techniques: 1) a contour reconstruction method along with edge-detection to quickly detect the candidate plates; 2) a simple zero-one-alternation scheme to effectively remove the fake top and bottom borders around plates to facilitate more accurate segmentation of characters on plates; 3) a set of techniques to augment the training data, incorporate SIFT features into the CNN network, and exploit transfer learning to obtain the initial parameters for more effective training; and 4) a two-phase verification procedure to determine the correct plate at low cost, a statistical filtering in the plate detection stage to quickly remove unwanted candidates, and the accurate CR results after the CR process to perform further plate verification without additional processing. We implement a complete LPDR system based on our algorithms. The experimental results demonstrate that our system can accurately recognize license plate in real-time. Additionally, it works robustly under various levels of illumination and noise, and in the presence of car movement. Compared to peer schemes, our system is not only among the most accurate ones but is also the fastest, and can be easily applied to other scenarios.",Olipldeanreinunsc,106.0,33.0,15.0
6839,Optical Character Recognition,401.0,image-to-markup generation with coarse-to-fine attention,1.0,12.0,5.0,201.0,1.0,2.6,185.4,57,http://arxiv.org/pdf/1709.06308v1,"Attention mechanisms have been widely applied in the Visual Question Answering (VQA) task, as they help to focus on the area-of-interest of both visual and textual information. To answer the questions correctly, the model needs to selectively target different areas of an image, which suggests that an attention-based model may benefit from an explicit attention supervision. In this work, we aim to address the problem of adding attention supervision to VQA models. Since there is a lack of human attention data, we first propose a Human Attention Network (HAN) to generate human-like attention maps, training on a recently released dataset called Human ATtention Dataset (VQA-HAT). Then, we apply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the human-like attention maps for all image-question pairs. The generated human-like attention map dataset for the VQA v2.0 dataset is named as Human-Like ATtention (HLAT) dataset. Finally, we apply human-like attention supervision to an attention-based VQA model. The experiments show that adding human-like supervision yields a more accurate attention together with a better performance, showing a promising future for human-like attention supervision in VQA.",Oimgewicoat,114.0,47.0,14.0
6840,Optical Character Recognition,401.0,exploring cross-image pixel contrast for semantic segmentation,1.0,17.0,5.0,201.0,1.0,2.6,187.4,58,http://arxiv.org/pdf/2101.11939v4,"Current semantic segmentation methods focus only on mining ""local"" context, i.e., dependencies between pixels within individual images, by context-aggregation modules (e.g., dilated convolution, neural attention) or structure-aware optimization criteria (e.g., IoU-like loss). However, they ignore ""global"" context of the training data, i.e., rich semantic relations between pixels across different images. Inspired by the recent advance in unsupervised contrastive representation learning, we propose a pixel-wise contrastive framework for semantic segmentation in the fully supervised setting. The core idea is to enforce pixel embeddings belonging to a same semantic class to be more similar than embeddings from different classes. It raises a pixel-wise metric learning paradigm for semantic segmentation, by explicitly exploring the structures of labeled pixels, which were rarely explored before. Our method can be effortlessly incorporated into existing segmentation frameworks without extra overhead during testing. We experimentally show that, with famous segmentation models (i.e., DeepLabV3, HRNet, OCR) and backbones (i.e., ResNet, HR-Net), our method brings consistent performance improvements across diverse datasets (i.e., Cityscapes, PASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our community to rethink the current de facto training paradigm in fully supervised semantic segmentation.",Oexcrpicofosese,26.0,86.0,5.0
6841,Optical Character Recognition,401.0,e2e-mlt - an unconstrained end-to-end method for multi-language scene text,1.0,19.0,5.0,201.0,1.0,2.6,188.2,59,http://arxiv.org/pdf/1711.11249v1,"Arbitrary-oriented text detection in the wild is a very challenging task, due to the aspect ratio, scale, orientation, and illumination variations. In this paper, we propose a novel method, namely Arbitrary-oriented Text (or ArbText for short) detector, for efficient text detection in unconstrained natural scene images. Specifically, we first adopt the circle anchors rather than the rectangular ones to represent bounding boxes, which is more robust to orientation variations. Subsequently, we incorporate a pyramid pooling module into the Single Shot MultiBox Detector framework, in order to simultaneously explore the local and global visual information, which can, therefore, generate more confidential detection results. Experiments on established scene-text datasets, such as the ICDAR 2015 and MSRA-TD500 datasets, have demonstrated the supe rior performance of the proposed method, compared to the state-of-the-art approaches.",Oe2-anunenmefomuscte,52.0,48.0,10.0
6842,Optical Character Recognition,401.0,an unsupervised model of orthographic variation for historical document transcription,1.0,20.0,5.0,201.0,1.0,2.6,188.6,60,http://arxiv.org/pdf/1503.02335v1,"Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.",Oanunmooforvafohidotr,11.0,23.0,0.0
6843,Optical Character Recognition,401.0,document rectification and illumination correction using a patch-based cnn,1.0,21.0,5.0,201.0,1.0,2.6,189.0,61,http://arxiv.org/pdf/1909.09470v1,"We propose a novel learning method to rectify document images with various distortion types from a single input image. As opposed to previous learning-based methods, our approach seeks to first learn the distortion flow on input image patches rather than the entire image. We then present a robust technique to stitch the patch results into the rectified document by processing in the gradient domain. Furthermore, we propose a second network to correct the uneven illumination, further improving the readability and OCR accuracy. Due to the less complex distortion present on the smaller image patches, our patch-based approach followed by stitching and illumination correction can significantly improve the overall accuracy in both the synthetic and real datasets.",Odoreanilcousapacn,10.0,70.0,2.0
6844,Optical Character Recognition,401.0,"image-based table recognition: data, model, and evaluation",1.0,22.0,5.0,201.0,1.0,2.6,189.4,62,http://arxiv.org/pdf/2110.15132v1,"High-quality Web tables are rich sources of information that can be used to populate Knowledge Graphs (KG). The focus of this paper is an evaluation of methods for table-to-class annotation, which is a sub-task of Table Interpretation (TI). We provide a formal definition for table classification as a machine learning task. We propose an experimental setup and we evaluate 5 fundamentally different approaches to find the best method for generating vector table representations. Our findings indicate that although transfer learning methods achieve high F1 score on the table classification task, dedicated table encoding models are a promising direction as they appear to capture richer semantics.",Oimtaredamoanev,37.0,46.0,8.0
6845,Optical Character Recognition,401.0,"lights, camera, action! a framework to improve nlp accuracy over ocr documents",1.0,23.0,5.0,201.0,1.0,2.6,189.8,63,http://arxiv.org/abs/hep-lat/9711017v1,"Recent developments in calculations of the light hadron spectrum are reviewed. Particular emphasis is placed on discussion of to what extent the quenched spectrum agrees with experiment. Recent progress, both for quenched and full QCD, in reducing scaling violation with the use of improved actions is presented.",Olicaacafrtoimnlacovocdo,0.0,26.0,0.0
6846,Optical Character Recognition,401.0,the newspaper navigator dataset: extracting and analyzing visual content from 16 million historic newspaper pages in chronicling america,1.0,24.0,5.0,201.0,1.0,2.6,190.2,64,http://arxiv.org/pdf/2005.01583v1,"Chronicling America is a product of the National Digital Newspaper Program, a partnership between the Library of Congress and the National Endowment for the Humanities to digitize historic newspapers. Over 16 million pages of historic American newspapers have been digitized for Chronicling America to date, complete with high-resolution images and machine-readable METS/ALTO OCR. Of considerable interest to Chronicling America users is a semantified corpus, complete with extracted visual content and headlines. To accomplish this, we introduce a visual content recognition model trained on bounding box annotations of photographs, illustrations, maps, comics, and editorial cartoons collected as part of the Library of Congress's Beyond Words crowdsourcing initiative and augmented with additional annotations including those of headlines and advertisements. We describe our pipeline that utilizes this deep learning model to extract 7 classes of visual content: headlines, photographs, illustrations, maps, comics, editorial cartoons, and advertisements, complete with textual content such as captions derived from the METS/ALTO OCR, as well as image embeddings for fast image similarity querying. We report the results of running the pipeline on 16.3 million pages from the Chronicling America corpus and describe the resulting Newspaper Navigator dataset, the largest dataset of extracted visual content from historic newspapers ever produced. The Newspaper Navigator dataset, finetuned visual content recognition model, and all source code are placed in the public domain for unrestricted re-use.",Othnenadaexananvicofr16mihinepaincham,7.0,68.0,1.0
6847,Optical Character Recognition,401.0,a hybrid approach to automatic corpus generation for chinese spelling check,1.0,27.0,5.0,201.0,1.0,2.6,191.4,65,http://arxiv.org/pdf/2008.12281v3,"Spell check is a useful application which processes noisy human-generated text. Spell check for Chinese poses unresolved problems due to the large number of characters, the sparse distribution of errors, and the dearth of resources with sufficient coverage of heterogeneous and shifting error domains. For Chinese spell check, filtering using confusion sets narrows the search space and makes finding corrections easier. However, most, if not all, confusion sets used to date are fixed and thus do not include new, shifting error domains. We propose a scalable adaptable filter that exploits hierarchical character embeddings to (1) obviate the need to handcraft confusion sets, and (2) resolve sparsity problems related to infrequent errors. Our approach compares favorably with competitive baselines and obtains SOTA results on the 2014 and 2015 Chinese Spelling Check Bake-off datasets.",Oahyaptoaucogefochspch,27.0,45.0,8.0
6848,Optical Character Recognition,401.0,parallel iterative edit models for local sequence transduction,1.0,28.0,5.0,201.0,1.0,2.6,191.8,66,http://arxiv.org/pdf/1910.02893v2,"We present a Parallel Iterative Edit (PIE) model for the problem of local sequence transduction arising in tasks like Grammatical error correction (GEC). Recent approaches are based on the popular encoder-decoder (ED) model for sequence to sequence learning. The ED model auto-regressively captures full dependency among output tokens but is slow due to sequential decoding. The PIE model does parallel decoding, giving up the advantage of modelling full dependency in the output, yet it achieves accuracy competitive with the ED model for four reasons: 1.~predicting edits instead of tokens, 2.~labeling sequences instead of generating sequences, 3.~iteratively refining predictions to capture dependencies, and 4.~factorizing logits over edits and their token argument to harness pre-trained language models like BERT. Experiments on tasks spanning GEC, OCR correction and spell correction demonstrate that the PIE model is an accurate and significantly faster alternative for local sequence transduction.",Opaitedmofolosetr,37.0,42.0,7.0
6849,Optical Character Recognition,401.0,larex - a semi-automatic open-source tool for layout analysis and region extraction on early printed books,1.0,29.0,5.0,201.0,1.0,2.6,192.2,67,http://arxiv.org/pdf/1811.07442v1,This paper describes an approach to automatically extracting floor plans from the kinds of incomplete measurements that could be acquired by an autonomous mobile robot. The approach proceeds by reasoning about extended structural layout surfaces which are automatically extracted from the available data. The scheme can be run in an online manner to build water tight representations of the environment. The system effectively speculates about room boundaries and free space regions which provides useful guidance to subsequent motion planning systems. Experimental results are presented on multiple data sets.,Ola-aseoptofolaananreexoneaprbo,11.0,11.0,2.0
6850,Optical Character Recognition,401.0,case study of a highly automated layout analysis and ocr of an incunabulum: 'der heiligen leben' (1488),1.0,30.0,5.0,201.0,1.0,2.6,192.6,68,http://arxiv.org/pdf/1701.07395v1,"This paper provides the first thorough documentation of a high quality digitization process applied to an early printed book from the incunabulum period (1450-1500). The entire OCR related workflow including preprocessing, layout analysis and text recognition is illustrated in detail using the example of 'Der Heiligen Leben', printed in Nuremberg in 1488. For each step the required time expenditure was recorded. The character recognition yielded excellent results both on character (97.57%) and word (92.19%) level. Furthermore, a comparison of a highly automated (LAREX) and a manual (Aletheia) method for layout analysis was performed. By considerably automating the segmentation the required human effort was reduced significantly from over 100 hours to less than six hours, resulting in only a slight drop in OCR accuracy. Realistic estimates for the human effort necessary for full text extraction from incunabula can be derived from this study. The printed pages of the complete work together with the OCR result is available online ready to be inspected and downloaded.",Ocastofahiaulaananocofanin'dhele(1,16.0,16.0,0.0
6851,Optical Character Recognition,401.0,"accurate, data-efficient, unconstrained text recognition with convolutional neural networks",1.0,34.0,5.0,201.0,1.0,2.6,194.2,69,http://arxiv.org/pdf/1812.11894v1,"Unconstrained text recognition is an important computer vision task, featuring a wide variety of different sub-tasks, each with its own set of challenges. One of the biggest promises of deep neural networks has been the convergence and automation of feature extractors from input raw signals, allowing for the highest possible performance with minimum required domain knowledge. To this end, we propose a data-efficient, end-to-end neural network model for generic, unconstrained text recognition. In our proposed architecture we strive for simplicity and efficiency without sacrificing recognition accuracy. Our proposed architecture is a fully convolutional network without any recurrent connections trained with the CTC loss function. Thus it operates on arbitrary input sizes and produces strings of arbitrary length in a very efficient and parallelizable manner. We show the generality and superiority of our proposed text recognition architecture by achieving state of the art results on seven public benchmark datasets, covering a wide spectrum of text recognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR, License Plate Recognition, and Scene Text Recognition. Our proposed architecture has won the ICFHR2018 Competition on Automated Text Recognition on a READ Dataset.",Oacdaunterewiconene,38.0,112.0,2.0
6852,Optical Character Recognition,401.0,table structure recognition using top-down and bottom-up cues,1.0,36.0,5.0,201.0,1.0,2.6,195.0,70,http://arxiv.org/pdf/2010.04565v1,"Tables are information-rich structured objects in document images. While significant work has been done in localizing tables as graphic objects in document images, only limited attempts exist on table structure recognition. Most existing literature on structure recognition depends on extraction of meta-features from the PDF document or on the optical character recognition (OCR) models to extract low-level layout features from the image. However, these methods fail to generalize well because of the absence of meta-features or errors made by the OCR when there is a significant variance in table layouts and text organization. In our work, we focus on tables that have complex structures, dense content, and varying layouts with no dependency on meta-features and/or OCR.   We present an approach for table structure recognition that combines cell detection and interaction modules to localize the cells and predict their row and column associations with other detected cells. We incorporate structural constraints as additional differential components to the loss function for cell detection. We empirically validate our method on the publicly available real-world datasets - ICDAR-2013, ICDAR-2019 (cTDaR) archival, UNLV, SciTSR, SciTSR-COMP, TableBank, and PubTabNet. Our attempt opens up a new direction for table structure recognition by combining top-down (table cells detection) and bottom-up (structure recognition) cues in visually understanding the tables.",Otastreustoanbocu,10.0,60.0,4.0
6853,Optical Character Recognition,401.0,gated recurrent convolution neural network for ocr,1.0,37.0,5.0,201.0,1.0,2.6,195.4,71,http://arxiv.org/abs/2106.02859v1,"The convolutional neural network (CNN) has become a basic model for solving many computer vision problems. In recent years, a new class of CNNs, recurrent convolution neural network (RCNN), inspired by abundant recurrent connections in the visual systems of animals, was proposed. The critical element of RCNN is the recurrent convolutional layer (RCL), which incorporates recurrent connections between neurons in the standard convolutional layer. With increasing number of recurrent computations, the receptive fields (RFs) of neurons in RCL expand unboundedly, which is inconsistent with biological facts. We propose to modulate the RFs of neurons by introducing gates to the recurrent connections. The gates control the amount of context information inputting to the neurons and the neurons' RFs therefore become adaptive. The resulting layer is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several computer vision tasks including object recognition, scene text recognition and object detection, and obtained much better results than the RCNN. In addition, when combined with other adaptive RF techniques, the GRCNN demonstrated competitive performance to the state-of-the-art models on benchmark datasets for these tasks. The codes are released at \href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}.",Ogareconenefooc,88.0,44.0,10.0
6854,Optical Character Recognition,41.0,an improved scene text extraction method using conditional random field and optical character recognition,4.0,201.0,1.0,114.0,3.0,2.5,126.9,72,http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520a708.pdf,"Over the past few years, research on scene text extraction has developed rapidly. Recently, condition random field (CRF) has been used to give connected components (CCs) 'text' or 'non-text' labels. However, a burning issue in CRF model comes from multiple text lines extraction. In this paper, we propose a two-step iterative CRF algorithm with a Belief Propagation inference and an OCR filtering stage. Two kinds of neighborhood relationship graph are used in the respective iterations for extracting multiple text lines. Furthermore, OCR confidence is used as an indicator for identifying the text regions, while a traditional OCR filter module only considered the recognition results. The first CRF iteration aims at finding certain text CCs, especially in multiple text lines, and sending uncertain CCs to the second iteration. The second iteration gives second chance for the uncertain CCs and filter false alarm CCs with the help of OCR. Experiments based on the public dataset of ICDAR 2005 prove that the proposed method is comparative with the existing algorithms.",Oanimscteexmeuscorafianopchre,44.0,14.0,1.0
6855,Optical Character Recognition,120.0,handwritten optical character recognition system for sindhi numerals,3.0,201.0,1.0,45.0,4.0,2.5,129.9,73,http://arxiv.org/abs/1706.00069v1,"Sindhi language is script language like Arabic and Persian. It's origin is 2500 years old and spoken in various countries in Asia. In this paper, we propose an Optical Character Recognition (OCR) system which recognizes handwritten Sindhi numeral expressions (i.e. Sindhi handwritten numeral strings) without using common input devices such as keyboard and storage device memory. Our experiments focus on character recognition which later can be used for various applications such as tutoring, mathematical kids games, and automatic telephone number conversion from sign boards in India and Pakistan. In our research, we investigate the correlation between the numeral shapes and apply famous state-of-the art classifier based on correlation based template matching. We experimentally show that template matching gives poor performance as the shapes of numerals are highly correlated. There exists little volume of literature to address OCR on Sindhi language but unavailability of benchmark dataset makes it difficult for researchers around the world to re-implement the literature frameworks. We provide two sets of images which can be used for training and prediction.",Ohaopchresyfosinu,10.0,16.0,0.0
6856,Optical Character Recognition,64.0,rwth ocr: a large vocabulary optical character recognition system for arabic scripts,4.0,201.0,1.0,104.0,3.0,2.5,130.8,74,http://arxiv.org/pdf/1001.5352v1,"We present a novel large vocabulary OCR system, which implements a confidence- and margin-based discriminative training approach for model adaptation of an HMM-based recognition system to handle multiple fonts, different handwriting styles, and their variations. Most current HMM approaches are HTK-based systems which are maximum likelihood (ML) trained and which try to adapt their models to different writing styles using writer adaptive training, unsupervised clustering, or additional writer-specific data. Here, discriminative training based on the maximum mutual information (MMI) and minimum phone error (MPE) criteria are used instead. For model adaptation during decoding, an unsupervised confidence-based discriminative training within a two-pass decoding process is proposed. Additionally, we use neural network-based features extracted by a hierarchical multi-layer perceptron (MLP) network either in a hybrid MLP/HMM approach or to discriminatively retrain a Gaussian HMM system in a tandem approach. The proposed framework and methods are evaluated for closed-vocabulary isolated handwritten word recognition on the IFN/ENIT-database Arabic handwriting database, where the word error rate is decreased by more than 50 % relative to an ML trained baseline system. Preliminary results for large vocabulary Arabic machine-printed text recognition tasks are presented on a novel publicly available newspaper database.",Orwocalavoopchresyfoarsc,33.0,79.0,1.0
6857,Optical Character Recognition,102.0,an efficient optical character recognition algorithm using artificial neural network by curvature properties of characters,3.0,201.0,1.0,66.0,4.0,2.5,130.8,75,http://arxiv.org/pdf/1904.12592v1,"Optical Character Recognition (OCR) has gained so much importance among the researchers now a day as it is an eminent sector for a Human Computer Interaction (HCI) System. For an efficient recognition system the primary need is a reliable feature extraction process. So far the feature extraction systems used are mainly based on the character pattern, enclosure or symmetry. Still another property which is based on the angular properties of the several predetermined positions can be used for the purpose of feature extraction process that is the main motivation of this work. The effectiveness of the algorithm has been discussed in the experimental result section where the performance has been compared for different number of feature used.",Oanefopchrealusarnenebycuprofch,14.0,16.0,0.0
6858,Optical Character Recognition,56.0,shape feature and fuzzy logic based offline devnagari handwritten optical character recognition,4.0,201.0,1.0,123.0,3.0,2.5,134.1,76,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.730.7794&rep=rep1&type=pdf,"In handwritten character recognition, benchmark database plays an important role in evaluating the performance of various algorithms and the results obtained by various researchers. In Devnagari script, there is lack of such official benchmark. This paper focuses on the generation of offline benchmark database for Devnagari handwritten numerals and characters. The present work generated 5137 and 20305 isolated samples for numeral and character database, respectively, from 750 writers of all ages, sex, education, and profession. The offline sample images are stored in TIFF image format as it occupies less memory. Also, the data is presented in binary level so that memory requirement is further reduced. It will facilitate research on handwriting recognition of Devnagari script through free access to the researchers.",Oshfeanfulobaofdehaopchre,49.0,0.0,6.0
6859,Optical Character Recognition,119.0,unsupervised feature learning for optical character recognition,3.0,201.0,1.0,67.0,4.0,2.5,136.2,77,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1073.9847&rep=rep1&type=pdf,"Most of the popular optical character recognition (OCR) architectures use a set of handcrafted features and a powerful classifier for isolated character classification. Success of these methods often depend on the suitability of these features for the language of interest. In recent years, whole word recognition based on Recurrent Neural Networks (RNN) has gained popularity. These methods use simple features such as raw pixel values or profiles. Success of these methods depend on the learning capabilities of these networks to encode the script and language information. In this work, we investigate the possibility of learning an appropriate set of features for designing OCR for a specific language. We learn the language specific features from the data with no supervision. This enables the seamless adaptation of the architecture across languages. In this work, we learn features using a stacked Restricted Boltzman Machines (RBM) and use it with the RNN based recognition solution. We validate our method on five different languages. In addition, these novel features also result in better convergence rate of the RNNs.",Ounfelefoopchre,5.0,16.0,0.0
6861,Optical Character Recognition,50.0,a study of car park control system using optical character recognition,4.0,201.0,1.0,148.0,3.0,2.5,139.8,78,https://www.researchgate.net/profile/Anton-Satria-Prabuwono-2/publication/224368698_A_Study_of_Car_Park_Control_System_Using_Optical_Character_Recognition/links/5f7c171392851c14bcb169b8/A-Study-of-Car-Park-Control-System-Using-Optical-Character-Recognition.pdf,"This paper presents a study and design of car park control system using optical character recognition (OCR) devices. The system uses client server environment. The administrator will monitor the system and the database from the server side. Furthermore the parking information will be displayed static based on the database shared by the server. Server application and database will be stored in the server. The result shows the system is capable to save log record that will ease tracking parking user, updating user and parking credit database as well as monitoring availability of parking spaces.",Oastofcapacosyusopchre,21.0,11.0,2.0
6862,Optical Character Recognition,130.0,an optical character recognition of machine printed oriya script,3.0,201.0,1.0,69.0,4.0,2.5,140.1,79,http://arxiv.org/pdf/2009.07433v1,"Character recognition research is one of the most challenging research areas which put many of the research areas of Digital Image processing like Noise Removal, Machine Learning, and Object Recognition under one roof. If we focus on India particularly, the field has huge amount of work to be done. In current technological era OCR systems for Indian scripts has great application opportunities with their significant use like digital libraries of literatures written in different Indian scripts. Oriya is one of the officially recognized languages of India, and is the official language of the state Orissa. Through this paper I am introducing a Character Recognition System for machine printed Oriya symbols. Paper describes in detail the Feature extraction process, which is carried out by some conventional with two novel enhancements to Structural Features like detection of vertical line in a character and detection of open space in lower zone of a character. Classification process is completed by implementing a novel combination of Binary tree and Naïve Bayesian classifier, which has never been used for Oriya script earlier.",Oanopchreofmaprorsc,9.0,7.0,0.0
6864,Optical Character Recognition,103.0,multi font and size optical character recognition using template matching,3.0,201.0,1.0,99.0,4.0,2.5,141.0,80,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.413.7805&rep=rep1&type=pdf,"Optical character recognition (OCR) is very popular research field since 1950's. A great work has been done for various scripts based on the different approaches used for the design of OCR system. There are various approaches used for the design of OCR system, But this paper provide a review of existing works in handwritten and typewritten recognition of characters from image irrespective of their position, size, and various font styles based on the different approaches such as a Template Matching, Fuzzy Logic, Neural Network, etc.",Omufoansiopchreustema,10.0,9.0,0.0
6865,Optical Character Recognition,62.0,optical character recognition for english and tamil using support vector machines,4.0,201.0,1.0,141.0,3.0,2.5,141.3,81,http://arxiv.org/pdf/0907.4960v1,"Optical Character Recognition is an evergreen area of research and is verily used in various real time applications. This paper proposes a new technique of Optical character Recognition using Gabor filters and Support Vector machines (SVM). This method proves to be very effective with the use of Gabor filters for feature extraction and SVM for developing the model. The model proposed is trained and validated for two languages – English and Tamil and the results are found to be very much encouraging. The model developed works for the entire character set in both the languages including symbols and numerals. In addition , the model can recognise the characetrs of six different fonts in English and Twelve different fonts in Tamil. The average accuracy of recognition for English is 97% and for Tamil it is 84%, which is achieved in just three iterations of training. The method can turn out to be a suitable candidate for future applications in this area.",Oopchrefoenantaussuvema,12.0,13.0,0.0
6866,Optical Character Recognition,72.0,optical character recognition based on least square support vector machine,4.0,201.0,1.0,138.0,3.0,2.5,143.4,82,http://arxiv.org/abs/1807.03613v1,"Optical character recognition (OCR) is a very active field for research and development, and has become one of the most successful applications of automatic pattern recognition. To avoid the curse of dimensionality and improve the recognition performance, an optical character recognition system based on image preprocessing technologies combined with Least Square Support Vector Machine (LS-SVM) has been developed, which first uses dynamic thresholding operation and robust gray value normalization to segment characters and extract features respectively, and then uses LS-SVM to classify characters based on features. The proposed method has been evaluated by carrying out recognition experiments on the optical characters of electronic components. The results show that the proposed method has a better recognition performance, and holds a lot of potential for developing robust recognition learning.",Oopchrebaonlesqsuvema,19.0,12.0,0.0
6867,Optical Character Recognition,47.0,combining multiple classifiers for faster optical character recognition,4.0,201.0,1.0,167.0,3.0,2.5,144.6,83,https://link.springer.com/content/pdf/10.1007/11669487_32.pdf,Traditional approaches to combining classifiers attempt to improve classification accuracy at the cost of increased processing. They may be viewed as providing an accuracy-speed trade-off: higher accuracy for lower speed. In this paper we present a novel approach to combining multiple classifiers to solve the inverse problem of significantly improving classification speeds at the cost of slightly reduced classification accuracy. We propose a cascade architecture for combining classifiers and cast the process of building such a cascade as a search and optimization problem. We present two algorithms based on steepest-descent and dynamic programming for producing approximate solutions fast. We also present a simulated annealing algorithm and a depth-first-search algorithm for finding optimal solutions. Results on handwritten optical character recognition indicate that a) a speedup of 4-9 times is possible with no increase in error and b) speedups of up to 15 times are possible when twice as many errors can be tolerated.,Ocomuclfofaopchre,24.0,9.0,4.0
6868,Optical Character Recognition,44.0,optical character recognition of bangla characters using neural network: a better approach,4.0,201.0,1.0,179.0,3.0,2.5,147.3,84,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.997.323&rep=rep1&type=pdf,"This is a complete Optical Character Recognition system for printed Bangla text with a perspective of implementation. Suggestions have been made on the basis of the problems confronted in developing the software. This paper describes the efficient ways involving line and word detection, zoning, character separations and character recognition. Employing Skewness correction, thinning and better approach in scaling have obviously enhanced the performance of the OCR system in comparison with the existing ones. Application of neural network in detection of characters has made the process even faster with optimum performance.",Oopchreofbachusneneabeap,24.0,4.0,1.0
6869,Optical Character Recognition,74.0,improving optical character recognition,4.0,201.0,1.0,158.0,3.0,2.5,150.0,85,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.518.5363&rep=rep1&type=pdf,"There is a clear need for optical character recognition in order to provide a fast and accurate method to search both existing images as well as large archives of existing paper documents. However, existing optical character recognition programs suffer from a flawed tradeoff between speed and accuracy, making it less attractive for large quantities of documents. This paper analyzes five different algorithms which operate completely independently of optical character recognition programs, but which have the combined effect of decreasing computational complexity and increasing overall accuracy. Finally, the paper proposes implementing each of these algorithms on the GPU, as well as optical character recognition programs themselves, in order to deliver another massive speed increase.",Oimopchre,6.0,5.0,0.0
6873,Optical Character Recognition,55.0,motion deblurring for optical character recognition,4.0,201.0,1.0,191.0,3.0,2.5,154.2,86,https://core.ac.uk/download/pdf/48627518.pdf,"This paper investigates the problem of blurring caused by motion during image capture of text documents. Motion blurring prevents proper optical character recognition of the document text contents. One area of such applications is to deblur name card images obtained from handheld cameras. In this paper, a complete motion deblurring procedure for document images has been proposed. The method handles both uniform linear motion blur and uniform acceleration motion blur. Experiments on synthetic and real-life blurred images prove the feasibility and reliability of this algorithm provided that the motion is not too irregular. The restoration procedure consumes only small amount of computation time.",Omodefoopchre,17.0,8.0,0.0
6874,Optical Character Recognition,94.0,the impact of a modified repeated-reading strategy paired with optical character recognition on the reading rates of students with visual impairments,4.0,201.0,1.0,181.0,3.0,2.5,162.9,87,https://files.eric.ed.gov/fulltext/EJ683404.pdf,"The repeated-reading strategy and optical character recognition were paired to demonstrate a functional relationship between the combined strategies and two factors: the reading rates of students with visual impairments and the students’ self-perceptions, or attitudes, toward reading. The results indicated that all five students increased their reading rates, and four students’ attitudes toward reading improved.",Othimofamorestpawiopchreonthreraofstwiviim,26.0,44.0,4.0
6875,Optical Character Recognition,2.0,optical character recognition technique algorithms.,5.0,201.0,1.0,201.0,1.0,2.2,141.3,88,http://www.jatit.org/volumes/Vol83No2/15Vol83No2.pdf,"Braille has empowered visually challenged community to read and write. But at the same time, it has created a gap due to widespread inability of non-Braille users to understand Braille scripts. This gap has fuelled researchers to propose Optical Braille Recognition techniques to convert Braille documents to natural language. The main motivation of this work is to cement the communication gap at academic institutions by translating personal documents of blind students. This has been accomplished by proposing an economical and effective technique which digitizes Braille documents using a smartphone camera. For any given Braille image, a dot detection mechanism based on Hough transform is proposed which is invariant to skewness, noise and other deterrents. The detected dots are then clustered into Braille cells using distance-based clustering algorithm. In succession, the standard physical parameters of each Braille cells are estimated for feature extraction and classification as natural language characters. The comprehensive evaluation of this technique on the proposed dataset of 54 Braille scripts has yielded into accuracy of 98.71%.",Oopchreteal,26.0,12.0,0.0
6876,Optical Character Recognition,3.0,optical character recognition for cursive handwriting,5.0,201.0,1.0,201.0,1.0,2.2,141.60000000000002,89,http://arxiv.org/pdf/1304.0421v1,Human eye can see and read what is written or displayed either in natural handwriting or in printed format. The same work in case the machine does is called handwriting recognition. Handwriting recognition can be broken down into two categories: off-line and on-line. ...,Oopchrefocuha,178.0,51.0,9.0
6877,Optical Character Recognition,10.0,review on optical character recognition,5.0,201.0,1.0,201.0,1.0,2.2,143.7,90,https://www.researchgate.net/profile/Muna-Ahmed/publication/334162853_REVIEW_ON_OPTICAL_CHARACTER_RECOGNITION/links/5d1af333a6fdcc2462b74595/REVIEW-ON-OPTICAL-CHARACTER-RECOGNITION.pdf,"Intensive research has been done on optical character recognition ocr and a large number of articles have been published on this topic during the last few decades. Many commercial OCR systems are now available in the market, but most of these systems work for Roman, Chinese, Japanese and Arabic characters. There are no sufficient number of works on Indian language character recognition especially Kannada script among 12 major scripts in India. This paper presents a review of existing work on printed Kannada script and their results. The characteristics of Kannada script and Kannada Character Recognition System kcr are discussed in detail. Finally fusion at the classifier level is proposed to increase the recognition accuracy.",Oreonopchre,0.0,15.0,0.0
6878,Optical Character Recognition,14.0,an optical character recognition,5.0,201.0,1.0,201.0,1.0,2.2,144.9,91,http://www.m-hikari.com/ces/ces2012/ces9-12-2012/meslehCES9-12-2012.pdf,"Precise character segmentation is the only solution towards higher Optical Character Recognition (OCR) accuracy. In cursive script, overlapped characters are serious issue in the process of character segmentations as characters are deprived from their discriminative parts using conventional linear segmentation strategy. Hence, non-linear segmentation is an utmost need to avoid loss of characters parts and to enhance character/script recognition accuracy. This paper presents an improved approach for non-linear segmentation of the overlapped characters in handwritten roman script. The proposed technique is composed of a sequence of heuristic rules based on geometrical features of characters to locate possible non-linear character boundaries in a cursive script word. However, to enhance efficiency, heuristic approach is integrated with trained ensemble neural network validation strategy for verification of character boundaries. Accordingly, correct boundaries are retained and incorrect are removed based on ensemble neural networks vote. Finally, based on verified valid segmentation points, characters are segmented non-linearly. For fair comparison CEDAR benchmark database is experimented. The experimental results are much better than conventional linear character segmentation techniques reported in the state of art. Ensemble neural network play vital role to enhance character segmentation accuracy as compared to individual neural networks.",Oanopchre,35.0,10.0,4.0
6879,Optical Character Recognition,113.0,artificial neural network based on optical character recognition,3.0,201.0,1.0,102.0,3.0,2.2,144.9,92,http://vtucs.com/wp-content/uploads/2016/05/Optical_Character_Recognition_IJERT.pdf,"The recognition of optical characters is known to be one of the earliest applications of Artificial Neural Networks. In this paper, a simplified neural approach to recognition of optical or visual characters is portrayed and discussed. OCR (Optical Character Recognition) System or to improve the quality of an existing one. The use of artificial neural network simplifies development of an optical character recognition application, while achieving highest quality of recognition and good performance. The ANN is trained using the Back Propagation algorithm. In the proposed system, each typed English letter is represented by binary numbers that are used as input to a simple feature extraction system whose output, in addition to the input, are fed to an ANN. KeywordsOptical character recognition, Artificial Neural Network, supervised learning, the Multi-Layer Perception, the back propagation algorithm.",Oarnenebaonopchre,7.0,2.0,0.0
6880,Optical Character Recognition,114.0,strip shredded document reconstruction using optical character recognition,3.0,201.0,1.0,111.0,3.0,2.2,147.89999999999998,93,http://arxiv.org/pdf/2007.00779v1,"Document reconstruction affects different areas such as archeology, philology and forensics. A reconstruction of fragmented writing materials allows to retrieve and to analyze the lost content. Due to the complexity of reconstruction, automated algorithms are necessary. A reconstruction methodology for shredded documents is presented in this paper which recognizes characters at the stripes' borders and matches them subsequently. In order to achieve this, an Optical Character Recognition (OCR) system is exploited, that is capable of recognizing partially visible characters by means of local features. Thus, no binarization needs to be performed. Preliminary results show the ability of matching shredded documents using the information of cut characters. (6 pages)",Ostshdoreusopchre,19.0,0.0,0.0
6881,Optical Character Recognition,25.0,real‐time optical character recognition on field programmable gate array for automatic number plate recognition system,5.0,201.0,1.0,201.0,1.0,2.2,148.2,94,https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-cds.2012.0339,"In this paper, we propose an automatic and mechanized license and number plate recognition (LNPR) system which can extract the license plate number of the vehicles passing through a given location using image processing algorithms. No additional devices such as GPS or radio frequency identification (RFID) need to be installed for implementing the proposed system. Using special cameras, the system takes pictures from each passing vehicle and forwards the image to the computer for being processed by the LPR software. Plate recognition software uses different algorithms such as localization, orientation, normalization, segmentation and finally optical character recognition (OCR). The resulting data is applied to compare with the records on a database. Experimental results reveal that the presented system successfully detects and recognizes the vehicle number plate on real images. This system can also be used for security and traffic control.",Oreopchreonfiprgaarfoaunuplresy,28.0,28.0,3.0
6884,Optical Character Recognition,33.0,a modern optical character recognition system in a real world clinical setting: some accuracy and feasibility observations.,5.0,201.0,1.0,201.0,1.0,2.2,150.60000000000002,95,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2244242/pdf/procamiasymp00001-0097.pdf,"Optical Character Recognition (OCR) has many real world applications. The existing methods normally detect where the characters are, and then recognize the character for each detected location. Thus the accuracy of characters recognition is impacted by the performance of characters detection. In this paper, we propose a method for recognizing characters without detecting the location of each character. This is done by converting the OCR task into an image captioning task. One advantage of the proposed method is that the labeled bounding boxes for the characters are not needed during training. The experimental results show the proposed method outperforms the existing methods on both the license plate recognition and the watermeter character recognition tasks. The proposed method is also deployed into a low-power (300mW) CNN accelerator chip connected to a Raspberry Pi 3 for on-device applications.",Oamoopchresyinarewoclsesoacanfeob,32.0,9.0,0.0
6885,Optical Character Recognition,135.0,devanagri character recognition model using deep convolution neural network,3.0,201.0,1.0,120.0,3.0,2.2,156.9,96,http://arxiv.org/pdf/1002.4007v1,"Abstract In recent times, there has been a significant increase in the use of deep learning in the field of computer vision and image analysis. Deep learning is a subfield of machine learning which uses artificial neural networks that is inspired by the structure and function of the human brain. Identifying hand written text by machines has been achieved remarkable success with the use of artificial neural networks. In Optical Character Recognition for hand written text, the majority of work has been done for the popular languages such as English, Arabic or Chinese languages. There is very limited work in the literature for the handwritten character recognition for Devanagri characters. In this paper, we focus on recognition of Devanagri characters using deep convolution neural networks. Devanagri lipi is responsible for twelve languages used in India. In this paper, we optimize the network by selecting best hyperparameters for the network. Experimental results show the effectiveness of the proposed approach on the benchmark dataset.",Odechremousdeconene,20.0,11.0,1.0
6886,Optical Character Recognition,172.0,malayalam handwritten character recognition using convolutional neural network,3.0,201.0,1.0,143.0,3.0,2.2,174.9,97,http://arxiv.org/pdf/2104.04437v1,"Optical Character Recognition is the process of converting an input text image into a machine encoded format. Different methods are used in OCR for different languages. The main steps of optical character recognition are pre-processing, segmentation and recognition. Recognizing handwritten text is harder than recognizing printed text. Convolutional Neural Network has shown remarkable improvement in recognizing characters of other languages. But CNNs have not been implemented for Malayalam handwritten characters yet. The proposed system uses Convolutional neural network to extract features. This is method different from the conventional method that requires handcrafted features that needs to be used for finding features in the text. We have tested the network against a newly constructed dataset of six Malayalam characters. This is method different from the conventional method that requires handcrafted features that needs to be used for finding features in the text.",Omahachreusconene,22.0,18.0,3.0
6887,Optical Character Recognition,196.0,gpu based optical character transcription for ancient inscription recognition,3.0,201.0,1.0,150.0,3.0,2.2,184.2,98,https://www.academia.edu/download/45255746/GPU_Based_Optical_Character_Transcriptio20160501-7679-1ketuk8.pdf,"Motivated by the challenging questions of todays sinologists we are developing an automated system for processing of ancient Chinese inscriptions (sutras). As these inscriptions are not accessible due to location or damage our input data are noisy images of paper showing the texture of stones together with the inscriptions transfered by charcoal or pencil. Due to the vast amount and large sizes of the images we adopted highly parallelized -- and therefore high-performance -- anisotropic filtering using standard computer hardware. Additionally characters are localized/segmented for further processing by Optical Character Recognition. Real results for ancient Chinese inscriptions, which are mimicking non-standardized handwritings are shown.",Ogpbaopchtrfoaninre,12.0,24.0,0.0
6888,Optical Character Recognition,166.0,machine learning methods for optical character recognition,3.0,201.0,1.0,184.0,3.0,2.2,185.4,99,https://perun.pmf.uns.ac.rs/radovanovic/dmsem/completed/2006/OCR.pdf,"Success of optical character recognition depends on a number of factors, two of which are feature extraction and classi cation algorithms. In this paper we look at the results of the application of a set of classi ers to datasets obtained through various basic feature extraction methods.",Omalemefoopchre,7.0,10.0,0.0
6889,Optical Character Recognition,182.0,an application of svm in character recognition with chain code,3.0,201.0,1.0,170.0,3.0,2.2,186.0,100,http://arxiv.org/pdf/1902.01544v1,"Artificial intelligence, pattern recognition and computer vision has a significant importance in the field of electronics and image processing. Optical character recognition (OCR) is one of the main aspects of pattern recognition and has evolved greatly since its beginning. OCR is a system which recognized the readable characters from optical data and converts it into digital form. Various methodologies have been developed for this purpose using different approaches. In this paper, general architecture of modern OCR system with details of each module is discussed. We applied Moore neighborhood tracing for extracting boundary of characters and then chain rule for feature extraction. In the classification stage for character recognition, SVM is trained and is applied on suitable example.",Oanapofsvinchrewichco,23.0,27.0,1.0
7163,Optical Flow Estimation,11.0,flownet 2.0: evolution of optical flow estimation with deep networks,5.0,8.0,5.0,1.0,5.0,5.0,6.8,1,https://openaccess.thecvf.com/content_cvpr_2017/papers/Ilg_FlowNet_2.0_Evolution_CVPR_2017_paper.pdf,"The FlowNet demonstrated that optical flow estimation can be cast as a learning problem. However, the state of the art with regard to the quality of the flow has still been defined by traditional methods. Particularly on small displacements and real-world data, FlowNet cannot compete with variational methods. In this paper, we advance the concept of end-to-end learning of optical flow and make it work really well. The large improvements in quality and speed are caused by three major contributions: first, we focus on the training data and show that the schedule of presenting data during training is very important. Second, we develop a stacked architecture that includes warping of the second image with intermediate optical flow. Third, we elaborate on small displacements by introducing a subnetwork specializing on small motions. FlowNet 2.0 is only marginally slower than the original FlowNet but decreases the estimation error by more than 50%. It performs on par with state-of-the-art methods, while running at interactive frame rates. Moreover, we present faster variants that allow optical flow computation at up to 140fps with accuracy matching the original FlowNet.",Ofl2.evofopfleswidene,1742.0,37.0,234.0
7164,Optical Flow Estimation,17.0,"models matter, so does training: an empirical study of cnns for optical flow estimation",5.0,15.0,5.0,3.0,5.0,5.0,12.0,2,https://arxiv.org/pdf/1809.05571,"We investigate two crucial and closely-related aspects of CNNs for optical flow estimation: models and training. First, we design a compact but effective CNN model, called PWC-Net, according to simple and well-established principles: pyramidal processing, warping, and cost volume processing. PWC-Net is 17 times smaller in size, 2 times faster in inference, and 11 percent more accurate on Sintel final than the recent FlowNet2 model. It is the winning entry in the optical flow competition of the robust vision challenge. Next, we experimentally analyze the sources of our performance gains. In particular, we use the same training procedure for PWC-Net to retrain FlowNetC, a sub-network of FlowNet2. The retrained FlowNetC is 56 percent more accurate on Sintel final than the previously trained one and even 5 percent more accurate than the FlowNet2 model. We further improve the training procedure and increase the accuracy of PWC-Net on Sintel by 10 percent and on KITTI 2012 and 2015 by 20 percent. Our newly trained model parameters and training protocols are available on https://github.com/NVlabs/PWC-Net.",Omomasodotranemstofcnfoopfles,82.0,79.0,11.0
7165,Optical Flow Estimation,8.0,liteflownet: a lightweight convolutional neural network for optical flow estimation,5.0,27.0,5.0,7.0,5.0,5.0,15.3,3,https://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_LiteFlowNet_A_Lightweight_CVPR_2018_paper.pdf,"FlowNet2 [14], the state-of-the-art convolutional neural network (CNN) for optical flow estimation, requires over 160M parameters to achieve accurate flow estimation. In this paper we present an alternative network that attains performance on par with FlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being 30 times smaller in the model size and 1.36 times faster in the running speed. This is made possible by drilling down to architectural details that might have been missed in the current frameworks: (1) We present a more effective flow inference approach at each pyramid level through a lightweight cascaded network. It not only improves flow estimation accuracy through early correction, but also permits seamless incorporation of descriptor matching in our network. (2) We present a novel flow regularization layer to ameliorate the issue of outliers and vague flow boundaries by using a feature-driven local convolution. (3) Our network owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2. Our code and trained models are available at github.com/twhui/LiteFlowNet.",Olialiconenefoopfles,305.0,40.0,61.0
7166,Optical Flow Estimation,9.0,optical flow estimation using a spatial pyramid network,5.0,30.0,5.0,9.0,5.0,5.0,17.4,4,https://openaccess.thecvf.com/content_cvpr_2017/papers/Ranjan_Optical_Flow_Estimation_CVPR_2017_paper.pdf,"We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions, these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small (",Oopflesusasppyne,544.0,61.0,66.0
7167,Optical Flow Estimation,23.0,a fusion approach for multi-frame optical flow estimation,5.0,14.0,5.0,18.0,5.0,5.0,17.9,5,https://arxiv.org/pdf/1810.10066,"To date, top-performing optical flow estimation methods only take pairs of consecutive frames into account. While elegant and appealing, the idea of using more than two frames has not yet produced state-of-the-art results. We present a simple, yet effective fusion approach for multi-frame optical flow that benefits from longer-term temporal cues. Our method first warps the optical flow from previous frames to the current, thereby yielding multiple plausible estimates. It then fuses the complementary information carried by these estimates into a new optical flow field. At the time of writing, our method ranks first among published results in the MPI Sintel and KITTI 2015 benchmarks. Our models will be available on https://github.com/NVlabs/PWC-Net.",Oafuapfomuopfles,30.0,50.0,4.0
7168,Optical Flow Estimation,22.0,flownet: learning optical flow with convolutional networks,5.0,17.0,5.0,55.0,4.0,4.7,29.9,6,http://openaccess.thecvf.com/content_iccv_2015/papers/Dosovitskiy_FlowNet_Learning_Optical_ICCV_2015_paper.pdf,"Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",Oflleopflwicone,2394.0,43.0,378.0
7169,Optical Flow Estimation,21.0,liteflownet3: resolving correspondence ambiguity for more accurate optical flow estimation,5.0,86.0,4.0,13.0,5.0,4.6,44.6,7,https://arxiv.org/pdf/2007.09319,"Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by exploring local flow consistency. To this end, each inaccurate optical flow is replaced with an accurate one from a nearby position through a novel warping of the flow field. LiteFlowNet3 not only achieves promising results on public benchmarks but also has a small model size and a fast runtime.",Olirecoamfomoacopfles,11.0,38.0,1.0
7170,Optical Flow Estimation,38.0,iterative residual refinement for joint optical flow and occlusion estimation,5.0,88.0,4.0,42.0,4.0,4.3,59.2,8,https://openaccess.thecvf.com/content_CVPR_2019/papers/Hur_Iterative_Residual_Refinement_for_Joint_Optical_Flow_and_Occlusion_Estimation_CVPR_2019_paper.pdf,"Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves state-of-the-art results for both optical flow and occlusion estimation across several standard datasets.",Oitrerefojoopflanoces,90.0,81.0,20.0
7171,Optical Flow Estimation,10.0,"a large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation",5.0,131.0,3.0,2.0,5.0,4.2,56.00000000000001,9,https://openaccess.thecvf.com/content_cvpr_2016/papers/Mayer_A_Large_Dataset_CVPR_2016_paper.pdf,"Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluation of scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.",Oaladatotrconefodiopflanscfles,1308.0,38.0,314.0
7172,Optical Flow Estimation,32.0,ev-flownet: self-supervised optical flow estimation for event-based cameras,5.0,117.0,3.0,17.0,5.0,4.2,61.50000000000001,10,https://arxiv.org/pdf/1802.06898,"Event-based cameras have shown great promise in a variety of situations where frame based cameras suffer, such as high speed motions and high dynamic range scenes. However, developing algorithms for event measurements requires a new class of hand crafted algorithms. Deep learning has shown great success in providing model free solutions to many problems in the vision community, but existing networks have been developed with frame based images in mind, and there does not exist the wealth of labeled data for events as there does for images for supervised training. To these points, we present EV-FlowNet, a novel self-supervised deep learning pipeline for optical flow estimation for event based cameras. In particular, we introduce an image based representation of a given event stream, which is fed into a self-supervised neural network as the sole input. The corresponding grayscale images captured from the same camera at the same time as the events are then used as a supervisory signal to provide a loss function at training time, given the estimated flow from the network. We show that the resulting network is able to accurately predict optical flow from events only in a variety of different scenes, with performance competitive to image based networks. This method not only allows for accurate estimation of dense optical flow, but also provides a framework for the transfer of other self-supervised methods to the event-based domain.",Oevseopflesfoevca,136.0,32.0,33.0
7173,Optical Flow Estimation,76.0,"geonet: unsupervised learning of dense depth, optical flow and camera pose",4.0,24.0,5.0,101.0,3.0,4.1,62.7,11,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.pdf,"We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and egomotion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.",Ogeunleofdedeopflancapo,559.0,58.0,83.0
7174,Optical Flow Estimation,132.0,"competitive collaboration: joint unsupervised learning of depth, camera motion, optical flow and motion segmentation",3.0,34.0,5.0,74.0,4.0,4.1,75.4,12,https://openaccess.thecvf.com/content_CVPR_2019/papers/Ranjan_Competitive_Collaboration_Joint_Unsupervised_Learning_of_Depth_Camera_Motion_Optical_CVPR_2019_paper.pdf,"We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.",Ococojounleofdecamoopflanmose,221.0,46.0,31.0
7175,Optical Flow Estimation,74.0,unsupervised learning of a hierarchical spiking neural network for optical flow estimation: from events to global motion perception,4.0,141.0,3.0,6.0,5.0,3.9,80.4,13,https://arxiv.org/pdf/1807.10936,"The combination of spiking neural networks and event-based vision sensors holds the potential of highly efficient and high-bandwidth optical flow estimation. This paper presents the first hierarchical spiking architecture in which motion (direction and speed) selectivity emerges in an unsupervised fashion from the raw stimuli generated with an event-based camera. A novel adaptive neuron model and stable spike-timing-dependent plasticity formulation are at the core of this neural network governing its spike-based processing and learning, respectively. After convergence, the neural architecture exhibits the main properties of biological visual motion systems, namely feature extraction and local and global motion perception. Convolutional layers with input synapses characterized by single and multiple transmission delays are employed for feature and local motion perception, respectively; while global motion selectivity emerges in a final fully-connected layer. The proposed solution is validated using synthetic and real event sequences. Along with this paper, we provide the cuSNN library, a framework that enables GPU-accelerated simulations of large-scale spiking neural networks. Source code and samples are available at https://github.com/tudelft/cuSNN.",Ounleofahispnenefoopflesfrevtoglmope,32.0,103.0,2.0
7176,Optical Flow Estimation,401.0,learning by analogy: reliable supervision from transformations for unsupervised optical flow estimation,1.0,83.0,4.0,4.0,5.0,3.4000000000000004,154.7,14,http://arxiv.org/pdf/2003.13045v2,"Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.",Olebyanresufrtrfounopfles,33.0,53.0,7.0
7177,Optical Flow Estimation,401.0,deep video super-resolution using hr optical flow estimation,1.0,93.0,4.0,5.0,5.0,3.4000000000000004,159.0,15,http://arxiv.org/pdf/2001.02129v1,"Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.",Odevisuushropfles,24.0,68.0,5.0
7178,Optical Flow Estimation,401.0,learning for video super-resolution through hr optical flow estimation,1.0,94.0,4.0,23.0,5.0,3.4000000000000004,164.8,16,http://arxiv.org/pdf/1809.08573v2,"Video super-resolution (SR) aims to generate a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The generation of accurate correspondence plays a significant role in video SR. It is demonstrated by traditional video SR methods that simultaneous SR of both images and optical flows can provide accurate correspondences and better SR results. However, LR optical flows are used in existing deep learning based methods for correspondence generation. In this paper, we propose an end-to-end trainable video SR framework to super-resolve both images and optical flows. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed according to the HR optical flows. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate the SR results. Extensive experiments demonstrate that HR optical flows provide more accurate correspondences than their LR counterparts and improve both accuracy and consistency performance. Comparative results on the Vid4 and DAVIS-10 datasets show that our framework achieves the state-of-the-art performance.",Olefovisuthhropfles,33.0,48.0,9.0
7179,Optical Flow Estimation,4.0,high accuracy optical flow estimation based on a theory for warping,5.0,201.0,1.0,8.0,5.0,3.4,84.00000000000001,17,https://link.springer.com/content/pdf/10.1007/978-3-540-24673-2_3.pdf,"We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.",Ohiacopflesbaonathfowa,2673.0,37.0,230.0
7180,Optical Flow Estimation,2.0,secrets of optical flow estimation and their principles,5.0,201.0,1.0,15.0,5.0,3.4,85.5,18,https://users.soe.ucsc.edu/~pang/200/f18/papers/2018/05539939.pdf,"The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that “classical” flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a nonlocal term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark.",Oseofopflesanthpr,1352.0,44.0,155.0
7181,Optical Flow Estimation,16.0,flow fields: dense correspondence fields for highly accurate large displacement optical flow estimation,5.0,201.0,1.0,11.0,5.0,3.4,88.5,19,http://openaccess.thecvf.com/content_iccv_2015/papers/Bailer_Flow_Fields_Dense_ICCV_2015_paper.pdf,"Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier-prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this article we present a dense correspondence field approach that is much less outlier-prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach does not require explicit regularization, smoothing (like median filtering) or a new data term. Instead we solely rely on patch matching techniques and a novel multi-scale matching strategy. We also present enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than modern descriptor matching techniques. We do so by initializing EpicFlow with our approach instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI 2012, KITTI 2015 and Middlebury. In this extended article of our former conference publication we further improve our approach in matching accuracy as well as runtime and present more experiments and insights.",Oflfidecofifohiacladiopfles,104.0,46.0,19.0
7182,Optical Flow Estimation,5.0,motion detail preserving optical flow estimation,5.0,201.0,1.0,27.0,5.0,3.4,90.0,20,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.896&rep=rep1&type=pdf,"We discuss the cause of a severe optical flow estimation problem that fine motion structures cannot always be correctly reconstructed in the commonly employed multi-scale variational framework. Our major finding is that significant and abrupt displacement transition wrecks small-scale motion structures in the coarse-to-fine refinement. A novel optical flow estimation method is proposed in this paper to address this issue, which reduces the reliance of the flow estimates on their initial values propagated from the coarser level and enables recovering many motion details in each scale. The contribution of this paper also includes adaption of the objective function and development of a new optimization procedure. The effectiveness of our method is borne out by experiments for both large- and small-displacement optical flow estimation.",Omodepropfles,523.0,47.0,74.0
7183,Optical Flow Estimation,12.0,unsupervised deep learning for optical flow estimation,5.0,201.0,1.0,21.0,5.0,3.4,90.3,21,https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPDFInterstitial/14388/13940,"Recent work has shown that optical flow estimation can be formulated as a supervised learning problem. Moreover, convolutional networks have been successfully applied to this task. However, supervised flow learning is obfuscated by the shortage of labeled training data. As a consequence, existing methods have to turn to large synthetic datasets for easily computer generated ground truth. In this work, we explore if a deep network for flow estimation can be trained without supervision. Using image warping by the estimated flow, we devise a simple yet effective unsupervised method for learning optical flow, by directly minimizing photometric consistency. We demonstrate that a flow network can be trained from endto-end using our unsupervised scheme. In some cases, our results come tantalizingly close to the performance of methods trained with full supervision.",Oundelefoopfles,205.0,46.0,17.0
7184,Optical Flow Estimation,3.0,a quantitative analysis of current practices in optical flow estimation and the principles behind them,5.0,201.0,1.0,33.0,5.0,3.4,91.20000000000002,22,https://link.springer.com/content/pdf/10.1007/s11263-013-0644-x.pdf,"The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that “classical” flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. One key implementation detail is the median filtering of intermediate flow fields during optimization. While this improves the robustness of classical methods it actually leads to higher energy solutions, meaning that these methods are not optimizing the original objective function. To understand the principles behind this phenomenon, we derive a new objective function that formalizes the median filtering heuristic. This objective function includes a non-local smoothness term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that can better preserve motion details. To take advantage of the trend towards video in wide-screen format, we further introduce an asymmetric pyramid downsampling scheme that enables the estimation of longer range horizontal motions. The methods are evaluated on the Middlebury, MPI Sintel, and KITTI datasets using the same parameter settings.",Oaquanofcuprinopflesanthprbeth,489.0,80.0,41.0
7185,Optical Flow Estimation,25.0,"occlusions, motion and depth boundaries with a generic network for disparity, optical flow or scene flow estimation",5.0,201.0,1.0,12.0,5.0,3.4,91.5,23,http://openaccess.thecvf.com/content_ECCV_2018/papers/Eddy_Ilg_Occlusions_Motion_and_ECCV_2018_paper.pdf,"Occlusions play an important role in disparity and optical flow estimation, since matching costs are not available in occluded areas and occlusions indicate depth or motion boundaries. Moreover, occlusions are relevant for motion segmentation and scene flow estimation. In this paper, we present an efficient learning-based approach to estimate occlusion areas jointly with disparities or optical flow. The estimated occlusions and motion boundaries clearly improve over the state-of-the-art. Moreover, we present networks with state-of-the-art performance on the popular KITTI benchmark and good generic performance. Making use of the estimated occlusions, we also show improved results on motion segmentation and scene flow estimation.",Oocmoandebowiagenefodiopflorscfles,115.0,52.0,11.0
7186,Optical Flow Estimation,18.0,what makes good synthetic training data for learning disparity and optical flow estimation?,5.0,201.0,1.0,20.0,5.0,3.4,91.8,24,https://arxiv.org/pdf/1801.06397,"The finding that very large networks can be trained efficiently and reliably has led to a paradigm shift in computer vision from engineered solutions to learning formulations. As a result, the research challenge shifts from devising algorithms to creating suitable and abundant training data for supervised learning. How to efficiently create such training data? The dominant data acquisition method in visual recognition is based on web data and manual annotation. Yet, for many computer vision problems, such as stereo or optical flow estimation, this approach is not feasible because humans cannot manually enter a pixel-accurate flow field. In this paper, we promote the use of synthetically generated data for the purpose of training deep networks on such tasks. We suggest multiple ways to generate such data and evaluate the influence of dataset properties on the performance and generalization properties of the resulting networks. We also demonstrate the benefit of learning schedules that use different types of data at selected stages of the training process.",Owhmagosytrdafoledianopfles,120.0,64.0,7.0
7187,Optical Flow Estimation,27.0,optical flow estimation in aerated flows,5.0,201.0,1.0,39.0,5.0,3.4,100.2,25,http://arxiv.org/pdf/1806.05666v2,"ABSTRACT Optical flow estimation is known from Computer Vision where it is used to determine obstacle movements through a sequence of images following an assumption of brightness conservation. This paper presents the first study on application of the optical flow method to aerated stepped spillway flows. For this purpose, the flow is captured with a high-speed camera and illuminated with a synchronized LED light source. The flow velocities, obtained using a basic Horn–Schunck method for estimation of the optical flow coupled with an image pyramid multi-resolution approach for image filtering, compare well with data from intrusive conductivity probe measurements. Application of the Horn–Schunck method yields densely populated flow field data sets with velocity information for every pixel. It is found that the image pyramid approach has the most significant effect on the accuracy compared to other image processing techniques. However, the final results show some dependency on the pixel intensity distribution, with better accuracy found for grey values between 100 and 150.",Oopflesinaefl,47.0,28.0,2.0
7188,Optical Flow Estimation,40.0,continual occlusion and optical flow estimation,5.0,201.0,1.0,29.0,5.0,3.4,101.1,26,https://arxiv.org/pdf/1811.01602,"Two optical flow estimation problems are addressed: (i) occlusion estimation and handling, and (ii) estimation from image sequences longer than two frames. The proposed ContinualFlow method estimates occlusions before flow, avoiding the use of flow corrupted by occlusions for their estimation. We show that providing occlusion masks as an additional input to flow estimation improves the standard performance metric by more than 25% on both KITTI and Sintel. As a second contribution, a novel method for incorporating information from past frames into flow estimation is introduced. The previous frame flow serves as an input to occlusion estimation and as a prior in occluded regions, i.e. those without visual correspondences. By continually using the previous frame flow, ContinualFlow performance improves further by 18% on KITTI and 7% on Sintel, achieving top performance on KITTI and Sintel.",Ocoocanopfles,21.0,43.0,4.0
7189,Optical Flow Estimation,35.0,displacement-invariant matching cost learning for accurate optical flow estimation,5.0,95.0,4.0,201.0,1.0,3.4,108.8,27,https://arxiv.org/pdf/2010.14851,"Learning matching costs has been shown to be critical to the success of the state-of-the-art deep stereo matching methods, in which 3D convolutions are applied on a 4D feature volume to learn a 3D cost volume. However, this mechanism has never been employed for the optical flow task. This is mainly due to the significantly increased search dimension in the case of optical flow computation, ie, a straightforward extension would require dense 4D convolutions in order to process a 5D feature volume, which is computationally prohibitive. This paper proposes a novel solution that is able to bypass the requirement of building a 5D feature volume while still allowing the network to learn suitable matching costs from data. Our key innovation is to decouple the connection between 2D displacements and learn the matching costs at each 2D displacement hypothesis independently, ie, displacement-invariant cost learning. Specifically, we apply the same 2D convolution-based matching net independently on each 2D displacement hypothesis to learn a 4D cost volume. Moreover, we propose a displacement-aware projection layer to scale the learned cost volume, which reconsiders the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. Extensive experiments show that our approach achieves state-of-the-art accuracy on various datasets, and outperforms all published optical flow methods on the Sintel benchmark.",Odimacolefoacopfles,10.0,52.0,1.0
7190,Optical Flow Estimation,7.0,tv-l1 optical flow estimation,5.0,201.0,1.0,48.0,4.0,3.1,96.9,28,http://www.ipol.im/pub/art/2013/26/article_lr.pdf,"This article describes an implementation of the optical flow estimation method introduced by Zach, Pock and Bischof in 2007. This method is based on the minimization of a functional containing a data term using the L 1 norm and a regularization term using the total variation of the flow. The main feature of this formulation is that it allows discontinuities in the flow field, while being more robust to noise than the classical approach by Horn and Schunck. The algorithm is an efficient numerical scheme, which solves a relaxed version of the problem by alternate minimization. Source Code A C implementation of this algorithm is provided. The source code and an online demo are accessible at the web page of this article 1 .",Otvopfles,273.0,9.0,20.0
7191,Optical Flow Estimation,51.0,single image optical flow estimation with an event camera,4.0,201.0,1.0,10.0,5.0,3.1,98.7,29,https://arxiv.org/pdf/2004.00347,"Event cameras are bio-inspired sensors that asynchronously report intensity changes in microsecond resolution. DAVIS can capture high dynamics of a scene and simultaneously output high temporal resolution events and low frame-rate intensity images. In this paper, we propose a single image (potentially blurred) and events based optical flow estimation approach. First, we demonstrate how events can be used to improve flow estimates. To this end, we encode the relation between flow and events effectively by presenting an event-based photometric consistency formulation. Then, we consider the special case of image blur caused by high dynamics in the visual environments and show that including the blur formation in our model further constrains flow estimation. This is in sharp contrast to existing works that ignore the blurred images while our formulation can naturally handle either blurred or sharp images to achieve accurate flow estimation. Finally, we reduce flow estimation, as well as image deblurring, to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data (with blurred and non-blurred images) show the superiority of our model in comparison to state-of-the-art approaches.",Osiimopfleswianevca,14.0,61.0,1.0
7192,Optical Flow Estimation,46.0,recurrent spatial pyramid cnn for optical flow estimation,4.0,201.0,1.0,25.0,5.0,3.1,101.7,30,https://openreview.net/pdf?id=QdUa5PuA4g,"Optical flow estimation plays an important role in many multimedia and computer vision tasks. Although great progress has been made in applying convolutional neural networks (CNNs) to estimate optical flow in recent works, it is still difficult for CNNs to generate optical flow with the desired effectiveness and efficiency. Compared to CNN-based methods, conventional variational methods normally perform to optimize an energy function and produce optical flow with more precise details. Inspired by the effectiveness of variational methods and deep CNNs, we propose a recurrent spatial pyramid (RecSPy) network for optical flow estimation. To deal with large displacements and to decrease the number of parameters, we formulate the spatial pyramid as a recurrent process, and adopt a CNN to refine optical flow at each spatial scale. Furthermore, to improve the results with more precise details, we propose an energy function that encodes structure and constancy constraints to help refine the optical flow at each spatial scale. The combination of the proposed RecSPy network and the proposed energy-based refinement enables our system to estimate optical flow effectively and efficiently. Experimental results on the benchmarks validate the effectiveness and efficiency of the proposed method.",Oresppycnfoopfles,26.0,58.0,4.0
7193,Optical Flow Estimation,28.0,"dense, accurate optical flow estimation with piecewise parametric model",5.0,201.0,1.0,46.0,4.0,3.1,102.6,31,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Yang_Dense_Accurate_Optical_2015_CVPR_paper.pdf,"This paper proposes a simple method for estimating dense and accurate optical flow field. It revitalizes an early idea of piecewise parametric flow model. A key innovation is that, we fit a flow field piecewise to a variety of parametric models, where the domain of each piece (i.e., each piece's shape, position and size) is determined adaptively, while at the same time maintaining a global inter-piece flow continuity constraint. We achieve this by a multi-model fitting scheme via energy minimization. Our energy takes into account both the piecewise constant model assumption and the flow field continuity constraint, enabling the proposed method to effectively handle both homogeneous motions and complex motions. The experiments on three public optical flow benchmarks (KITTI, MPI Sintel, and Middlebury) show the superiority of our method compared with the state of the art: it achieves top-tier performances on all the three benchmarks.",Odeacopfleswipipamo,106.0,55.0,5.0
7194,Optical Flow Estimation,47.0,patch-based video denoising with optical flow estimation,4.0,201.0,1.0,34.0,5.0,3.1,104.7,32,http://arxiv.org/pdf/2003.04279v3,"A novel image sequence denoising algorithm is presented. The proposed approach takes advantage of the self-similarity and redundancy of adjacent frames. The algorithm is inspired by fusion algorithms, and as the number of frames increases, it tends to a pure temporal average. The use of motion compensation by regularized optical flow methods permits robust patch comparison in a spatiotemporal volume. The use of principal component analysis ensures the correct preservation of fine texture and details. An extensive comparison with the state-of-the-art methods illustrates the superior performance of the proposed approach, with improved texture and detail reconstruction.",Opavidewiopfles,81.0,49.0,7.0
7195,Optical Flow Estimation,66.0,non-local weighted regularization for optical flow estimation,4.0,201.0,1.0,16.0,5.0,3.1,105.0,33,http://arxiv.org/pdf/1904.12599v1,"Abstract Global optical flow characterizes the overall motion while local optical flow characterizes the individual's movement. Therefore, both global and local optical flow is important to analyze the motion of each object in an image and of the camera. In order to robustly and accurately estimate the optical flow which includes both global and local motion information, this paper proposes a novel variational framework with non-local weighted regularization for optical flow estimation (NLWOF). The proposed NLWOF strategy includes the following two key parts: Firstly, non-local prior is used a regularization to robustly estimate the local optical flow due to that the similar structure patches using in non-local weight can suppress noise interference. Secondly, the solution of the NLWOF model can be approximated by the employment of the Lagrange–Euler formula and the approximation of the Laplace operator. Experimental results in both quantitation and qualitation verify the effectiveness of the NLWOF scheme, and even better than those of the state-of-the-arts.",Onowerefoopfles,8.0,40.0,0.0
7196,Optical Flow Estimation,69.0,optical flow estimation using dual self-attention pyramid networks,4.0,201.0,1.0,14.0,5.0,3.1,105.3,34,http://arxiv.org/pdf/1611.00850v2,"Recently, optical flow estimation benefits greatly from deep learning based techniques. Most approaches use encoder-decoder architecture (U-Net) or spatial pyramid network (SPN) to learn optical flow. Both U-Net and SPN can extract multi-scale features and can predict optical flow directly. However, existing networks ignore to exploit the global information among channel features and inter-spatial relationship of features. In this paper, we propose a dual self-attention pyramid network, which adaptively integrates local features with their global dependencies and focuses on important features and suppresses unimportant features. Specifically, we introduce two types of attention modules into SPN, which emphasizes meaningful features along channel and spatial axes. The channel attention can adaptively re-weight channel-wise features by considering interdependencies among channels. Moreover, the spatial attention can utilize global contextual information to emphasize or suppress features in different spatial locations. In addition, two attention modules are embedded into each pyramidal level, which can refine features at different scale. We evaluate our method on MPI-Sintel and KITTI. The experimental results show that using the dual self-attention module can improve the representation power of network and further increase the accuracy of optical flow estimation.",Oopflesusdusepyne,10.0,48.0,0.0
7197,Optical Flow Estimation,1.0,optical flow estimation,5.0,201.0,1.0,89.0,4.0,3.1,107.4,35,http://www.cs.utoronto.ca/~jepson/csc420/notes/flowChapter05.pdf,Optical flow is the velocity vector field of the projected environmental surfaces when a viewing system moves relative to the environment.,Oopfles,0.0,115.0,0.0
7198,Optical Flow Estimation,30.0,optical flow estimation for flame detection in videos,5.0,201.0,1.0,67.0,4.0,3.1,109.5,36,http://arxiv.org/pdf/2003.12680v1,"Computational vision-based flame detection has drawn significant attention in the past decade with camera surveillance systems becoming ubiquitous. Whereas many discriminating features, such as color, shape, texture, etc., have been employed in the literature, this paper proposes a set of motion features based on motion estimators. The key idea consists of exploiting the difference between the turbulent, fast, fire motion, and the structured, rigid motion of other objects. Since classical optical flow methods do not model the characteristics of fire motion (e.g., non-smoothness of motion, non-constancy of intensity), two optical flow methods are specifically designed for the fire detection task: optimal mass transport models fire with dynamic texture, while a data-driven optical flow scheme models saturated flames. Then, characteristic features related to the flow magnitudes and directions are computed from the flow fields to discriminate between fire and non-fire motion. The proposed features are tested on a large video database to demonstrate their practical usefulness. Moreover, a novel evaluation method is proposed by fire simulations that allow for a controlled environment to analyze parameter influences, such as flame saturation, spatial resolution, frame rate, and random noise.",Oopflesfofldeinvi,93.0,44.0,3.0
7199,Optical Flow Estimation,36.0,constrained optical flow estimation as a matching problem,5.0,201.0,1.0,71.0,4.0,3.1,112.5,37,http://www.cvc.uab.es/~mozerov/2013TIP_OF.pdf,"In general, discretization in the motion vector domain yields an intractable number of labels. In this paper, we propose an approach that can reduce general optical flow to the constrained matching problem by pre-estimating a 2-D disparity labeling map of the desired discrete motion vector function. One of the goals of the proposed paper is estimating coarse distribution of motion vectors and then utilizing this distribution as global constraints for discrete optical flow estimation. This pre-estimation is done with a simple frame-to-frame correlation technique also known as the digital symmetric-phase-only-filter (SPOF). We discover a strong correlation between the output of the SPOF and the motion vector distribution of the related optical flow. A two step matching paradigm for optical flow estimation is applied: pixel accuracy (integer flow) and subpixel accuracy estimation. The matching problem is solved by global optimization. Experiments on the Middlebury optical flow datasets confirm our intuitive assumptions about strong correlation between motion vector distribution of optical flow and maximal peaks of SPOF outputs. The overall performance of the proposed method is promising and achieves state-of-the-art results on the Middlebury benchmark.",Ocoopflesasamapr,41.0,43.0,4.0
7200,Optical Flow Estimation,87.0,unsupervised learning for optical flow estimation using pyramid convolution lstm,4.0,201.0,1.0,22.0,5.0,3.1,113.1,38,https://arxiv.org/pdf/1907.11628,"Most of current Convolution Neural Network (CNN) based methods for optical flow estimation focus on learning optical flow on synthetic datasets with groundtruth, which is not practical. In this paper, we propose an unsupervised optical flow estimation framework named PCLNet. It uses pyramid Convolution LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which allows flexibly estimating multi-frame optical flows from any video clip. Besides, by decoupling motion feature learning and optical flow representation, our method avoids complex short-cut connections used in existing frameworks while improving accuracy of optical flow estimation. Moreover, different from those methods using specialized CNN architectures for capturing motion, our framework directly learns optical flow from the features of generic CNNs and thus can be easily embedded in any CNN based frameworks for other tasks. Extensive experiments have verified that our method not only estimates optical flow effectively and accurately, but also obtains comparable performance on action recognition.",Ounlefoopflesuspycols,10.0,19.0,0.0
7201,Optical Flow Estimation,80.0,variational method for joint optical flow estimation and edge-aware image restoration,4.0,201.0,1.0,35.0,5.0,3.1,114.9,39,https://www.sciencedirect.com/science/article/am/pii/S0031320316303430,"The most popular optical flow algorithms rely on optimizing the energy function that integrates a data term and a smoothness term. In contrast to this traditional framework, we derive a new objective function that couples optical flow estimation and image restoration. Our method is inspired by the recent successes of edge-aware constraints (EAC) in preserving edges in general gradient domain image filtering. By incorporating an EAC image fidelity term (IFT) in the conventional variational model, the new energy function can simultaneously estimate optical flow and restore images with preserved edges, in a bidirectional manner. For the energy minimization, we rewrite the EAC into gradient form and optimize the IFT with Euler-Lagrange equations. We can thus apply the image restoration by analytically solving a system of linear equations. Our EAC-combined IFT is easy to implement and can be seamlessly integrated into various optical flow functions suggested in literature. Extensive experiments on public optical flow benchmarks demonstrate that our method outperforms the current state-of-the-art in optical flow estimation and image restoration. HighlightsIncorporating an EAC added IFT to the variational model to form a new energy function, which can estimate optical flow and restore images jointly.The EAC can be rewritten into the first-order gradient form, and is beneficial for preserving edges and minimization.Input images can be fast restored by optimizing the Euler-Lagrange equations of the EAC integrated IFT.",Ovamefojoopflesanedimre,25.0,59.0,1.0
7202,Optical Flow Estimation,26.0,optical flow estimation using learned sparse model,5.0,201.0,1.0,99.0,4.0,3.1,117.9,40,http://mmlab.ie.cuhk.edu.hk/2011/iccv11_optical_flow.pdf,"Optical flow estimation is a fundamental and ill-posed problem in computer vision. To recover a dense flow field, appropriate spatial constraints have to be enforced. Recent advances exploit higher order spatial regularization, and achieve the top performance on the Middlebury benchmark. In this work, we revisit learning-based approach, and propose a learned sparse model to patch-wisely regularize the flow field. In particular, our method is based on multi-scale spatial regularization, which benefits from first-order spatial regularity and our learned, higher order sparse model. To obtain accurate flow estimation, we propose a sequential optimization scheme to solve the corresponding energy minimization problem. Moreover, as the errors in intermediate flow estimates are usually dense with large variations, we further propose flow-driven and image-driven approaches to address the problem of outliers. Experiments on the Middlebury benchmark show that our method is competitive with the state-of-the-art.",Oopflesuslespmo,42.0,38.0,3.0
7203,Optical Flow Estimation,401.0,self-supervised monocular scene flow estimation,1.0,68.0,4.0,58.0,4.0,3.1,164.9,41,http://arxiv.org/pdf/2004.04143v2,"Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation - obtaining 3D structure and 3D motion from two temporally consecutive images - is a highly ill-posed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.",Osemoscfles,33.0,69.0,1.0
7204,Optical Flow Estimation,41.0,large displacement optical flow: descriptor matching in variational motion estimation,4.0,201.0,1.0,52.0,4.0,2.8,108.3,42,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.5756&rep=rep1&type=pdf,"Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied.",Oladiopfldemainvamoes,1265.0,46.0,163.0
7205,Optical Flow Estimation,73.0,real-time traffic flow parameter estimation from uav video based on ensemble classifier and optical flow,4.0,201.0,1.0,45.0,4.0,2.8,115.8,43,http://faculty.washington.edu/ker27/pdfs/TITS2018uav.pdf,"Recently, the availability of unmanned aerial vehicle (UAV) opens up new opportunities for smart transportation applications, such as automatic traffic data collection. In such a trend, detecting vehicles and extracting traffic parameters from UAV video in a fast and accurate manner is becoming crucial in many prospective applications. However, from the methodological perspective, several limitations have to be addressed before the actual implementation of UAV. This paper proposes a new and complete analysis framework for traffic flow parameter estimation from UAV video. This framework addresses the well-concerned issues on UAV’s irregular ego-motion, low estimation accuracy in dense traffic situation, and high computational complexity by designing and integrating four stages. In the first two stages an ensemble classifier (Haar cascade + convolutional neural network) is developed for vehicle detection, and in the last two stages a robust traffic flow parameter estimation method is developed based on optical flow and traffic flow theory. The proposed ensemble classifier is demonstrated to outperform the state-of-the-art vehicle detectors that designed for UAV-based vehicle detection. Traffic flow parameter estimations in both free flow and congested traffic conditions are evaluated, and the results turn out to be very encouraging. The dataset with 20,000 image samples used in this study is publicly accessible for benchmarking at http://www.uwstarlab.org/research.html.",Oretrflpaesfruavibaonenclanopfl,103.0,42.0,1.0
7206,Optical Flow Estimation,65.0,moving vehicle detection based on optical flow estimation of edge,4.0,201.0,1.0,54.0,4.0,2.8,116.1,44,http://arxiv.org/abs/1803.04523v3,"This paper proposed a moving vehicle detection algorithm based on optical flow estimation on an edge image. Using the Canny operator, the image edge is obtained and refined. Then a set of feature points is extracted from the edge image. The pyramid model of Lucas-Kanade optical flow is used to calculate the optical flow information of the feature point set. A new algorithm, which is called the weighted Kmeans optical flow clustering algorithm, is proposed to cluster feature points and used to identify vehicle pattern of the feature point set on the optical flow so that the moving vehicle can be efficiently extracted from the complicated dynamic background. In this paper the vehicle videos, which are captured by a camera on a moving car, are used as the test data set. The experimental results show that this algorithm can effectively detect the moving vehicles in the videos from the camera on the moving car.",Omovedebaonopflesofed,20.0,11.0,1.0
7207,Optical Flow Estimation,56.0,robust optical flow estimation,4.0,201.0,1.0,75.0,4.0,2.8,119.7,45,https://www.ipol.im/pub/art/2013/21/article_lr.pdf,"In this paper a new robust estimator of the optical flow field is shown. Arising from the techniques based on the O.EC. (Optical Flow Constrain), we develop an estimation that takes several measures around a given pixel and discards the erroneous ones, using a voting schema. In this way, velocities that have more accumulated votes are taken as the correct ones. A preliminary comparison with other well known techniques is shown, and clearly our approach performs quite bette6 even in the cases where two o r more movements are present in the analyzed pixels.",Oroopfles,23.0,28.0,3.0
7208,Optical Flow Estimation,52.0,mirrorflow: exploiting symmetries in joint optical flow and occlusion estimation,4.0,201.0,1.0,80.0,4.0,2.8,120.0,46,http://openaccess.thecvf.com/content_ICCV_2017/papers/Hur_MirrorFlow_Exploiting_Symmetries_ICCV_2017_paper.pdf,"Optical flow estimation is one of the most studied problems in computer vision, yet recent benchmark datasets continue to reveal problem areas of today’s approaches. Occlusions have remained one of the key challenges. In this paper, we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions. In contrast to many state-ofthe- art methods that consider occlusions as outliers, possibly filtered out during post-processing, we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow. The key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images. Specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model jointly estimates optical flow in both forward and backward direction, as well as consistent occlusion maps in both views. We demonstrate significant performance benefits on standard benchmarks, especially from the occlusiondisocclusion symmetry. On the challenging KITTI dataset we report the most accurate two-frame results to date.",Omiexsyinjoopflanoces,63.0,55.0,8.0
7209,Optical Flow Estimation,49.0,probflow: joint optical flow and uncertainty estimation,4.0,201.0,1.0,96.0,4.0,2.8,123.9,47,https://openaccess.thecvf.com/content_ICCV_2017/papers/Wannenwetsch_ProbFlow_Joint_Optical_ICCV_2017_paper.pdf,"Optical flow estimation remains challenging due to untextured areas, motion boundaries, occlusions, and more. Thus, the estimated flow is not equally reliable across the image. To that end, post-hoc confidence measures have been introduced to assess the per-pixel reliability of the flow. We overcome the artificial separation of optical flow and confidence estimation by introducing a method that jointly predicts optical flow and its underlying uncertainty. Starting from common energy-based formulations, we rely on the corresponding posterior distribution of the flow given the images. We derive a variational inference scheme based on mean field, which incorporates best practices from energy minimization. An uncertainty measure is obtained along the flow at every pixel as the (marginal) entropy of the variational distribution. We demonstrate the flexibility of our probabilistic approach by applying it to two different energies and on two benchmarks. We not only obtain flow results that are competitive with the underlying energy minimization approach, but also a reliable uncertainty measure that significantly outperforms existing post-hoc approaches.",Oprjoopflanunes,28.0,52.0,2.0
7210,Optical Flow Estimation,93.0,local all-pass filters for optical flow estimation,4.0,201.0,1.0,56.0,4.0,2.8,125.1,48,https://www.ee.cuhk.edu.hk/~tblu/monsite/pdfs/gilliam1501.pdf,"The optical flow is a velocity field that describes the motion of pixels within a sequence (or set) of images. Its estimation plays an important role in areas such as motion compensation, object tracking and image registration. In this paper, we present a novel framework to estimate the optical flow using local all-pass filters. Instead of using the optical flow equation, the framework is based on relating one image to another, on a local level, using an all-pass filter and then extracting the optical flow from the filter. Using this framework, we present a fast novel algorithm for estimating a smoothly varying optical flow, which we term the Local All-Pass (LAP) algorithm. We demonstrate that this algorithm is consistent and accurate, and that it outperforms three state-of-the-art algorithms when estimating constant and smoothly varying flows. We also show initial competitive results for real images.",Oloalfifoopfles,21.0,34.0,1.0
7211,Optical Flow Estimation,82.0,observation model based on scale interactions for optical flow estimation,4.0,201.0,1.0,69.0,4.0,2.8,125.7,49,http://arxiv.org/abs/0902.2299v1,"In this paper, an original observation model for multiresolution optical flow estimation is introduced. Multiresolution frameworks, often based on coarse-to-fine warping strategies, are widely used by state-of-the-art optical flow methods. They allow the recovery of large motions by successive estimations of the flow field at several resolution levels. Although such approaches perform very efficiently and usually lead to faster minimizations, they generally consider independent problems at each resolution levels and do not exploit the existing interactions between scales (especially the influences of fine scales on larger ones). In this paper, we tackle this issue by proposing a flexible framework, inspired from fluid mechanics, able to partly counter these limitations. For each resolution level, our process filters the equations of interest and decomposes the key variables into resolved (i.e., at a given resolution) and unresolved (i.e., at finer resolutions) components. This enables to derive a new data term that takes into account, at coarse resolutions, the influence of their unresolved parts. From this new term, we propose two different estimation strategies, depending on whether we explicitly know the type of relations between the different scales (as for physical processes) or not. In order to test the efficiency of this new observation model, we have embedded it in a simple multiresolution Lucas-Kanade estimator. Comparing the usual optical flow constraint equation with this new term in the same motion estimation procedure, it clearly appears that the proposed term leads to more consistent estimates and prevents from errors propagation apparition during the estimation. In all situations (synthetic, real, physical images or not), our new term is able to greatly improve the results compared with usual conservation constraints.",Oobmobaonscinfoopfles,15.0,51.0,1.0
7212,Optical Flow Estimation,6.0,fluid experimental flow estimation based on an optical-flow scheme,5.0,201.0,1.0,147.0,3.0,2.8,126.3,50,https://www.sites.univ-rennes2.fr/costel/liama/publis/pdf/corpetti_etal_2006_EIF.pdf,"We present in this paper a novel approach dedicated to the measurement of velocity in fluid experimental flows through image sequences. Unlike most of the methods based on particle image velocimetry (PIV) approaches used in that context, the proposed technique is an extension of “optical-flow” schemes used in the computer vision community, which includes a specific enhancement for fluid mechanics applications. The method we propose enables to provide accurate dense motion fields. It includes an image based integrated version of the continuity equation. This model is associated to a regularization functional, which preserve divergence and vorticity blobs of the motion field. The method was applied on synthetic images and on real experiments carried out to allow a thorough comparison with a state-of-the-art PIV method in conditions of strong local free shear.",Oflexflesbaonanopsc,231.0,51.0,17.0
7213,Optical Flow Estimation,90.0,optical flow estimation using laplacian mesh energy,4.0,201.0,1.0,79.0,4.0,2.8,131.1,51,https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Optical_Flow_Estimation_2013_CVPR_paper.pdf,"In this paper we present a novel non-rigid optical flow algorithm for dense image correspondence and non-rigid registration. The algorithm uses a unique Laplacian Mesh Energy term to encourage local smoothness whilst simultaneously preserving non-rigid deformation. Laplacian deformation approaches have become popular in graphics research as they enable mesh deformations to preserve local surface shape. In this work we propose a novel Laplacian Mesh Energy formula to ensure such sensible local deformations between image pairs. We express this wholly within the optical flow optimization, and show its application in a novel coarse-to-fine pyramidal approach. Our algorithm achieves the state-of-the-art performance in all trials on the Garg et al. dataset, and top tier performance on the Middlebury evaluation.",Oopflesuslameen,30.0,20.0,0.0
7214,Optical Flow Estimation,133.0,dense descriptors for optical flow estimation: a comparative study,3.0,201.0,1.0,40.0,5.0,2.8,132.3,52,https://www.mdpi.com/2313-433X/3/1/12/pdf,"Estimating the displacements of intensity patterns between sequential frames is a very well-studied problem, which is usually referred to as optical flow estimation. The first assumption among many of the methods in the field is the brightness constancy during movements of pixels between frames. This assumption is proven to be not true in general, and therefore, the use of photometric invariant constraints has been studied in the past. One other solution can be sought by use of structural descriptors rather than pixels for estimating the optical flow. Unlike sparse feature detection/description techniques and since the problem of optical flow estimation tries to find a dense flow field, a dense structural representation of individual pixels and their neighbors is computed and then used for matching and optical flow estimation. Here, a comparative study is carried out by extending the framework of SIFT-flow to include more dense descriptors, and comprehensive comparisons are given. Overall, the work can be considered as a baseline for stimulating more interest in the use of dense descriptors for optical flow estimation.",Odedefoopflesacost,11.0,67.0,0.0
7215,Optical Flow Estimation,14.0,discrete-continuous optimization for optical flow estimation,5.0,201.0,1.0,184.0,3.0,2.8,139.8,53,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.431.5388&rep=rep1&type=pdf,"The accurate estimation of optical flow is a challenging task, which is often posed as an energy minimization problem. Most top-performing methods approach this using continuous optimization algorithms. In many cases, the employed models are assumed to be convex to ensure tractability of the optimization problem. This is in contrast to the related problem of narrow-baseline stereo matching, where the top-performing methods employ powerful discrete optimization algorithms such as graph cuts and message-passing to optimize highly non-convex energies. 
 
In this chapter, we demonstrate how similar non-convex energies can be formulated and optimized in the context of optical flow estimation using a combination of discrete and continuous techniques. Starting with a set of candidate solutions that are produced by either fast continuous flow estimation algorithms or sparse feature matching, the proposed method iteratively fuses these candidate solutions by the computation of minimum cuts on graphs. The obtained continuous-valued result is then further improved using local gradient descent. Experimentally, we demonstrate that the proposed energy is an accurate model and that the proposed discrete-continuous optimization scheme not only finds lower energy solutions than traditional discrete or continuous optimization techniques, but also leads to very accurate flow estimates.",Odiopfoopfles,23.0,37.0,0.0
7216,Optical Flow Estimation,33.0,optical flow estimation with uncertainties through dynamic mrfs,5.0,201.0,1.0,175.0,3.0,2.8,142.8,54,https://www.researchgate.net/profile/Georgios-Tziritas/publication/221364529_Optical_flow_estimation_with_uncertainties_through_dynamic_MRFs/links/004635149f96a75c28000000/Optical-flow-estimation-with-uncertainties-through-dynamic-MRFs.pdf,"In this paper, we propose a novel dynamic discrete framework to address image morphing with application to optical flow estimation. We reformulate the problem using a number of discrete displacements, and therefore the estimation of the morphing parameters becomes a tractable matching criteria independent combinatorial problem which is solved through the FastPD algorithm. In order to overcome the main limitation of discrete approaches (low dimensionality of the label space is unable to capture the continuous nature of the expected solution), we introduce a dynamic behavior in the model where the plausible discrete deformations (displacements) are varying in space (across the domain) and time (different states of the process - successive morphing states) according to the local uncertainty of the obtained solution.",Oopfleswiunthdymr,94.0,33.0,3.0
7217,Optical Flow Estimation,20.0,coarse to over-fine optical flow estimation,5.0,201.0,1.0,195.0,3.0,2.8,144.9,55,https://www.math.nyu.edu/~eyal/papers/opticflow.pdf,"We present a readily applicable way to go beyond the accuracy limits of current optical flow estimators. Modern optical flow algorithms employ the coarse to fine approach. We suggest to upgrade this class of algorithms, by adding over-fine interpolated levels to the pyramid. Theoretical analysis of the coarse to over-fine approach explains its advantages in handling flow-field discontinuities and simulations show its benefit for sub-pixel motion. By applying the suggested technique to various multi-scale optical flow algorithms, we reduced the estimation error by 10-30% on the common test sequences. Using the coarse to over-fine technique, we obtain optical flow estimation results that are currently the best for benchmark sequences.",Ocotoovopfles,61.0,22.0,1.0
7218,Optical Flow Estimation,197.0,an optical flow-based method for velocity field of fluid flow estimation,3.0,201.0,1.0,31.0,5.0,2.8,148.8,56,http://arxiv.org/pdf/1306.2461v3,"The aim of this paper is to present a method for estimating flow-velocity vector fields using the Lucas-Kanade algorithm. The optical flow measurements are based on the Particle Image Velocimetry (PIV) technique, which is commonly used in fluid mechanics laboratories in both research institutes and industry. Common approaches for an optical characterization of velocity fields base on computation of partial derivatives of the image intensity using finite differences. Nevertheless, the accuracy of velocity field computations is low due to the fact that an exact estimation of spatial derivatives is very difficult in presence of rapid intensity changes in the PIV images, caused by particles having small diameters. The method discussed in this paper solves this problem by interpolating the PIV images using Gaussian radial basis functions. This provides a significant improvement in the accuracy of the velocity estimation but, more importantly, allows for the evaluation of the derivatives in intermediate points between pixels. Numerical analysis proves that the method is able to estimate even a separate vector for each particle with a 5× 5 px2 window, whereas a classical correlation-based method needs at least 4 particle images. With the use of a specialized multi-step hybrid approach to data analysis the method improves the estimation of the particle displacement far above 1 px.",Oanopflmefovefiofflfles,6.0,19.0,0.0
7219,Optical Flow Estimation,31.0,simultaneous higher-order optical flow estimation and decomposition,5.0,201.0,1.0,199.0,3.0,2.8,149.4,57,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.570.1421&rep=rep1&type=pdf,"We study the estimation and decomposition of optical flows from highly nonrigid motions. To this end, recent methods from image decomposition into structural and textural parts are combined with variational optical flow estimation. The approaches we suggest amount to minimizing discrete convex functionals using second-order cone programming. Higher-order regularization is necessary in order to accurately recover important flow structure like vortices, and to incorporate key physical properties such as vanishing divergence. For proper discretization, we apply the finite mimetic difference method, which preserves the identities fulfilled by the continuous differential operators. Numerical examples demonstrate the feasibility of the complex approaches.",Osihiopflesande,57.0,54.0,0.0
7220,Optical Flow Estimation,401.0,upgrading optical flow to 3d scene flow through optical expansion,1.0,116.0,3.0,72.0,4.0,2.7,188.3,58,http://arxiv.org/pdf/1908.07734v1,"We describe an approach for upgrading 2D optical flow to 3D scene flow. Our key insight is that dense optical expansion – which can be reliably inferred from monocular frame pairs – reveals changes in depth of scene elements, e.g., things moving closer will get bigger. When integrated with camera intrinsics, optical expansion can be converted into a normalized 3D scene flow vectors that provide meaningful directions of 3D movement, but not their magnitude (due to an underlying scale ambiguity). Normalized scene flow can be further “upgraded” to the true 3D scene flow knowing depth in one frame. We show that dense optical expansion between two views can be learned from annotated optical flow maps or unlabeled video sequences, and applied to a variety of dynamic 3D perception tasks including optical scene flow, LiDAR scene flow, time-to-collision estimation and depth estimation, often demonstrating significant improvement over the prior art.",Oupopflto3dscflthopex,15.0,70.0,4.0
7221,Optical Flow Estimation,401.0,what matters in unsupervised optical flow,1.0,3.0,5.0,201.0,1.0,2.6,181.8,59,http://arxiv.org/pdf/2006.04902v2,"We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.",Owhmainunopfl,33.0,44.0,9.0
7222,Optical Flow Estimation,401.0,perceiver io: a general architecture for structured inputs & outputs,1.0,4.0,5.0,201.0,1.0,2.6,182.2,60,http://arxiv.org/pdf/2103.03206v2,"Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",Opeioagearfostin&ou,4.0,101.0,0.0
7223,Optical Flow Estimation,401.0,depth-aware video frame interpolation,1.0,5.0,5.0,201.0,1.0,2.6,182.6,61,http://arxiv.org/pdf/2005.06684v1,"Deep Neural Networks are increasingly used in video frame interpolation tasks such as frame rate changes as well as generating fake face videos. Our project aims to apply recent advances in Deep video interpolation to increase the temporal resolution of fluorescent microscopy time-lapse movies. To our knowledge, there is no previous work that uses Convolutional Neural Networks (CNN) to generate frames between two consecutive microscopy images. We propose a fully convolutional autoencoder network that takes as input two images and generates upto seven intermediate images. Our architecture has two encoders each with a skip connection to a single decoder. We evaluate the performance of several variants of our model that differ in network architecture and loss function. Our best model out-performs state of the art video frame interpolation algorithms. We also show qualitative and quantitative comparisons with state-of-the-art video frame interpolation algorithms. We believe deep video interpolation represents a new approach to improve the time-resolution of fluorescent microscopy.",Odevifrin,143.0,43.0,46.0
7224,Optical Flow Estimation,401.0,semantic flow for fast and accurate scene parsing,1.0,6.0,5.0,201.0,1.0,2.6,183.0,62,http://arxiv.org/pdf/2002.10120v3,"In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used -- atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at \url{https://github.com/lxtGH/SFSegNets}.",Oseflfofaanacscpa,27.0,85.0,0.0
7225,Optical Flow Estimation,401.0,super slomo: high quality estimation of multiple intermediate frames for video interpolation,1.0,7.0,5.0,201.0,1.0,2.6,183.4,63,http://arxiv.org/pdf/1712.00080v2,"Given two consecutive frames, video interpolation aims at generating intermediate frame(s) to form both spatially and temporally coherent video sequences. While most existing methods focus on single-frame interpolation, we propose an end-to-end convolutional neural network for variable-length multi-frame video interpolation, where the motion interpretation and occlusion reasoning are jointly modeled. We start by computing bi-directional optical flow between the input images using a U-Net architecture. These flows are then linearly combined at each time step to approximate the intermediate bi-directional optical flows. These approximate flows, however, only work well in locally smooth regions and produce artifacts around motion boundaries. To address this shortcoming, we employ another U-Net to refine the approximated flow and also predict soft visibility maps. Finally, the two input images are warped and linearly fused to form each intermediate frame. By applying the visibility maps to the warped images before fusion, we exclude the contribution of occluded pixels to the interpolated intermediate frame to avoid artifacts. Since none of our learned network parameters are time-dependent, our approach is able to produce as many intermediate frames as needed. We use 1,132 video clips with 240-fps, containing 300K individual video frames, to train our network. Experimental results on several datasets, predicting different numbers of interpolated frames, demonstrate that our approach performs consistently better than existing methods.",Osuslhiquesofmuinfrfoviin,308.0,41.0,81.0
7226,Optical Flow Estimation,401.0,deep flow-guided video inpainting,1.0,9.0,5.0,201.0,1.0,2.6,184.2,64,http://arxiv.org/pdf/2101.11080v1,"This paper studies video inpainting detection, which localizes an inpainted region in a video both spatially and temporally. In particular, we introduce VIDNet, Video Inpainting Detection Network, which contains a two-stream encoder-decoder architecture with attention module. To reveal artifacts encoded in compression, VIDNet additionally takes in Error Level Analysis frames to augment RGB frames, producing multimodal features at different levels with an encoder. Exploring spatial and temporal relationships, these features are further decoded by a Convolutional LSTM to predict masks of inpainted regions. In addition, when detecting whether a pixel is inpainted or not, we present a quad-directional local attention module that borrows information from its surrounding pixels from four directions. Extensive experiments are conducted to validate our approach. We demonstrate, among other things, that VIDNet not only outperforms by clear margins alternative inpainting detection methods but also generalizes well on novel videos that are unseen during training.",Odeflviin,65.0,35.0,12.0
7227,Optical Flow Estimation,401.0,temporal interlacing network,1.0,13.0,5.0,201.0,1.0,2.6,185.8,65,http://arxiv.org/pdf/2001.06499v1,"For a long time, the vision community tries to learn the spatio-temporal representation by combining convolutional neural network together with various temporal models, such as the families of Markov chain, optical flow, RNN and temporal convolution. However, these pipelines consume enormous computing resources due to the alternately learning process for spatial and temporal information. One natural question is whether we can embed the temporal information into the spatial one so the information in the two domains can be jointly learned once-only. In this work, we answer this question by presenting a simple yet powerful operator -- temporal interlacing network (TIN). Instead of learning the temporal features, TIN fuses the two kinds of information by interlacing spatial representations from the past to the future, and vice versa. A differentiable interlacing target can be learned to control the interlacing process. In this way, a heavy temporal model is replaced by a simple interlacing operator. We theoretically prove that with a learnable interlacing target, TIN performs equivalently to the regularized temporal convolution network (r-TCN), but gains 4% more accuracy with 6x less latency on 6 challenging benchmarks. These results push the state-of-the-art performances of video understanding by a considerable margin. Not surprising, the ensemble model of the proposed TIN won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. Code is made available to facilitate further research at https://github.com/deepcs233/TIN",Oteinne,12.0,33.0,1.0
7228,Optical Flow Estimation,401.0,learning to see through obstructions,1.0,18.0,5.0,201.0,1.0,2.6,187.8,66,http://arxiv.org/pdf/2004.01180v1,"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera. Our method leverages the motion differences between the background and the obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. The learning-based layer reconstruction allows us to accommodate potential errors in the flow estimation and brittle assumptions such as brightness consistency. We show that training on synthetically generated data transfers well to real images. Our results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",Oletosethob,13.0,47.0,2.0
7229,Optical Flow Estimation,401.0,video frame interpolation via adaptive separable convolution,1.0,19.0,5.0,201.0,1.0,2.6,188.2,67,http://arxiv.org/pdf/1708.01692v1,"Standard video frame interpolation methods first estimate optical flow between input frames and then synthesize an intermediate frame guided by motion. Recent approaches merge these two steps into a single convolution process by convolving input frames with spatially adaptive kernels that account for motion and re-sampling simultaneously. These methods require large kernels to handle large motion, which limits the number of pixels whose kernels can be estimated at once due to the large memory demand. To address this problem, this paper formulates frame interpolation as local separable convolution over input frames using pairs of 1D kernels. Compared to regular 2D kernels, the 1D kernels require significantly fewer parameters to be estimated. Our method develops a deep fully convolutional neural network that takes two input frames and estimates pairs of 1D kernels for all pixels simultaneously. Since our method is able to estimate kernels and synthesizes the whole video frame at once, it allows for the incorporation of perceptual loss to train the neural network to produce visually pleasing frames. This deep neural network is trained end-to-end using widely available video data without any human annotation. Both qualitative and quantitative experiments show that our method provides a practical solution to high-quality video frame interpolation.",Ovifrinviadseco,353.0,78.0,95.0
7230,Optical Flow Estimation,401.0,supervision by registration and triangulation for landmark detection,1.0,20.0,5.0,201.0,1.0,2.6,188.6,68,http://arxiv.org/abs/2101.09866v1,"We present Supervision by Registration and Triangulation (SRT), an unsupervised approach that utilizes unlabeled multi-view video to improve the accuracy and precision of landmark detectors. Being able to utilize unlabeled data enables our detectors to learn from massive amounts of unlabeled data freely available and not be limited by the quality and quantity of manual human annotations. To utilize unlabeled data, there are two key observations: (1) the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. (2) the detections of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. Registration and multi-view consistency are sources of supervision that do not require manual labeling, thus it can be leveraged to augment existing training data during detector training. End-to-end training is made possible by differentiable registration and 3D triangulation modules. Experiments with 11 datasets and a newly proposed metric to measure precision demonstrate accuracy and precision improvements in landmark detection on both images and video. Code is available at https://github.com/D-X-Y/landmark-detection.",Osubyreantrfolade,8.0,69.0,1.0
7231,Optical Flow Estimation,401.0,learning correspondence from the cycle-consistency of time,1.0,22.0,5.0,201.0,1.0,2.6,189.4,69,http://arxiv.org/pdf/1906.00158v1,"There have been different strategies to improve the performance of a machine learning model, e.g., increasing the depth, width, and/or nonlinearity of the model, and using ensemble learning to aggregate multiple base/weak learners in parallel or in series. This paper proposes a novel strategy called patch learning (PL) for this problem. It consists of three steps: 1) train an initial global model using all training data; 2) identify from the initial global model the patches which contribute the most to the learning error, and train a (local) patch model for each such patch; and, 3) update the global model using training data that do not fall into any patch. To use a PL model, we first determine if the input falls into any patch. If yes, then the corresponding patch model is used to compute the output. Otherwise, the global model is used. We explain in detail how PL can be implemented using fuzzy systems. Five regression problems on 1D/2D/3D curve fitting, nonlinear system identification, and chaotic time-series prediction, verified its effectiveness. To our knowledge, the PL idea has not appeared in the literature before, and it opens up a promising new line of research in machine learning.",Olecofrthcyofti,205.0,95.0,21.0
7232,Optical Flow Estimation,401.0,deep multi-scale video prediction beyond mean square error,1.0,23.0,5.0,201.0,1.0,2.6,189.8,70,http://arxiv.org/pdf/1511.05440v6,"Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",Odemuviprbemesqer,1371.0,43.0,155.0
7233,Optical Flow Estimation,401.0,real-time action recognition with enhanced motion vector cnns,1.0,25.0,5.0,201.0,1.0,2.6,190.6,71,http://arxiv.org/pdf/2105.09188v1,"Existing image-to-image translation (I2IT) methods are either constrained to low-resolution images or long inference time due to their heavy computational burden on the convolution of high-resolution feature maps. In this paper, we focus on speeding-up the high-resolution photorealistic I2IT tasks based on closed-form Laplacian pyramid decomposition and reconstruction. Specifically, we reveal that the attribute transformations, such as illumination and color manipulation, relate more to the low-frequency component, while the content details can be adaptively refined on high-frequency components. We consequently propose a Laplacian Pyramid Translation Network (LPTN) to simultaneously perform these two tasks, where we design a lightweight network for translating the low-frequency component with reduced resolution and a progressive masking strategy to efficiently refine the high-frequency ones. Our model avoids most of the heavy computation consumed by processing high-resolution feature maps and faithfully preserves the image details. Extensive experimental results on various tasks demonstrate that the proposed method can translate 4K images in real-time using one normal GPU while achieving comparable transformation performance against existing methods. Datasets and codes are available: https://github.com/csjliang/LPTN.",Oreacrewienmovecn,308.0,41.0,30.0
7234,Optical Flow Estimation,401.0,hidden two-stream convolutional networks for action recognition,1.0,29.0,5.0,201.0,1.0,2.6,192.2,72,http://arxiv.org/pdf/1704.00389v4,"Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.",Ohitwconefoacre,203.0,81.0,21.0
7235,Optical Flow Estimation,401.0,time lens: event-based video frame interpolation,1.0,31.0,5.0,201.0,1.0,2.6,193.0,73,http://arxiv.org/pdf/2010.08188v2,"Video generation models often operate under the assumption of fixed frame rates, which leads to suboptimal performance when it comes to handling flexible frame rates (e.g., increasing the frame rate of the more dynamic portion of the video as well as handling missing video frames). To resolve the restricted nature of existing video generation models' ability to handle arbitrary timesteps, we propose continuous-time video generation by combining neural ODE (Vid-ODE) with pixel-level video processing techniques. Using ODE-ConvGRU as an encoder, a convolutional version of the recently proposed neural ODE, which enables us to learn continuous-time dynamics, Vid-ODE can learn the spatio-temporal dynamics of input videos of flexible frame rates. The decoder integrates the learned dynamics function to synthesize video frames at any given timesteps, where the pixel-level composition technique is used to maintain the sharpness of individual frames. With extensive experiments on four real-world video datasets, we verify that the proposed Vid-ODE outperforms state-of-the-art approaches under various video generation settings, both within the trained time range (interpolation) and beyond the range (extrapolation). To the best of our knowledge, Vid-ODE is the first work successfully performing continuous-time video generation using real-world videos.",Otileevvifrin,2.0,48.0,0.0
7236,Optical Flow Estimation,401.0,three-stream 3d/1d cnn for fine-grained action classification and segmentation in table tennis,1.0,32.0,5.0,201.0,1.0,2.6,193.4,74,http://arxiv.org/abs/2109.14306v1,"This paper proposes a fusion method of modalities extracted from video through a three-stream network with spatio-temporal and temporal convolutions for fine-grained action classification in sport. It is applied to TTStroke-21 dataset which consists of untrimmed videos of table tennis games. The goal is to detect and classify table tennis strokes in the videos, the first step of a bigger scheme aiming at giving feedback to the players for improving their performance. The three modalities are raw RGB data, the computed optical flow and the estimated pose of the player. The network consists of three branches with attention blocks. Features are fused at the latest stage of the network using bilinear layers. Compared to previous approaches, the use of three modalities allows faster convergence and better performances on both tasks: classification of strokes with known temporal boundaries and joint segmentation and classification. The pose is also further investigated in order to offer richer feedback to the athletes.",Oth3dcnfofiacclanseintate,0.0,64.0,0.0
7238,Optical Flow Estimation,401.0,robust ego and object 6-dof motion estimation and tracking,1.0,35.0,5.0,201.0,1.0,2.6,194.6,75,http://arxiv.org/pdf/2007.13993v1,"The problem of tracking self-motion as well as motion of objects in the scene using information from a camera is known as multi-body visual odometry and is a challenging task. This paper proposes a robust solution to achieve accurate estimation and consistent track-ability for dynamic multi-body visual odometry. A compact and effective framework is proposed leveraging recent advances in semantic instance-level segmentation and accurate optical flow estimation. A novel formulation, jointly optimizing SE(3) motion and optical flow is introduced that improves the quality of the tracked points and the motion estimation accuracy. The proposed approach is evaluated on the virtual KITTI Dataset and tested on the real KITTI Dataset, demonstrating its applicability to autonomous driving applications. For the benefit of the community, we make the source code public.",Oroeganob6-moesantr,2.0,28.0,1.0
7239,Optical Flow Estimation,401.0,video enhancement with task-oriented flow,1.0,37.0,5.0,201.0,1.0,2.6,195.4,76,https://arxiv.org/pdf/1711.09078,"With the prosperity of digital video industry, video frame interpolation has arisen continuous attention in computer vision community and become a new upsurge in industry. Many learning-based methods have been proposed and achieved progressive results. Among them, a recent algorithm named quadratic video interpolation (QVI) achieves appealing performance. It exploits higher-order motion information (e.g. acceleration) and successfully models the estimation of interpolated flow. However, its produced intermediate frames still contain some unsatisfactory ghosting, artifacts and inaccurate motion, especially when large and complex motion occurs. In this work, we further improve the performance of QVI from three facets and propose an enhanced quadratic video interpolation (EQVI) model. In particular, we adopt a rectified quadratic flow prediction (RQFP) formulation with least squares method to estimate the motion more accurately. Complementary with image pixel-level blending, we introduce a residual contextual synthesis network (RCSN) to employ contextual information in high-dimensional feature space, which could help the model handle more complicated scenes and motion patterns. Moreover, to further boost the performance, we devise a novel multi-scale fusion network (MS-Fusion) which can be regarded as a learnable augmentation process. The proposed EQVI model won the first place in the AIM2020 Video Temporal Super-Resolution Challenge.",Ovienwitafl,280.0,78.0,76.0
7240,Optical Flow Estimation,401.0,future frame prediction for anomaly detection â a new baseline,1.0,38.0,5.0,201.0,1.0,2.6,195.8,77,http://arxiv.org/pdf/1712.09867v3,"Anomaly detection in videos refers to the identification of events that do not conform to expected behavior. However, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which cannot guarantee a larger reconstruction error for an abnormal event. In this paper, we propose to tackle the anomaly detection problem within a video prediction framework. To the best of our knowledge, this is the first work that leverages the difference between a predicted future frame and its ground truth to detect an abnormal event. To predict a future frame with higher quality for normal events, other than the commonly used appearance (spatial) constraints on intensity and gradient, we also introduce a motion (temporal) constraint in video prediction by enforcing the optical flow between predicted frames and ground truth frames to be consistent, and this is the first work that introduces a temporal constraint into the video prediction task. Such spatial and motion constraints facilitate the future frame prediction for normal events, and consequently facilitate to identify those abnormal events that do not conform the expectation. Extensive experiments on both a toy dataset and some publicly available datasets validate the effectiveness of our method in terms of robustness to the uncertainty in normal events and the sensitivity to abnormal events.",Ofufrprfoandeâaneba,316.0,54.0,83.0
7241,Optical Flow Estimation,401.0,hybrid neural fusion for full-frame video stabilization,1.0,40.0,5.0,201.0,1.0,2.6,196.6,78,http://arxiv.org/pdf/2102.06205v4,"Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",Ohynefufofuvist,1.0,88.0,0.0
7242,Optical Flow Estimation,61.0,unflow: unsupervised learning of optical flow with a bidirectional census loss,4.0,201.0,1.0,102.0,3.0,2.5,129.3,79,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16502/16319,"In the era of end-to-end deep learning, many advances in computer vision are driven by large amounts of labeled data. In the optical flow setting, however, obtaining dense per-pixel ground truth for real scenes is difficult and thus such data is rare. Therefore, recent end-to-end convolutional networks for optical flow rely on synthetic datasets for supervision, but the domain mismatch between training and test scenarios continues to be a challenge. Inspired by classical energy-based optical flow methods, we design an unsupervised loss based on occlusion-aware bidirectional flow estimation and the robust census transform to circumvent the need for ground truth flow. On the KITTI benchmarks, our unsupervised approach outperforms previous unsupervised deep networks by a large margin, and is even more accurate than similar supervised methods trained on synthetic datasets alone. By optionally fine-tuning on the KITTI training data, our method achieves competitive optical flow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling generic pre-training of supervised networks for datasets with limited amounts of ground truth.",Oununleofopflwiabicelo,284.0,39.0,51.0
7243,Optical Flow Estimation,105.0,variational optical flow estimation based on stick tensor voting,3.0,201.0,1.0,78.0,4.0,2.5,135.3,80,https://www.academia.edu/download/33031672/06482636.pdf,"Variational optical flow techniques allow the estimation of flow fields from spatio-temporal derivatives. They are based on minimizing a functional that contains a data term and a regularization term. Recently, numerous approaches have been presented for improving the accuracy of the estimated flow fields. Among them, tensor voting has been shown to be particularly effective in the preservation of flow discontinuities. This paper presents an adaptation of the data term by using anisotropic stick tensor voting in order to gain robustness against noise and outliers with significantly lower computational cost than (full) tensor voting. In addition, an anisotropic complementary smoothness term depending on directional information estimated through stick tensor voting is utilized in order to preserve discontinuity capabilities of the estimated flow fields. Finally, a weighted non-local term that depends on both the estimated directional information and the occlusion state of pixels is integrated during the optimization process in order to denoise the final flow field. The proposed approach yields state-of-the-art results on the Middlebury benchmark.",Ovaopflesbaonsttevo,24.0,29.0,0.0
7244,Optical Flow Estimation,118.0,optical flow estimation in cardiac ct images using the steered hermite transform,3.0,201.0,1.0,76.0,4.0,2.5,138.60000000000002,81,https://www.researchgate.net/profile/Ernesto-Moya-Albor/publication/257344400_Optical_flow_estimation_in_cardiac_CT_images_using_the_steered_Hermite_transform/links/59e8f28d0f7e9bc89b6df6e0/Optical-flow-estimation-in-cardiac-CT-images-using-the-steered-Hermite-transform.pdf,"This paper describes a new method to estimate the heart's motion in computer tomography images with the inclusion of a bio-inspired image representation model. Our proposal is based on the polynomial decomposition of each of the images using the steered Hermite transform as a representation of the local characteristics of images from an perceptual approach within a multiresolution scheme. The Hermite transform is a model that incorporates some of the more important properties of the first stages of the human visual system, such as the overlapping Gaussian receptive fields, the Gaussian derivative model of early vision and the multiresolution analysis. We propose an approach for optical flow estimation that incorporates image structure information extracted from the steered Hermite coefficients, that is later used as local motion constraints in a differential estimation method that involves several of the constraints seen in the current differential methods, which allows obtaining accurate flows. Considering the importance of understanding the movement of certain structures such as left ventricular and myocardial wall for better medical diagnosis, our main goal is to find an estimation method useful to assist diagnosis tasks in computer tomography images.",Oopflesincactimusthsthetr,19.0,61.0,0.0
7245,Optical Flow Estimation,83.0,moving object tracking using optical flow and motion vector estimation,4.0,201.0,1.0,112.0,3.0,2.5,138.9,82,https://www.researchgate.net/profile/Kiran-Kale/publication/308808625_Moving_object_tracking_using_optical_flow_and_motion_vector_estimation/links/5b64af190f7e9bd7ae92e993/Moving-object-tracking-using-optical-flow-and-motion-vector-estimation.pdf,"Moving object detection and tracking is an evolving research field due to its wide applications in traffic surveillance, 3D reconstruction, motion analysis (human and non-human), activity recognition, medical imaging etc. However real time object tracking is a challenging task due to dynamic tacking environment and different limiting parameters like view point, anthropometric variation, dimensions of an object, cluttered background, camera motions, occlusion etc. In this paper, we have developed new object detection and tracking algorithm which makes use of optical flow in conjunction with motion vector estimation for object detection and tracking in a sequence of frames. The optical flow gives valuable information about the object movement even if no quantitative parameters are computed. The motion vector estimation technique can provide an estimation of object position from consecutive frames which increases the accuracy of this algorithm and helps to provide robust result irrespective of image blur and cluttered background. The use of median filter with this algorithm makes it more robust in the presence of noise. The developed algorithm is applied to wide range of standard and real time datasets with different illumination (indoor and outdoor), object speed etc. The obtained results indicates that the developed algorithm over performs over conventional methods and state of art methods of object tracking.",Omoobtrusopflanmovees,46.0,10.0,0.0
7246,Optical Flow Estimation,145.0,highly accurate optical flow estimation on superpixel tree,3.0,201.0,1.0,51.0,4.0,2.5,139.20000000000002,83,https://www.researchgate.net/profile/Yinlin-Hu/publication/305732251_Highly_accurate_optical_flow_estimation_on_superpixel_tree/links/59eea7c5aca272029ddf79e7/Highly-accurate-optical-flow-estimation-on-superpixel-tree.pdf,"Formulated as a pixel-labeling problem, optical flow estimation using efficient edge-aware filtering has shown great success recently. However, the typical challenge that restricts the range of applicability of this method is the computational complexity mainly caused by the testing of every hypothetical label in the whole label space, which is usually large in an optical flow estimation. In this paper, we present an effective and efficient two-level filter-based optical flow algorithm connected by an accurate non-local matching. With the key observation that the optical flow of the pixels from the same compact superpixels is highly coherent, we propose a novel superpixel tree representation of an image to obtain an accurate superpixel flow. We find that if filtered separately, the candidate label space of the pixels from each superpixel is drastically reduced with the known superpixel flow. We also suggest a refined label selection strategy that is more accurate than the usual winner-takes-all manner. The proposed method, called Highly Accurate flow on Superpixel Tree (HastFlow) is validated on Middlebury and MPI-Sintel, and outperforms all filter-based methods both in accuracy and efficiency. A robust superpixel tree representation of an image is proposed.A hybrid filtering method for optical flow is proposed based on the tree.Much faster than the local filtering methods",Ohiacopflesonsutr,12.0,38.0,0.0
7247,Optical Flow Estimation,86.0,optical flow with geometric occlusion estimation and fusion of multiple frames,4.0,201.0,1.0,114.0,3.0,2.5,140.4,84,https://www.cis.upenn.edu/~cjtaylor/PUBLICATIONS/pdfs/KennedyEMMCVPR2014.pdf,"Optical flow research has made significant progress in recent years and it can now be computed efficiently and accurately for many images. However, complex motions, large displacements, and difficult imaging conditions are still problematic. In this paper, we present a framework for estimating optical flow which leads to improvements on these difficult cases by 1) estimating occlusions and 2) using additional temporal information. First, we divide the image into discrete triangles and show how this allows for occluded regions to be naturally estimated and directly incorporated into the optimization algorithm. We additionally propose a novel method of dealing with temporal information in image sequences by using “inertial estimates” of the flow. These estimates are combined using a classifier-based fusion scheme, which significantly improves results. These contributions are evaluated on three different optical flow datasets, and we achieve state-of-the-art results on MPI-Sintel.",Oopflwigeocesanfuofmufr,74.0,39.0,7.0
7248,Optical Flow Estimation,107.0,optical flow-based 3d human motion estimation from monocular video,3.0,201.0,1.0,93.0,4.0,2.5,140.4,85,https://arxiv.org/pdf/1703.00177,"This paper presents a method to estimate 3D human pose and body shape from monocular videos. While recent approaches infer the 3D pose from silhouettes and landmarks, we exploit properties of optical flow to temporally constrain the reconstructed motion. We estimate human motion by minimizing the difference between computed flow fields and the output of our novel flow renderer. By just using a single semi-automatic initialization step, we are able to reconstruct monocular sequences without joint annotation. Our test scenarios demonstrate that optical flow effectively regularizes the under-constrained problem of human shape and motion estimation from monocular video.",Oopfl3dhumoesfrmovi,39.0,49.0,0.0
7249,Optical Flow Estimation,67.0,real-time velocity estimation based on optical flow and disparity matching,4.0,201.0,1.0,137.0,3.0,2.5,141.6,86,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.713.1513&rep=rep1&type=pdf,"A high update rate of metric velocity values is crucial for a robust operation of navigation control loops of mobile robots such as micro aerial vehicles (MAVs). An efficient way for obtaining metric velocity of robots without external reference are image-based optical flow measurements, scaled with the distance between camera and the observed scene. However, since optical flow and stereo vision are computationally intensive tasks, metric optical flow calculations on embedded systems are typically only possible at limited frame rate. In this work, we therefore present an FPGA-based platform with the capability of calculating real-time metric optical flow at 127 frames per second and 376×240 resolution. Radial undistortion, image rectification, disparity estimation and optical flow calculation tasks are performed on a single FPGA without the need for external memory. The platform is perfectly suited for mobile robots or MAVs due to its low weight and low power consumption.",Oreveesbaonopflandima,50.0,9.0,2.0
7250,Optical Flow Estimation,88.0,optical flow for incompressible turbulence motion estimation,4.0,201.0,1.0,118.0,3.0,2.5,142.20000000000002,87,https://core.ac.uk/download/pdf/52841865.pdf,"We propose in this paper a new formulation of optical flow dedicated to 2D incompressible turbulent flows. It consists in minimizing an objective function constituted by an observation term and a regularization one. The observation term is based on the transport equation of the passive scalar field. For non-fully resolved scalar images, we propose to use the mixed model in large eddy simulation to determine the interaction between large scales and unresolved ones. The regularization term is based on the continuity equation of 2D incompressible flows. Compared to prototypical method, this regularizer preserves more vortex structures by eliminating constraints over the vorticity field. The evaluation of the proposed formulation is done over synthetic and experimental images, and the improvements in term of estimation are discussed.",Oopflfointumoes,32.0,73.0,0.0
7251,Optical Flow Estimation,134.0,quaternion based optical flow estimation for robust object tracking,3.0,201.0,1.0,73.0,4.0,2.5,142.5,88,http://arxiv.org/pdf/2107.08149v1,"We propose a quaternion optical flow algorithm for robust object tracking. Unlike previous works of color optical flow methods that treat color as separating channels, the proposed algorithm exploits quaternion representation of color and processes color as a holistic signal. In this way, it enables more accurate flow estimation at the pixel locations of spatial color variations, and reduces tracking errors by leaving more features points at their correct locations on the target. For successful and efficient object tracking, we also proposed a novel type of quaternion color corners that are reliable features during tracking. Together with grayscale corners, they form a good feature point set, especially when used with the proposed quaternion optical flow algorithm. We conduct a quantitative evaluation on publicly available dataset to verify the efficacy of the proposed algorithm. And object tracking experiments demonstrate that robust tracking can be achieved for real-time applications.",Oqubaopflesforoobtr,11.0,16.0,0.0
7252,Optical Flow Estimation,137.0,lucas-kanade optical flow estimation on the ti c66x digital signal processor,3.0,201.0,1.0,70.0,4.0,2.5,142.5,89,https://www.researchgate.net/profile/Fan-Zhang-55/publication/280313352_Lucas-Kanade_Optical_Flow_estimation_on_the_TI_C66x_digital_signal_processor/links/55b1ecda08aec0e5f4312ff6/Lucas-Kanade-Optical-Flow-estimation-on-the-TI-C66x-digital-signal-processor.pdf,"Optical flow is a computer vision operation that seeks to calculate the apparent motion of features across two consecutive frames of a video sequence. It is an important constituent kernel in many automated intelligence, surveillance, and reconnaissance applications. Different optical flow algorithms represent points in the trade off space of accuracy and cost, but in general all are extremely computationally expensive. In this paper we describe an implementation and tuning of the dense pyramidal Lucas-Kanade Optical Flow method on the Texas Instruments C66x, a 10 Watt embedded digital signal processor (DSP). By using aggressive manual optimization, we achieve 90% of its peak theoretical floating point throughput, resulting in an energy efficiency that is 8.2X that of a modern Intel CPU and 2.0X that of a modern NVIDIA GPU. We believe this is a major step toward the ability to deploy mobile systems that are capable of complex computer vision applications, and real-time optical flow in particular.",Oluopflesonthtic6disipr,19.0,15.0,1.0
7253,Optical Flow Estimation,122.0,a decoupled approach to illumination-robust optical flow estimation,3.0,201.0,1.0,86.0,4.0,2.5,142.8,90,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.708.2072&rep=rep1&type=pdf,"Despite continuous improvements in optical flow in the last three decades, the ability for optical flow algorithms to handle illumination variation is still an unsolved challenge. To improve the ability to interpret apparent object motion in video containing illumination variation, an illumination-robust optical flow method is designed. This method decouples brightness into reflectance and illumination components using a stochastic technique; reflectance is given higher weight to ensure robustness against illumination, which is suppressed. Illumination experiments using the Middlebury and University of Oulu databases demonstrate the decoupled method's improvement when compared with state-of-the-art. In addition, a novel technique is implemented to visualize optical flow output, which is especially useful to compare different optical flow methods in the absence of the ground truth.",Oadeaptoilopfles,10.0,31.0,0.0
7254,Optical Flow Estimation,96.0,optical flow background estimation for real-time pan/tilt camera object tracking,4.0,201.0,1.0,124.0,3.0,2.5,146.4,91,http://arxiv.org/pdf/1907.08816v1,"Abstract As Computer Vision (CV) techniques develop, pan/tilt camera systems are able to enhance data capture capabilities over static camera systems. In order for these systems to be effective for metrology purposes, they will need to respond to the test article in real-time with a minimum of additional uncertainty. A methodology is presented here for obtaining high-resolution, high frame-rate images, of objects traveling at speeds ⩾1.2 m/s at 1 m from the camera by tracking the moving texture of an object. Strong corners are determined and used as flow points using implementations on a graphic processing unit (GPU), resulting in significant speed-up over central processing units (CPU). Based on directed pan/tilt motion, a pixel-to-pixel relationship is used to estimate whether optical flow points fit background motion, dynamic motion or noise. To smooth variation, a two-dimensional position and velocity vector is used with a Kalman filter to predict the next required position of the camera so the object stays centered in the image. High resolution images can be stored by a parallel process resulting in a high frame rate procession of images for post-processing. The results provide real-time tracking on a portable system using a pan/tilt unit for generic moving targets where no training is required and camera motion is observed from high accuracy encoders opposed to image correlation.",Oopflbaesforepacaobtr,49.0,36.0,2.0
7255,Optical Flow Estimation,89.0,on-board velocity estimation and closed-loop control of a quadrotor uav based on optical flow,4.0,201.0,1.0,132.0,3.0,2.5,146.70000000000002,92,https://www.researchgate.net/profile/Heinrich-Buelthoff/publication/261416515_On-board_Velocity_Estimation_and_Closed-loop_Control_of_a_Quadrotor_UAV_based_on_Optical_Flow/links/540e08290cf2d8daaacd3c28/On-board-Velocity-Estimation-and-Closed-loop-Control-of-a-Quadrotor-UAV-based-on-Optical-Flow.pdf,"Robot vision became a field of increasing importance in micro aerial vehicle robotics with the availability of small and light hardware. While most approaches rely on external ground stations because of the need of high computational power, we will present a full autonomous setup using only on-board hardware. Our work is based on the continuous homography constraint to recover ego-motion from optical flow. Thus we are able to provide an efficient fall back routine for any kind of UAV (Unmanned Aerial Vehicles) since we rely solely on a monocular camera and on on-board computation. In particular, we devised two variants of the classical continuous 4-point algorithm and provided an extensive experimental evaluation against a known ground truth. The results show that our approach is able to recover the ego-motion of a flying UAV in realistic conditions and by only relying on the limited on-board computational power. Furthermore, we exploited the velocity estimation for closing the loop and controlling the motion of the UAV online.",Oonveesanclcoofaquuabaonopfl,89.0,23.0,1.0
7256,Optical Flow Estimation,140.0,optical flow estimation for motion-compensated compression,3.0,201.0,1.0,81.0,4.0,2.5,146.70000000000002,93,https://www.researchgate.net/profile/Wei-Chen-354/publication/259443518_OFEMCC/links/02e7e52b9979ed9ac0000000/OFEMCC.pdf,"The computation of optical flow within an image sequence is one of the most widely used techniques in computer vision. In this paper, we present a new approach to estimate the velocity field for motion-compensated compression. It is derived by a nonlinear system using the direct temporal integral of the brightness conservation constraint equation or the Displaced Frame Difference (DFD) equation. To solve the nonlinear system of equations, an adaptive framework is used, which employs velocity field modeling, a nonlinear least-squares model, Gauss-Newton and Levenberg-Marquardt techniques, and an algorithm of the progressive relaxation of the over-constraint. The three criteria by which successful motion-compensated compression is judged are 1.) The fidelity with which the estimated optical flow matches the ground truth motion, 2.) The relative absence of artifacts and ''dirty window'' effects for frame interpolation, and 3.) The cost to code the motion vector field. We base our estimated flow field on a single minimized target function, which leads to motion-compensated predictions without incurring penalties in any of these three criteria. In particular, we compare our proposed algorithm results with those from Block-Matching Algorithms (BMA), and show that with nearly the same number of displacement vectors per fixed block size, the performance of our algorithm exceeds that of BMA in all the three above points. We also test the algorithm on synthetic and natural image sequences, and use it to demonstrate applications for motion-compensated compression.",Oopflesfomoco,13.0,52.0,1.0
7257,Optical Flow Estimation,155.0,robust optical flow estimation for continuous blurred scenes using rgb-motion imaging and directional filtering,3.0,201.0,1.0,68.0,4.0,2.5,147.3,94,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.6960&rep=rep1&type=pdf,"Optical flow estimation is a difficult task given real-world video footage with camera and object blur. In this paper, we combine a 3D pose&position tracker with an RGB sensor allowing us to capture video footage together with 3D camera motion. We show that the additional camera motion information can be embedded into a hybrid optical flow framework by interleaving an iterative blind deconvolution and warping based minimization scheme. Such a hybrid framework significantly improves the accuracy of optical flow estimation in scenes with strong blur. Our approach yields improved overall performance against three state-of-the-art baseline methods applied to our proposed ground truth sequences, as well as in several other real-world sequences captured by our novel imaging system.",Oroopflesfocoblscusrgimandifi,15.0,21.0,1.0
7258,Optical Flow Estimation,149.0,robust local optical flow estimation using bilinear equations for sparse motion estimation,3.0,201.0,1.0,85.0,4.0,2.5,150.6,95,https://www.researchgate.net/profile/Tobias-Senst/publication/272356500_Presentation_BERLOF_Senst_ICIP_2013_publish/links/54e30c130cf2edaea09492a7/Presentation-BERLOF-Senst-ICIP-2013-publish.pdf,"This article presents a theoretical framework to decrease the computation effort of the Robust Local Optical Flow method which is based on the Lucas Kanade method. We show mathematically, how to transform the iterative scheme of the feature tracker into a system of bilinear equations and thus estimate the motion vectors directly by analyzing its zeros. Furthermore, we show that it is possible to parallelise our approach efficiently on a GPU, thus, outperforming the current OpenCV-OpenCL implementation of the pyramidal Lucas Kanade method in terms of runtime and accuracy. Finally, an evaluation is given for the Middlebury Optical Flow and the KITTI datasets.",Oroloopflesusbieqfospmoes,12.0,14.0,0.0
7259,Optical Flow Estimation,151.0,tv-l1 optical flow estimation with image details recovering based on modified census transform,3.0,201.0,1.0,83.0,4.0,2.5,150.6,96,http://arxiv.org/pdf/1711.07837v1,"This paper proposes an improved optical flow estimation approach based on the total variational L1 minimization technique with weighted median filter. Furthermore, recovering image details using modified census transform algorithm improves the overall accuracy of estimating large scale displacements optical flow. On the other hand, the use of the Taylor expansion approximation in most of the optical flow approaches limits the ability to estimate movement of fast objects. Hence, a coarse-to-fine scheme is used to overcome such a problem of the cost of losing small details in the interpolation process where initial values are propagated from the coarse level to the fine one. The proposed algorithm improves the accuracy of the estimation process by integrating the correspondence results of the modified census transform into the coarse-to-fine module in order to recover the lost details. The outcome of the proposed approach yields state-of-the-art results on the Middlebury optical flow evaluations.",Otvopfleswiimderebaonmocetr,16.0,19.0,0.0
7260,Optical Flow Estimation,50.0,a naturalistic open source movie for optical flow evaluation,4.0,201.0,1.0,185.0,3.0,2.5,150.9,97,https://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_44.pdf,"Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further research on optical flow estimation is needed. To validate the use of synthetic data, we compare the image- and flow-statistics of Sintel to those of real films and videos and show that they are similar. The data set, metrics, and evaluation website are publicly available.",Oanaopsomofoopflev,1235.0,22.0,239.0
7261,Optical Flow Estimation,45.0,symmetrical dense optical flow estimation with occlusions detection,4.0,201.0,1.0,192.0,3.0,2.5,151.5,98,https://link.springer.com/content/pdf/10.1007/3-540-47969-4_48.pdf,"Traditional techniques of dense optical flow estimation do not generally yield symmetrical solutions: the results will differ if they are applied between images I1 and I2 or between images I2 and I1. In this work, we present a method to recover a dense optical flow field map from two images, while explicitely taking into account the symmetry across the images as well as possible occlusions in the flow field. The idea is to consider both displacements vectors from I1 to I2 and I2 to I1 and to minimise an energy functional that explicitely encodes all those properties. This variational problem is then solved using the gradient flow defined by the Euler–Lagrange equations associated to the energy. To prove the importance of the concepts of symmetry and occlusions for optical flow computation, we have extended a classical approach to handle those. Experiments clearly show the added value of these properties to improve the accuracy of the computed flows.Figures appear in color in the online version of this paper.",Osydeopfleswiocde,109.0,53.0,6.0
7262,Optical Flow Estimation,183.0,optical flow for image-based river velocity estimation,3.0,201.0,1.0,61.0,4.0,2.5,153.60000000000002,99,https://hal.inria.fr/hal-01930498/file/Khalid-18.pdf,"We present a novel motion estimation technique for image-based river velocimetry. It is based on the so-called optical flow, which is a well developed method for rigid motion estimation in image sequences, devised in computer vision community. Contrary to PIV (Particle Image Velocimetry) techniques, optical flow formulation is flexible enough to incorporate physics equations that govern the observed quantity motion. Over the past years, it has been adopted by experimental fluid dynamics community where many new models were introduced to better represent different fluids motions, (see (Heitz et al., 2010) for a review). Our optical flow is based on the scalar transport equation and is augmented with a weighted diffusion term to compensate for small scale (non-captured) contributions. Additionally, since there is no ground truth data for such type of image sequences, we present a new evaluation method to assess the results. It is based on trajectory reconstruction of few Lagrangian particles of interest and a direct comparison against their manually-reconstructed trajectories. The new motion estimation technique outperformed traditional optical flow and PIV-based methods.",Oopflfoimrivees,6.0,39.0,0.0
7263,Optical Flow Estimation,64.0,optical flow estimation using fourier mellin transform,4.0,201.0,1.0,181.0,3.0,2.5,153.9,100,https://researchsystem.canberra.edu.au/ws/files/9082906/2009000193.pdf,"In this paper, we propose a novel method of computing the optical flow using the Fourier Mellin Transform (FMT). Each image in a sequence is divided into a regular grid of patches and the optical flow is estimated by calculating the phase correlation of each pair of co-sited patches using the FMT. By applying the FMT in calculating the phase correlation, we are able to estimate not only the pure translation, as limited in the case of the basic phase correlation techniques, but also the scale and rotation motion of image patches, i.e. full similarity transforms. Moreover, the motion parameters of each patch can be estimated to sub-pixel accuracy based on a recently proposed algorithm that uses a 2D esinc function in fitting the data from the phase correlation output. We also improve the estimation of the optical flow by presenting a method of smoothing the field by using a vector weighted average filter. Finally, experimental results, using publicly available data sets are presented, demonstrating the accuracy and improvements of our method over previous optical flow methods.",Oopflesusfometr,32.0,28.0,1.0
7557,Pose estimation,25.0,openpose: realtime multi-person 2d pose estimation using part affinity fields,5.0,5.0,5.0,3.0,5.0,5.0,10.4,1,https://arxiv.org/pdf/1812.08008,"Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.",Popremu2dpoesuspaaffi,1434.0,88.0,286.0
7558,Pose estimation,29.0,realtime multi-person 2d pose estimation using part affinity fields,5.0,6.0,5.0,1.0,5.0,5.0,11.4,2,http://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.pdf,"We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.",Premu2dpoesuspaaffi,3395.0,81.0,579.0
7559,Pose estimation,10.0,densepose: dense human pose estimation in the wild,5.0,3.0,5.0,26.0,5.0,5.0,12.0,3,https://openaccess.thecvf.com/content_cvpr_2018/papers/Guler_DensePose_Dense_Human_CVPR_2018_paper.pdf,"In this work we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efficient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence 'in the wild', namely in the presence of background, occlusions and scale variations. We improve our training set's effectiveness by training an inpainting network that can fill in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary materials, data, code, and videos are provided on the project page http://densepose.org.",Pdedehupoesinthwi,643.0,54.0,79.0
7560,Pose estimation,19.0,deep high-resolution representation learning for human pose estimation,5.0,20.0,5.0,4.0,5.0,5.0,14.9,4,http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_Deep_High-Resolution_Representation_Learning_for_Human_Pose_Estimation_CVPR_2019_paper.pdf,"In this paper, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. In addition, we show the superiority of our network in pose tracking on the PoseTrack dataset. The code and models have been publicly available at https://github.com/leoxiaobin/deep-high-resolution-net.pytorch.",Pdehirelefohupoes,935.0,105.0,195.0
7561,Pose estimation,1.0,rmpe: regional multi-person pose estimation,5.0,15.0,5.0,38.0,5.0,5.0,17.7,5,http://openaccess.thecvf.com/content_ICCV_2017/papers/Fang_RMPE_Regional_Multi-Person_ICCV_2017_paper.pdf,"Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76:7 mAP on the MPII (multi person) dataset[3]. Our model and source codes are made publicly available.",Prmremupoes,629.0,70.0,65.0
7562,Pose estimation,15.0,simple baselines for human pose estimation and tracking,5.0,21.0,5.0,19.0,5.0,5.0,18.6,6,http://openaccess.thecvf.com/content_ECCV_2018/papers/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.pdf,"There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at this https URL.",Psibafohupoesantr,675.0,36.0,160.0
7563,Pose estimation,7.0,2d human pose estimation: new benchmark and state of the art analysis,5.0,22.0,5.0,41.0,4.0,4.7,23.2,7,http://openaccess.thecvf.com/content_cvpr_2014/papers/Andriluka_2D_Human_Pose_2014_CVPR_paper.pdf,"Human pose estimation has made significant progress during the last years. However current datasets are limited in their coverage of the overall pose estimation challenges. Still these serve as the common sources to evaluate, train and compare different models on. In this paper we introduce a novel benchmark ""MPII Human Pose"" that makes a significant advance in terms of diversity and difficulty, a contribution that we feel is required for future developments in human body models. This comprehensive dataset was collected using an established taxonomy of over 800 human activities [1]. The collected images cover a wider variety of human activities than previous datasets including various recreational, occupational and householding activities, and capture people from a wider range of viewpoints. We provide a rich set of labels including positions of body joints, full 3D torso and head orientation, occlusion labels for joints and body parts, and activity labels. For each image we provide adjacent video frames to facilitate the use of motion information. Given these rich annotations we perform a detailed analysis of leading human pose estimation approaches and gaining insights for the success and failures of these methods.",P2dhupoesnebeanstoftharan,1333.0,29.0,301.0
7564,Pose estimation,58.0,higherhrnet: scale-aware representation learning for bottom-up human pose estimation,4.0,18.0,5.0,7.0,5.0,4.7,26.7,8,http://openaccess.thecvf.com/content_CVPR_2020/papers/Cheng_HigherHRNet_Scale-Aware_Representation_Learning_for_Bottom-Up_Human_Pose_Estimation_CVPR_2020_paper.pdf,"Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6% AP), suggesting its robustness in crowded scene.",Phiscrelefobohupoes,122.0,45.0,23.0
7565,Pose estimation,5.0,a simple yet effective baseline for 3d human pose estimation,5.0,56.0,4.0,31.0,5.0,4.6,33.2,9,https://openaccess.thecvf.com/content_ICCV_2017/papers/Martinez_A_Simple_yet_ICCV_2017_paper.pdf,"Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3- dimensional positions.,,With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, ""lifting"" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by about 30% on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (i.e., using images as input) yields state of the art results – this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.",Pasiyeefbafo3dhupoes,637.0,66.0,137.0
7566,Pose estimation,31.0,stacked hourglass networks for human pose estimation,5.0,59.0,4.0,2.0,5.0,4.6,33.5,10,http://arxiv.org/pdf/1603.06937v2,"This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.",Psthonefohupoes,2890.0,57.0,428.0
7567,Pose estimation,74.0,a general optimization-based framework for global pose estimation with multiple sensors,4.0,36.0,5.0,61.0,4.0,4.4,54.900000000000006,11,https://arxiv.org/pdf/1901.03642,"Accurate state estimation is a fundamental problem for autonomous robots. To achieve locally accurate and globally drift-free state estimation, multiple sensors with complementary properties are usually fused together. Local sensors (camera, IMU, LiDAR, etc) provide precise pose within a small region, while global sensors (GPS, magnetometer, barometer, etc) supply noisy but globally drift-free localization in a largescale environment. In this paper, we propose a sensor fusion framework to fuse local states with global sensors, which achieves locally accurate and globally drift-free pose estimation. Local estimations, produced by existing VO/VIO approaches, are fused with global sensors in a pose graph optimization. Within the graph optimization, local estimations are aligned into a global coordinate. Meanwhile, the accumulated drifts are eliminated. We evaluate the performance of our system on public datasets and with real-world experiments. Results are compared against other state-of-the-art algorithms. We highlight that our system is a general framework, which can easily fuse various global sensors in a unified pose graph optimization. Our implementations are open source.",Pageopfrfoglpoeswimuse,61.0,25.0,3.0
7568,Pose estimation,8.0,deeppose: human pose estimation via deep neural networks,5.0,60.0,4.0,59.0,4.0,4.3,44.1,12,https://openaccess.thecvf.com/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf,We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images.,Pdehupoesvidenene,1747.0,28.0,121.0
7569,Pose estimation,43.0,cascaded pyramid network for multi-person pose estimation,4.0,76.0,4.0,25.0,5.0,4.3,50.8,13,https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Cascaded_Pyramid_Network_CVPR_2018_paper.pdf,"The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these ""hard"" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the ""simple"" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the ""hard"" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.",Pcapynefomupoes,571.0,48.0,98.0
7570,Pose estimation,69.0,densefusion: 6d object pose estimation by iterative dense fusion,4.0,72.0,4.0,9.0,5.0,4.3,52.2,14,https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_DenseFusion_6D_Object_Pose_Estimation_by_Iterative_Dense_Fusion_CVPR_2019_paper.pdf,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",Pde6dobpoesbyitdefu,299.0,46.0,75.0
7571,Pose estimation,73.0,crowdpose: efficient crowded scenes pose estimation and a new benchmark,4.0,53.0,4.0,33.0,5.0,4.3,53.0,15,http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_CrowdPose_Efficient_Crowded_Scenes_Pose_Estimation_and_a_New_Benchmark_CVPR_2019_paper.pdf,"Multi-person pose estimation is fundamental to many computer vision tasks and has made significant progress in recent years. However, few previous methods explored the problem of pose estimation in crowded scenes while it remains challenging and inevitable in many scenarios. Moreover, current benchmarks cannot provide an appropriate evaluation for such cases. In this paper, we propose a novel and efficient method to tackle the problem of pose estimation in the crowd and a new dataset to better evaluate algorithms. Our model consists of two key components: joint-candidate single person pose estimation (SPPE) and global maximum joints association. With multi-peak prediction for each joint and global association using the graph model, our method is robust to inevitable interference in crowded scenes and very efficient in inference. The proposed method surpasses the state-of-the-art methods on CrowdPose dataset by 5.2 mAP and results on MSCOCO dataset demonstrate the generalization ability of our method.",Pcrefcrscpoesananebe,128.0,46.0,26.0
7572,Pose estimation,91.0,posecnn: a convolutional neural network for 6d object pose estimation in cluttered scenes,4.0,80.0,4.0,18.0,5.0,4.3,64.7,16,https://arxiv.org/pdf/1711.00199,"Estimating the 6D pose of known objects is important for robots to interact with the real world. The problem is challenging due to the variety of objects as well as the complexity of a scene caused by clutter and occlusions between objects. In this work, we introduce PoseCNN, a new Convolutional Neural Network for 6D object pose estimation. PoseCNN estimates the 3D translation of an object by localizing its center in the image and predicting its distance from the camera. The 3D rotation of the object is estimated by regressing to a quaternion representation. We also introduce a novel loss function that enables PoseCNN to handle symmetric objects. In addition, we contribute a large scale video dataset for 6D object pose estimation named the YCB-Video dataset. Our dataset provides accurate 6D poses of 21 objects from the YCB dataset observed in 92 videos with 133,827 frames. We conduct extensive experiments on our YCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is highly robust to occlusions, can handle symmetric objects, and provide accurate pose estimation using only color images as input. When using depth data to further refine the poses, our approach achieves state-of-the-art results on the challenging OccludedLINEMOD dataset. Our code and dataset are available at this https URL.",Ppoaconenefo6dobpoesinclsc,708.0,39.0,165.0
7573,Pose estimation,145.0,real-time 2d multi-person pose estimation on cpu: lightweight openpose,3.0,34.0,5.0,68.0,4.0,4.1,77.5,17,https://arxiv.org/pdf/1811.12004,"In this work we adapt multi-person pose estimation architecture to use it on edge devices. We follow the bottom-up approach from OpenPose, the winner of COCO 2016 Keypoints Challenge, because of its decent quality and robustness to number of people inside the frame. With proposed network design and optimized post-processing code the full solution runs at 28 frames per second (fps) on Intel$\unicode{xAE}$ NUC 6i7KYB mini PC and 26 fps on Core$^{TM}$ i7-6850K CPU. The network model has 4.1M parameters and 9 billions floating-point operations (GFLOPs) complexity, which is just ~15% of the baseline 2-stage OpenPose with almost the same quality. The code and model are available as a part of Intel$\unicode{xAE}$ OpenVINO$^{TM}$ Toolkit.",Pre2dmupoesoncpliop,54.0,15.0,11.0
7574,Pose estimation,41.0,rethinking on multi-stage networks for human pose estimation,4.0,52.0,4.0,52.0,4.0,4.0,48.7,18,https://arxiv.org/pdf/1901.00148,"Existing pose estimation approaches fall into two categories: single-stage and multi-stage methods. While multi-stage methods are seemingly more suited for the task, their performance in current practice is not as good as single-stage methods. This work studies this issue. We argue that the current multi-stage methods' unsatisfactory performance comes from the insufficiency in various design choices. We propose several improvements, including the single-stage module design, cross stage feature aggregation, and coarse-to-fine supervision. The resulting method establishes the new state-of-the-art on both MS COCO and MPII Human Pose dataset, justifying the effectiveness of a multi-stage architecture. The source code is publicly available for further research.",Preonmunefohupoes,85.0,50.0,7.0
7575,Pose estimation,75.0,lifting from the deep: convolutional 3d pose estimation from a single image,4.0,99.0,4.0,81.0,4.0,4.0,86.4,19,http://openaccess.thecvf.com/content_cvpr_2017/papers/Tome_Lifting_From_the_CVPR_2017_paper.pdf,"We propose a unified formulation for the problem of 3D human pose estimation from a single raw RGB image that reasons jointly about 2D joint estimation and 3D pose reconstruction to improve both tasks. We take an integrated approach that fuses probabilistic knowledge of 3D human pose with a multi-stage CNN architecture and uses the knowledge of plausible 3D landmark locations to refine the search for better 2D locations. The entire process is trained end-to-end, is extremely efficient and obtains state-of-the-art results on Human3.6M outperforming previous approaches both on 2D and 3D errors.",Plifrthdeco3dpoesfrasiim,380.0,56.0,28.0
7576,Pose estimation,11.0,fast human pose estimation,5.0,115.0,3.0,54.0,4.0,3.9,65.5,20,http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf,"Existing human pose estimation approaches often only consider how to improve the model generalisation performance, but putting aside the significant efficiency problem. This leads to the development of heavy models with poor scalability and cost-effectiveness in practical use. In this work, we investigate the under-studied but practically critical pose model efficiency problem. To this end, we present a new Fast Pose Distillation (FPD) model learning strategy. Specifically, the FPD trains a lightweight pose neural network architecture capable of executing rapidly with low computational cost. It is achieved by effectively transferring the pose structure knowledge of a strong teacher network. Extensive evaluations demonstrate the advantages of our FPD method over a broad range of state-of-the-art pose estimation approaches in terms of model cost-effectiveness on two standard benchmark datasets, MPII Human Pose and Leeds Sports Pose.",Pfahupoes,74.0,42.0,12.0
7577,Pose estimation,24.0,learning feature pyramids for human pose estimation,5.0,154.0,3.0,80.0,4.0,3.9,92.8,21,http://openaccess.thecvf.com/content_ICCV_2017/papers/Yang_Learning_Feature_Pyramids_ICCV_2017_paper.pdf,"Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multibranch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.",Plefepyfohupoes,318.0,71.0,31.0
7578,Pose estimation,66.0,towards 3d human pose estimation in the wild: a weakly-supervised approach,4.0,181.0,3.0,39.0,5.0,3.9,103.9,22,http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhou_Towards_3D_Human_ICCV_2017_paper.pdf,"In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.,, We propose a weakly-supervised transfer learning method that uses mixed 2D and 3D labels in a unified deep neutral network that presents two-stage cascaded structure. Our network augments a state-of-the-art 2D pose estimation sub-network with a 3D depth regression sub-network. Unlike previous two stage approaches that train the two sub-networks sequentially and separately, our training is end-to-end and fully exploits the correlation between the 2D pose and depth estimation sub-tasks. The deep features are better learnt through shared representations. In doing so, the 3D pose labels in controlled lab environments are transferred to in the wild images. In addition, we introduce a 3D geometric constraint to regularize the 3D pose prediction, which is effective in the absence of ground truth depth labels. Our method achieves competitive results on both 2D and 3D benchmarks.",Pto3dhupoesinthwiaweap,364.0,37.0,52.0
7579,Pose estimation,37.0,deepcut: joint subset partition and labeling for multi person pose estimation,5.0,172.0,3.0,82.0,4.0,3.9,104.5,23,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pishchulin_DeepCut_Joint_Subset_CVPR_2016_paper.pdf,"This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identifies occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by first detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form configurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation.",Pdejosupaanlafomupepoes,636.0,63.0,53.0
7580,Pose estimation,46.0,vnect: real-time 3d human pose estimation with a single rgb camera,4.0,198.0,3.0,40.0,5.0,3.9,105.0,24,https://arxiv.org/pdf/1705.01583.pdf%C3%AF%C2%BC%E2%80%B0%C3%A6%CB%86%E2%80%93[%C3%A8%C2%BF%E2%84%A2%C3%A4%C2%B8%E2%82%AC%C3%A4%C2%B8%C2%AA]%C3%AF%C2%BC%CB%86https://arxiv.org/,"We present the first real-time method to capture the full global 3D skeletal pose of a human in a stable, temporally consistent manner using a single RGB camera. Our method combines a new convolutional neural network (CNN) based pose regressor with kinematic skeleton fitting. Our novel fully-convolutional pose formulation regresses 2D and 3D joint positions jointly in real time and does not require tightly cropped input frames. A real-time kinematic skeleton fitting method uses the CNN output to yield temporally stable 3D global pose reconstructions on the basis of a coherent kinematic skeleton. This makes our approach the first monocular RGB method usable in real-time applications such as 3D character control---thus far, the only monocular methods for such applications employed specialized RGB-D cameras. Our method's accuracy is quantitatively on par with the best offline 3D monocular RGB pose estimation methods. Our results are qualitatively comparable to, and sometimes better than, results from monocular RGB-D approaches, such as the Kinect. However, we show that our approach is more broadly applicable than RGB-D solutions, i.e., it works for outdoor scenes, community videos, and low quality commodity RGB cameras.",Pvnre3dhupoeswiasirgca,600.0,127.0,58.0
7581,Pose estimation,401.0,distribution-aware coordinate representation for human pose estimation,1.0,17.0,5.0,12.0,5.0,3.8,130.7,25,http://arxiv.org/pdf/2003.07232v1,"While being the de facto standard coordinate representation for human pose estimation, heatmap has not been investigated in-depth. This work fills this gap. For the first time, we find that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for the performance. We further probe the design limitations of the standard coordinate decoding method, and propose a more principled distributionaware decoding method. Also, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating unbiased/accurate heatmaps. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoints (DARK) method. Serving as a model-agnostic plug-in, DARK brings about significant performance boost to existing human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO. Besides, DARK achieves the 2nd place entry in the ICCV 2019 COCO Keypoints Challenge. The code is available online.",Pdicorefohupoes,76.0,42.0,15.0
7582,Pose estimation,401.0,3d human pose estimation in video with temporal convolutions and semi-supervised training,1.0,26.0,5.0,11.0,5.0,3.8,134.0,26,http://arxiv.org/abs/2109.07353v1,"In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D",P3dhupoesinviwitecoansetr,304.0,72.0,67.0
7583,Pose estimation,68.0,multi-context attention for human pose estimation,4.0,97.0,4.0,101.0,3.0,3.7,89.5,27,https://openaccess.thecvf.com/content_cvpr_2017/papers/Chu_Multi-Context_Attention_for_CVPR_2017_paper.pdf,"In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on detailed descriptions for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive field, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. Code has been made publicly available.",Pmuatfohupoes,413.0,52.0,19.0
7584,Pose estimation,178.0,fsa-net: learning fine-grained structure aggregation for head pose estimation from a single image,3.0,91.0,4.0,49.0,4.0,3.7,104.5,28,http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_FSA-Net_Learning_Fine-Grained_Structure_Aggregation_for_Head_Pose_Estimation_From_CVPR_2019_paper.pdf,"This paper proposes a method for head pose estimation from a single image. Previous methods often predict head poses through landmark or depth estimation and would require more computation than necessary. Our method is based on regression and feature aggregation. For having a compact model, we employ the soft stagewise regression scheme. Existing feature aggregation methods treat inputs as a bag of features and thus ignore their spatial relationship in a feature map. We propose to learn a fine-grained structure mapping for spatially grouping features before aggregation. The fine-grained structure provides part-based information and pooled values. By utilizing learnable and non-learnable importance over the spatial location, different model variants can be generated and form a complementary ensemble. Experiments show that our method outperforms the state-of-the-art methods including both the landmark-free ones and the ones based on landmark or depth estimation. With only a single RGB frame as input, our method even outperforms methods utilizing multi-modality information (RGB-D, RGB-Time) on estimating the yaw angle. Furthermore, the memory overhead of our model is 100 times smaller than those of previous methods.",Pfslefistagfohepoesfrasiim,90.0,53.0,22.0
7585,Pose estimation,63.0,pretraining boosts out-of-domain robustness for pose estimation,4.0,29.0,5.0,201.0,1.0,3.5,90.8,29,https://openaccess.thecvf.com/content/WACV2021/papers/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.pdf,"Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both ""within-domain"" and ""out-of-domain"" (unseen horse) benchmarking - this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness.",Pprboourofopoes,39.0,73.0,4.0
7586,Pose estimation,98.0,simple pose: rethinking and improving a bottom-up approach for multi-person pose estimation,4.0,33.0,5.0,201.0,1.0,3.5,102.9,30,https://ojs.aaai.org/index.php/AAAI/article/download/6797/6651,"Human pose estimation - the process of recognizing human keypoints in a given image - is one of the most important tasks in computer vision and has a wide range of applications including movement diagnostics, surveillance, or self-driving vehicle. The accuracy of human keypoint prediction is increasingly improved thanks to the burgeoning development of deep learning. Most existing methods solved human pose estimation by generating heatmaps in which the ith heatmap indicates the location confidence of the ith keypoint. In this paper, we introduce novel network structures referred to as multi-resolution representation learning for human keypoint prediction. At different resolutions in the learning process, our networks branch off and use extra layers to learn heatmap generation. We firstly consider the architectures for generating the multi-resolution heatmaps after obtaining the lowest-resolution feature maps. Our second approach allows learning during the process of feature extraction in which the heatmaps are generated at each resolution of the feature extractor. The first and second approaches are referred to as multi-resolution heatmap learning and multi-resolution feature map learning respectively. Our architectures are simple yet effective, achieving good performance. We conducted experiments on two common benchmarks for human pose estimation: MSCOCO and MPII dataset. The code is made publicly available at https://github.com/tqtrunghnvn/SimMRPose.",Psiporeanimaboapfomupoes,19.0,30.0,0.0
7587,Pose estimation,401.0,3d hand shape and pose estimation from a single rgb image,1.0,96.0,4.0,22.0,5.0,3.4000000000000004,165.29999999999998,31,http://arxiv.org/pdf/2109.13879v1,"This work addresses a novel and challenging problem of estimating the full 3D hand shape and pose from a single RGB image. Most current methods in 3D hand analysis from monocular RGB images only focus on estimating the 3D locations of hand keypoints, which cannot fully express the 3D shape of hand. In contrast, we propose a Graph Convolutional Neural Network (Graph CNN) based method to reconstruct a full 3D mesh of hand surface that contains richer information of both 3D hand shape and pose. To train networks with full supervision, we create a large-scale synthetic dataset containing both ground truth 3D meshes and 3D poses. When fine-tuning the networks on real-world datasets without 3D ground truth, we propose a weakly-supervised approach by leveraging the depth map as a weak supervision in training. Through extensive evaluations on our proposed new datasets and two public datasets, we show that our proposed method can produce accurate and reasonable 3D hand mesh, and can achieve superior 3D hand pose estimation accuracy when compared with state-of-the-art methods.",P3dhashanpoesfrasirgim,180.0,80.0,26.0
7588,Pose estimation,108.0,2d/3d pose estimation and action recognition using multitask deep learning,3.0,129.0,3.0,89.0,4.0,3.3,110.7,32,http://openaccess.thecvf.com/content_cvpr_2018/papers/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.pdf,"Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efficient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-to-end leads to significantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.",P2dpoesanacreusmudele,257.0,64.0,23.0
7589,Pose estimation,44.0,pose guided structured region ensemble network for cascaded hand pose estimation,4.0,201.0,1.0,8.0,5.0,3.1,96.0,33,https://arxiv.org/pdf/1708.03416,"Abstract Hand pose estimation from single depth images is an essential topic in computer vision and human computer interaction. Despite recent advancements in this area promoted by convolutional neural networks, accurate hand pose estimation is still a challenging problem. In this paper we propose a novel approach named as pose guided structured region ensemble network (Pose-REN) to boost the performance of hand pose estimation. Under the guidance of an initially estimated pose, the proposed method extracts regions from the feature maps of convolutional neural network and generates more optimal and representative features for hand pose estimation. The extracted feature regions are then integrated hierarchically according to the topology of hand joints by tree-structured fully connections to regress the refined hand pose. The final hand pose is obtained by an iterative cascaded method. Comprehensive experiments on public hand pose datasets demonstrate that our proposed method outperforms state-of-the-art algorithms.",Ppogustreennefocahapoes,112.0,64.0,20.0
7590,Pose estimation,14.0,pifpaf: composite fields for human pose estimation,5.0,201.0,1.0,43.0,4.0,3.1,97.5,34,http://openaccess.thecvf.com/content_CVPR_2019/papers/Kreiss_PifPaf_Composite_Fields_for_Human_Pose_Estimation_CVPR_2019_paper.pdf,"We propose a new bottom-up method for multi-person 2D human pose estimation that is particularly well suited for urban mobility such as self-driving cars and delivery robots. The new method, PifPaf, uses a Part Intensity Field (PIF) to localize body parts and a Part Association Field (PAF) to associate body parts with each other to form full human poses. Our method outperforms previous methods at low resolution and in crowded, cluttered and occluded scenes thanks to (i) our new composite field PAF encoding fine-grained information and (ii) the choice of Laplace loss for regressions which incorporates a notion of uncertainty. Our architecture is based on a fully convolutional, single-shot, box-free design. We perform on par with the existing state-of-the-art bottom-up method on the standard COCO keypoint task and produce state-of-the-art results on a modified COCO keypoint task for the transportation domain.",Ppicofifohupoes,135.0,45.0,23.0
7591,Pose estimation,57.0,deeplabcut: markerless pose estimation of user-defined body parts with deep learning,4.0,201.0,1.0,27.0,5.0,3.1,105.6,35,https://drive.google.com/file/d/12N9kHAOuo48kkNg20AAzS70VkyJX1Hao/view,"Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (~200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.Using a deep learning approach to track user-defined body parts during various behaviors across multiple species, the authors show that their toolbox, called DeepLabCut, can achieve human accuracy with only a few hundred frames of training data.",Pdemapoesofusbopawidele,615.0,103.0,79.0
7592,Pose estimation,54.0,"face detection, pose estimation, and landmark localization in the wild",4.0,201.0,1.0,32.0,5.0,3.1,106.2,36,http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.661.3510&rep=rep1&type=pdf,"We present a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Our model is based on a mixtures of trees with a shared pool of parts; we model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. We show that tree-structured models are surprisingly effective at capturing global elastic deformation, while being easy to optimize unlike dense graph structures. We present extensive results on standard face benchmarks, as well as a new “in the wild” annotated dataset, that suggests our system advances the state-of-the-art, sometimes considerably, for all three tasks. Though our model is modestly trained with hundreds of faces, it compares favorably to commercial systems trained with billions of examples (such as Google Picasa and face.com).",Pfadepoesanlalointhwi,2079.0,40.0,359.0
7593,Pose estimation,13.0,towards accurate multi-person pose estimation in the wild,5.0,201.0,1.0,79.0,4.0,3.1,108.0,37,https://openaccess.thecvf.com/content_cvpr_2017/papers/Papandreou_Towards_Accurate_Multi-Person_CVPR_2017_paper.pdf,"We propose a method for multi-person detection and 2-D pose estimation that achieves state-of-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. In the first stage, we predict the location and scale of boxes which are likely to contain people, for this we use the Faster RCNN detector. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. Trained on COCO data alone, our final system achieves average precision of 0.649 on the COCO test-dev set and the 0.643 test-standard sets, outperforming the winner of the 2016 COCO keypoints challenge and other recent state-of-art. Further, by using additional in-house labeled data we obtain an even higher average precision of 0.685 on the test-dev set and 0.673 on the test-standard set, more than 5% absolute improvement compared to the previous best performing method on the same dataset.",Ptoacmupoesinthwi,467.0,55.0,35.0
7594,Pose estimation,22.0,human pose estimation with iterative error feedback,5.0,201.0,1.0,99.0,4.0,3.1,116.7,38,http://openaccess.thecvf.com/content_cvpr_2016/papers/Carreira_Human_Pose_Estimation_CVPR_2016_paper.pdf,"Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classification tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.",Phupoeswiiterfe,521.0,53.0,38.0
7595,Pose estimation,60.0,single-network whole-body pose estimation,4.0,98.0,4.0,201.0,1.0,3.1,117.5,39,http://openaccess.thecvf.com/content_ICCV_2019/papers/Hidalgo_Single-Network_Whole-Body_Pose_Estimation_ICCV_2019_paper.pdf,"For human pose estimation in still images, this paper proposes three semi- and weakly-supervised learning schemes. While recent advances of convolutional neural networks improve human pose estimation using supervised training data, our focus is to explore the semi- and weakly-supervised schemes. Our proposed schemes initially learn conventional model(s) for pose estimation from a small amount of standard training images with human pose annotations. For the first semi-supervised learning scheme, this conventional pose model detects candidate poses in training images with no human annotation. From these candidate poses, only true-positives are selected by a classifier using a pose feature representing the configuration of all body parts. The accuracies of these candidate pose estimation and true-positive pose selection are improved by action labels provided to these images in our second and third learning schemes, which are semi- and weakly-supervised learning. While the first and second learning schemes select only poses that are similar to those in the supervised training data, the third scheme selects more true-positive poses that are significantly different from any supervised poses. This pose selection is achieved by pose clustering using outlier pose detection with Dirichlet process mixtures and the Bayes factor. The proposed schemes are validated with large-scale human pose datasets.",Psiwhpoes,29.0,81.0,6.0
7596,Pose estimation,33.0,worldwide pose estimation using 3d point clouds,5.0,201.0,1.0,98.0,4.0,3.1,119.70000000000002,40,https://link.springer.com/content/pdf/10.1007/978-3-642-33718-5_2.pdf,"We address the problem of determining where a photo was taken by estimating a full 6-DOF-plus-intrincs camera pose with respect to a large georegistered 3D point cloud, bringing together research on image localization, landmark recognition, and 3D pose estimation. Our method scales to datasets with hundreds of thousands of images and tens of millions of 3D points through the use of two new techniques: a co-occurrence prior for RANSAC and bidirectional matching of image features with 3D points. We evaluate our method on several large data sets, and show state-of-the-art results on landmark recognition as well as the ability to locate cameras to within meters, requiring only seconds per query.",Pwopoesus3dpocl,297.0,32.0,43.0
7597,Pose estimation,99.0,monocular 3d human pose estimation in the wild using improved cnn supervision,4.0,201.0,1.0,36.0,5.0,3.1,120.9,41,https://arxiv.org/pdf/1611.09813.pdf?source=post_page---------------------------,"We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows better in-the-wild performance than existing annotated data, which is further improved in conjunction with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in tandem with algorithmic and data contributions is crucial for general 3D body pose estimation.",Pmo3dhupoesinthwiusimcnsu,390.0,97.0,107.0
7598,Pose estimation,401.0,fine-grained head pose estimation without keypoints,1.0,62.0,4.0,74.0,4.0,3.1,167.29999999999998,42,http://arxiv.org/pdf/1710.00925v5,"Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.",Pfihepoeswike,204.0,40.0,38.0
7599,Pose estimation,401.0,fast and robust multi-person 3d pose estimation from multiple views,1.0,75.0,4.0,65.0,4.0,3.1,169.8,43,http://arxiv.org/pdf/1901.04111v1,"This paper addresses the problem of 3D pose estimation for multiple people in a few calibrated camera views. The main challenge of this problem is to find the cross-view correspondences among noisy and incomplete 2D pose predictions. Most previous methods address this challenge by directly reasoning in 3D using a pictorial structure model, which is inefficient due to the huge state space. We propose a fast and robust approach to solve this problem. Our key idea is to use a multi-way matching algorithm to cluster the detected 2D poses in all views. Each resulting cluster encodes 2D poses of the same person across different views and consistent correspondences across the keypoints, from which the 3D pose of each person can be effectively inferred. The proposed convex optimization based multi-way matching algorithm is efficient and robust against missing and false detections, without knowing the number of people in the scene. Moreover, we propose to combine geometric and appearance cues for cross-view matching. The proposed approach achieves significant performance gains from the state-of-the-art (96.3% vs. 90.6% and 96.9% vs. 88% on the Campus and Shelf datasets, respectively), while being efficient for real-time applications.",Pfaanromu3dpoesfrmuvi,58.0,48.0,17.0
7600,Pose estimation,401.0,camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image,1.0,90.0,4.0,48.0,4.0,3.1,170.70000000000002,44,http://arxiv.org/pdf/2103.16792v1,"Although significant improvement has been achieved recently in 3D human pose estimation, most of the previous methods only treat a single-person case. In this work, we firstly propose a fully learning-based, camera distance-aware top-down approach for 3D multi-person pose estimation from a single RGB image. The pipeline of the proposed system consists of human detection, absolute 3D human root localization, and root-relative 3D single-person pose estimation modules. Our system achieves comparable results with the state-of-the-art 3D single-person pose estimation models without any groundtruth information and significantly outperforms previous 3D multi-person pose estimation methods on publicly available datasets. The code is available in \footnote{\url{https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE}}\textsuperscript{,}\footnote{\url{https://github.com/mks0601/3DMPPE_POSENET_RELEASE}}.",Pcaditoapfo3dmupoesfrasirgim,100.0,62.0,23.0
7601,Pose estimation,401.0,cross view fusion for 3d human pose estimation,1.0,100.0,4.0,56.0,4.0,3.1,177.10000000000002,45,http://arxiv.org/pdf/1909.01203v1,"We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm).",Pcrvifufo3dhupoes,67.0,44.0,14.0
7602,Pose estimation,147.0,single-shot multi-person 3d pose estimation from monocular rgb,3.0,103.0,3.0,118.0,3.0,3.0,120.70000000000002,46,https://arxiv.org/pdf/1712.03453,"We propose a new single-shot method for multi-person 3D pose estimation in general scenes from a monocular RGB camera. Our approach uses novel occlusion-robust pose-maps (ORPM) which enable full body pose inference even under strong partial occlusions by other people and objects in the scene. ORPM outputs a fixed number of maps which encode the 3D joint locations of all people in the scene. Body part associations [8] allow us to infer 3D pose for an arbitrary number of people without explicit bounding box prediction. To train our approach we introduce MuCo-3DHP, the first large scale training data set showing real images of sophisticated multi-person interactions and occlusions. We synthesize a large corpus of multi-person images by compositing images of individual people (with ground truth from mutli-view performance capture). We evaluate our method on our new challenging 3D annotated multi-person test set MuPoTs-3D where we achieve state-of-the-art performance. To further stimulate research in multi-person 3D pose estimation, we will make our new datasets, and associated code publicly available for research purposes.",Psimu3dpoesfrmorg,128.0,70.0,25.0
7603,Pose estimation,401.0,look into person: joint body parsing & pose estimation network and a new benchmark,1.0,119.0,3.0,17.0,5.0,3.0,173.0,47,http://arxiv.org/pdf/1804.01984v1,"Human parsing and pose estimation have recently received considerable interest due to their substantial application potentials. However, the existing datasets have limited numbers of images and annotations and lack a variety of human appearances and coverage of challenging cases in unconstrained environments. In this paper, we introduce a new benchmark named “Look into Person (LIP)” that provides a significant advancement in terms of scalability, diversity, and difficulty, which are crucial for future developments in human-centric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels and 16 body joints, which are captured from a broad range of viewpoints, occlusions, and background complexities. Using these rich annotations, we perform detailed analyses of the leading human parsing and pose estimation approaches, thereby obtaining insights into the successes and failures of these methods. To further explore and take advantage of the semantic correlation of these two tasks, we propose a novel joint human parsing and pose estimation network to explore efficient context modeling, which can simultaneously predict parsing and pose with extremely high quality. Furthermore, we simplify the network to solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into the parsing results without resorting to extra supervision. The datasets, code and models are available at http://www.sysu-hcp.net/lip/.",Ploinpejobopa&poesneananebe,170.0,55.0,29.0
7604,Pose estimation,401.0,deep learning-based human pose estimation: a survey,1.0,141.0,3.0,21.0,5.0,3.0,183.0,48,http://arxiv.org/pdf/2012.13392v3,"Human pose estimation has received significant attention recently due to its various applications in the real world. As the performance of the state-of-the-art human pose estimation methods can be improved by deep learning, this paper presents a comprehensive survey of deep learning based human pose estimation methods and analyzes the methodologies employed. We summarize and discuss recent works with a methodology-based taxonomy. Single-person and multi-person pipelines are first reviewed separately. Then, the deep learning techniques applied in these pipelines are compared and analyzed. The datasets and metrics used in this task are also discussed and compared. The aim of this survey is to make every step in the estimation pipelines interpretable and to provide readers a readily comprehensible explanation. Moreover, the unsolved problems and challenges for future research are discussed.",Pdelehupoesasu,58.0,320.0,2.0
7605,Pose estimation,401.0,normalized object coordinate space for category-level 6d object pose and size estimation,1.0,142.0,3.0,34.0,5.0,3.0,187.3,49,http://arxiv.org/pdf/1901.02970v2,"The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to ``instance-level'' 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce a \textbf{Normalized Object Coordinate Space (NOCS)}---a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.",Pnoobcospfoca6dobpoansies,161.0,55.0,37.0
7606,Pose estimation,115.0,the devil is in the details: delving into unbiased data processing for human pose estimation,3.0,51.0,4.0,201.0,1.0,2.8,115.2,50,http://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,"Being a fundamental component in training and inference, data processing has not been systematically considered in human pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of human pose estimation evolution is in the biased data processing. Specifically, by investigating the standard data processing in state-of-the-art approaches mainly including coordinate system transformation and keypoint format transformation (i.e., encoding and decoding), we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is a statistical error in some keypoint format transformation methods. Two problems couple together, significantly degrade the pose estimation performance and thus lay a trap for the research community. This trap has given bone to many suboptimal remedies, which are always unreported, confusing but influential. By causing failure in reproduction and unfair in comparison, the unreported remedies seriously impedes the technological development. To tackle this dilemma from the source, we propose Unbiased Data Processing (UDP) consist of two technique aspect for the two aforementioned problems respectively (i.e., unbiased coordinate system transformation and unbiased keypoint format transformation). As a model-agnostic approach and a superior solution, UDP successfully pushes the performance boundary of human pose estimation and offers a higher and more reliable baseline for research community. Code is public available in https://github.com/HuangJunJie2017/UDP-Pose",Pthdeisinthdedeinundaprfohupoes,37.0,37.0,5.0
7607,Pose estimation,83.0,segmentation-driven 6d object pose estimation,4.0,201.0,1.0,47.0,4.0,2.8,119.4,51,https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Segmentation-Driven_6D_Object_Pose_Estimation_CVPR_2019_paper.pdf,"The most recent trend in estimating the 6D pose of rigid objects has been to train deep networks to either directly regress the pose from the image or to predict the 2D locations of 3D keypoints, from which the pose can be obtained using a PnP algorithm. In both cases, the object is treated as a global entity, and a single pose estimate is computed. As a consequence, the resulting techniques can be vulnerable to large occlusions. In this paper, we introduce a segmentation-driven 6D pose estimation framework where each visible part of the objects contributes a local pose prediction in the form of 2D keypoint locations. We then use a predicted measure of confidence to combine these pose candidates into a robust set of 3D-to-2D correspondences, from which a reliable pose estimate can be obtained. We outperform the state-of-the-art on the challenging Occluded-LINEMOD and YCB-Video datasets, which is evidence that our approach deals well with multiple poorly-textured objects occluding each other. Furthermore, it relies on a simple enough architecture to achieve real-time performance.",Pse6dobpoes,94.0,57.0,13.0
7608,Pose estimation,32.0,human pose estimation via convolutional part heatmap regression,5.0,201.0,1.0,109.0,3.0,2.8,122.7,52,http://arxiv.org/abs/1609.01743v1,"This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/~psxab5/.",Phupoesvicopahere,382.0,48.0,21.0
7609,Pose estimation,125.0,monocular human pose estimation: a survey of deep learning-based methods,3.0,201.0,1.0,16.0,5.0,2.8,122.7,53,https://arxiv.org/pdf/2006.01423,"Abstract Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.",Pmohupoesasuofdeleme,75.0,211.0,1.0
7610,Pose estimation,85.0,occlusion-aware networks for 3d human pose estimation in video,4.0,201.0,1.0,58.0,4.0,2.8,123.3,54,https://openaccess.thecvf.com/content_ICCV_2019/papers/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.pdf,"Occlusion is a key problem in 3D human pose estimation from a monocular video. To address this problem, we introduce an occlusion-aware deep-learning framework. By employing estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint, we filter out the unreliable estimations of occluded keypoints. When occlusion occurs, we have incomplete 2D keypoints and feed them to our 2D and 3D temporal convolutional networks (2D and 3D TCNs) that enforce temporal smoothness to produce a complete 3D pose. By using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints. Training the occlusion-aware 3D TCN requires pairs of a 3D pose and a 2D pose with occlusion labels. As no such a dataset is available, we introduce a ``Cylinder Man Model'' to approximate the occupation of body parts in 3D space. By projecting the model onto a 2D plane in different viewing angles, we obtain and label the occluded keypoints, providing us plenty of training data. In addition, we use this model to create a pose regularization constraint, preferring the 2D estimations of unreliable keypoints to be occluded. Our method outperforms state-of-the-art methods on Human 3.6M and HumanEva-I datasets.",Pocnefo3dhupoesinvi,64.0,50.0,6.0
7611,Pose estimation,70.0,sparseness meets deepness: 3d human pose estimation from monocular video,4.0,201.0,1.0,87.0,4.0,2.8,127.5,55,https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Sparseness_Meets_Deepness_CVPR_2016_paper.pdf,"This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.",Pspmede3dhupoesfrmovi,345.0,67.0,47.0
7612,Pose estimation,87.0,human pose estimation with spatial contextual information,4.0,201.0,1.0,72.0,4.0,2.8,128.1,56,https://arxiv.org/pdf/1901.01760,"We explore the importance of spatial contextual information in human pose estimation. Most state-of-the-art pose networks are trained in a multi-stage manner and produce several auxiliary predictions for deep supervision. With this principle, we present two conceptually simple and yet computational efficient modules, namely Cascade Prediction Fusion (CPF) and Pose Graph Neural Network (PGNN), to exploit underlying contextual information. Cascade prediction fusion accumulates prediction maps from previous stages to extract informative signals. The resulting maps also function as a prior to guide prediction at following stages. To promote spatial correlation among joints, our PGNN learns a structured representation of human pose as a graph. Direct message passing between different joints is enabled and spatial relation is captured. These two modules require very limited computational complexity. Experimental results demonstrate that our method consistently outperforms previous methods on MPII and LSP benchmark.",Phupoeswispcoin,44.0,60.0,3.0
7613,Pose estimation,92.0,aligning latent spaces for 3d hand pose estimation,4.0,201.0,1.0,70.0,4.0,2.8,129.0,57,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Aligning_Latent_Spaces_for_3D_Hand_Pose_Estimation_ICCV_2019_paper.pdf,"Hand pose estimation from monocular RGB inputs is a highly challenging task. Many previous works for monocular settings only used RGB information for training despite the availability of corresponding data in other modalities such as depth maps. In this work, we propose to learn a joint latent representation that leverages other modalities as weak labels to boost the RGB-based hand pose estimator. By design, our architecture is highly flexible in embedding various diverse modalities such as heat maps, depth maps and point clouds. In particular, we find that encoding and decoding the point cloud of the hand surface can improve the quality of the joint latent representation. Experiments show that with the aid of other modalities during training, our proposed method boosts the accuracy of RGB-based hand pose estimation systems and significantly outperforms state-of-the-art on two public benchmarks.",Pallaspfo3dhapoes,44.0,32.0,5.0
7614,Pose estimation,26.0,pose estimation for augmented reality: a hands-on survey,5.0,201.0,1.0,139.0,3.0,2.8,129.9,58,https://hal.inria.fr/hal-01246370/document,"Augmented reality (AR) allows to seamlessly insert virtual objects in an image sequence. In order to accomplish this goal, it is important that synthetic elements are rendered and aligned in the scene in an accurate and visually acceptable way. The solution of this problem can be related to a pose estimation or, equivalently, a camera localization process. This paper aims at presenting a brief but almost self-contented introduction to the most important approaches dedicated to vision-based camera localization along with a survey of several extension proposed in the recent years. For most of the presented approaches, we also provide links to code of short examples. This should allow readers to easily bridge the gap between theoretical aspects and practical implementations.",Ppoesfoaureahasu,304.0,177.0,13.0
7615,Pose estimation,21.0,flowing convnets for human pose estimation in videos,5.0,201.0,1.0,172.0,3.0,2.8,138.3,59,http://openaccess.thecvf.com/content_iccv_2015/papers/Pfister_Flowing_ConvNets_for_ICCV_2015_paper.pdf,"The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps, (ii) spatial fusion layers that learn an implicit spatial model, (iii) optical flow is used to align heatmap predictions from neighbouring frames, and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also [5, 35] in the high precision region).",Pflcofohupoesinvi,413.0,51.0,29.0
7616,Pose estimation,20.0,articulated pose estimation with flexible mixtures-of-parts,5.0,201.0,1.0,177.0,3.0,2.8,139.5,60,https://vision.ics.uci.edu/papers/YangR_CVPR_2011/YangR_CVPR_2011.pdf,"We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50% while being orders of magnitude faster.",Parpoeswiflmi,1105.0,43.0,196.0
7617,Pose estimation,16.0,recurrent human pose estimation,5.0,201.0,1.0,182.0,3.0,2.8,139.8,61,https://arxiv.org/pdf/1605.02914,"We propose a ConvNet model for predicting 2D human body poses in an image. The model regresses a heatmap representation for each body keypoint, and is able to learn and represent both the part appearances and the context of the part configuration. We make the following three contributions: (i) an architecture combining a feed forward module with a recurrent module, where the recurrent module can be run iteratively to improve the performance; (ii) the model can be trained end-to-end and from scratch, with auxiliary losses incorporated to improve performance; (iii) we investigate whether keypoint visibility can also be predicted. The model is evaluated on two benchmark datasets. The result is a simple architecture that achieves performance on par with the state of the art, but without the complexity of a graphical model stage (or layers).",Prehupoes,229.0,51.0,17.0
7618,Pose estimation,64.0,generic 3d representation via pose estimation and matching,4.0,113.0,3.0,201.0,1.0,2.7,124.7,62,http://arxiv.org/abs/1710.08247v1,"Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross-modality pose estimation). In the context of the core supervised tasks, we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification (unlike SIFT and the majority of learned features). We also show 6DOF camera pose estimation given a pair local image patches. The accuracy of both supervised tasks come comparable to humans. Finally, we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information, and conclude with a discussion on the learned representation and open research questions.",Pge3drevipoesanma,75.0,68.0,5.0
7619,Pose estimation,401.0,ssd-6d: making rgb-based 3d detection and 6d pose estimation great again,1.0,131.0,3.0,42.0,4.0,2.7,185.3,63,https://openaccess.thecvf.com/content_ICCV_2017/papers/Kehl_SSD-6D_Making_RGB-Based_ICCV_2017_paper.pdf,"We present a novel method for detecting 3D model instances and estimating their 6D poses from RGB data in a single shot. To this end, we extend the popular SSD paradigm to cover the full 6D pose space and train on synthetic model data only. Our approach competes or surpasses current state-of-the-art methods that leverage RGBD data on multiple challenging datasets. Furthermore, our method produces these results at around 10Hz, which is many times faster than the related methods. For the sake of reproducibility, we make our trained networks and detection code publicly available.",Pssmarg3ddean6dpoesgrag,473.0,43.0,103.0
7620,Pose estimation,401.0,exploiting temporal context for 3d human pose estimation in the wild,1.0,179.0,3.0,53.0,4.0,2.7,207.8,64,http://arxiv.org/pdf/1905.04266v1,"We present a bundle-adjustment-based algorithm for recovering accurate 3D human pose and meshes from monocular videos. Unlike previous algorithms which operate on single frames, we show that reconstructing a person over an entire sequence gives extra constraints that can resolve ambiguities. This is because videos often give multiple views of a person, yet the overall body shape does not change and 3D positions vary slowly. Our method improves not only on standard mocap-based datasets like Human 3.6M -- where we show quantitative improvements -- but also on challenging in-the-wild datasets such as Kinetics. Building upon our algorithm, we present a new dataset of more than 3 million frames of YouTube videos from Kinetics with automatically generated 3D poses and meshes. We show that retraining a single-frame 3D pose estimator on this data improves accuracy on both real-world and mocap data by evaluating on the 3DPW and HumanEVA datasets.",Pextecofo3dhupoesinthwi,111.0,64.0,15.0
7621,Pose estimation,401.0,mask r-cnn,1.0,1.0,5.0,201.0,1.0,2.6,181.0,65,http://arxiv.org/pdf/1703.06870v3,"We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron",Pmar-,7336.0,52.0,1571.0
7622,Pose estimation,401.0,discovery of latent 3d keypoints via end-to-end geometric reasoning,1.0,2.0,5.0,201.0,1.0,2.6,181.4,66,http://arxiv.org/pdf/1807.03146v2,"This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors. Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task. We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation. The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at http://keypointnet.github.io/.",Pdiofla3dkeviengere,150.0,70.0,16.0
7623,Pose estimation,401.0,non-local neural networks,1.0,4.0,5.0,201.0,1.0,2.6,182.2,67,http://arxiv.org/pdf/cs/0504056v1,The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.,Pnonene,3202.0,57.0,500.0
7624,Pose estimation,401.0,convolutional pose machines,1.0,7.0,5.0,201.0,1.0,2.6,183.4,68,http://arxiv.org/pdf/1602.00134v4,"Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",Pcopoma,1839.0,50.0,241.0
7625,Pose estimation,401.0,"view-invariant, occlusion-robust probabilistic embedding for human pose",1.0,10.0,5.0,201.0,1.0,2.6,184.6,69,http://arxiv.org/pdf/2010.13321v2,"Recognition of human poses and actions is crucial for autonomous systems to interact smoothly with people. However, cameras generally capture human poses in 2D as images and videos, which can have significant appearance variations across viewpoints that make the recognition tasks challenging. To address this, we explore recognizing similarity in 3D human body poses from 2D information, which has not been well-studied in existing works. Here, we propose an approach to learning a compact view-invariant embedding space from 2D body joint keypoints, without explicitly predicting 3D poses. Input ambiguities of 2D poses from projection and occlusion are difficult to represent through a deterministic mapping, and therefore we adopt a probabilistic formulation for our embedding space. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 3D pose estimation models. We also show that by training a simple temporal embedding model, we achieve superior performance on pose sequence retrieval and largely reduce the embedding dimension from stacking frame-based embeddings for efficient large-scale retrieval. Furthermore, in order to enable our embeddings to work with partially visible input, we further investigate different keypoint occlusion augmentation strategies during training. We demonstrate that these occlusion augmentations significantly improve retrieval performance on partial 2D input poses. Results on action recognition and video alignment demonstrate that using our embeddings without any additional training achieves competitive performance relative to other models specifically trained for each task.",Pviocpremfohupo,1.0,86.0,0.0
7626,Pose estimation,401.0,an analysis of svd for deep rotation estimation,1.0,12.0,5.0,201.0,1.0,2.6,185.4,70,http://arxiv.org/pdf/2006.14616v1,"Symmetric orthogonalization via SVD, and closely related procedures, are well-known techniques for projecting matrices onto $O(n)$ or $SO(n)$. These tools have long been used for applications in computer vision, for example optimal 3D alignment problems solved by orthogonal Procrustes, rotation averaging, or Essential matrix decomposition. Despite its utility in different settings, SVD orthogonalization as a procedure for producing rotation matrices is typically overlooked in deep learning models, where the preferences tend toward classic representations like unit quaternions, Euler angles, and axis-angle, or more recently-introduced methods. Despite the importance of 3D rotations in computer vision and robotics, a single universally effective representation is still missing. Here, we explore the viability of SVD orthogonalization for 3D rotations in neural networks. We present a theoretical analysis that shows SVD is the natural choice for projecting onto the rotation group. Our extensive quantitative analysis shows simply replacing existing representations with the SVD orthogonalization procedure obtains state of the art performance in many deep learning applications covering both supervised and unsupervised training.",Pananofsvfoderoes,12.0,64.0,2.0
7627,Pose estimation,401.0,deep high-resolution representation learning for visual recognition,1.0,13.0,5.0,201.0,1.0,2.6,185.8,71,http://arxiv.org/pdf/1905.01932v1,"Understanding how cities visually differ from each others is interesting for planners, residents, and historians. We investigate the interpretation of deep features learned by convolutional neural networks (CNNs) for city recognition. Given a trained city recognition network, we first generate weighted masks using the known Grad-CAM technique and to select the most discriminate regions in the image. Since the image classification label is the city name, it contains no information of objects that are class-discriminate, we investigate the interpretability of deep representations with two methods. (i) Unsupervised method is used to cluster the objects appearing in the visual explanations. (ii) A pretrained semantic segmentation model is used to label objects in pixel level, and then we introduce statistical measures to quantitatively evaluate the interpretability of discriminate objects. The influence of network architectures and random initializations in training, is studied on the interpretability of CNN features for city recognition. The results suggest that network architectures would affect the interpretability of learned visual representations greater than different initializations.",Pdehirelefovire,486.0,206.0,101.0
7628,Pose estimation,401.0,high-resolution representations for labeling pixels and regions,1.0,19.0,5.0,201.0,1.0,2.6,188.2,72,http://arxiv.org/abs/1102.1442v1,"SN 1996cr, located in the Circinus Galaxy (3.7 Mpc, z ~ 0.001) was non-detected in X-rays at ~ 1000 days yet brightened to ~ 4 x 10^{39} erg/s (0.5-8 keV) after 10 years (Bauer et al. 2008). A 1-D hydrodynamic model of the ejecta-CSM interaction produces good agreement with the measured X-ray light curves and spectra at multiple epochs. We conclude that the progenitor of SN 1996cr could have been a massive star, M > 30 M_solar, which went from an RSG to a brief W-R phase before exploding within its ~ 0.04 pc wind-blown shell (Dwarkadas et al. 2010). Further analysis of the deep Chandra HETG observations allows line-shape fitting of a handful of bright Si and Fe lines in the spectrum. The line shapes are well fit by axisymmetric emission models with an axis orientation ~ 55 degrees to our line-of-sight. In the deep 2009 epoch the higher ionization Fe XXVI emission is constrained to high lattitudes: the Occam-est way to get the Fe H-like emission coming from high latitude/polar regions is to have more CSM at/around the poles than at mid and lower lattitudes, along with a symmetric ejecta explosion/distribution. Similar CSM/ejecta characterization may be possible for other SNe and, with higher-throughput X-ray observations, for gamma-ray burst remnants as well.",Phirefolapianre,247.0,138.0,57.0
7629,Pose estimation,401.0,"relocalization, global optimization and map merging for monocular visual-inertial slam",1.0,23.0,5.0,201.0,1.0,2.6,189.8,73,http://arxiv.org/pdf/1803.01549v1,"The monocular visual-inertial system (VINS), which consists one camera and one low-cost inertial measurement unit (IMU), is a popular approach to achieve accurate 6-DOF state estimation. However, such locally accurate visual-inertial odometry is prone to drift and cannot provide absolute pose estimation. Leveraging history information to relocalize and correct drift has become a hot topic. In this paper, we propose a monocular visual-inertial SLAM system, which can relocalize camera and get the absolute pose in a previous-built map. Then 4-DOF pose graph optimization is performed to correct drifts and achieve global consistent. The 4-DOF contains x, y, z, and yaw angle, which is the actual drifted direction in the visual-inertial system. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. Current map and previous map can be merged together by the global pose graph optimization. We validate the accuracy of our system on public datasets and compare against other state-of-the-art algorithms. We also evaluate the map merging ability of our system in the large-scale outdoor environment. The source code of map reuse is integrated into our public code, VINS-Mono.",Preglopanmamefomovisl,27.0,34.0,2.0
7630,Pose estimation,401.0,face alignment in full pose range: a 3d total solution,1.0,24.0,5.0,201.0,1.0,2.6,190.2,74,http://arxiv.org/abs/1804.01005v1,"Face alignment, which fits a face model to an image and extracts the semantic meanings of facial pixels, has been an important topic in the computer vision community. However, most algorithms are designed for faces in small to medium poses (yaw angle is smaller than 45 degrees), which lack the ability to align faces in large poses up to 90 degrees. The challenges are three-fold. Firstly, the commonly used landmark face model assumes that all the landmarks are visible and is therefore not suitable for large poses. Secondly, the face appearance varies more drastically across large poses, from the frontal view to the profile view. Thirdly, labelling landmarks in large poses is extremely challenging since the invisible landmarks have to be guessed. In this paper, we propose to tackle these three challenges in an new alignment framework termed 3D Dense Face Alignment (3DDFA), in which a dense 3D Morphable Model (3DMM) is fitted to the image via Cascaded Convolutional Neural Networks. We also utilize 3D information to synthesize face images in profile views to provide abundant samples for training. Experiments on the challenging AFLW database show that the proposed approach achieves significant improvements over the state-of-the-art methods.",Pfaalinfuporaa3dtoso,172.0,93.0,23.0
7631,Pose estimation,401.0,revisiting unreasonable effectiveness of data in deep learning era,1.0,25.0,5.0,201.0,1.0,2.6,190.6,75,http://arxiv.org/pdf/1707.02968v2,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",Preunefofdaindeleer,930.0,43.0,42.0
7632,Pose estimation,401.0,imagenet performance correlates with pose estimation robustness and generalization on out-of-domain data,1.0,27.0,5.0,201.0,1.0,2.6,191.4,76,http://arxiv.org/pdf/2109.13228v1,"Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.",Pimpecowipoesroangeonouda,2.0,36.0,0.0
7633,Pose estimation,401.0,deep learning tools for the measurement of animal behavior in neuroscience,1.0,28.0,5.0,201.0,1.0,2.6,191.8,77,http://arxiv.org/abs/1909.13868v2,"Recent advances in computer vision have made accurate, fast and robust measurement of animal behavior a reality. In the past years powerful tools specifically designed to aid the measurement of behavior have come to fruition. Here we discuss how capturing the postures of animals - pose estimation - has been rapidly advancing with new deep learning methods. While challenges still remain, we envision that the fast-paced development of new deep learning tools will rapidly change the landscape of realizable real-world neuroscience.",Pdeletofothmeofanbeinne,98.0,143.0,3.0
7634,Pose estimation,401.0,unsupervised learning of depth and ego-motion from video,1.0,37.0,5.0,201.0,1.0,2.6,195.4,78,http://arxiv.org/pdf/2103.00369v2,"Although depth extraction with passive sensors has seen remarkable improvement with deep learning, these approaches may fail to obtain correct depth if they are exposed to environments not observed during training. Online adaptation, where the neural network trains while deployed, with unsupervised learning provides a convenient solution. However, online adaptation causes a neural network to forget the past. Thus, past training is wasted and the network is not able to provide good results if it observes past scenes. This work deals with practical online-adaptation where the input is online and temporally-correlated, and training is completely unsupervised. Regularization and replay-based methods without task boundaries are proposed to avoid catastrophic forgetting while adapting to online data. Experiments are performed on different datasets with both structure-from-motion and stereo. Results of forgetting as well as adaptation are provided, which are superior to recent methods. The proposed approach is more inline with the artificial general intelligence paradigm as the neural network learns the scene where it is deployed without any supervision (target labels and tasks) and without forgetting about the past. Code is available at github.com/umarKarim/cou_stereo and github.com/umarKarim/cou_sfm.",Punleofdeanegfrvi,1310.0,62.0,309.0
7635,Pose estimation,401.0,a multiresolution 3d morphable face model and fitting framework,1.0,39.0,5.0,201.0,1.0,2.6,196.2,79,http://arxiv.org/pdf/1606.00474v1,"Face analysis techniques have become a crucial component of human-machine interaction in the fields of assistive and humanoid robotics. However, the variations in head-pose that arise naturally in these environments are still a great challenge. In this paper, we present a real-time capable 3D face modelling framework for 2D in-the-wild images that is applicable for robotics. The fitting of the 3D Morphable Model is based exclusively on automatically detected landmarks. After fitting, the face can be corrected in pose and transformed back to a frontal 2D representation that is more suitable for face recognition. We conduct face recognition experiments with non-frontal images from the MUCT database and uncontrolled, in the wild images from the PaSC database, the most challenging face recognition database to date, showing an improved performance. Finally, we present our SCITOS G5 robot system, which incorporates our framework as a means of image pre-processing for face analysis.",Pamu3dmofamoanfifr,171.0,20.0,20.0
7636,Pose estimation,401.0,polarized self-attention: towards high-quality pixel-wise regression,1.0,40.0,5.0,201.0,1.0,2.6,196.6,80,http://arxiv.org/pdf/1212.4315v1,"Many approaches to sentiment analysis rely on lexica where words are tagged with their prior polarity - i.e. if a word out of context evokes something positive or something negative. In particular, broad-coverage resources like SentiWordNet provide polarities for (almost) every word. Since words can have multiple senses, we address the problem of how to compute the prior polarity of a word starting from the polarity of each sense and returning its polarity strength as an index between -1 and 1. We compare 14 such formulae that appear in the literature, and assess which one best approximates the human judgement of prior polarities, with both regression and classification models.",Pposetohipire,1.0,72.0,0.0
7637,Pose estimation,45.0,joint training of a convolutional network and a graphical model for human pose estimation,4.0,201.0,1.0,141.0,3.0,2.5,136.2,81,http://papers.nips.cc/paper/5573-stochastic-network-design-in-bidirected-trees.pdf,This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.,Pjotrofaconeanagrmofohupoes,1124.0,47.0,85.0
7638,Pose estimation,129.0,multimodal face-pose estimation with multitask manifold deep learning,3.0,201.0,1.0,57.0,4.0,2.5,136.2,82,https://arxiv.org/pdf/1712.06467,"Face-pose estimation aims at estimating the gazing direction with two-dimensional face images. It gives important communicative information and visual saliency. However, it is challenging because of lights, background, face orientations, and appearance visibility. Therefore, a descriptive representation of face images and mapping it to poses are critical. In this paper, we use multimodal data and propose a novel face-pose estimation framework named multitask manifold deep learning (<inline-formula><tex-math notation=""LaTeX"">$\text{M}^2\text{DL}$</tex-math></inline-formula>). It is based on feature extraction with improved convolutional neural networks (CNNs) and multimodal mapping relationship with multitask learning. In the proposed CNNs, manifold regularized convolutional layers learn the relationship between outputs of neurons in a low-rank space. Besides, in the proposed mapping relationship learning method, different modals of face representations are naturally combined by applying multitask learning with incoherent sparse and low-rank learning with a least-squares loss. Experimental results on three challenging benchmark datasets demonstrate the performance of <inline-formula><tex-math notation=""LaTeX"">$\text{M}^2\text{DL}$</tex-math></inline-formula>.",Pmufaeswimumadele,135.0,94.0,0.0
7639,Pose estimation,150.0,pix2pose: pixel-wise coordinate regression of objects for 6d pose estimation,3.0,201.0,1.0,44.0,4.0,2.5,138.6,83,https://openaccess.thecvf.com/content_ICCV_2019/papers/Park_Pix2Pose_Pixel-Wise_Coordinate_Regression_of_Objects_for_6D_Pose_Estimation_ICCV_2019_paper.pdf,"Estimating the 6D pose of objects using only RGB images remains challenging because of problems such as occlusion and symmetries. It is also difficult to construct 3D models with precise texture without expert knowledge or specialized scanning devices. To address these problems, we propose a novel pose estimation method, Pix2Pose, that predicts the 3D coordinates of each object pixel without textured models. An auto-encoder architecture is designed to estimate the 3D coordinates and expected errors per pixel. These pixel-wise predictions are then used in multiple stages to form 2D-3D correspondences to directly compute poses with the PnP algorithm with RANSAC iterations. Our method is robust to occlusion by leveraging recent achievements in generative adversarial training to precisely recover occluded parts. Furthermore, a novel loss function, the transformer loss, is proposed to handle symmetric objects by guiding predictions to the closest symmetric pose. Evaluations on three different benchmark datasets containing symmetric and occluded objects show our method outperforms the state of the art using only RGB images.",Ppipicoreofobfo6dpoes,115.0,38.0,23.0
7640,Pose estimation,59.0,point-to-point regression pointnet for 3d hand pose estimation,4.0,201.0,1.0,137.0,3.0,2.5,139.20000000000002,84,http://openaccess.thecvf.com/content_ECCV_2018/papers/Liuhao_Ge_Point-to-Point_Regression_PointNet_ECCV_2018_paper.pdf,"Convolutional Neural Networks (CNNs)-based methods for 3D hand pose estimation with depth cameras usually take 2D depth images as input and directly regress holistic 3D hand pose. Different from these methods, our proposed Point-to-Point Regression PointNet directly takes the 3D point cloud as input and outputs point-wise estimations, i.e., heat-maps and unit vector fields on the point cloud, representing the closeness and direction from every point in the point cloud to the hand joint. The point-wise estimations are used to infer 3D joint locations with weighted fusion. To better capture 3D spatial information in the point cloud, we apply a stacked network architecture for PointNet with intermediate supervision, which is trained end-to-end. Experiments show that our method can achieve outstanding results when compared with state-of-the-art methods on three challenging hand pose datasets.",Pporepofo3dhapoes,84.0,58.0,13.0
7641,Pose estimation,117.0,pose from shape: deep pose estimation for arbitrary 3d objects,3.0,201.0,1.0,88.0,4.0,2.5,141.9,85,https://arxiv.org/pdf/1906.05105,"Most deep pose estimation methods need to be trained for specific object instances or categories. In this work we propose a completely generic deep pose estimation approach, which does not require the network to have been trained on relevant categories, nor objects in a category to have a canonical pose. We believe this is a crucial step to design robotic systems that can interact with new objects in the wild not belonging to a predefined category. Our main insight is to dynamically condition pose estimation with a representation of the 3D shape of the target object. More precisely, we train a Convolutional Neural Network that takes as input both a test image and a 3D model, and outputs the relative 3D pose of the object in the input image with respect to the 3D model. We demonstrate that our method boosts performances for supervised category pose estimation on standard benchmarks, namely Pascal3D+, ObjectNet3D and Pix3D, on which we provide results superior to the state of the art. More importantly, we show that our network trained on everyday man-made objects from ShapeNet generalizes without any additional training to completely new types of 3D objects by providing results on the LINEMOD dataset as well as on natural entities such as animals from ImageNet.",Ppofrshdepoesfoar3dob,20.0,67.0,3.0
7642,Pose estimation,149.0,crossinfonet: multi-task information sharing based hand pose estimation,3.0,201.0,1.0,76.0,4.0,2.5,147.9,86,https://openaccess.thecvf.com/content_CVPR_2019/papers/Du_CrossInfoNet_Multi-Task_Information_Sharing_Based_Hand_Pose_Estimation_CVPR_2019_paper.pdf,"This paper focuses on the topic of vision based hand pose estimation from single depth map using convolutional neural network (CNN). Our main contributions lie in designing a new pose regression network architecture named CrossInfoNet. The proposed CrossInfoNet decomposes hand pose estimation task into palm pose estimation sub-task and finger pose estimation sub-task, and adopts two-branch crossconnection structure to share the beneficial complementary information between the sub-tasks. Our work is inspired by multi-task information sharing mechanism, which has been few discussed in hand pose estimation using depth data in previous publications. In addition, we propose a heat-map guided feature extraction structure to get better feature maps, and train the complete network end-to-end. The effectiveness of the proposed CrossInfoNet is evaluated with extensively self-comparative experiments and in comparison with state-of-the-art methods on four public hand pose datasets. The code is available.",Pcrmuinshbahapoes,35.0,50.0,10.0
7643,Pose estimation,142.0,point-to-pose voting based hand pose estimation using residual permutation equivariant layer,3.0,201.0,1.0,84.0,4.0,2.5,148.2,87,https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Point-To-Pose_Voting_Based_Hand_Pose_Estimation_Using_Residual_Permutation_Equivariant_CVPR_2019_paper.pdf,"Recently, 3D input data based hand pose estimation methods have shown state-of-the-art performance, because 3D data capture more spatial information than the depth image. Whereas 3D voxel-based methods need a large amount of memory, PointNet based methods need tedious preprocessing steps such as K-nearest neighbour search for each point. In this paper, we present a novel deep learning hand pose estimation method for an unordered point cloud. Our method takes 1024 3D points as input and does not require additional information. We use Permutation Equivariant Layer (PEL) as the basic element, where a residual network version of PEL is proposed for the hand pose estimation task. Furthermore, we propose a voting-based scheme to merge information from individual points to the final pose output. In addition to the pose estimation task, the voting-based scheme can also provide point cloud segmentation result without ground-truth for segmentation. We evaluate our method on both NYU dataset and the Hands2017Challenge dataset, where our method outperforms recent state-of-theart methods.",Ppovobahapoesusrepeeqla,36.0,44.0,1.0
7644,Pose estimation,161.0,disentangling latent hands for image synthesis and pose estimation,3.0,201.0,1.0,66.0,4.0,2.5,148.5,88,https://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Disentangling_Latent_Hands_for_Image_Synthesis_and_Pose_Estimation_CVPR_2019_paper.pdf,"Hand image synthesis and pose estimation from RGB images are both highly challenging tasks due to the large discrepancy between factors of variation ranging from image background content to camera viewpoint. To better analyze these factors of variation, we propose the use of disentangled representations and a disentangled variational autoencoder (dVAE) that allows for specific sampling and inference of these factors. The derived objective from the variational lower bound as well as the proposed training strategy are highly flexible, allowing us to handle crossmodal encoders and decoders as well as semi-supervised learning scenarios. Experiments show that our dVAE can synthesize highly realistic images of the hand specifiable by both pose and image background content and also estimate 3D hand poses from RGB images with accuracy competitive with state-of-the-art on two public benchmarks.",Pdilahafoimsyanpoes,54.0,46.0,4.0
7645,Pose estimation,97.0,"depth-based hand pose estimation: methods, data, and challenges",4.0,201.0,1.0,135.0,3.0,2.5,150.0,89,https://arxiv.org/pdf/1504.06378,"Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and have released software and evaluation code. We summarize important conclusions here: (1) Coarse pose estimation appears viable for scenes with isolated hands. However, high precision pose estimation [required for immersive virtual reality and cluttered scenes (where hands may be interacting with nearby objects and surfaces) remain a challenge. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.",Pdehapoesmedaanch,85.0,87.0,8.0
7646,Pose estimation,153.0,3d human pose estimation from a single image via distance matrix regression,3.0,201.0,1.0,85.0,4.0,2.5,151.8,90,http://openaccess.thecvf.com/content_cvpr_2017/papers/Moreno-Noguer_3D_Human_Pose_CVPR_2017_paper.pdf,"This paper addresses the problem of 3D human pose estimation from a single image. We follow a standard two-step pipeline by first detecting the 2D position of the N body joints, and then using these observations to infer 3D pose. For the first step, we use a recent CNN-based detector. For the second step, most existing approaches perform 2N-to-3N regression of the Cartesian joint coordinates. We show that more precise pose estimates can be obtained by representing both the 2D and 3D human poses using NxN distance matrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network architectures, which by construction, enforce positivity and symmetry of the predicted matrices. The approach has also the advantage to naturally handle missing observations and allowing to hypothesize the position of non-observed joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate consistent performance gains over state-of-the-art. Qualitative evaluation on the images in-the-wild of the LSP dataset, using the regressor learned on Human3.6M, reveals very promising generalization results.",P3dhupoesfrasiimvidimare,275.0,63.0,28.0
7647,Pose estimation,164.0,self-supervised 3d hand pose estimation through training by fitting,3.0,201.0,1.0,78.0,4.0,2.5,153.0,91,http://openaccess.thecvf.com/content_CVPR_2019/papers/Wan_Self-Supervised_3D_Hand_Pose_Estimation_Through_Training_by_Fitting_CVPR_2019_paper.pdf,"We present a self-supervision method for 3D hand pose estimation from depth maps. We begin with a neural network initialized with synthesized data and fine-tune it on real but unlabelled depth maps by minimizing a set of data-fitting terms. By approximating the hand surface with a set of spheres, we design a differentiable hand renderer to align estimates by comparing the rendered and input depth maps. In addition, we place a set of priors including a data-driven term to further regulate the estimate's kinematic feasibility. Our method makes highly accurate estimates comparable to current supervised methods which require large amounts of labelled training samples, thereby advancing state-of-the-art in unsupervised learning for hand pose estimation.",Pse3dhapoesthtrbyfi,47.0,59.0,2.0
7648,Pose estimation,61.0,relative camera pose estimation using convolutional neural networks,4.0,201.0,1.0,183.0,3.0,2.5,153.6,92,https://arxiv.org/pdf/1702.01381,"This paper presents a convolutional neural network based approach for estimating the relative pose between two cameras. The proposed network takes RGB images from both cameras as input and directly produces the relative rotation and translation as output. The system is trained in an end-to-end manner utilising transfer learning from a large scale classification dataset. The introduced approach is compared with widely used local feature based methods (SURF, ORB) and the results indicate a clear improvement over the baseline. In addition, a variant of the proposed architecture containing a spatial pyramid pooling (SPP) layer is evaluated and shown to further improve the performance.",Precapoesusconene,128.0,28.0,5.0
7649,Pose estimation,95.0,global pose estimation with an attention-based recurrent network,4.0,201.0,1.0,152.0,3.0,2.5,154.5,93,http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/Parisotto_Global_Pose_Estimation_CVPR_2018_paper.pdf,"The ability for an agent to localize itself within an environment is crucial for many real-world applications. For unknown environments, Simultaneous Localization and Mapping (SLAM) enables incremental and concurrent building of and localizing within a map. We present a new, differentiable architecture, Neural Graph Optimizer, progressing towards a complete neural network solution for SLAM by designing a system composed of a local pose estimation model, a novel pose selection module, and a novel graph optimization process. The entire architecture is trained in an end-to-end fashion, enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering. We demonstrate the effectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom environment.",Pglpoeswianatrene,43.0,56.0,6.0
7650,Pose estimation,93.0,region ensemble network: improving convolutional network for hand pose estimation,4.0,201.0,1.0,162.0,3.0,2.5,156.9,94,https://arxiv.org/pdf/1702.02447,"Hand pose estimation from monocular depth images is an important and challenging problem for human-computer interaction. Recently deep convolutional networks (ConvNet) with sophisticated design have been employed to address it, but the improvement over traditional methods is not so apparent. To promote the performance of directly 3D coordinate regression, we propose a tree-structured Region Ensemble Network (REN), which partitions the convolution outputs into regions and integrates the results from multiple regressors on each regions. Compared with multi-model ensemble, our model is completely end-to-end training. The experimental results demonstrate that our approach achieves the best performance among state-of-the-arts on two public datasets.",Preenneimconefohapoes,116.0,30.0,22.0
7651,Pose estimation,198.0,generating multiple hypotheses for 3d human pose estimation with mixture density network,3.0,201.0,1.0,62.0,4.0,2.5,158.4,95,http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Generating_Multiple_Hypotheses_for_3D_Human_Pose_Estimation_With_Mixture_CVPR_2019_paper.pdf,"3D human pose estimation from a monocular image or 2D joints is an ill-posed problem because of depth ambiguity and occluded joints. We argue that 3D human pose estimation from a monocular input is an inverse problem where multiple feasible solutions can exist. In this paper, we propose a novel approach to generate multiple feasible hypotheses of the 3D pose from 2D joints. In contrast to existing deep learning approaches which minimize a mean square error based on an unimodal Gaussian distribution, our method is able to generate multiple feasible hypotheses of 3D pose based on a multimodal mixture density networks. Our experiments show that the 3D poses estimated by our approach from an input of 2D joints are consistent in 2D reprojections, which supports our argument that multiple solutions exist for the 2D-to-3D inverse problem. Furthermore, we show state-of-the-art performance on the Human3.6M dataset in both best hypothesis and multi-view settings, and we demonstrate the generalization capacity of our model by testing on the MPII and MPI-INF-3DHP datasets. Our code is available at the project website.",Pgemuhyfo3dhupoeswimidene,61.0,33.0,6.0
7652,Pose estimation,80.0,towards accurate marker-less human shape and pose estimation over time,4.0,201.0,1.0,184.0,3.0,2.5,159.6,96,https://arxiv.org/pdf/1707.07548,"Existing markerless motion capture methods often assume known backgrounds, static cameras, and sequence specific motion priors, limiting their application scenarios. Here we present a fully automatic method that, given multi-view videos, estimates 3D human pose and body shape. We take the recently proposed SMPLify method \cite{bogo2016keep} as the base method and extend it in several ways. First we fit a 3D human body model to 2D features detected in multi-view images. Second, we use a CNN method to segment the person in each image and fit the 3D body model to the contours, further improving accuracy. Third we utilize a generic and robust DCT temporal prior to handle the left and right side swapping issue sometimes introduced by the 2D pose estimator. Validation on standard benchmarks shows our results are comparable to the state of the art and also provide a realistic 3D shape avatar. We also demonstrate accurate results on HumanEva and on challenging monocular sequences of dancing from YouTube.",Ptoacmahushanpoesovti,119.0,58.0,5.0
7653,Pose estimation,100.0,poseidon: face-from-depth for driver pose estimation,4.0,201.0,1.0,173.0,3.0,2.5,162.3,97,https://openaccess.thecvf.com/content_cvpr_2017/papers/Borghi_POSEidon_Face-From-Depth_for_CVPR_2017_paper.pdf,"Fast and accurate upper-body and head pose estimation is a key task for automatic monitoring of driver attention, a challenging context characterized by severe illumination changes, occlusions and extreme poses. In this work, we present a new deep learning framework for head localization and pose estimation on depth images. The core of the proposal is a regressive neural network, called POSEidon, which is composed of three independent convolutional nets followed by a fusion layer, specially conceived for understanding the pose by depth. In addition, to recover the intrinsic value of face appearance for understanding head position and orientation, we propose a new Face-from-Depth model for learning image faces from depth. Results in face reconstruction are qualitatively impressive. We test the proposed framework on two public datasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new challenging dataset mainly inspired by the automotive setup. Results show that our method overcomes all recent state-of-art works, running in real time at more than 30 frames per second.",Ppofafodrpoes,84.0,60.0,8.0
7654,Pose estimation,90.0,hand3d: hand pose estimation using 3d neural network,4.0,201.0,1.0,189.0,3.0,2.5,164.1,98,https://arxiv.org/pdf/1704.02224,"We propose a novel 3D neural network architecture for 3D hand pose estimation from a single depth image. Different from previous works that mostly run on 2D depth image domain and require intermediate or post process to bring in the supervision from 3D space, we convert the depth map to a 3D volumetric representation, and feed it into a 3D convolutional neural network(CNN) to directly produce the pose in 3D requiring no further process. Our system does not require the ground truth reference point for initialization, and our network architecture naturally integrates both local feature and global context in 3D space. To increase the coverage of the hand pose space of the training data, we render synthetic depth image by transferring hand pose from existing real image datasets. We evaluation our algorithm on two public benchmarks and achieve the state-of-the-art performance. The synthetic hand pose dataset will be available.",Phahapoesus3dnene,62.0,30.0,4.0
7655,Pose estimation,401.0,depth-based 3d hand pose estimation: from current achievements to future goals,1.0,124.0,3.0,113.0,3.0,2.4000000000000004,203.8,99,http://arxiv.org/pdf/1712.03917v2,"In this paper, we strive to answer two questions: What is the current state of 3D hand pose estimation from depth images? And, what are the next challenges that need to be tackled? Following the successful Hands In the Million Challenge (HIM2017), we investigate the top 10 state-of-the-art methods on three tasks: single frame 3D pose estimation, 3D hand tracking, and hand pose estimation during object interaction. We analyze the performance of different CNN structures with regard to hand shape, joint visibility, view point and articulation distributions. Our findings include: (1) isolated 3D hand pose estimation achieves low mean errors (10 mm) in the view point range of [70, 120] degrees, but it is far from being solved for extreme view points; (2) 3D volumetric representations outperform 2D CNNs, better capturing the spatial structure of the depth data; (3) Discriminative methods still generalize poorly to unseen hand shapes; (4) While joint occlusions pose a challenge for most methods, explicit modeling of structure constraints can significantly narrow the gap between errors on visible and occluded joints.",Pde3dhapoesfrcuactofugo,159.0,81.0,9.0
7656,Pose estimation,401.0,multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge,1.0,132.0,3.0,126.0,3.0,2.4000000000000004,210.9,100,http://arxiv.org/pdf/1609.09475v3,"Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC) [1]. A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multiview RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th-place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://apc.cs.princeton.edu/",Pmusedelefo6dpoesinthampich,289.0,25.0,16.0
7932,Semantic segmentation,14.0,fully convolutional networks for semantic segmentation,5.0,50.0,4.0,1.0,5.0,4.6,24.5,1,https://openaccess.thecvf.com/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf,"Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.",Sfuconefosese,12514.0,78.0,1706.0
7933,Semantic segmentation,33.0,linknet: exploiting encoder representations for efficient semantic segmentation,5.0,82.0,4.0,93.0,4.0,4.3,70.6,2,https://arxiv.org/pdf/1707.03718.pdf%5D,"Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 GFLOPs for processing an image of resolution 3 × 640 × 360. It gives state-of-the-art performance on CamVid and comparable results on Cityscapes dataset. We also compare our networks processing time on NVIDIA GPU and embedded system device with existing state-of-the-art architectures for different image resolutions.",Sliexenrefoefsese,433.0,35.0,63.0
7934,Semantic segmentation,6.0,ccnet: criss-cross attention for semantic segmentation,5.0,110.0,3.0,12.0,5.0,4.2,49.4,3,http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_CCNet_Criss-Cross_Attention_for_Semantic_Segmentation_ICCV_2019_paper.pdf,"Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet.",Scccratfosese,591.0,93.0,72.0
7935,Semantic segmentation,49.0,asymmetric non-local neural networks for semantic segmentation,4.0,97.0,4.0,87.0,4.0,4.0,79.6,4,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhu_Asymmetric_Non-Local_Neural_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf,"The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git.",Sasnonenefosese,192.0,60.0,16.0
7936,Semantic segmentation,96.0,gated-scnn: gated shape cnns for semantic segmentation,4.0,100.0,4.0,55.0,4.0,4.0,85.3,5,http://openaccess.thecvf.com/content_ICCV_2019/papers/Takikawa_Gated-SCNN_Gated_Shape_CNNs_for_Semantic_Segmentation_ICCV_2019_paper.pdf,"Current state-of-the-art methods for image segmentation form a dense image representation where the color, shape and texture information are all processed together inside a deep CNN. This however may not be ideal as they contain very different type of information relevant for recognition. Here, we propose a new two-stream CNN architecture for semantic segmentation that explicitly wires shape information as a separate processing branch, i.e. shape stream, that processes information in parallel to the classical stream. Key to this architecture is a new type of gates that connect the intermediate layers of the two streams. Specifically, we use the higher-level activations in the classical stream to gate the lower-level activations in the shape stream, effectively removing noise and helping the shape stream to only focus on processing the relevant boundary-related information. This enables us to use a very shallow architecture for the shape stream that operates on the image-level resolution. Our experiments show that this leads to a highly effective architecture that produces sharper predictions around object boundaries and significantly boosts performance on thinner and smaller objects. Our method achieves state-of-the-art performance on the Cityscapes benchmark, in terms of both mask (mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong baselines.",Sgagashcnfosese,205.0,67.0,34.0
7937,Semantic segmentation,61.0,expectation-maximization attention networks for semantic segmentation,4.0,98.0,4.0,95.0,4.0,4.0,86.0,6,https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Expectation-Maximization_Attention_Networks_for_Semantic_Segmentation_ICCV_2019_paper.pdf,"Self-attention mechanism has been widely used for various tasks. It is designed to compute the representation of each position by a weighted sum of the features at all positions. Thus, it can capture long-range relations for computer vision tasks. However, it is computationally consuming. Since the attention maps are computed w.r.t all other positions. In this paper, we formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed. By a weighted summation upon these bases, the resulting representation is low-rank and deprecates noisy information from the input. The proposed Expectation-Maximization Attention (EMA) module is robust to the variance of input and is also friendly in memory and computation. Moreover, we set up the bases maintenance and normalization methods to stabilize its training procedure. We conduct extensive experiments on popular semantic segmentation benchmarks including PASCAL VOC, PASCAL Context, and COCO Stuff, on which we set new records.",Sexatnefosese,150.0,44.0,16.0
7938,Semantic segmentation,2.0,context encoding for semantic segmentation,5.0,113.0,3.0,48.0,4.0,3.9,60.2,7,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Context_Encoding_for_CVPR_2018_paper.pdf,"Recent work has made significant progress in improving spatial resolution for pixelwise labeling with Fully Convolutional Network (FCN) framework by employing Dilated/Atrous convolution, utilizing multi-scale features and refining boundaries. In this paper, we explore the impact of global contextual information in semantic segmentation by introducing the Context Encoding Module, which captures the semantic context of scenes and selectively highlights class-dependent featuremaps. The proposed Context Encoding Module significantly improves semantic segmentation results with only marginal extra computation cost over FCN. Our approach has achieved new state-of-the-art results 51.7% mIoU on PASCAL-Context, 85.9% mIoU on PASCAL VOC 2012. Our single model achieves a final score of 0.5567 on ADE20K test set, which surpasses the winning entry of COCO-Place Challenge 2017. In addition, we also explore how the Context Encoding Module can improve the feature representation of relatively shallow networks for the image classification on CIFAR-10 dataset. Our 14 layer network has achieved an error rate of 3.45%, which is comparable with state-of-the-art approaches with over 10Ã— more layers. The source code for the complete system are publicly available1.",Scoenfosese,620.0,68.0,86.0
7939,Semantic segmentation,11.0,fast-scnn: fast semantic segmentation network,5.0,101.0,3.0,75.0,4.0,3.9,66.2,8,https://arxiv.org/pdf/1902.04502,"The encoder-decoder framework is state-of-the-art for offline semantic image segmentation. Since the rise in autonomous systems, real-time computation is increasingly desirable. In this paper, we introduce fast segmentation convolutional neural network (Fast-SCNN), an above real-time semantic segmentation model on high resolution image data (1024x2048px) suited to efficient computation on embedded devices with low memory. Building on existing two-branch methods for fast segmentation, we introduce our `learning to downsample' module which computes low-level features for multiple resolution branches simultaneously. Our network combines spatial detail at high resolution with deep features extracted at lower resolution, yielding an accuracy of 68.0% mean intersection over union at 123.5 frames per second on Cityscapes. We also show that large scale pre-training is unnecessary. We thoroughly validate our metric in experiments with ImageNet pre-training and the coarse labeled data of Cityscapes. Finally, we show even faster computation with competitive results on subsampled inputs, without any network modifications.",Sfafasesene,144.0,42.0,28.0
7940,Semantic segmentation,1.0,bisenet: bilateral segmentation network for real-time semantic segmentation,5.0,139.0,3.0,49.0,4.0,3.9,70.6,9,https://openaccess.thecvf.com/content_ECCV_2018/papers/Changqian_Yu_BiSeNet_Bilateral_Segmentation_ECCV_2018_paper.pdf,"Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048 \(\times \) 1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.",Sbibiseneforesese,650.0,41.0,111.0
7941,Semantic segmentation,15.0,icnet for real-time semantic segmentation on high-resolution images,5.0,143.0,3.0,50.0,4.0,3.9,76.7,10,https://openaccess.thecvf.com/content_ECCV_2018/papers/Hengshuang_Zhao_ICNet_for_Real-Time_ECCV_2018_paper.pdf,"We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.",Sicforeseseonhiim,641.0,54.0,110.0
7942,Semantic segmentation,63.0,rich feature hierarchies for accurate object detection and semantic segmentation,4.0,147.0,3.0,4.0,5.0,3.9,78.9,11,https://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf,"Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",Srifehifoacobdeansese,15395.0,64.0,2056.0
7943,Semantic segmentation,39.0,enet: a deep neural network architecture for real-time semantic segmentation,5.0,144.0,3.0,47.0,4.0,3.9,83.39999999999999,12,https://arxiv.org/pdf/1606.02147.pdf)%C3%AC%20%E2%82%AC,"The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18$\times$ faster, requires 75$\times$ less FLOPs, has 79$\times$ less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.",Senadenenearforesese,1066.0,35.0,215.0
7944,Semantic segmentation,5.0,co-occurrent features in semantic segmentation,5.0,151.0,3.0,100.0,4.0,3.9,91.9,13,http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Co-Occurrent_Features_in_Semantic_Segmentation_CVPR_2019_paper.pdf,"Recent work has achieved great success in utilizing global contextual information for semantic segmentation, including increasing the receptive field and aggregating pyramid feature representations. In this paper, we go beyond global context and explore the fine-grained representation using co-occurrent features by introducing Co-occurrent Feature Model, which predicts the distribution of co-occurrent features for a given target. To leverage the semantic context in the co-occurrent features, we build an Aggregated Co-occurrent Feature (ACF) Module by aggregating the probability of the co-occurrent feature with the co-occurrent context. ACF Module learns a fine-grained spatial invariant representation to capture co-occurrent context information across the scene. Our approach significantly improves the segmentation results using FCN and achieves superior performance 54.0% mIoU on Pascal Context, 87.2% mIoU on Pascal VOC 2012 and 44.89% mIoU on ADE20K datasets. The source code and complete system will be publicly available upon publication.",Scofeinsese,123.0,63.0,20.0
7945,Semantic segmentation,401.0,encoder-decoder with atrous separable convolution for semantic image segmentation,1.0,3.0,5.0,5.0,5.0,3.8,123.0,14,http://arxiv.org/pdf/1706.05587v3,"Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0\% and 82.1\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at \url{this https URL}.",Senwiatsecofoseimse,3833.0,92.0,670.0
7947,Semantic segmentation,401.0,semantic image segmentation with deep convolutional nets and fully connected crfs,1.0,10.0,5.0,20.0,5.0,3.8,130.3,15,http://arxiv.org/pdf/1709.00516v1,"Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ""semantic image segmentation""). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our ""DeepLab"" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",Sseimsewideconeanfucocr,3056.0,59.0,374.0
7948,Semantic segmentation,42.0,erfnet: efficient residual factorized convnet for real-time semantic segmentation,4.0,141.0,3.0,66.0,4.0,3.6000000000000005,88.8,16,http://www.robesafe.com/personal/roberto.arroyo/docs/Romera17tits.pdf,"Semantic segmentation is a challenging task that addresses most of the perception needs of intelligent vehicles (IVs) in an unified way. Deep neural networks excel at this task, as they can be trained end-to-end to accurately classify multiple object categories in an image at pixel level. However, a good tradeoff between high quality and computational resources is yet not present in the state-of-the-art semantic segmentation approaches, limiting their application in real vehicles. In this paper, we propose a deep architecture that is able to run in real time while providing accurate semantic segmentation. The core of our architecture is a novel layer that uses residual connections and factorized convolutions in order to remain efficient while retaining remarkable accuracy. Our approach is able to run at over 83 FPS in a single Titan X, and 7 FPS in a Jetson TX1 (embedded device). A comprehensive set of experiments on the publicly available Cityscapes data set demonstrates that our system achieves an accuracy that is similar to the state of the art, while being orders of magnitude faster to compute than other architectures that achieve top precision. The resulting tradeoff makes our model an ideal approach for scene understanding in IV applications. The code is publicly available at: https://github.com/Eromera/erfnet",Serefrefacoforesese,525.0,38.0,109.0
7949,Semantic segmentation,51.0,3d semantic segmentation with submanifold sparse convolutional networks,4.0,160.0,3.0,69.0,4.0,3.6000000000000005,100.0,17,https://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf,"Convolutional networks are the de-facto standard for analyzing spatio-temporal data such as images, videos, and 3D shapes. Whilst some of this data is naturally dense (e.g., photos), many other data sources are inherently sparse. Examples include 3D point clouds that were obtained using a LiDAR scanner or RGB-D camera. Standard ""dense"" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce new sparse convolutional operations that are designed to process spatially-sparse data more efficiently, and use them to develop spatially-sparse convolutional networks. We demonstrate the strong performance of the resulting models, called submanifold sparse convolutional networks (SS-CNs), on two tasks involving semantic segmentation of 3D point clouds. In particular, our models outperform all prior state-of-the-art on the test set of a recent semantic segmentation competition.",S3dsesewisuspcone,433.0,26.0,75.0
7950,Semantic segmentation,60.0,fully convolutional instance-aware semantic segmentation,4.0,146.0,3.0,84.0,4.0,3.6000000000000005,101.6,18,https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Fully_Convolutional_Instance-Aware_CVPR_2017_paper.pdf,"We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation [29] and instance mask proposal [5]. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The network architecture is highly integrated and efficient. It achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. Code would be released at https://github.com/daijifeng001/TA-FCN.",Sfucoinsese,614.0,49.0,60.0
7951,Semantic segmentation,56.0,improving semantic segmentation via video propagation and label relaxation,4.0,165.0,3.0,89.0,4.0,3.6000000000000005,109.5,19,http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Improving_Semantic_Segmentation_via_Video_Propagation_and_Label_Relaxation_CVPR_2019_paper.pdf,"Semantic segmentation requires large amounts of pixel-wise annotations to learn accurate models. In this paper, we present a video prediction-based methodology to scale up training sets by synthesizing new training samples in order to improve the accuracy of semantic segmentation networks. We exploit video prediction models' ability to predict future frames in order to also predict future labels. A joint propagation strategy is also proposed to alleviate mis-alignments in synthesized samples. We demonstrate that training segmentation models on datasets augmented by the synthesized samples leads to significant improvements in accuracy. Furthermore, we introduce a novel boundary label relaxation technique that makes training robust to annotation noise and propagation artifacts along object boundaries. Our proposed methods achieve state-of-the-art mIoUs of 83.5% on Cityscapes and 82.9% on CamVid. Our single model, without model ensembles, achieves 72.8% mIoU on the KITTI semantic segmentation test set, which surpasses the winning entry of the ROB challenge 2018.",Simsesevivipranlare,168.0,48.0,21.0
7952,Semantic segmentation,21.0,learning a discriminative feature network for semantic segmentation,5.0,168.0,3.0,127.0,3.0,3.6,111.6,20,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Learning_a_Discriminative_CVPR_2018_paper.pdf,"Most existing methods of semantic segmentation still suffer from two aspects of challenges: intra-class inconsistency and inter-class indistinction. To tackle these two problems, we propose a Discriminative Feature Network (DFN), which contains two sub-networks: Smooth Network and Border Network. Specifically, to handle the intra-class inconsistency problem, we specially design a Smooth Network with Channel Attention Block and global average pooling to select the more discriminative features. Furthermore, we propose a Border Network to make the bilateral features of boundary distinguishable with deep semantic boundary supervision. Based on our proposed DFN, we achieve state-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean IOU on Cityscapes dataset.",Sleadifenefosese,416.0,47.0,28.0
7953,Semantic segmentation,136.0,advent: adversarial entropy minimization for domain adaptation in semantic segmentation,3.0,183.0,3.0,13.0,5.0,3.6,117.9,21,http://openaccess.thecvf.com/content_CVPR_2019/papers/Vu_ADVENT_Adversarial_Entropy_Minimization_for_Domain_Adaptation_in_Semantic_Segmentation_CVPR_2019_paper.pdf,"Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real-world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging “synthetic-2-real” set-ups and show that the approach can also be used for detection.",Sadadenmifodoadinsese,333.0,58.0,86.0
7954,Semantic segmentation,401.0,hierarchical multi-scale attention for semantic segmentation,1.0,92.0,4.0,16.0,5.0,3.4000000000000004,161.9,22,http://arxiv.org/pdf/2005.10821v1,"Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).",Shimuatfosese,99.0,45.0,12.0
7955,Semantic segmentation,7.0,learning deconvolution network for semantic segmentation,5.0,201.0,1.0,15.0,5.0,3.4,87.0,23,https://openaccess.thecvf.com/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf,"We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.",Sledenefosese,2146.0,32.0,107.0
7956,Semantic segmentation,16.0,object-contextual representations for semantic segmentation,5.0,201.0,1.0,9.0,5.0,3.4,87.9,24,https://arxiv.org/pdf/1909.11065,"In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, % the representation similarity we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context, and COCO-Stuff. Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission ""HRNet + OCR + SegFix"" achieves the 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: this https URL and this https URL.",Sobrefosese,245.0,104.0,34.0
7957,Semantic segmentation,31.0,refinenet: multi-path refinement networks for high-resolution semantic segmentation,5.0,201.0,1.0,7.0,5.0,3.4,91.8,25,http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf,"Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",Sremurenefohisese,1536.0,47.0,183.0
7958,Semantic segmentation,23.0,the one hundred layers tiramisu: fully convolutional densenets for semantic segmentation,5.0,201.0,1.0,23.0,5.0,3.4,94.20000000000002,26,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Jegou_The_One_Hundred_CVPR_2017_paper.pdf,"State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions.,,,,,, Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train.,,,,,, In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets.",Sthonhulatifucodefosese,1074.0,47.0,132.0
7959,Semantic segmentation,27.0,learning dynamic routing for semantic segmentation,5.0,201.0,1.0,31.0,5.0,3.4,97.8,27,https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Learning_Dynamic_Routing_for_Semantic_Segmentation_CVPR_2020_paper.pdf,"Recently, numerous handcrafted and searched networks have been applied for semantic segmentation. However, previous works intend to handle inputs with various scales in pre-defined static architectures, such as FCN, U-Net, and DeepLab series. This paper studies a conceptually new method to alleviate the scale variance in semantic representation, named dynamic routing. The proposed framework generates data-dependent routes, adapting to the scale distribution of each image. To this end, a differentiable gating function, called soft conditional gate, is proposed to select scale transform paths on the fly. In addition, the computational cost can be further reduced in an end-to-end manner by giving budget constraints to the gating function. We further relax the network level routing space to support multi-path propagations and skip-connections in each forward, bringing substantial network capacity. To demonstrate the superiority of the dynamic property, we compare with several static architectures, which can be modeled as special cases in the routing space. Extensive experiments are conducted on Cityscapes and PASCAL VOC 2012 to illustrate the effectiveness of the dynamic framework. Code is available at https://github.com/yanwei-li/DynamicRouting.",Sledyrofosese,43.0,48.0,7.0
7960,Semantic segmentation,168.0,interlaced sparse self-attention for semantic segmentation,3.0,99.0,4.0,150.0,3.0,3.4,135.0,28,https://arxiv.org/pdf/1907.12273.pdf(%C3%A6%C2%AD%C2%A3%C3%A5%C5%93%C2%A8%C3%A9%CB%9C%E2%80%A6%C3%A8%C2%AF%C2%BB%C3%A7%C5%A1%E2%80%9E%C3%A4%C2%B8%E2%82%AC%C3%A7%C2%AF%E2%80%A1),"In this paper, we present a so-called interlaced sparse self-attention approach to improve the efficiency of the \emph{self-attention} mechanism for semantic segmentation. The main idea is that we factorize the dense affinity matrix as the product of two sparse affinity matrices. There are two successive attention modules each estimating a sparse affinity matrix. The first attention module is used to estimate the affinities within a subset of positions that have long spatial interval distances and the second attention module is used to estimate the affinities within a subset of positions that have short spatial interval distances. These two attention modules are designed so that each position is able to receive the information from all the other positions. In contrast to the original self-attention module, our approach decreases the computation and memory complexity substantially especially when processing high-resolution feature maps. We empirically verify the effectiveness of our approach on six challenging semantic segmentation benchmarks.",Sinspsefosese,42.0,65.0,3.0
7961,Semantic segmentation,3.0,understanding convolution for semantic segmentation,5.0,201.0,1.0,46.0,4.0,3.1,95.1,29,https://arxiv.org/pdf/1702.08502.pdf](Unsupervised,"Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the ""gridding issue""caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1% mIOU in the test set at the time of submission. We also have achieved state-of-theart overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC.",Suncofosese,746.0,54.0,71.0
7962,Semantic segmentation,48.0,randla-net: efficient semantic segmentation of large-scale point clouds,4.0,201.0,1.0,8.0,5.0,3.1,97.20000000000002,30,http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_RandLA-Net_Efficient_Semantic_Segmentation_of_Large-Scale_Point_Clouds_CVPR_2020_paper.pdf,"We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale point clouds. In this paper, we introduce RandLA-Net, an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. The key to our approach is to use random point sampling instead of more complex point selection approaches. Although remarkably computation and memory efficient, random sampling can discard key features by chance. To overcome this, we introduce a novel local feature aggregation module to progressively increase the receptive field for each 3D point, thereby effectively preserving geometric details. Extensive experiments show that our RandLA-Net can process 1 million points in a single pass with up to 200x faster than existing approaches. Moreover, our RandLA-Net clearly surpasses state-of-the-art approaches for semantic segmentation on two large-scale benchmarks Semantic3D and SemanticKITTI.",Sraefseseoflapocl,238.0,75.0,38.0
7963,Semantic segmentation,34.0,the synthia dataset: a large collection of synthetic images for semantic segmentation of urban scenes,5.0,201.0,1.0,45.0,4.0,3.1,104.1,31,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf,"Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images, thus, having a sufficient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation - in particular, when using a DCNN paradigm. In order to answer this question we have generated a synthetic collection of diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments with DCNNs that show how the inclusion of SYNTHIA in the training stage significantly improves performance on the semantic segmentation task.",Sthsydaalacoofsyimfoseseofursc,1150.0,49.0,232.0
7964,Semantic segmentation,10.0,denseaspp for semantic segmentation in street scenes,5.0,201.0,1.0,79.0,4.0,3.1,107.1,32,http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf,"Semantic image segmentation is a basic street scene understanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of semantic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly encoded. To remedy this problem, atrous convolution[14]was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution, Atrous Spatial Pyramid Pooling (ASPP)[2] was proposed to concatenate multiple atrous-convolved features using different dilation rates into a final feature representation. Although ASPP is able to generate multi-scale features, we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end, we propose Densely connected Atrous Spatial Pyramid Pooling (DenseASPP), which connects a set of atrous convolutional layers in a dense way, such that it generates multi-scale features that not only cover a larger scale range, but also cover that scale range densely, without significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapes[4] and achieve state-of-the-art performance.",Sdefoseseinstsc,531.0,26.0,65.0
7965,Semantic segmentation,79.0,bidirectional learning for domain adaptation of semantic segmentation,4.0,201.0,1.0,17.0,5.0,3.1,109.2,33,http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Bidirectional_Learning_for_Domain_Adaptation_of_Semantic_Segmentation_CVPR_2019_paper.pdf,"Domain adaptation for semantic image segmentation is very necessary since manually labeling large datasets with pixel-level labels is expensive and time consuming. Existing domain adaptation techniques either work on limited datasets, or yield not so good performance compared with supervised learning. In this paper, we propose a novel bidirectional learning framework for domain adaptation of segmentation. Using the bidirectional learning, the image translation model and the segmentation adaptation model can be learned alternatively and promote to each other.Furthermore, we propose a self-supervised learning algorithm to learn a better segmentation adaptation model and in return improve the image translation model. Experiments show that our method superior to the state-of-the-art methods in domain adaptation of segmentation with a big margin. The source code is available at https://github.com/liyunsheng13/BDL",Sbilefodoadofsese,209.0,44.0,51.0
7966,Semantic segmentation,94.0,panet: few-shot image semantic segmentation with prototype alignment,4.0,201.0,1.0,19.0,5.0,3.1,114.3,34,http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf,"Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%.",Spafeimsesewipral,154.0,29.0,48.0
7967,Semantic segmentation,68.0,rethinking semantic segmentation from a sequence-to-sequence perspective with transformers,4.0,89.0,4.0,201.0,1.0,3.1,116.3,35,https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.pdf,"Spatially-adaptive normalization is remarkably successful recently in conditional semantic image synthesis, which modulates the normalized activation with spatially-varying transformations learned from semantic layouts, to preserve the semantic information from being washed away. Despite its impressive performance, a more thorough understanding of the true advantages inside the box is still highly demanded, to help reduce the significant computation and parameter overheads introduced by these new structures. In this paper, from a return-on-investment point of view, we present a deep analysis of the effectiveness of SPADE and observe that its advantages actually come mainly from its semantic-awareness rather than the spatial-adaptiveness. Inspired by this point, we propose class-adaptive normalization (CLADE), a lightweight variant that is not adaptive to spatial positions or layouts. Benefited from this design, CLADE greatly reduces the computation cost while still being able to preserve the semantic information during the generation. Extensive experiments on multiple challenging datasets demonstrate that while the resulting fidelity is on par with SPADE, its overhead is much cheaper than SPADE. Take the generator for ADE20k dataset as an example, the extra parameter and computation cost introduced by CLADE are only 4.57% and 0.07% while that of SPADE are 39.21% and 234.73% respectively.",Sresesefrasepewitr,168.0,68.0,22.0
7968,Semantic segmentation,24.0,stacked deconvolutional network for semantic segmentation,5.0,201.0,1.0,96.0,4.0,3.1,116.4,36,https://arxiv.org/pdf/1708.04943,"Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and bring the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which enhances the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-ofthe- art results on four datasets, including PASCAL VOC 2012, CamVid, GATECH, COCO Stuff. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6% in the test set.",Sstdenefosese,147.0,43.0,7.0
7969,Semantic segmentation,36.0,large-scale point cloud semantic segmentation with superpoint graphs,5.0,201.0,1.0,85.0,4.0,3.1,116.7,37,https://openaccess.thecvf.com/content_cvpr_2018/papers/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.pdf,"We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of large-scale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).",Slapoclsesewisugr,516.0,68.0,54.0
7970,Semantic segmentation,28.0,survey on semantic segmentation using deep learning techniques,5.0,201.0,1.0,99.0,4.0,3.1,118.5,38,https://www.sciencedirect.com/science/article/am/pii/S092523121930181X,"Abstract Semantic segmentation is a challenging task in computer vision systems. A lot of methods have been developed to tackle this problem ranging from autonomous vehicles, human-computer interaction, to robotics, medical research, agriculture and so on. Many of these methods have been built using the deep learning paradigm that has shown a salient performance. For this reason, we propose to survey these methods by, first categorizing them into ten different classes according to the common concepts underlying their architectures. Second, by providing an overview of the publicly available datasets on which they have been assessed. In addition, we present the common evaluation matrix used to measure their accuracy. Moreover, we focus on some of the methods and look closely at their architectures in order to find out how they have achieved their reported performances. Finally, we conclude by discussing some of the open problems and their possible solutions.",Ssuonseseusdelete,140.0,228.0,3.0
7971,Semantic segmentation,93.0,rethinking bisenet for real-time semantic segmentation,4.0,88.0,4.0,201.0,1.0,3.1,123.4,39,https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Rethinking_BiSeNet_for_Real-Time_Semantic_Segmentation_CVPR_2021_paper.pdf,"BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design. To handle these problems, we propose a novel and efficient structure named Short-Term Dense Concatenate network (STDC network) by removing structure redundancy. Specifically, we gradually reduce the dimension of feature maps and use the aggregation of them for image representation, which forms the basic module of STDC network. In the decoder, we propose a Detail Aggregation module by integrating the learning of spatial information into low-level layers in single-stream manner. Finally, the low-level features and deep features are fused to predict the final segmentation results. Extensive experiments on Cityscapes and CamVid dataset demonstrate the effectiveness of our method by achieving promising trade-off between segmentation accuracy and inference speed. On Cityscapes, we achieve 71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti, which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0 FPS while inferring on higher resolution images.",Srebiforesese,7.0,32.0,0.0
7973,Semantic segmentation,401.0,improving semantic segmentation via decoupled body and edge supervision,1.0,90.0,4.0,64.0,4.0,3.1,175.5,40,http://arxiv.org/pdf/2007.10035v2,"Existing semantic segmentation approaches either aim to improve the object's inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires \textit{explicitly} modeling the object \textit{body} and \textit{edge}, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including \textit{Cityscapes}, \textit{CamVid}, \textit{KIITI} and \textit{BDD} show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU \% on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (\url{this https URL}).",Simsesevideboanedsu,37.0,73.0,0.0
7974,Semantic segmentation,45.0,learning to adapt structured output space for semantic segmentation,4.0,201.0,1.0,52.0,4.0,2.8,109.5,41,https://openaccess.thecvf.com/content_cvpr_2018/papers/Tsai_Learning_to_Adapt_CVPR_2018_paper.pdf,"Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the state-of-the-art methods in terms of accuracy and visual quality.",Sletoadstouspfosese,585.0,45.0,158.0
7975,Semantic segmentation,89.0,a survey of loss functions for semantic segmentation,4.0,201.0,1.0,43.0,4.0,2.8,120.0,42,https://arxiv.org/pdf/2006.14822,"Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.",Sasuoflofufosese,59.0,26.0,4.0
7976,Semantic segmentation,92.0,single-stage semantic segmentation from image labels,4.0,201.0,1.0,44.0,4.0,2.8,121.2,43,http://openaccess.thecvf.com/content_CVPR_2020/papers/Araslanov_Single-Stage_Semantic_Segmentation_From_Image_Labels_CVPR_2020_paper.pdf,"Recent years have seen a rapid growth in new approaches improving the accuracy of semantic segmentation in a weakly supervised setting, i.e. with only image-level labels available for training. However, this has come at the cost of increased model complexity and sophisticated multi-stage training procedures. This is in contrast to earlier work that used only a single stage -- training one segmentation network on image labels -- which was abandoned due to inferior segmentation accuracy. In this work, we first define three desirable properties of a weakly supervised method: local consistency, semantic fidelity, and completeness. Using these properties as guidelines, we then develop a segmentation-based network model and a self-supervised training scheme to train for semantic masks from image-level annotations in a single stage. We show that despite its simplicity, our method achieves results that are competitive with significantly more complex pipelines, substantially outperforming earlier single-stage methods.",Ssisesefrimla,30.0,67.0,7.0
7977,Semantic segmentation,108.0,mining cross-image semantics for weakly supervised semantic segmentation,3.0,201.0,1.0,29.0,5.0,2.8,121.5,44,https://arxiv.org/pdf/2007.01947,"This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the classifier discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually benefiting semantic segmentation learning. More essentially, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its efficacy and generalizability. Moreover, our approach ranked 1st place in the Weakly-Supervised Semantic Segmentation Track of CVPR2020 Learning from Imperfect Data Challenge.",Smicrsefowesusese,55.0,96.0,8.0
7978,Semantic segmentation,54.0,dfanet: deep feature aggregation for real-time semantic segmentation,4.0,201.0,1.0,91.0,4.0,2.8,123.9,45,http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_DFANet_Deep_Feature_Aggregation_for_Real-Time_Semantic_Segmentation_CVPR_2019_paper.pdf,"This paper introduces an extremely efficient CNN architecture named DFANet for semantic segmentation under resource constraints. Our proposed network starts from a single lightweight backbone and aggregates discriminative features through sub-network and sub-stage cascade respectively. Based on the multi-scale feature propagation, DFANet substantially reduces the number of parameters, but still obtains sufficient receptive field and enhances the model learning ability, which strikes a balance between the speed and segmentation performance. Experiments on Cityscapes and CamVid datasets demonstrate the superior performance of DFANet with 8$\times$ less FLOPs and 2$\times$ faster than the existing state-of-the-art real-time semantic segmentation methods while providing comparable accuracy. Specifically, it achieves 70.3\% Mean IOU on the Cityscapes test dataset with only 1.7 GFLOPs and a speed of 160 FPS on one NVIDIA Titan X card, and 71.3\% Mean IOU with 3.4 GFLOPs while inferring on a higher resolution image.",Sdfdefeagforesese,157.0,40.0,22.0
7979,Semantic segmentation,37.0,knowledge adaptation for efficient semantic segmentation,5.0,201.0,1.0,113.0,3.0,2.8,125.4,46,https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Knowledge_Adaptation_for_Efficient_Semantic_Segmentation_CVPR_2019_paper.pdf,"Both accuracy and efficiency are of significant importance to the task of semantic segmentation. Existing deep FCNs suffer from heavy computations due to a series of high-resolution feature maps for preserving the detailed knowledge in dense estimation. Although reducing the feature map resolution (i.e., applying a large overall stride) via subsampling operations (e.g., polling and convolution striding) can instantly increase the efficiency, it dramatically decreases the estimation accuracy. To tackle this dilemma, we propose a knowledge distillation method tailored for semantic segmentation to improve the performance of the compact FCNs with large overall stride. To handle the inconsistency between the features of the student and teacher network, we optimize the feature similarity in a transferred latent domain formulated by utilizing a pre-trained autoencoder. Moreover, an affinity distillation module is proposed to capture the long-range dependency by calculating the non local interactions across the whole image. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three popular benchmarks: Pascal VOC, Cityscapes and Pascal Context. Built upon a highly competitive baseline, our proposed method can improve the performance of a student network by 2.5% (mIOU boosts from 70.2 to 72.7 on the cityscapes test set) and can train a better compact model with only 8% float operations (FLOPS) of a model that achieves comparable performances.",Sknadfoefsese,80.0,34.0,3.0
7980,Semantic segmentation,22.0,a review on deep learning techniques applied to semantic segmentation,5.0,201.0,1.0,133.0,3.0,2.8,126.9,47,https://arxiv.org/pdf/1704.06857,"Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.",Sareondeleteaptosese,709.0,120.0,30.0
7981,Semantic segmentation,117.0,temporally distributed networks for fast video semantic segmentation,3.0,201.0,1.0,38.0,5.0,2.8,126.9,48,http://openaccess.thecvf.com/content_CVPR_2020/papers/Hu_Temporally_Distributed_Networks_for_Fast_Video_Semantic_Segmentation_CVPR_2020_paper.pdf,"We present TDNet, a temporally distributed network designed for fast and accurate video semantic segmentation. We observe that features extracted from a certain high-level layer of a deep CNN can be approximated by composing features extracted from several shallower sub-networks. Leveraging the inherent temporal continuity in videos, we distribute these sub-networks over sequential frames. Therefore, at each time step, we only need to perform a lightweight computation to extract a sub-features group from a single sub-network. The full features used for segmentation are then recomposed by application of a novel attention propagation module that compensates for geometry deformation between frames. A grouped knowledge distillation loss is also introduced to further improve the representation power at both full and sub-feature levels. Experiments on Cityscapes, CamVid, and NYUD-v2 demonstrate that our method achieves state-of-the-art accuracy with significantly faster speed and lower latency.",Stedinefofavisese,38.0,70.0,7.0
7982,Semantic segmentation,72.0,structured knowledge distillation for semantic segmentation,4.0,201.0,1.0,90.0,4.0,2.8,129.0,49,http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf,"In this paper, we investigate the issue of knowledge distillation for training compact semantic segmentation networks by making use of cumbersome networks. We start from the straightforward scheme, pixel-wise distillation, which applies the distillation scheme originally introduced for image classification and performs knowledge distillation for each pixel separately. We further propose to distill the structured knowledge from cumbersome networks into compact networks, which is motivated by the fact that semantic segmentation is a structured prediction problem. We study two such structured distillation schemes: (i) pair-wise distillation that distills the pairwise similarities, and (ii) holistic distillation that uses adversarial training to distill holistic knowledge. The effectiveness of our knowledge distillation approaches is demonstrated by extensive experiments on three scene parsing datasets: Cityscapes, Camvid and ADE20K.",Sstkndifosese,155.0,65.0,17.0
7983,Semantic segmentation,66.0,object region mining with adversarial erasing: a simple classification to semantic segmentation approach,4.0,201.0,1.0,97.0,4.0,2.8,129.3,50,https://openaccess.thecvf.com/content_cvpr_2017/papers/Wei_Object_Region_Mining_CVPR_2017_paper.pdf,"We investigate a principle way to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems. Classification networks are only responsive to small and sparse discriminative regions from the object of interest, which deviates from the requirement of the segmentation task that needs to localize dense, interior and integral regions for pixel-wise inference. To mitigate this gap, we propose a new adversarial erasing approach for localizing and expanding object regions progressively. Starting with a single small object region, our proposed approach drives the classification network to sequentially discover new and complement object regions by erasing the current mined regions in an adversarial manner. These localized regions eventually constitute a dense and complete object region for learning semantic segmentation. To further enhance the quality of the discovered regions by adversarial erasing, an online prohibitive segmentation learning approach is developed to collaborate with adversarial erasing by providing auxiliary segmentation supervision modulated by the more reliable classification scores. Despite its apparent simplicity, the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union (mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new state-of-the-arts.",Sobremiwiaderasicltoseseap,411.0,35.0,52.0
7984,Semantic segmentation,75.0,graph attention convolution for point cloud semantic segmentation,4.0,201.0,1.0,88.0,4.0,2.8,129.3,51,https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Graph_Attention_Convolution_for_Point_Cloud_Semantic_Segmentation_CVPR_2019_paper.pdf,"Standard convolution is inherently limited for semantic segmentation of point cloud due to its isotropy about features. It neglects the structure of an object, results in poor object delineation and small spurious regions in the segmentation result. This paper proposes a novel graph attention convolution (GAC), whose kernels can be dynamically carved into specific shapes to adapt to the structure of an object. Specifically, by assigning proper attentional weights to different neighboring points, GAC is designed to selectively focus on the most relevant part of them according to their dynamically learned features. The shape of the convolution kernel is then determined by the learned distribution of the attentional weights. Though simple, GAC can capture the structured features of point clouds for fine-grained segmentation and avoid feature contamination between objects. Theoretically, we provided a thorough analysis on the expressive capabilities of GAC to show how it can learn about the features of point clouds. Empirically, we evaluated the proposed GAC on challenging indoor and outdoor datasets and achieved the state-of-the-art results in both scenarios.",Sgratcofopoclsese,196.0,59.0,20.0
7985,Semantic segmentation,95.0,dual super-resolution learning for semantic segmentation,4.0,201.0,1.0,70.0,4.0,2.8,129.9,52,https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf,"Current state-of-the-art semantic segmentation methods often apply high-resolution input to attain high performance, which brings large computation budgets and limits their applications on resource-constrained devices. In this paper, we propose a simple and flexible two-stream framework named Dual Super-Resolution Learning (DSRL) to effectively improve the segmentation accuracy without introducing extra computation costs. Specifically, the proposed method consists of three parts: Semantic Segmentation Super-Resolution (SSSR), Single Image Super-Resolution (SISR) and Feature Affinity (FA) module, which can keep high-resolution representations with low-resolution input while simultaneously reducing the model computation complexity. Moreover, it can be easily generalized to other tasks, e.g., human pose estimation. This simple yet effective method leads to strong representations and is evidenced by promising performance on both semantic segmentation and human pose estimation. Specifically, for semantic segmentation on CityScapes, we can achieve $\geq$2\% higher mIoU with similar FLOPs, and keep the performance with 70\% FLOPs. For human pose estimation, we can gain $\geq$2\% mAP with the same FLOPs and maintain mAP with $30\%$ fewer FLOPs. Code and models are available at \url{https://github.com/wanglixilinx/DSRL}.",Sdusulefosese,25.0,39.0,2.0
7986,Semantic segmentation,91.0,adversarial learning for semi-supervised semantic segmentation,4.0,201.0,1.0,92.0,4.0,2.8,135.3,53,https://arxiv.org/pdf/1802.07934,"We propose a method for semi-supervised semantic segmentation using an adversarial network. While most existing discriminators are trained to classify input images as real or fake on the image level, we design a discriminator in a fully convolutional manner to differentiate the predicted probability maps from the ground truth segmentation distribution with the consideration of the spatial resolution. We show that the proposed discriminator can be used to improve semantic segmentation accuracy by coupling the adversarial loss with the standard cross entropy loss of the proposed model. In addition, the fully convolutional discriminator enables semi-supervised learning through discovering the trustworthy regions in predicted results of unlabeled images, thereby providing additional supervisory signals. In contrast to existing methods that utilize weakly-labeled images, our method leverages unlabeled images to enhance the segmentation model. Experimental results on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the effectiveness of the proposed algorithm.",Sadlefosesese,195.0,47.0,44.0
7987,Semantic segmentation,99.0,learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation,4.0,201.0,1.0,86.0,4.0,2.8,135.9,54,https://openaccess.thecvf.com/content_cvpr_2018/papers/Ahn_Learning_Pixel-Level_Semantic_CVPR_2018_paper.pdf,"The deficiency of segmentation labels is one of the main obstacles to semantic segmentation in the wild. To alleviate this issue, we present a novel framework that generates segmentation labels of images given their image-level class labels. In this weakly supervised setting, trained models have been known to segment local discriminative parts rather than the entire object area. Our solution is to propagate such local responses to nearby areas which belong to the same semantic entity. To this end, we propose a Deep Neural Network (DNN) called AffinityNet that predicts semantic affinity between a pair of adjacent image coordinates. The semantic propagation is then realized by random walk with the affinities predicted by AffinityNet. More importantly, the supervision employed to train AffinityNet is given by the initial discriminative part segmentation, which is incomplete as a segmentation annotation but sufficient for learning semantic affinities within small image areas. Thus the entire framework relies only on image-level class labels and does not require any extra data or annotations. On the PASCAL VOC 2012 dataset, a DNN learned with segmentation labels generated by our method outperforms previous models trained with the same level of supervision, and is even as competitive as those relying on stronger supervision.",Slepiseafwiimsufowesusese,226.0,41.0,52.0
7988,Semantic segmentation,98.0,instance-aware semantic segmentation via multi-task network cascades,4.0,201.0,1.0,98.0,4.0,2.8,139.20000000000002,55,https://openaccess.thecvf.com/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf,"Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.",Sinsesevimuneca,912.0,36.0,83.0
7989,Semantic segmentation,164.0,part-aware prototype network for few-shot semantic segmentation,3.0,201.0,1.0,33.0,5.0,2.8,139.5,56,https://arxiv.org/pdf/2007.06309,"Few-shot semantic segmentation aims to learn to segment new object classes with only a few annotated examples, which has a wide range of real-world applications. Most existing methods either focus on the restrictive setting of one-way few-shot segmentation or suffer from incomplete coverage of object regions. In this paper, we propose a novel few-shot semantic segmentation framework based on the prototype representation. Our key idea is to decompose the holistic class representation into a set of part-aware prototypes, capable of capturing diverse and fine-grained object features. In addition, we propose to leverage unlabeled data to enrich our part-aware prototypes, resulting in better modeling of intra-class variations of semantic objects. We develop a novel graph neural network model to generate and enhance the proposed part-aware prototypes based on labeled and unlabeled images. Extensive experimental evaluations on two benchmarks show that our method outperforms the prior art with a sizable margin.",Spaprnefofesese,42.0,41.0,12.0
7990,Semantic segmentation,13.0,exfuse: enhancing feature fusion for semantic segmentation,5.0,201.0,1.0,188.0,3.0,2.8,140.70000000000002,57,https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenli_Zhang_ExFuse_Enhancing_Feature_ECCV_2018_paper.pdf,"Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0\% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9\% mean IoU, which outperforms the previous state-of-the-art results.",Sexenfefufosese,241.0,43.0,16.0
7991,Semantic segmentation,187.0,learning texture invariant representation for domain adaptation of semantic segmentation,3.0,201.0,1.0,24.0,5.0,2.8,143.7,58,https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Learning_Texture_Invariant_Representation_for_Domain_Adaptation_of_Semantic_Segmentation_CVPR_2020_paper.pdf,"Since annotating pixel-level labels for semantic segmentation is laborious, leveraging synthetic data is an attractive solution. However, due to the domain gap between synthetic domain and real domain, it is challenging for a model trained with synthetic data to generalize to real data. In this paper, considering the fundamental difference between the two domains as the texture, we propose a method to adapt to the target domain's texture. First, we diversity the texture of synthetic images using a style transfer algorithm. The various textures of generated images prevent a segmentation model from overfitting to one specific (synthetic) texture. Then, we fine-tune the model with self-training to get direct supervision of the target texture. Our results achieve state-of-the-art performance and we analyze the properties of the model trained on the stylized dataset with extensive experiments.",Sleteinrefodoadofsese,70.0,33.0,4.0
7992,Semantic segmentation,190.0,unsupervised intra-domain adaptation for semantic segmentation through self-supervision,3.0,201.0,1.0,21.0,5.0,2.8,143.70000000000002,59,http://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Unsupervised_Intra-Domain_Adaptation_for_Semantic_Segmentation_Through_Self-Supervision_CVPR_2020_paper.pdf,"Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model, from this adaptation, we separate target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard subdomain. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.",Suninadfosesethse,97.0,40.0,19.0
7993,Semantic segmentation,194.0,polarnet: an improved grid representation for online lidar point clouds semantic segmentation,3.0,201.0,1.0,25.0,5.0,2.8,146.1,60,http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_PolarNet_An_Improved_Grid_Representation_for_Online_LiDAR_Point_Clouds_CVPR_2020_paper.pdf,"The requirement of fine-grained perception by autonomous driving systems has resulted in recently increased research in the online semantic segmentation of single-scan LiDAR. Emerging datasets and technological advancements have enabled researchers to benchmark this problem and improve the applicable semantic segmentation algorithms. Still, online semantic segmentation of LiDAR scans in autonomous driving applications remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware, (2) points are distributed unevenly across space, and (3) an increasing number of more fine-grained semantic classes. The combination of the aforementioned challenges motivates us to propose a new LiDAR-specific, KNN-free segmentation algorithm - PolarNet. Instead of using common spherical or bird's-eye-view projection, our polar bird's-eye-view representation balances the points per grid and thus indirectly redistributes the network's attention over the long-tailed points distribution over the radial axis in polar coordination. We find that our encoding scheme greatly increases the mIoU in three drastically different real urban LiDAR single-scan segmentation datasets while retaining ultra low latency and near real-time throughput.",Spoanimgrrefoonlipoclsese,63.0,44.0,13.0
7994,Semantic segmentation,26.0,semantic segmentation using adversarial networks,5.0,201.0,1.0,194.0,3.0,2.8,146.4,61,https://arxiv.org/pdf/1611.08408.pdf%5D,Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.,Sseseusadne,517.0,36.0,34.0
7995,Semantic segmentation,38.0,segcloud: semantic segmentation of 3d point clouds,5.0,201.0,1.0,187.0,3.0,2.8,147.9,62,https://arxiv.org/pdf/1710.07563,"3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks(NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-to-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-the-art on all datasets.",Sseseseof3dpocl,347.0,78.0,31.0
7996,Semantic segmentation,401.0,the surprising impact of mask-head architecture on novel class segmentation,1.0,1.0,5.0,201.0,1.0,2.6,181.0,63,http://arxiv.org/pdf/2104.00613v2,"Instance segmentation models today are very accurate when trained on large annotated datasets, but collecting mask annotations at scale is prohibitively expensive. We address the partially supervised instance segmentation problem in which one can train on (significantly cheaper) bounding boxes for all categories but use masks only for a subset of categories. In this work, we focus on a popular family of models which apply differentiable cropping to a feature map and predict a mask based on the resulting crop. Under this family, we study Mask R-CNN and discover that instead of its default strategy of training the mask-head with a combination of proposals and groundtruth boxes, training the mask-head with only groundtruth boxes dramatically improves its performance on novel classes. This training strategy also allows us to take advantage of alternative mask-head architectures, which we exploit by replacing the typical mask-head of 2-4 layers with significantly deeper off-the-shelf architectures (e.g. ResNet, Hourglass models). While many of these architectures perform similarly when trained in fully supervised mode, our main finding is that they can generalize to novel classes in dramatically different ways. We call this ability of mask-heads to generalize to unseen classes the strong mask generalization effect and show that without any specialty modules or losses, we can achieve state-of-the-art results in the partially supervised COCO instance segmentation benchmark. Finally, we demonstrate that our effect is general, holding across underlying detection methodologies (including anchor-based, anchor-free or no detector at all) and across different backbone networks. Code and pre-trained models are available at https://git.io/deepmac.",Sthsuimofmaaronnoclse,1.0,58.0,0.0
7997,Semantic segmentation,401.0,searching for mobilenetv3,1.0,2.0,5.0,201.0,1.0,2.6,181.4,64,http://arxiv.org/pdf/1905.02244v5,"We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",Ssefomo,1040.0,61.0,208.0
7998,Semantic segmentation,401.0,pyramid scene parsing network,1.0,6.0,5.0,201.0,1.0,2.6,183.0,65,http://arxiv.org/pdf/1612.01105v2,"Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",Spyscpane,4603.0,47.0,771.0
7999,Semantic segmentation,401.0,deep residual learning for image recognition,1.0,8.0,5.0,201.0,1.0,2.6,183.8,66,http://arxiv.org/pdf/1612.05400v1,"Hashing aims at generating highly compact similarity preserving code words which are well suited for large-scale image retrieval tasks.   Most existing hashing methods first encode the images as a vector of hand-crafted features followed by a separate binarization step to generate hash codes. This two-stage process may produce sub-optimal encoding. In this paper, for the first time, we propose a deep architecture for supervised hashing through residual learning, termed Deep Residual Hashing (DRH), for an end-to-end simultaneous representation learning and hash coding. The DRH model constitutes four key elements: (1) a sub-network with multiple stacked residual blocks; (2) hashing layer for binarization; (3) supervised retrieval loss function based on neighbourhood component analysis for similarity preserving embedding; and (4) hashing related losses and regularisation to control the quantization error and improve the quality of hash coding. We present results of extensive experiments on a large public chest x-ray image database with co-morbidities and discuss the outcome showing substantial improvements over the latest state-of-the art methods.",Sderelefoimre,79331.0,61.0,15629.0
8000,Semantic segmentation,401.0,searching for efficient multi-scale architectures for dense image prediction,1.0,14.0,5.0,201.0,1.0,2.6,186.2,67,http://arxiv.org/pdf/1809.04184v1,"The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7\% on Cityscapes (street scene parsing), 71.3\% on PASCAL-Person-Part (person-part segmentation), and 87.9\% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.",Ssefoefmuarfodeimpr,235.0,100.0,21.0
8001,Semantic segmentation,401.0,mask r-cnn,1.0,15.0,5.0,201.0,1.0,2.6,186.6,68,http://arxiv.org/pdf/1703.06870v3,"We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron",Smar-,7336.0,52.0,1571.0
8002,Semantic segmentation,401.0,learning to segment every thing,1.0,18.0,5.0,201.0,1.0,2.6,187.8,69,http://arxiv.org/pdf/1711.10370v2,"Most methods for object instance segmentation require all training examples to be labeled with segmentation masks. This requirement makes it expensive to annotate new categories and has restricted instance segmentation models to ~100 well-annotated classes. The goal of this paper is to propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables training instance segmentation models on a large set of categories all of which have box annotations, but only a small fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment 3000 visual concepts using box annotations from the Visual Genome dataset and mask annotations from the 80 classes in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a first step towards instance segmentation models that have broad comprehension of the visual world.",Sletoseevth,207.0,42.0,18.0
8003,Semantic segmentation,401.0,pointly-supervised instance segmentation,1.0,19.0,5.0,201.0,1.0,2.6,188.2,70,http://arxiv.org/pdf/2010.11681v2,"Panoptic Segmentation aims to provide an understanding of background (stuff) and instances of objects (things) at a pixel level. It combines the separate tasks of semantic segmentation (pixel level classification) and instance segmentation to build a single unified scene understanding task. Typically, panoptic segmentation is derived by combining semantic and instance segmentation tasks that are learned separately or jointly (multi-task networks). In general, instance segmentation networks are built by adding a foreground mask estimation layer on top of object detectors or using instance clustering methods that assign a pixel to an instance center. In this work, we present a fully convolution neural network that learns instance segmentation from semantic segmentation and instance contours (boundaries of things). Instance contours along with semantic segmentation yield a boundary aware semantic segmentation of things. Connected component labeling on these results produces instance segmentation. We merge semantic and instance segmentation results to output panoptic segmentation. We evaluate our proposed method on the CityScapes dataset to demonstrate qualitative and quantitative performances along with several ablation studies. Our overview video can be accessed from url:https://youtu.be/wBtcxRhG3e0.",Spoinse,1.0,62.0,1.0
8004,Semantic segmentation,401.0,panoptic-deeplab,1.0,20.0,5.0,201.0,1.0,2.6,188.6,71,http://arxiv.org/pdf/1911.10194v3,"In this work, we introduce Panoptic-DeepLab, a simple, strong, and fast system for panoptic segmentation, aiming to establish a solid baseline for bottom-up methods that can achieve comparable performance of two-stage methods while yielding fast inference speed. In particular, Panoptic-DeepLab adopts the dual-ASPP and dual-decoder structures specific to semantic, and instance segmentation, respectively. The semantic segmentation branch is the same as the typical design of any semantic segmentation model (e.g., DeepLab), while the instance segmentation branch is class-agnostic, involving a simple instance center regression. As a result, our single Panoptic-DeepLab simultaneously ranks first at all three Cityscapes benchmarks, setting the new state-of-art of 84.2% mIoU, 39.0% AP, and 65.5% PQ on test set. Additionally, equipped with MobileNetV3, Panoptic-DeepLab runs nearly in real-time with a single 1025x2049 image (15.8 frames per second), while achieving a competitive performance on Cityscapes (54.1 PQ% on test set). On Mapillary Vistas test set, our ensemble of six models attains 42.7% PQ, outperforming the challenge winner in 2018 by a healthy margin of 1.5%. Finally, our Panoptic-DeepLab also performs on par with several top-down approaches on the challenging COCO dataset. For the first time, we demonstrate a bottom-up approach could deliver state-of-the-art results on panoptic segmentation.",Spa,106.0,91.0,22.0
8005,Semantic segmentation,401.0,panoptic feature pyramid networks,1.0,23.0,5.0,201.0,1.0,2.6,189.8,72,http://arxiv.org/pdf/1901.02446v2,"The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.",Spafepyne,305.0,68.0,79.0
8006,Semantic segmentation,401.0,path aggregation network for instance segmentation,1.0,24.0,5.0,201.0,1.0,2.6,190.2,73,http://arxiv.org/pdf/1803.01534v4,"The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet",Spaagnefoinse,1042.0,72.0,139.0
8007,Semantic segmentation,401.0,a novel region of interest extraction layer for instance segmentation,1.0,25.0,5.0,201.0,1.0,2.6,190.6,74,http://arxiv.org/pdf/2004.13665v2,"Given the wide diffusion of deep neural network architectures for computer vision tasks, several new applications are nowadays more and more feasible. Among them, a particular attention has been recently given to instance segmentation, by exploiting the results achievable by two-stage networks (such as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex architectures, a crucial role is played by the Region of Interest (RoI) extraction layer, devoted to extracting a coherent subset of features from a single Feature Pyramid Network (FPN) layer attached on top of a backbone.   This paper is motivated by the need to overcome the limitations of existing RoI extractors which select only one (the best) layer from FPN. Our intuition is that all the layers of FPN retain useful information. Therefore, the proposed layer (called Generic RoI Extractor - GRoIE) introduces non-local building blocks and attention mechanisms to boost the performance.   A comprehensive ablation study at component level is conducted to find the best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can be integrated seamlessly with every two-stage architecture for both object detection and instance segmentation tasks. Therefore, the improvements brought about by the use of GRoIE in different state-of-the-art architectures are also evaluated. The proposed layer leads up to gain a 1.1% AP improvement on bounding box detection and 1.7% AP improvement on instance segmentation.   The code is publicly available on GitHub repository at https://github.com/IMPLabUniPr/mmdetection/tree/groie_dev",Sanoreofinexlafoinse,9.0,28.0,0.0
8008,Semantic segmentation,401.0,seesaw loss for long-tailed instance segmentation,1.0,26.0,5.0,201.0,1.0,2.6,191.0,75,http://arxiv.org/pdf/2008.10032v4,"Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem, we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles. Code is available at https://github.com/open-mmlab/mmdetection.",Sselofoloinse,11.0,61.0,3.0
8009,Semantic segmentation,401.0,deep high-resolution representation learning for visual recognition,1.0,32.0,5.0,201.0,1.0,2.6,193.4,76,http://arxiv.org/pdf/1905.01932v1,"Understanding how cities visually differ from each others is interesting for planners, residents, and historians. We investigate the interpretation of deep features learned by convolutional neural networks (CNNs) for city recognition. Given a trained city recognition network, we first generate weighted masks using the known Grad-CAM technique and to select the most discriminate regions in the image. Since the image classification label is the city name, it contains no information of objects that are class-discriminate, we investigate the interpretability of deep representations with two methods. (i) Unsupervised method is used to cluster the objects appearing in the visual explanations. (ii) A pretrained semantic segmentation model is used to label objects in pixel level, and then we introduce statistical measures to quantitatively evaluate the interpretability of discriminate objects. The influence of network architectures and random initializations in training, is studied on the interpretability of CNN features for city recognition. The results suggest that network architectures would affect the interpretability of learned visual representations greater than different initializations.",Sdehirelefovire,486.0,206.0,101.0
8010,Semantic segmentation,401.0,mask scoring r-cnn,1.0,36.0,5.0,201.0,1.0,2.6,195.0,77,http://arxiv.org/pdf/1903.00241v1,"Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent and noticeable gain with different models, and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation. The source code of our method is available at \url{https://github.com/zjhuang22/maskscoring_rcnn}.",Smascr-,306.0,39.0,62.0
8011,Semantic segmentation,401.0,hybrid task cascade for instance segmentation,1.0,37.0,5.0,201.0,1.0,2.6,195.4,78,http://arxiv.org/pdf/1901.07518v2,"Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a single HTC obtains 38.4 and 1.5 improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in the COCO 2018 Challenge Object Detection Task. Code is available at: https://github.com/open-mmlab/mmdetection.",Shytacafoinse,393.0,48.0,80.0
8012,Semantic segmentation,401.0,"deformable convnets v2: more deformable, better results",1.0,38.0,5.0,201.0,1.0,2.6,195.8,79,http://arxiv.org/pdf/1811.11168v2,"The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation.",Sdecov2modebere,509.0,45.0,53.0
8013,Semantic segmentation,401.0,"how to train your vit? data, augmentation, and regularization in vision transformers",1.0,39.0,5.0,201.0,1.0,2.6,196.2,80,http://arxiv.org/pdf/2106.10270v1,"Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.",Shototryovidaauanreinvitr,13.0,43.0,1.0
8014,Semantic segmentation,44.0,efficient piecewise training of deep structured models for semantic segmentation,4.0,201.0,1.0,131.0,3.0,2.5,132.9,81,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf,"Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information, specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78:0 on the challenging PASCAL VOC 2012 dataset.",Sefpitrofdestmofosese,752.0,56.0,53.0
8015,Semantic segmentation,57.0,pyramid attention network for semantic segmentation,4.0,201.0,1.0,126.0,3.0,2.5,135.3,82,https://arxiv.org/pdf/1805.10180.pdf],"A Pyramid Attention Network(PAN) is proposed to exploit the impact of global contextual information in semantic segmentation. Different from most existing works, we combine attention mechanism and spatial pyramid to extract precise dense features for pixel labeling instead of complicated dilated convolution and artificially designed decoder networks. Specifically, we introduce a Feature Pyramid Attention module to perform spatial pyramid attention structure on high-level output and combining global pooling to learn a better feature representation, and a Global Attention Upsample module on each decoder layer to provide global context as a guidance of low-level features to select category localization details. The proposed approach achieves state-of-the-art performance on PASCAL VOC 2012 and Cityscapes benchmarks with a new record of mIoU accuracy 84.0% on PASCAL VOC 2012, while training without COCO dataset.",Spyatnefosese,283.0,41.0,33.0
8016,Semantic segmentation,135.0,weakly supervised semantic segmentation with boundary exploration,3.0,201.0,1.0,53.0,4.0,2.5,136.8,83,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123710341.pdf,"Weakly supervised semantic segmentation with image-level labels has attracted a lot of attention recently because these labels are already available in most datasets. To obtain semantic segmentation under weak supervision, this paper presents a simple yet effective approach based on the idea of explicitly exploring object boundaries from training images to keep coincidence of segmentation and boundaries. Specifically, we synthesize boundary annotations by exploiting coarse localization maps obtained from CNN classifier, and use annotations to train the proposed network called BENet which further excavates more object boundaries to provide constraints for segmentation. Finally generated pseudo annotations of training images are used to supervise an off-the-shelf segmentation network. We evaluate the proposed method on PASCAL VOC 2012 benchmark and the final results achieve 65.7% and 66.6% mIoU scores on val and test sets respectively, which outperforms previous methods trained under image-level supervision.",Swesusesewiboex,25.0,39.0,3.0
8017,Semantic segmentation,134.0,squeeze-and-attention networks for semantic segmentation,3.0,201.0,1.0,57.0,4.0,2.5,137.7,84,http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhong_Squeeze-and-Attention_Networks_for_Semantic_Segmentation_CVPR_2020_paper.pdf,"The recent integration of attention mechanisms into segmentation networks improves their representational capabilities through a great emphasis on more informative features. However, these attention mechanisms ignore an implicit sub-task of semantic segmentation and are constrained by the grid structure of convolution kernels. In this paper, we propose a novel squeeze-and-attention network (SANet) architecture that leverages an effective squeeze-and-attention (SA) module to account for two distinctive characteristics of segmentation: i) pixel-group attention, and ii) pixel-wise prediction. Specifically, the proposed SA modules impose pixel-group attention on conventional convolution by introducing an 'attention' convolutional channel, thus taking into account spatial-channel inter-dependencies in an efficient manner. The final segmentation results are produced by merging outputs from four hierarchical stages of a SANet to integrate multi-scale contexts for obtaining an enhanced pixel-wise prediction. Empirical experiments on two challenging public datasets validate the effectiveness of the proposed SANets, which achieves 83.2 % mIoU (without COCO pre-training) on PASCAL VOC and a state-of-the-art mIoU of 54.4 % on PASCAL Context.",Ssqnefosese,49.0,52.0,2.0
8018,Semantic segmentation,143.0,synthesize then compare: detecting failures and anomalies for semantic segmentation,3.0,201.0,1.0,54.0,4.0,2.5,139.5,85,https://arxiv.org/pdf/2003.08440,"The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, \emph{i.e.}, 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation.",Ssythcodefaananfosese,23.0,52.0,5.0
8019,Semantic segmentation,131.0,virtual multi-view fusion for 3d semantic segmentation,3.0,201.0,1.0,71.0,4.0,2.5,141.0,86,https://arxiv.org/pdf/2007.13138,"Semantic segmentation of 3D meshes is an important problem for 3D scene understanding. In this paper we revisit the classic multiview representation of 3D meshes and study several techniques that make them effective for 3D semantic segmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our method effectively chooses different virtual views of the 3D mesh and renders multiple 2D channels for training an effective 2D semantic segmentation model. Features from multiple per view predictions are finally fused on 3D mesh vertices to predict mesh semantic segmentation labels. Using the large scale indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual views enable more effective training of 2D semantic segmentation networks than previous multiview approaches. When the 2D per pixel predictions are aggregated on 3D surfaces, our virtual multiview fusion method is able to achieve significantly better 3D semantic segmentation results compared to all prior multiview approaches and competitive with recent 3D convolution approaches.",Svimufufo3dsese,25.0,53.0,1.0
8020,Semantic segmentation,161.0,learning integral objects with intra-class discriminator for weakly-supervised semantic segmentation,3.0,201.0,1.0,41.0,4.0,2.5,141.0,87,http://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Learning_Integral_Objects_With_Intra-Class_Discriminator_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2020_paper.pdf,"Image-level weakly-supervised semantic segmentation (WSSS) aims at learning semantic segmentation by adopting only image class labels. Existing approaches generally rely on class activation maps (CAM) to generate pseudo-masks and then train segmentation models. The main difficulty is that the CAM estimate only covers partial foreground objects. In this paper, we argue that the critical factor preventing to obtain the full object mask is the classification boundary mismatch problem in applying the CAM to WSSS. Because the CAM is optimized by the classification task, it focuses on the discrimination across different image-level classes. However, the WSSS requires to distinguish pixels sharing the same image-level class to separate them into the foreground and the background. To alleviate this contradiction, we propose an efficient end-to-end Intra-Class Discriminator (ICD) framework, which learns intra-class boundaries to help separate the foreground and the background within each image-level class. Without bells and whistles, our approach achieves the state-of-the-art performance of image label based WSSS, with mIoU 68.0% on the VOC 2012 semantic segmentation benchmark, demonstrating the effectiveness of the proposed approach.",Sleinobwiindifowesese,33.0,42.0,4.0
8021,Semantic segmentation,124.0,espnet: efficient spatial pyramid of dilated convolutions for semantic segmentation,3.0,201.0,1.0,80.0,4.0,2.5,141.6,88,http://openaccess.thecvf.com/content_ECCV_2018/papers/Sachin_Mehta_ESPNet_Efficient_Spatial_ECCV_2018_paper.pdf,"We introduce a fast and efficient convolutional neural network, ESPNet, for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP), which is efficient in terms of computation, memory, and power. ESPNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet, while its category-wise accuracy is only 8% less. We evaluated EPSNet on a variety of semantic segmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively.",Sesefsppyofdicofosese,332.0,65.0,59.0
8022,Semantic segmentation,65.0,acfnet: attentional class feature network for semantic segmentation,4.0,201.0,1.0,147.0,3.0,2.5,144.0,89,http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_ACFNet_Attentional_Class_Feature_Network_for_Semantic_Segmentation_ICCV_2019_paper.pdf,"Recent works have made great progress in semantic segmentation by exploiting richer context, most of which are designed from a spatial perspective. In contrast to previous works, we present the concept of class center which extracts the global context from a categorical perspective. This class-level context describes the overall representation of each class in an image. We further propose a novel module, named Attentional Class Feature (ACF) module, to calculate and adaptively combine different class centers according to each pixel. Based on the ACF module, we introduce a coarse-to-fine segmentation network, called Attentional Class Feature Network (ACFNet), which can be composed of an ACF module and any off-the-shell segmentation network (base network). In this paper, we use two types of base networks to evaluate the effectiveness of ACFNet. We achieve new state-of-the-art performance of 81.85% mIoU on Cityscapes dataset with only finely annotated data used for training.",Sacatclfenefosese,75.0,52.0,2.0
8023,Semantic segmentation,138.0,unsupervised domain adaptation for semantic segmentation via class-balanced self-training,3.0,201.0,1.0,76.0,4.0,2.5,144.60000000000002,90,http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf,"Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world “wild tasks” where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as “domain gap”, and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training (ST) procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of ST, we also propose a novel class-balanced self-training (CBST) framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.",Sundoadfoseseviclse,456.0,54.0,60.0
8024,Semantic segmentation,163.0,linking points with labels in 3d: a review of point cloud semantic segmentation,3.0,201.0,1.0,51.0,4.0,2.5,144.60000000000002,91,https://arxiv.org/pdf/1908.08854,"Ripe with possibilities offered by deep-learning techniques and useful in applications related to remote sensing, computer vision, and robotics, 3D point cloud semantic segmentation (PCSS) and point cloud segmentation (PCS) are attracting increasing interest. This article summarizes available data sets and relevant studies on recent developments in PCSS and PCS.",Slipowilain3dareofpoclsese,66.0,212.0,2.0
8025,Semantic segmentation,157.0,contextual-relation consistent domain adaptation for semantic segmentation,3.0,201.0,1.0,62.0,4.0,2.5,146.1,92,https://arxiv.org/pdf/2007.02424,"Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 to Cityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods.",Scocodoadfosese,38.0,71.0,2.0
8026,Semantic segmentation,43.0,stc: a simple to complex framework for weakly-supervised semantic segmentation,4.0,201.0,1.0,179.0,3.0,2.5,147.0,93,https://arxiv.org/pdf/1509.03150,"Recently, significant improvement has been made on semantic object segmentation due to the development of deep convolutional neural networks (DCNNs). Training such a DCNN usually relies on a large number of images with pixel-level segmentation masks, and annotating these images is very costly in terms of both finance and human effort. In this paper, we propose a simple to complex (STC) framework in which only image-level annotations are utilized to learn DCNNs for semantic segmentation. Specifically, we first train an initial segmentation network called Initial-DCNN with the saliency maps of simple images (i.e., those with a single category of major object(s) and clean background). These saliency maps can be automatically obtained by existing bottom-up salient object detection techniques, where no supervision information is needed. Then, a better network called Enhanced-DCNN is learned with supervision from the predicted segmentation masks of simple images based on the Initial-DCNN as well as the image-level annotations. Finally, more pixel-level segmentation masks of complex images (two or more categories of objects with cluttered background), which are inferred by using Enhanced-DCNN and image-level annotations, are utilized as the supervision information to learn the Powerful-DCNN for semantic segmentation. Our method utilizes 40K simple images from Flickr.com and 10K complex images from PASCAL VOC for step-wisely boosting the segmentation network. Extensive experimental results on PASCAL VOC 2012 segmentation benchmark well demonstrate the superiority of the proposed STC framework compared with other state-of-the-arts.",Sstasitocofrfowesese,323.0,56.0,30.0
8027,Semantic segmentation,80.0,one-shot learning for semantic segmentation,4.0,201.0,1.0,145.0,3.0,2.5,147.9,94,https://arxiv.org/pdf/1709.03410,"Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3 times faster.",Sonlefosese,203.0,42.0,71.0
8028,Semantic segmentation,50.0,towards bridging semantic gap to improve semantic segmentation,4.0,201.0,1.0,176.0,3.0,2.5,148.2,95,http://openaccess.thecvf.com/content_ICCV_2019/papers/Pang_Towards_Bridging_Semantic_Gap_to_Improve_Semantic_Segmentation_ICCV_2019_paper.pdf,"Aggregating multi-level features is essential for capturing multi-scale context information for precise scene semantic segmentation. However, the improvement by directly fusing shallow features and deep features becomes limited as the semantic gap between them increases. To solve this problem, we explore two strategies for robust feature fusion. One is enhancing shallow features using a semantic enhancement module (SeEM) to alleviate the semantic gap between shallow features and deep features. The other strategy is feature attention, which involves discovering complementary information (i.e., boundary information) from low-level features to enhance high-level features for precise segmentation. By embedding these two strategies, we construct a parallel feature pyramid towards improving multi-level feature fusion. A Semantic Enhanced Network called SeENet is constructed with the parallel pyramid to implement precise segmentation. Experiments on three benchmark datasets demonstrate the effectiveness of our method for robust multi-level feature aggregation. As a result, our SeENet has achieved better performance than other state-of-the-art methods for semantic segmentation.",Stobrsegatoimsese,39.0,67.0,0.0
8029,Semantic segmentation,188.0,prototype mixture models for few-shot semantic segmentation,3.0,201.0,1.0,42.0,4.0,2.5,149.4,96,https://arxiv.org/pdf/2008.03898,"Few-shot segmentation is challenging because objects within the support and query images could significantly differ in appearance and pose. Using a single prototype acquired directly from the support image to segment the query image causes semantic ambiguity. In this paper, we propose prototype mixture models (PMMs), which correlate diverse image regions with multiple prototypes to enforce the prototype-based semantic representation. Estimated by an Expectation-Maximization algorithm, PMMs incorporate rich channel-wised and spatial semantics from limited support images. Utilized as representations as well as classifiers, PMMs fully leverage the semantics to activate objects in the query image while depressing background regions in a duplex manner. Extensive experiments on Pascal VOC and MS-COCO datasets show that PMMs significantly improve upon state-of-the-arts. Particularly, PMMs improve 5-shot segmentation performance on MS-COCO by up to 5.82\% with only a moderate cost for model size and inference speed.",Sprmimofofesese,50.0,40.0,11.0
8030,Semantic segmentation,90.0,incremental learning techniques for semantic segmentation,4.0,201.0,1.0,141.0,3.0,2.5,149.7,97,http://openaccess.thecvf.com/content_ICCVW_2019/papers/TASK-CV/Michieli_Incremental_Learning_Techniques_for_Semantic_Segmentation_ICCVW_2019_paper.pdf,"Deep learning architectures exhibit a critical drop of performance due to catastrophic forgetting when they are required to incrementally learn new tasks. Contemporary incremental learning frameworks focus on image classification and object detection while in this work we formally introduce the incremental learning problem for semantic segmentation in which a pixel-wise labeling is considered. To tackle this task we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We propose various approaches working both on the output logits and on intermediate features. In opposition to some recent frameworks, we do not store any image from previously learned classes and only the last model is needed to preserve high accuracy on these classes. The experimental evaluation on the Pascal VOC2012 dataset shows the effectiveness of the proposed approaches.",Sinletefosese,50.0,47.0,16.0
8031,Semantic segmentation,175.0,unsupervised domain adaptation in semantic segmentation: a review,3.0,201.0,1.0,60.0,4.0,2.5,150.9,98,https://www.mdpi.com/2227-7080/8/2/35/pdf,"The aim of this paper is to give an overview of the recent advancements in the Unsupervised Domain Adaptation (UDA) of deep networks for semantic segmentation. This task is attracting a wide interest since semantic segmentation models require a huge amount of labeled data and the lack of data fitting specific requirements is the main limitation in the deployment of these techniques. This field has been recently explored and has rapidly grown with a large number of ad-hoc approaches. This motivates us to build a comprehensive overview of the proposed methodologies and to provide a clear categorization. In this paper, we start by introducing the problem, its formulation and the various scenarios that can be considered. Then, we introduce the different levels at which adaptation strategies may be applied: namely, at the input (image) level, at the internal features representation and at the output level. Furthermore, we present a detailed overview of the literature in the field, dividing previous methods based on the following (non mutually exclusive) categories: adversarial learning, generative-based, analysis of the classifier discrepancies, self-teaching, entropy minimization, curriculum learning and multi-task learning. Novel research directions are also briefly introduced to give a hint of interesting open problems in the field. Finally, a comparison of the performance of the various methods in the widely used autonomous driving scenario is presented.",Sundoadinseseare,38.0,126.0,2.0
8032,Semantic segmentation,46.0,the role of context for object detection and semantic segmentation in the wild,4.0,201.0,1.0,192.0,3.0,2.5,151.8,99,https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Mottaghi_The_Role_of_2014_CVPR_paper.pdf,"In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.",Sthroofcofoobdeanseseinthwi,801.0,47.0,139.0
8033,Semantic segmentation,53.0,fully convolutional adaptation networks for semantic segmentation,4.0,201.0,1.0,193.0,3.0,2.5,154.20000000000002,100,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Fully_Convolutional_Adaptation_CVPR_2018_paper.pdf,"The recent advances in deep neural networks have convincingly demonstrated high capability in learning vision models on large datasets. Nevertheless, collecting expert labeled datasets especially with pixel-level annotations is an extremely expensive process. An appealing alternative is to render synthetic data (e.g., computer games) and generate ground truth automatically. However, simply applying the models learnt on synthetic images may lead to high generalization error on real images due to domain shift. In this paper, we facilitate this issue from the perspectives of both visual appearance-level and representation-level domain adaptation. The former adapts source-domain images to appear as if drawn from the ""style"" in the target domain and the latter attempts to learn domain-invariant representations. Specifically, we present Fully Convolutional Adaptation Networks (FCAN), a novel deep architecture for semantic segmentation which combines Appearance Adaptation Networks (AAN) and Representation Adaptation Networks (RAN). AAN learns a transformation from one domain to the other in the pixel space and RAN is optimized in an adversarial learning manner to maximally fool the domain discriminator with the learnt source and target representations. Extensive experiments are conducted on the transfer from GTA5 (game videos) to Cityscapes (urban street scenes) on semantic segmentation and our proposal achieves superior results when comparing to state-of-the-art unsupervised adaptation techniques. More remarkably, we obtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an unsupervised setting.",Sfucoadnefosese,211.0,38.0,19.0
8291,Style Transfer,3.0,perceptual losses for real-time style transfer and super-resolution,5.0,6.0,5.0,1.0,5.0,5.0,3.6,1,http://arxiv.org/pdf/1603.08155v1,"We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",Speloforesttransu,5173.0,73.0,453.0
8292,Style Transfer,6.0,arbitrary style transfer in real-time with adaptive instance normalization,5.0,21.0,5.0,3.0,5.0,5.0,11.1,2,http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf,"Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",Sarsttrinrewiadinno,1513.0,64.0,274.0
8293,Style Transfer,11.0,photorealistic style transfer via wavelet transforms,5.0,28.0,5.0,18.0,5.0,5.0,19.9,3,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.pdf,"Recent style transfer models have provided promising artistic results. However, given a photograph as a reference style, existing methods are limited by spatial distortions or unrealistic artifacts, which should not happen in real photographs. We introduce a theoretically sound correction to the network architecture that remarkably enhances photorealism and faithfully transfers the style. The key ingredient of our method is wavelet transforms that naturally fits in deep networks. We propose a wavelet corrected transfer based on whitening and coloring transforms (WCT2) that allows features to preserve their structural information and statistical properties of VGG feature space during stylization. This is the first and the only end-to-end model that can stylize a 1024x1024 resolution image in 4.7 seconds, giving a pleasing and photorealistic quality without any post-processing. Last but not least, our model provides a stable video stylization without temporal constraints. Our code, generated images, pre-trained models and supplementary documents are all available at https://github.com/ClovaAI/WCT2.",Sphsttrviwatr,102.0,41.0,30.0
8294,Style Transfer,9.0,style transfer from non-parallel text by cross-alignment,5.0,37.0,5.0,16.0,5.0,5.0,22.3,4,https://arxiv.org/pdf/1705.09655.pdf?source=post_page---------------------------,"This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broad family of problems including machine translation, decipherment, and sentiment modification. The key challenge is to separate the content from other aspects such as style. We assume a shared latent content distribution across different text corpora, and propose a method that leverages refined alignment of latent representations to perform style transfer. The transferred sentences from one style should match example sentences from the other style as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.",Ssttrfrnotebycr,484.0,32.0,138.0
8295,Style Transfer,54.0,deep learning for text style transfer: a survey,4.0,18.0,5.0,36.0,5.0,4.7,34.2,5,https://arxiv.org/pdf/2011.00416,"Text style transfer (TST) is an important task in natural language generation (NLG), which aims to control certain attributes in the generated text, such as politeness, emotion, humor, and many others. It has a long history in the field of natural language processing (NLP), and recently has re-gained significant attention thanks to the promising performance brought by deep neural models. In this paper, we present a systematic survey of the research on neural text style transfer, spanning over 100 representative articles since the first neural text style transfer work in 2017. We discuss the task formulation, existing datasets and subtasks, evaluation, as well as the rich methodologies in the presence of parallel and non-parallel data. We also provide discussions on a variety of important topics regarding the future development of TST. Our curated paper list is at this https URL",Sdelefotesttrasu,12.0,218.0,2.0
8296,Style Transfer,30.0,reformulating unsupervised style transfer as paraphrase generation,5.0,91.0,4.0,6.0,5.0,4.6,47.2,6,https://arxiv.org/pdf/2010.05700,"Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs. However, many existing systems purportedly designed for style transfer inherently warp the input's meaning through attribute transfer, which changes semantic properties such as sentiment. In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data. Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations. We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants. Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.",Sreunsttraspage,38.0,111.0,7.0
8297,Style Transfer,12.0,a style-aware content loss for real-time hd style transfer,5.0,30.0,5.0,128.0,3.0,4.4,54.0,7,http://openaccess.thecvf.com/content_ECCV_2018/papers/Artsiom_Sanakoyeu_A_Style-aware_Content_ECCV_2018_paper.pdf,"Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.",Sastcoloforehdsttr,85.0,55.0,9.0
8298,Style Transfer,28.0,style transfer by relaxed optimal transport and self-similarity,5.0,53.0,4.0,47.0,4.0,4.3,43.7,8,https://openaccess.thecvf.com/content_CVPR_2019/papers/Kolkin_Style_Transfer_by_Relaxed_Optimal_Transport_and_Self-Similarity_CVPR_2019_paper.pdf,"The goal of style transfer algorithms is to render the content of one image using the style of another. We propose Style Transfer by Relaxed Optimal Transport and Self-Similarity (STROTSS), a new optimization-based style transfer algorithm. We extend our method to allow user specified point-to-point or region-to-region control over visual similarity between the style image and the output. Such guidance can be used to either achieve a particular visual effect or correct errors made by unconstrained style transfer. In order to quantitatively compare our method to prior work, we conduct a large-scale user study designed to assess the style-content tradeoff across settings in style transfer algorithms. Our results indicate that for any desired level of content preservation, our method provides higher quality stylization than prior work.",Ssttrbyreoptranse,59.0,29.0,18.0
8299,Style Transfer,31.0,style transformer: unpaired text style transfer without disentangled latent representation,5.0,88.0,4.0,44.0,4.0,4.3,57.7,9,https://arxiv.org/pdf/1905.05621,"Disentangling the content and style in the latent space is prevalent in unpaired text style transfer. However, two major issues exist in most of the current neural models. 1) It is difficult to completely strip the style information from the semantics for a sentence. 2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content. In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.",Ssttruntesttrwidilare,73.0,41.0,16.0
8300,Style Transfer,58.0,style transfer for anime sketches with enhanced residual u-net and auxiliary classifier gan,4.0,5.0,5.0,159.0,3.0,4.1,67.1,10,https://arxiv.org/pdf/1706.03319,"Recently, with the revolutionary neural style transferring methods, creditable paintings can be synthesized automatically from content images and style images. However, when it comes to the task of applying a painting's style to an anime sketch, these methods will just randomly colorize sketch lines as outputs and fail in the main task: specific style transfer. In this paper, we integrated residual U-net to apply the style to the gray-scale sketch with auxiliary classifier generative adversarial network (AC-GAN). The whole process is automatic and fast. Generated results are creditable in the quality of art style as well as colorization.",Ssttrfoanskwienreu-anauclga,91.0,23.0,6.0
8301,Style Transfer,43.0,controllable artistic text style transfer via shape-matching gan,4.0,43.0,4.0,60.0,4.0,4.0,48.1,11,http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Controllable_Artistic_Text_Style_Transfer_via_Shape-Matching_GAN_ICCV_2019_paper.pdf,"Artistic text style transfer is the task of migrating the style from a source image to the target text to create artistic typography. Recent style transfer methods have considered texture control to enhance usability. However, controlling the stylistic degree in terms of shape deformation remains an important open challenge. In this paper, we present the first text style transfer network that allows for real-time control of the crucial stylistic degree of the glyph through an adjustable parameter. Our key contribution is a novel bidirectional shape matching framework to establish an effective glyph-style mapping at various deformation levels without paired ground truth. Based on this idea, we propose a scale-controllable module to empower a single network to continuously characterize the multi-scale shape features of the style image and transfer these features to the target text. The proposed method demonstrates its superiority over previous state-of-the-arts in generating diverse, controllable and high-quality stylized text.",Scoartesttrvishga,35.0,35.0,3.0
8302,Style Transfer,57.0,domain-specific mappings for generative adversarial style transfer,4.0,92.0,4.0,59.0,4.0,4.0,71.60000000000001,12,https://arxiv.org/pdf/2008.02198,"Style transfer generates an image whose content comes from one image and style from the other. Image-to-image translation approaches with disentangled representations have been shown effective for style transfer between two image categories. However, previous methods often assume a shared domain-invariant content space, which could compromise the content representation power. For addressing this issue, this paper leverages domain-specific mappings for remapping latent features in the shared content space to domain-specific content spaces. This way, images can be encoded more properly for style transfer. Experiments show that the proposed method outperforms previous style transfer methods, particularly on challenging scenarios that would require semantic correspondences between images. Code and results are available at this https URL.",Sdomafogeadsttr,8.0,36.0,2.0
8303,Style Transfer,38.0,stroke controllable fast style transfer with adaptive receptive fields,5.0,71.0,4.0,140.0,3.0,4.0,81.80000000000001,13,https://openaccess.thecvf.com/content_ECCV_2018/papers/Yongcheng_Jing_Stroke_Controllable_Fast_ECCV_2018_paper.pdf,"Recently, in the community of Neural Style Transfer, several algorithms are proposed to transfer an artistic style in real-time, which is known as Fast Style Transfer. However, controlling the stroke size in stylized results still remains an open challenge. To achieve controllable stroke sizes, several attempts were made including training multiple models and resizing the input image in a variety of scales, respectively. However, their results are not promising regarding the efficiency and quality. In this paper, we present a stroke controllable style transfer network that incorporates different stroke sizes into one single model. Firstly, by analyzing the factors that influence the stroke size, we adopt the idea that both the receptive field and the style image scale should be taken into consideration for most cases. Then we propose a StrokePyramid module to endow the network with adaptive receptive fields, and two training strategies to achieve faster convergence and augment new stroke sizes upon a trained model respectively. Finally, by combining the proposed runtime control techniques, our network can produce distinct stroke sizes in different output images or different spatial regions within the same output image. The experimental results demonstrate that with almost the same number of parameters as the previous Fast Style Transfer algorithm, our network can transfer an artistic style in a stroke controllable manner.",Sstcofasttrwiadrefi,54.0,50.0,7.0
8304,Style Transfer,33.0,meta networks for neural style transfer,5.0,86.0,4.0,200.0,3.0,4.0,104.3,14,https://arxiv.org/pdf/1709.04111,"In this paper we propose a new method to get the specified network parameters through one time feed-forward propagation of the meta networks and explore the application to neural style transfer. Recent works on style transfer typically need to train image transformation networks for every new style, and the style is encoded in the network parameters by enormous iterations of stochastic gradient descent. To tackle these issues, we build a meta network which takes in the style image and produces a corresponding image transformations network directly. Compared with optimization-based methods for every style, our meta networks can handle an arbitrary new style within $19ms$ seconds on one modern GPU card. The fast image transformation network generated by our meta network is only 449KB, which is capable of real-time executing on a mobile device. We also investigate the manifold of the style transfer networks by operating the hidden features from meta networks. Experiments have well validated the effectiveness of our method. Code and trained models has been released this https URL",Smenefonesttr,16.0,46.0,1.0
8305,Style Transfer,17.0,arbitrary style transfer with style-attentional networks,5.0,105.0,3.0,50.0,4.0,3.9,62.1,15,http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_Arbitrary_Style_Transfer_With_Style-Attentional_Networks_CVPR_2019_paper.pdf,"Arbitrary style transfer aims to synthesize a content image with the style of an image to create a third image that has never been seen before. Recent arbitrary style transfer algorithms find it challenging to balance the content structure and the style patterns. Moreover, simultaneously maintaining the global and local style patterns is difficult due to the patch-based mechanism. In this paper, we introduce a novel style-attentional network (SANet) that efficiently and flexibly integrates the local style patterns according to the semantic spatial distribution of the content image. A new identity loss function and multi-level feature embeddings enable our SANet and decoder to preserve the content structure as much as possible while enriching the style patterns. Experimental results demonstrate that our algorithm synthesizes stylized images in real-time that are higher in quality than those produced by the state-of-the-art algorithms.",Sarsttrwistne,49.0,34.0,11.0
8306,Style Transfer,15.0,attention-aware multi-stroke style transfer,5.0,113.0,3.0,51.0,4.0,3.9,65.0,16,https://openaccess.thecvf.com/content_CVPR_2019/papers/Yao_Attention-Aware_Multi-Stroke_Style_Transfer_CVPR_2019_paper.pdf,"Neural style transfer has drawn considerable attention from both academic and industrial field. Although visual effect and efficiency have been significantly improved, existing methods are unable to coordinate spatial distribution of visual attention between the content image and stylized image, or render diverse level of detail via different brush strokes. In this paper, we tackle these limitations by developing an attention-aware multi-stroke style transfer model. We first propose to assemble self-attention mechanism into a style-agnostic reconstruction autoencoder framework, from which the attention map of a content image can be derived. By performing multi-scale style swap on content features and style features, we produce multiple feature maps reflecting different stroke patterns. A flexible fusion strategy is further presented to incorporate the salient characteristics from the attention map, which allows integrating multiple stroke patterns into different spatial regions of the output image harmoniously. We demonstrate the effectiveness of our method, as well as generate comparable stylized images with multiple stroke patterns against the state-of-the-art methods.",Satmusttr,47.0,40.0,11.0
8307,Style Transfer,4.0,evaluating style transfer for text,5.0,157.0,3.0,54.0,4.0,3.9,80.2,17,https://arxiv.org/pdf/1904.02295,"Research in the area of style transfer for text is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment: direction-corrected Earth Mover’s Distance, Word Mover’s Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined models exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.",Sevsttrfote,42.0,27.0,11.0
8308,Style Transfer,29.0,a hierarchical reinforced sequence operation method for unsupervised text style transfer,5.0,139.0,3.0,61.0,4.0,3.9,82.6,18,https://arxiv.org/pdf/1906.01833,"Unsupervised text style transfer aims to alter text styles while preserving the content, without aligned data for supervision. Existing seq2seq methods face three challenges: 1) the transfer is weakly interpretable, 2) generated outputs struggle in content preservation, and 3) the trade-off between content and style is intractable. To address these challenges, we propose a hierarchical reinforced sequence operation method, named Point-Then-Operate (PTO), which consists of a high-level agent that proposes operation positions and a low-level agent that alters the sentence. We provide comprehensive training objectives to control the fluency, style, and content of the outputs and a mask-based inference algorithm that allows for multi-step revision based on the single-step trained agents. Experimental results on two text style transfer datasets show that our method significantly outperforms recent methods and effectively addresses the aforementioned challenges.",Sahireseopmefountesttr,31.0,28.0,6.0
8309,Style Transfer,39.0,etnet: error transition network for arbitrary style transfer,5.0,109.0,3.0,98.0,4.0,3.9,84.69999999999999,19,https://arxiv.org/pdf/1910.12056,"Numerous valuable efforts have been devoted to achieving arbitrary style transfer since the seminal work of Gatys et al. However, existing state-of-the-art approaches often generate insufficiently stylized results under challenging cases. We believe a fundamental reason is that these approaches try to generate the stylized result in a single shot and hence fail to fully satisfy the constraints on semantic structures in the content images and style patterns in the style images. Inspired by the works on error-correction, instead, we propose a self-correcting model to predict what is wrong with the current stylization and refine it accordingly in an iterative manner. For each refinement, we transit the error features across both the spatial and scale domain and invert the processed features into a residual image, with a network we call Error Transition Network (ETNet). The proposed model improves over the state-of-the-art methods with better semantic structures and more adaptive style pattern details. Various qualitative and quantitative experiments show that the key concept of both progressive strategy and error-correction leads to better results. Code and models are available at https://github.com/zhijieW94/ETNet.",Setertrnefoarsttr,10.0,39.0,1.0
8310,Style Transfer,70.0,two-stage peer-regularized feature recombination for arbitrary image style transfer,4.0,160.0,3.0,8.0,5.0,3.9,87.4,20,https://openaccess.thecvf.com/content_CVPR_2020/papers/Svoboda_Two-Stage_Peer-Regularized_Feature_Recombination_for_Arbitrary_Image_Style_Transfer_CVPR_2020_paper.pdf,"This paper introduces a neural style transfer model to generate a stylized image conditioning on a set of examples describing the desired style. The proposed solution produces high-quality images even in the zero-shot setting and allows for more freedom in changes to the content geometry. This is made possible by introducing a novel Two-Stage Peer-Regularization Layer that recombines style and content in latent space by means of a custom graph convolutional layer. Contrary to the vast majority of existing solutions, our model does not depend on any pre-trained networks for computing perceptual losses and can be trained fully end-to-end thanks to a new set of cyclic losses that operate directly in latent space and not on the RGB images. An extensive ablation study confirms the usefulness of the proposed losses and of the Two-Stage Peer-Regularization Layer, with qualitative results that are competitive with respect to the current state of the art using a single model for all presented styles. This opens the door to more abstract and artistic neural image generation scenarios, along with simpler deployment of the model.",Stwpeferefoarimsttr,16.0,51.0,4.0
8311,Style Transfer,401.0,neural style transfer: a review,1.0,16.0,5.0,4.0,5.0,3.8,127.9,21,http://arxiv.org/pdf/cond-mat/9705270v1,"The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: https://osf.io/f8tu4/.",Snesttrare,283.0,186.0,13.0
8312,Style Transfer,401.0,deep photo style transfer,1.0,8.0,5.0,29.0,5.0,3.8,132.2,22,http://arxiv.org/pdf/1703.07511v3,"This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.",Sdephsttr,417.0,20.0,56.0
8313,Style Transfer,401.0,unpaired motion style transfer from video to animation,1.0,25.0,5.0,12.0,5.0,3.8,133.9,23,http://arxiv.org/abs/2005.05751v1,"Transferring the motion style from one animation clip to another, while preserving the motion content of the latter, has been a long-standing problem in character animation. Most existing data-driven approaches are supervised and rely on paired data, where motions with the same content are performed in different styles. In addition, these approaches are limited to transfer of styles that were seen during training. In this paper, we present a novel data-driven framework for motion style transfer, which learns from an unpaired collection of motions with style labels, and enables transferring motion styles not observed during training. Furthermore, our framework is able to extract motion styles directly from videos, bypassing 3D reconstruction, and apply them to the 3D input motion. Our style transfer network encodes motions into two latent codes, for content and for style, each of which plays a different role in the decoding (synthesis) process. While the content code is decoded into the output motion by several temporal convolutional layers, the style code modifies deep features via temporally invariant adaptive instance normalization (AdaIN). Moreover, while the content code is encoded from 3D joint rotations, we learn a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos. Our results are comparable to the state-of-the-art, despite not requiring paired training data, and outperform other methods when transferring previously unseen styles. To our knowledge, we are the first to demonstrate style transfer directly from videos to 3D animations - an ability which enables one to extend the set of style examples far beyond motions captured by MoCap systems.",Sunmosttrfrvitoan,21.0,60.0,5.0
8314,Style Transfer,401.0,exploring contextual word-level style relevance for unsupervised style transfer,1.0,23.0,5.0,30.0,5.0,3.8,138.5,24,http://arxiv.org/pdf/2005.02049v1,"Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation. In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously. In this way, this model is endowed with the ability to automatically predict the style relevance of each output word. Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer. Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms. Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.",Sexcowostrefounsttr,13.0,27.0,2.0
8315,Style Transfer,401.0,universal style transfer via feature transforms,1.0,35.0,5.0,27.0,5.0,3.8,142.4,25,http://arxiv.org/pdf/1705.08086v2,"Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.",Sunsttrvifetr,403.0,34.0,106.0
8316,Style Transfer,82.0,groove2groove: one-shot music style transfer with supervision from synthetic data,4.0,111.0,3.0,67.0,4.0,3.6000000000000005,89.1,26,https://hal.archives-ouvertes.fr/hal-02923548/document,"Style transfer is the process of changing the style of an image, video, audio clip or musical piece so as to match the style of a given example. Even though the task has interesting practical applications within the music industry, it has so far received little attention from the audio and music processing community. In this article, we present Groove2Groove, a one-shot style transfer method for symbolic music, focusing on the case of accompaniment styles in popular music and jazz. We propose an encoder-decoder neural network for the task, along with a synthetic data generation scheme to supply it with parallel training examples. This synthetic parallel data allows us to tackle the style transfer problem using end-to-end supervised learning, employing powerful techniques used in natural language processing. We experimentally demonstrate the performance of the model on style transfer using existing and newly proposed metrics, and also explore the possibility of style interpolation.",Sgronmusttrwisufrsyda,7.0,47.0,0.0
8317,Style Transfer,56.0,wavelet domain style transfer for an effective perception-distortion tradeoff in single image super-resolution,4.0,151.0,3.0,83.0,4.0,3.6000000000000005,102.1,27,https://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Wavelet_Domain_Style_Transfer_for_an_Effective_Perception-Distortion_Tradeoff_in_ICCV_2019_paper.pdf,"In single image super-resolution (SISR), given a low-resolution (LR) image, one wishes to ﬁnd a high-resolution (HR) version of it which is both accurate and photorealistic. Recently, it has been shown that there exists a fundamental tradeoff between low distortion and high perceptual quality, and the generative adversarial network (GAN) is demonstrated to approach the perception-distortion (PD) bound effectively. In this paper, we propose a novel method based on wavelet domain style transfer (WDST), which achieves a better PD tradeoff than the GAN based methods. Speciﬁcally, we propose to use 2D stationary wavelet transform (SWT) to decompose one image into low-frequency and high-frequency sub-bands. For the low-frequency sub-band, we improve its objective quality through an enhancement network. For the high-frequency sub-band, we propose to use WDST to effectively improve its perceptual quality. By feat of the perfect reconstruction property of wavelets, these sub-bands can be re-combined to obtain an image which has simultaneously high objective and perceptual quality. The numerical results on various datasets show that our method achieves the best trade-off between the distortion and perceptual quality among the existing state-of-the-art SISR methods.",Swadosttrfoanefpetrinsiimsu,29.0,35.0,0.0
8318,Style Transfer,95.0,automatic semantic style transfer using deep convolutional neural networks and soft masks,4.0,155.0,3.0,88.0,4.0,3.6000000000000005,116.9,28,https://arxiv.org/pdf/1708.09641,"This paper presents an automatic image synthesis method to transfer the style of an example image to a content image. When standard neural style transfer approaches are used, the textures and colours in different semantic regions of the style image are often applied inappropriately to the content image, ignoring its semantic layout and ruining the transfer result. In order to reduce or avoid such effects, we propose a novel method based on automatically segmenting the objects and extracting their soft semantic masks from the style and content images, in order to preserve the structure of the content image while having the style transferred. Each soft mask of the style image represents a specific part of the style image, corresponding to the soft mask of the content image with the same semantics. Both the soft masks and source images are provided as multichannel input to an augmented deep CNN framework for style transfer which incorporates a generative Markov random field model. The results on various images show that our method outperforms the most recent techniques.",Sausesttrusdeconeneansoma,22.0,59.0,0.0
8319,Style Transfer,75.0,learning from multi-domain artistic images for arbitrary style transfer,4.0,175.0,3.0,93.0,4.0,3.6000000000000005,120.4,29,https://arxiv.org/pdf/1805.09987,"We propose a fast feed-forward network for arbitrary style transfer, which can generate stylized image for previously unseen content and style image pairs. Besides the traditional content and style representation based on deep features and statistics for textures, we use adversarial networks to regularize the generation of stylized images. Our adversarial network learns the intrinsic property of image styles from large-scale multi-domain artistic images. The adversarial training is challenging because both the input and output of our generator are diverse multi-domain images. We use a conditional generator that stylized content by shifting the statistics of deep features, and a conditional discriminator based on the coarse category of styles. Moreover, we propose a mask module to spatially decide the stylization level and stabilize adversarial training by avoiding mode collapse. As a side effect, our trained discriminator can be applied to rank and select representative stylized images. We qualitatively and quantitatively evaluate the proposed method, and compare with recent style transfer methods. We release our code and model at https://github.com/nightldj/behance_release.",Slefrmuarimfoarsttr,16.0,64.0,2.0
8320,Style Transfer,8.0,arbitrary style transfer with deep feature reshuffle,5.0,147.0,3.0,129.0,3.0,3.6,99.9,30,http://openaccess.thecvf.com/content_cvpr_2018/papers/Gu_Arbitrary_Style_Transfer_CVPR_2018_paper.pdf,"This paper introduces a novel method by reshuffling deep features (i.e., permuting the spacial locations of a feature map) of the style image for arbitrary style transfer. We theoretically prove that our new style loss based on reshuffle connects both global and local style losses respectively used by most parametric and non-parametric neural style transfer methods. This simple idea can effectively address the challenging issues in existing style transfer methods. On one hand, it can avoid distortions in local style patterns, and allow semantic-level transfer, compared with neural parametric methods. On the other hand, it can preserve globally similar appearance to the style image, and avoid wash-out artifacts, compared with neural non-parametric methods. Based on the proposed loss, we also present a progressive feature-domain optimization approach. The experiments show that our method is widely applicable to various styles, and produces better quality than existing methods.",Sarsttrwidefere,79.0,44.0,9.0
8321,Style Transfer,27.0,laplacian-steered neural style transfer,5.0,177.0,3.0,173.0,3.0,3.6,130.79999999999998,31,https://arxiv.org/pdf/1707.01253,"Neural Style Transfer based on Convolutional Neural Networks (CNN) aims to synthesize a new image that retains the high-level structure of a content image, rendered in the low-level texture of a style image. This is achieved by constraining the new image to have high-level CNN features similar to the content image, and lower-level CNN features similar to the style image. However in the traditional optimization objective, low-level features of the content image are absent, and the low-level features of the style image dominate the low-level detail structures of the new image. Hence in the synthesized image, many details of the content image are lost, and a lot of inconsistent and unpleasing artifacts appear. As a remedy, we propose to steer image synthesis with a novel loss function: the Laplacian loss. The Laplacian matrix (""Laplacian"" in short), produced by a Laplacian operator, is widely used in computer vision to detect edges and contours. The Laplacian loss measures the difference of the Laplacians, and correspondingly the difference of the detail structures, between the content image and a new image. It is flexible and compatible with the traditional style transfer constraints. By incorporating the Laplacian loss, we obtain a new optimization objective for neural style transfer named Lapstyle. Minimizing this objective will produce a stylized image that better preserves the detail structures of the content image and eliminates the artifacts. Experiments show that Lapstyle produces more appealing stylized images with less artifacts, without compromising their ""stylishness"".",Slanesttr,52.0,28.0,6.0
8322,Style Transfer,72.0,drafting and revision: laplacian pyramid network for fast high-quality artistic style transfer,4.0,9.0,5.0,201.0,1.0,3.5,85.5,32,https://openaccess.thecvf.com/content/CVPR2021/papers/Lin_Drafting_and_Revision_Laplacian_Pyramid_Network_for_Fast_High-Quality_Artistic_CVPR_2021_paper.pdf,"Artistic style transfer aims at migrating the style from an example image to a content image. Currently, optimization-based methods have achieved great stylization quality, but expensive time cost restricts their practical applications. Meanwhile, feed-forward methods still fail to synthesize complex style, especially when holistic global and local patterns exist. Inspired by the common painting process of drawing a draft and revising the details, we introduce a novel feed-forward method named Laplacian Pyramid Network (LapStyle). LapStyle first transfers global style patterns in low-resolution via a Drafting Network. It then revises the local details in high-resolution via a Revision Network, which hallucinates a residual image according to the draft and the image textures extracted by Laplacian filtering. Higher resolution details can be easily generated by stacking Revision Networks with multiple Laplacian pyramid levels. The final stylized image is obtained by aggregating outputs of all pyramid levels. %We also introduce a patch discriminator to better learn local patterns adversarially. Experiments demonstrate that our method can synthesize high quality stylized images in real time, where holistic style patterns are properly transferred.",Sdranrelapynefofahiarsttr,2.0,37.0,0.0
8323,Style Transfer,401.0,controlling perceptual factors in neural style transfer,1.0,33.0,5.0,100.0,4.0,3.5,163.5,33,http://arxiv.org/pdf/1611.07865v2,"Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method to introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. We also describe how these methods can be used to more efficiently produce large size, high-quality stylisation. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer.",Scopefainnesttr,266.0,28.0,18.0
8324,Style Transfer,401.0,collaborative distillation for ultra-resolution universal style transfer,1.0,65.0,4.0,9.0,5.0,3.4000000000000004,149.0,34,http://arxiv.org/pdf/2003.08436v2,"Universal style transfer methods typically leverage rich representations from deep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on large collections of images. Despite the effectiveness, its application is heavily constrained by the large model size to handle ultra-resolution images given limited memory. In this work, we present a new knowledge distillation method (named Collaborative Distillation) for encoder-decoder based neural style transfer to reduce the convolutional filters. The main idea is underpinned by a finding that the encoder-decoder pairs construct an exclusive collaborative relationship, which is regarded as a new kind of knowledge for style transfer models. Moreover, to overcome the feature size mismatch when applying collaborative distillation, a linear embedding loss is introduced to drive the student network to learn a linear embedding of the teacher’s features. Extensive experiments show the effectiveness of our method when applied to different universal style transfer approaches (WCT and AdaIN), even if the model size is reduced by 15.5 times. Especially, on WCT with the compressed models, we achieve ultra-resolution (over 40 megapixels) universal style transfer on a 12GB GPU for the first time. Further experiments on optimization-based stylization scheme show the generality of our algorithm on different stylization paradigms. Our code and trained models are available at https://github.com/mingsun-tse/collaborative-distillation.",Scodifoulunsttr,22.0,76.0,3.0
8325,Style Transfer,401.0,"delete, retrieve, generate: a simple approach to sentiment and style transfer",1.0,56.0,4.0,25.0,5.0,3.4000000000000004,150.2,35,http://arxiv.org/pdf/1908.09368v1,"We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., “screen is just the right size” to “screen is too small”). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., “too small”). Our strongest method extracts content words by deleting phrases associated with the sentence’s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.",Sderegeasiaptoseansttr,294.0,24.0,102.0
8326,Style Transfer,401.0,a probabilistic formulation of unsupervised text style transfer,1.0,79.0,4.0,5.0,5.0,3.4000000000000004,153.4,36,http://arxiv.org/pdf/2002.03912v3,"We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.",Saprfoofuntesttr,46.0,37.0,15.0
8327,Style Transfer,401.0,style transfer through back-translation,1.0,70.0,4.0,26.0,5.0,3.4000000000000004,156.10000000000002,37,http://arxiv.org/pdf/1811.12704v1,"Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.",Ssttrthba,234.0,50.0,43.0
8328,Style Transfer,401.0,disentangled representation learning for non-parallel text style transfer,1.0,83.0,4.0,38.0,5.0,3.4000000000000004,164.9,38,http://arxiv.org/pdf/2101.07496v1,"This paper tackles the problem of disentangling the latent representations of style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task and adversarial objectives, for style prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled in the latent space. This disentangled latent representation learning can be applied to style transfer on non-parallel corpora. We achieve high performance in terms of transfer accuracy, content preservation, and language fluency, in comparison to various previous approaches.",Sdirelefonotesttr,123.0,48.0,21.0
8329,Style Transfer,1.0,image style transfer using convolutional neural networks,5.0,201.0,1.0,2.0,5.0,3.4,81.3,39,https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf,"Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.",Simsttrusconene,2759.0,35.0,356.0
8330,Style Transfer,24.0,dynamic instance normalization for arbitrary style transfer,5.0,201.0,1.0,15.0,5.0,3.4,92.1,40,https://ojs.aaai.org/index.php/AAAI/article/view/5862/5718,"Prior normalization methods rely on affine transformations to produce arbitrary image style transfers, of which the parameters are computed in a pre-defined way. Such manually-defined nature eventually results in the high-cost and shared encoders for both style and content encoding, making style transfer systems cumbersome to be deployed in resource-constrained environments like on the mobile-terminal side. In this paper, we propose a new and generalized normalization module, termed as Dynamic Instance Normalization (DIN), that allows for flexible and more efficient arbitrary style transfers. Comprising an instance normalization and a dynamic convolution, DIN encodes a style image into learnable convolution parameters, upon which the content image is stylized. Unlike conventional methods that use shared complex encoders to encode content and style, the proposed DIN introduces a sophisticated style encoder, yet comes with a compact and lightweight content encoder for fast inference. Experimental results demonstrate that the proposed approach yields very encouraging results on challenging style patterns and, to our best knowledge, for the first time enables an arbitrary style transfer using MobileNet-based lightweight architecture, leading to a reduction factor of more than twenty in computational cost as compared to existing approaches. Furthermore, the proposed DIN provides flexible support for state-of-the-art convolutional operations, and thus triggers novel functionalities, such as uniform-stroke placement for non-natural images and automatic spatial-stroke control.",Sdyinnofoarsttr,31.0,50.0,2.0
8331,Style Transfer,36.0,parallel data augmentation for formality style transfer,5.0,201.0,1.0,13.0,5.0,3.4,95.1,41,https://arxiv.org/pdf/2005.07522,"The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset.",Spadaaufofosttr,19.0,37.0,4.0
8332,Style Transfer,40.0,rethinking style transfer: from pixels to parameterized brushstrokes,5.0,90.0,4.0,201.0,1.0,3.4,108.3,42,https://openaccess.thecvf.com/content/CVPR2021/papers/Kotovenko_Rethinking_Style_Transfer_From_Pixels_to_Parameterized_Brushstrokes_CVPR_2021_paper.pdf,"There have been many successful implementations of neural style transfer in recent years. In most of these works, the stylization process is confined to the pixel domain. However, we argue that this representation is unnatural because paintings usually consist of brushstrokes rather than pixels. We propose a method to stylize images by optimizing parameterized brushstrokes instead of pixels and further introduce a simple differentiable rendering mechanism. Our approach significantly improves visual quality and enables additional control over the stylization process such as controlling the flow of brushstrokes through user input. We provide qualitative and quantitative evaluations that show the efficacy of the proposed parameterized representation.",Sresttrfrpitopabr,3.0,67.0,1.0
8333,Style Transfer,46.0,reconet: real-time coherent video style transfer network,4.0,104.0,3.0,147.0,3.0,3.3000000000000003,99.5,43,https://arxiv.org/pdf/1807.01197,"Image style transfer models based on convolutional neural networks usually suffer from high temporal inconsistency when applied to videos. Some video style transfer models have been proposed to improve temporal consistency, yet they fail to guarantee fast processing speed, nice perceptual style quality and high temporal consistency at the same time. In this paper, we propose a novel real-time video style transfer model, ReCoNet, which can generate temporally coherent style transfer videos while maintaining favorable perceptual styles. A novel luminance warping constraint is added to the temporal loss at the output level to capture luminance changes between consecutive frames and increase stylization stability under illumination effects. We also propose a novel feature-map-level temporal loss to further enhance temporal consistency on traceable objects. Experimental results indicate that our model exhibits outstanding performance both qualitatively and quantitatively.",Srerecovisttrne,27.0,35.0,3.0
8334,Style Transfer,41.0,selective style transfer for text,4.0,196.0,3.0,106.0,3.0,3.3000000000000003,122.5,44,https://arxiv.org/pdf/1906.01466,"This paper explores the possibilities of image style transfer applied to text maintaining the original transcriptions. Results on different text domains (scene text, machine printed text and handwritten text) and cross-modal results demonstrate that this is feasible, and open different research lines. Furthermore, two architectures for selective style transfer, which means transferring style to only desired image pixels, are proposed. Finally, scene text selective style transfer is evaluated as a data augmentation technique to expand scene text detection datasets, resulting in a boost of text detectors performance. Our implementation of the described models is publicly available.",Ssesttrfote,11.0,26.0,0.0
8335,Style Transfer,401.0,unsupervised text style transfer using language models as discriminators,1.0,15.0,5.0,112.0,3.0,3.2,159.9,45,http://arxiv.org/pdf/2109.07812v1,"Binary classifiers are often employed as discriminators in GAN-based unsupervised style transfer systems to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with this approach is that the error signal provided by the discriminator can be unstable and is sometimes insufficient to train the generator to produce fluent language. In this paper, we propose a new technique that uses a target domain language model as the discriminator, providing richer and more stable token-level feedback during the learning process. We train the generator to minimize the negative log likelihood (NLL) of generated sentences, evaluated by the language model. By using a continuous approximation of discrete sampling under the generator, our model can be trained using back-propagation in an end- to-end fashion. Moreover, our empirical results show that when using a language model as a structured discriminator, it is possible to forgo adversarial steps during training, making the process more stable. We compare our model with previous work using convolutional neural networks (CNNs) as discriminators and show that our approach leads to improved performance on three tasks: word substitution decipherment, sentiment modification, and related language translation.",Suntesttruslamoasdi,188.0,49.0,23.0
8336,Style Transfer,401.0,structured content preservation for unsupervised text style transfer,1.0,14.0,5.0,146.0,3.0,3.2,169.7,46,http://arxiv.org/pdf/1810.06526v2,"Text style transfer aims to modify the style of a sentence while keeping its content unchanged. Recent style transfer systems often fail to faithfully preserve the content after changing the style. This paper proposes a structured content preserving model that leverages linguistic information in the structured fine-grained supervisions to better preserve the style-independent content during style transfer. In particular, we achieve the goal by devising rich model objectives based on both the sentence's lexical information and a language model that conditions on content. The resulting model therefore is encouraged to retain the semantic meaning of the target sentences. We perform extensive experiments that compare our model to other existing approaches in the tasks of sentiment and political slant transfer. Our model achieves significant improvement in terms of both content preservation and style transfer in automatic and human evaluation.",Sstcoprfountesttr,33.0,31.0,6.0
8337,Style Transfer,401.0,fast patch-based style transfer of arbitrary style,1.0,36.0,5.0,126.0,3.0,3.2,172.5,47,http://arxiv.org/pdf/1808.04537v1,"Artistic style transfer is an image synthesis problem where the content of an image is reproduced with the style of another. Recent works show that a visually appealing style transfer can be achieved by using the hidden activations of a pretrained convolutional neural network. However, existing methods either apply (i) an optimization procedure that works for any style image but is very expensive, or (ii) an efficient feedforward network that only allows a limited number of trained styles. In this work we propose a simpler optimization objective based on local matching that combines the content structure and style textures in a single layer of the pretrained network. We show that our objective has desirable properties such as a simpler optimization landscape, intuitive parameter tuning, and consistent frame-by-frame performance on video. Furthermore, we use 80,000 natural images and 80,000 paintings to train an inverse network that approximates the result of the optimization. This results in a procedure for artistic style transfer that is efficient but also allows arbitrary content and style images.",Sfapasttrofarst,207.0,41.0,32.0
8338,Style Transfer,401.0,artistic style transfer for videos,1.0,11.0,5.0,162.0,3.0,3.2,173.3,48,http://arxiv.org/abs/1604.08610v2,"In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.",Sarsttrfovi,170.0,12.0,24.0
8339,Style Transfer,401.0,multi-content gan for few-shot font style transfer,1.0,40.0,5.0,130.0,3.0,3.2,175.3,49,http://arxiv.org/pdf/1712.00516v1,"In this work, we focus on the challenge of taking partial observations of highly-stylized text and generalizing the observations to generate unobserved glyphs in the ornamented typeface. To generate a set of multi-content images following a consistent style from very few examples, we propose an end-to-end stacked conditional GAN model considering content along channels and style along network layers. Our proposed network transfers the style of given glyphs to the contents of unseen ones, capturing highly stylized fonts found in the real-world such as those on movie posters or infographics. We seek to transfer both the typographic stylization (ex. serifs and ears) as well as the textual stylization (ex. color gradients and effects.) We base our experiments on our collected data set including 10,000 fonts with different styles and demonstrate effective generalization from a very small number of observed glyphs.",Smugafofefosttr,164.0,39.0,13.0
8340,Style Transfer,401.0,preserving color in neural artistic style transfer,1.0,10.0,5.0,172.0,3.0,3.2,175.9,50,http://arxiv.org/pdf/1606.05897v1,"This note presents an extension to the neural artistic style transfer algorithm (Gatys et al.). The original algorithm transforms an image to have the style of another given image. For example, a photograph can be transformed to have the style of a famous painting. Here we address a potential shortcoming of the original method: the algorithm transfers the colors of the original painting, which can alter the appearance of the scene in undesirable ways. We describe simple linear methods for transferring style while preserving colors.",Sprcoinnearsttr,94.0,6.0,7.0
8341,Style Transfer,7.0,content and style disentanglement for artistic style transfer,5.0,201.0,1.0,48.0,4.0,3.1,96.9,51,http://openaccess.thecvf.com/content_ICCV_2019/papers/Kotovenko_Content_and_Style_Disentanglement_for_Artistic_Style_Transfer_ICCV_2019_paper.pdf,"Artists rarely paint in a single style throughout their career. More often they change styles or develop variations of it. In addition, artworks in different styles and even within one style depict real content differently: while Picasso's Blue Period displays a vase in a blueish tone but as a whole, his Cubist works deconstruct the object. To produce artistically convincing stylizations, style transfer models must be able to reflect these changes and variations. Recently many works have aimed to improve the style transfer task, but neglected to address the described observations. We present a novel approach which captures particularities of style and the variations within and separates style and content. This is achieved by introducing two novel losses: a fixpoint triplet style loss to learn subtle variations within one style or between different styles and a disentanglement loss to ensure that the stylization is not conditioned on the real input photo. In addition the paper proposes various evaluation methods to measure the importance of both losses on the validity, quality and variability of final stylizations. We provide qualitative results to demonstrate the performance of our approach.",Scoanstdifoarsttr,56.0,36.0,3.0
8342,Style Transfer,22.0,reinforcement learning based text style transfer without parallel training corpus,5.0,201.0,1.0,70.0,4.0,3.1,108.0,52,https://arxiv.org/pdf/1903.10671,"Text style transfer rephrases a text from a source style (e.g., informal) to a target style (e.g., formal) while keeping its original meaning. Despite the success existing works have achieved using a parallel corpus for the two styles, transferring text style has proven significantly more challenging when there is no parallel training corpus. In this paper, we address this challenge by using a reinforcement-learning-based generator-evaluator architecture. Our generator employs an attention-based encoder-decoder to transfer a sentence from the source style to the target style. Our evaluator is an adversarially trained style discriminator with semantic and syntactic constraints that score the generated sentence for style, meaning preservation, and fluency. Experimental results on two different style transfer tasks–sentiment transfer, and formality transfer–show that our model outperforms state-of-the-art approaches.Furthermore, we perform a manual evaluation that demonstrates the effectiveness of the proposed method using subjective metrics of generated text quality.",Srelebatesttrwipatrco,45.0,43.0,2.0
8343,Style Transfer,20.0,filter style transfer between photos,5.0,201.0,1.0,79.0,4.0,3.1,110.1,53,https://arxiv.org/pdf/2007.07925,"Over the past few years, image-to-image style transfer has risen to the frontiers of neural image processing. While conventional methods were successful in various tasks such as color and texture transfer between images, none could effectively work with the custom filter effects that are applied by users through various platforms like Instagram. In this paper, we introduce a new concept of style transfer, Filter Style Transfer (FST). Unlike conventional style transfer, new technique FST can extract and transfer custom filter style from a filtered style image to a content image. FST first infers the original image from a filtered reference via image-to-image translation. Then it estimates filter parameters from the difference between them. To resolve the ill-posed nature of reconstructing the original image from the reference, we represent each pixel color of an image to class mean and deviation. Besides, to handle the intra-class color variation, we propose an uncertainty based weighted least square method for restoring an original image. To the best of our knowledge, FST is the first style transfer method that can transfer custom filter effects between FHD image under 2ms on a mobile device without any textual context loss.",Sfisttrbeph,5.0,36.0,0.0
8344,Style Transfer,76.0,image generation by gan and style transfer for agar plate image segmentation,4.0,201.0,1.0,34.0,5.0,3.1,113.4,54,http://arxiv.org/pdf/2007.12578v1,"BACKGROUND AND OBJECTIVES
Deep learning models and specifically Convolutional Neural Networks (CNNs) are becoming the leading approach in many computer vision tasks, including medical image analysis. Nevertheless, the CNN training usually requires large sets of supervised data, which are often difficult and expensive to obtain in the medical field. To address the lack of annotated images, image generation is a promising method, which is becoming increasingly popular in the computer vision community. In this paper, we present a new approach to the semantic segmentation of bacterial colonies in agar plate images, based on deep learning and synthetic image generation, to increase the training set size. Indeed, semantic segmentation of bacterial colony is the basis for infection recognition and bacterial counting in Petri plate analysis.


METHODS
A convolutional neural network (CNN) is used to separate the bacterial colonies from the background. To face the lack of annotated images, a novel engine is designed - which exploits a generative adversarial network to capture the typical distribution of the bacterial colonies on agar plates - to generate synthetic data. Then, bacterial colony patches are superimposed on existing background images, taking into account both the local appearance of the background and the intrinsic opacity of the bacterial colonies, and a style transfer algorithm is used for further improve visual realism.


RESULTS
The proposed deep learning approach has been tested on the only public dataset available with pixel-level annotations for bacterial colony semantic segmentation in agar plates. The role of including synthetic data in the training of a segmentation CNN has been evaluated, showing how comparable performances can be obtained with respect to the use of real images. Qualitative results are also reported for a second public dataset in which the segmentation annotations are not provided.


CONCLUSIONS
The use of a small set of real data, together with synthetic images, allows obtaining comparable results with respect to using a complete set of real images. Therefore, the proposed synthetic data generator is able to address the scarcity of biomedical data and provides a scalable and cheap alternative to human ground-truth supervision.",Simgebygaansttrfoagplimse,16.0,64.0,0.0
8345,Style Transfer,93.0,contextual text style transfer,4.0,201.0,1.0,40.0,5.0,3.1,120.3,55,https://arxiv.org/pdf/2005.00136,"We introduce a new task, Contextual Text Style Transfer - translating a sentence into a desired style with its surrounding context taken into account. This brings two key challenges to existing style transfer approaches: (I) how to preserve the semantic meaning of target sentence and its consistency with surrounding context during transfer; (ii) how to train a robust model with limited labeled data accompanied by context. To realize high-quality style transfer with natural context preservation, we propose a Context-Aware Style Transfer (CAST) model, which uses two separate encoders for each input sentence and its surrounding context. A classifier is further trained to ensure contextual consistency of the generated sentence. To compensate for the lack of parallel data, additional self-reconstruction and back-translation losses are introduced to leverage non-parallel data in a semi-supervised fashion. Two new benchmarks, Enron-Context and Reddit-Context, are introduced for formality and offensiveness style transfer. Experimental results on these datasets demonstrate the effectiveness of the proposed CAST model over state-of-the-art methods across style accuracy, content preservation and contextual consistency metrics.",Scotesttr,11.0,43.0,2.0
8346,Style Transfer,401.0,deformable style transfer,1.0,57.0,4.0,58.0,4.0,3.1,160.5,56,http://arxiv.org/pdf/2003.11038v2,"Both geometry and texture are fundamental aspects of visual style. Existing style transfer methods, however, primarily focus on texture, almost entirely ignoring geometry. We propose deformable style transfer (DST), an optimization-based approach that jointly stylizes the texture and geometry of a content image to better match a style image. Unlike previous geometry-aware stylization methods, our approach is neither restricted to a particular domain (such as human faces), nor does it require training sets of matching style/content pairs. We demonstrate our method on a diverse set of content and style images including portraits, animals, objects, scenes, and paintings. Code has been made publicly available at this https URL.",Sdesttr,8.0,35.0,2.0
8347,Style Transfer,401.0,automated deep photo style transfer,1.0,97.0,4.0,94.0,4.0,3.1,187.3,57,http://arxiv.org/pdf/1901.03915v1,"Photorealism is a complex concept that cannot easily be formulated mathematically. Deep Photo Style Transfer is an attempt to transfer the style of a reference image to a content image while preserving its photorealism. This is achieved by introducing a constraint that prevents distortions in the content image and by applying the style transfer independently for semantically different parts of the images. In addition, an automated segmentation process is presented that consists of a neural network based segmentation method followed by a semantic grouping step. To further improve the results a measure for image aesthetics is used and elaborated. If the content and the style image are sufficiently similar, the result images look very realistic. With the automation of the image segmentation the pipeline becomes completely independent from any user interaction, which allows for new applications.",Saudephsttr,7.0,21.0,3.0
8348,Style Transfer,401.0,lagrangian neural style transfer for fluids,1.0,103.0,3.0,19.0,5.0,3.0,167.2,58,http://arxiv.org/abs/2005.00803v1,"Artistically controlling the shape, motion and appearance of fluid simulations pose major challenges in visual effects production. In this paper, we present a neural style transfer approach from images to 3D fluids formulated in a Lagrangian viewpoint. Using particles for style transfer has unique benefits compared to grid-based techniques. Attributes are stored on the particles and hence are trivially transported by the particle motion. This intrinsically ensures temporal consistency of the optimized stylized structure and notably improves the resulting quality. Simultaneously, the expensive, recursive alignment of stylization velocity fields of grid approaches is unnecessary, reducing the computation time to less than an hour and rendering neural flow stylization practical in production settings. Moreover, the Lagrangian representation improves artistic control as it allows for multi-fluid stylization and consistent color transfer from images, and the generality of the method enables stylization of smoke and liquids likewise.",Slanesttrfofl,14.0,107.0,3.0
8349,Style Transfer,401.0,style transfer in text: exploration and evaluation,1.0,107.0,3.0,17.0,5.0,3.0,168.2,59,http://arxiv.org/pdf/2110.10481v1,"Style transfer is an important problem in natural language processing (NLP). However, the progress in language style transfer is lagged behind other domains, such as computer vision, mainly because of the lack of parallel data and principle evaluation metrics. In this paper, we propose to learn style transfer with non-parallel data. We explore two models to achieve this goal, and the key idea behind the proposed models is to learn separate content representations and style representations using adversarial networks. We also propose novel evaluation metrics which measure two aspects of style transfer: transfer strength and content preservation. We access our models and the evaluation metrics on two tasks: paper-news title transfer, and positive-negative review transfer. Results show that the proposed content preservation metric is highly correlate to human judgments, and the proposed models are able to generate sentences with higher style transfer strength and similar content preservation score comparing to auto-encoder.",Ssttrinteexanev,314.0,40.0,76.0
8350,Style Transfer,401.0,diversified arbitrary style transfer via deep feature perturbation,1.0,163.0,3.0,10.0,5.0,3.0,188.5,60,http://arxiv.org/pdf/1909.08223v3,"Image style transfer is an underdetermined problem, where a large number of solutions can satisfy the same constraint (the content and style). Although there have been some efforts to improve the diversity of style transfer by introducing an alternative diversity loss, they have restricted generalization, limited diversity and poor scalability. In this paper, we tackle these limitations and propose a simple yet effective method for diversified arbitrary style transfer. The key idea of our method is an operation called deep feature perturbation (DFP), which uses an orthogonal random noise matrix to perturb the deep image feature maps while keeping the original style information unchanged. Our DFP operation can be easily integrated into many existing WCT (whitening and coloring transform)-based methods, and empower them to generate diverse results for arbitrary styles. Experimental results demonstrate that this learning-free and universal method can greatly increase the diversity while maintaining the quality of stylization.",Sdiarsttrvidefepe,18.0,37.0,3.0
8351,Style Transfer,42.0,transport-based neural style transfer for smoke simulations,4.0,201.0,1.0,71.0,4.0,2.8,114.3,61,https://arxiv.org/pdf/1905.07442,"Artistically controlling fluids has always been a challenging task. Optimization techniques rely on approximating simulation states towards target velocity or density field configurations, which are often handcrafted by artists to indirectly control smoke dynamics. Patch synthesis techniques transfer image textures or simulation features to a target flow field. However, these are either limited to adding structural patterns or augmenting coarse flows with turbulent structures, and hence cannot capture the full spectrum of different styles and semantically complex structures. In this paper, we propose the first Transport-based Neural Style Transfer (TNST) algorithm for volumetric smoke data. Our method is able to transfer features from natural images to smoke simulations, enabling general content-aware manipulations ranging from simple patterns to intricate motifs. The proposed algorithm is physically inspired, since it computes the density transport from a source input smoke to a desired target configuration. Our transport-based approach allows direct control over the divergence of the stylization velocity field by optimizing incompressible and irrotational potentials that transport smoke towards stylization. Temporal consistency is ensured by transporting and aligning subsequent stylized velocities, and 3D reconstructions are computed by seamlessly merging stylizations from different camera viewpoints.",Strnesttrfosmsi,29.0,88.0,3.0
8352,Style Transfer,111.0,revision in continuous space: unsupervised text style transfer without adversarial learning,3.0,201.0,1.0,21.0,5.0,2.8,120.0,62,https://ojs.aaai.org/index.php/AAAI/article/view/6355/6211,"Typical methods for unsupervised text style transfer often rely on two key ingredients: 1) seeking the explicit disentanglement of the content and the attributes, and 2) troublesome adversarial learning. In this paper, we show that neither of these components is indispensable. We propose a new framework that utilizes the gradients to revise the sentence in a continuous space during inference to achieve text style transfer. Our method consists of three key components: a variational auto-encoder (VAE), some attribute predictors (one for each attribute), and a content predictor. The VAE and the two types of predictors enable us to perform gradient-based optimization in the continuous space, which is mapped from sentences in a discrete space, to find the representation of a target sentence with the desired attributes and preserved content. Moreover, the proposed method naturally has the ability to simultaneously manipulate multiple fine-grained attributes, such as sentence length and the presence of specific words, when performing text style transfer tasks. Compared with previous adversarial learning based methods, the proposed method is more interpretable, controllable and easier to train. Extensive experimental studies on three popular text style transfer tasks show that the proposed method significantly outperforms five state-of-the-art methods.",Sreincospuntesttrwiadle,18.0,40.0,2.0
8353,Style Transfer,63.0,improving style transfer with calibrated metrics,4.0,201.0,1.0,78.0,4.0,2.8,122.70000000000002,63,http://openaccess.thecvf.com/content_WACV_2020/papers/Yeh_Improving_Style_Transfer_with_Calibrated_Metrics_WACV_2020_paper.pdf,"Style transfer produces a transferred image which is a rendering of a content image in the manner of a style image. We seek to understand how to improve style transfer.To do so requires quantitative evaluation procedures, but current evaluation is qualitative, mostly involving user studies. We describe a novel quantitative evaluation procedure. Our procedure relies on two statistics: the Effectiveness (E) statistic measures the extent that a given style has been transferred to the target, and the Coherence (C) statistic measures the extent to which the original image’s content is preserved. Our statistics are calibrated to human preference: targets with larger values of E and C will reliably be preferred by human subjects in comparisons of style and content, respectively.We use these statistics to investigate relative performance of a number of Neural Style Transfer (NST) methods, revealing a number of intriguing properties. Admissible methods lie on a Pareto frontier (i.e. improving E reduces C, or vice versa). Three methods are admissible: Universal style transfer produces very good C but weak E; modifying the optimization used for Gatys’ loss produces a method with strong E and strong C; and a modified cross-layer method has slightly better E at strong cost in C. While the histogram loss improves the E statistics of Gatys’ method, it does not make the method admissible. Surprisingly, style weights have relatively little effect in improving EC scores, and most variability in transfer is explained by the style itself (meaning experimenters can be misguided by selecting styles). Our GitHub Link is available.1",Simsttrwicame,5.0,30.0,0.0
8354,Style Transfer,2.0,real-time neural style transfer for videos,5.0,201.0,1.0,142.0,3.0,2.8,123.6,64,https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf,"Recent research endeavors have shown the potential of using feed-forward convolutional neural networks to accomplish fast style transfer for images. In this work, we take one step further to explore the possibility of exploiting a feed-forward network to perform style transfer for videos and simultaneously maintain temporal consistency among stylized video frames. Our feed-forward network is trained by enforcing the outputs of consecutive frames to be both well stylized and temporally consistent. More specifically, a hybrid loss is proposed to capitalize on the content information of input frames, the style information of a given style image, and the temporal information of consecutive frames. To calculate the temporal loss during the training stage, a novel two-frame synergic training mechanism is proposed. Compared with directly applying an existing image style transfer method to videos, our proposed method employs the trained network to yield temporally consistent stylized videos which are much more visually pleasant. In contrast to the prior video style transfer method which relies on time-consuming optimization on the fly, our method runs in real time while generating competitive visual results.",Srenesttrfovi,116.0,32.0,16.0
8355,Style Transfer,50.0,efficient neural networks for real-time motion style transfer,4.0,201.0,1.0,95.0,4.0,2.8,123.9,65,https://dl.acm.org/doi/pdf/10.1145/3340254,"Style is an intrinsic, inescapable part of human motion. It complements the content of motion to convey meaning, mood, and personality. Existing state-of-the-art motion style methods require large quantities of example data and intensive computational resources at runtime. To ensure output quality, such style transfer applications are often run on desktop machine with GPUs and significant memory. In this paper, we present a fast and expressive neural network-based motion style transfer method that generates stylized motion with quality comparable to the state of the art method, but uses much less computational power and a much smaller memory footprint. Our method also allows the output to be adjusted in a latent style space, something not offered in previous approaches. Our style transfer model is implemented using three multi-layered networks: a pose network, a timing network and a foot-contact network. A one-hot style vector serves as an input control knob and determines the stylistic output of these networks. During training, the networks are trained with a large motion capture database containing heterogeneous actions and various styles. Joint information vectors together with one-hot style vectors are extracted from motion data and fed to the networks. Once the network has been trained, the database is no longer needed on the device, thus removing the large memory requirement of previous motion style methods. At runtime, our model takes novel input and allows real-valued numbers to be specified in the style vector, which can be used for interpolation, extrapolation or mixing of styles. With much lower memory and computational requirements, our networks are efficient and fast enough for real-time use on mobile devices. Requiring no information about future states, the style transfer can be performed in an online fashion. We validate our result both quantitatively and perceptually, confirming its effectiveness and improvement over previous approaches.",Sefneneforemosttr,15.0,38.0,0.0
8356,Style Transfer,10.0,neural style transfer for audio spectograms,5.0,201.0,1.0,141.0,3.0,2.8,125.7,66,https://arxiv.org/pdf/1801.01589,"There has been fascinating work on creating artistic transformations of images by Gatys. This was revolutionary in how we can in some sense alter the 'style' of an image while generally preserving its 'content'. In our work, we present a method for creating new sounds using a similar approach, treating it as a style-transfer problem, starting from a random-noise input signal and iteratively using back-propagation to optimize the sound to conform to filter-outputs from a pre-trained neural architecture of interest. 
For demonstration, we investigate two different tasks, resulting in bandwidth expansion/compression, and timbral transfer from singing voice to musical instruments. A feature of our method is that a single architecture can generate these different audio-style-transfer types using the same set of parameters which otherwise require different complex hand-tuned diverse signal processing pipelines.",Snesttrfoausp,48.0,7.0,2.0
8357,Style Transfer,21.0,style transfer as unsupervised machine translation,5.0,201.0,1.0,132.0,3.0,2.8,126.3,67,https://arxiv.org/pdf/1808.07894,"Language style transferring rephrases text with specific stylistic attributes while preserving the original attribute-independent content. One main challenge in learning a style transfer system is a lack of parallel data where the source sentence is in one style and the target sentence in another style. With this constraint, in this paper, we adapt unsupervised machine translation methods for the task of automatic style transfer. We first take advantage of style-preference information and word embedding similarity to produce pseudo-parallel data with a statistical machine translation (SMT) framework. Then the iterative back-translation approach is employed to jointly train two neural machine translation (NMT) based transfer systems. To control the noise generated during joint training, a style classifier is introduced to guarantee the accuracy of style transfer and penalize bad candidates in the generated pseudo data. Experiments on benchmark datasets show that our proposed method outperforms previous state-of-the-art models in terms of both accuracy of style transfer and quality of input-output correspondence.",Ssttrasunmatr,70.0,32.0,12.0
8358,Style Transfer,19.0,midi-vae: modeling dynamics and instrumentation of music with applications to style transfer,5.0,201.0,1.0,135.0,3.0,2.8,126.6,68,https://arxiv.org/pdf/1809.07600,"We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.",Smimodyaninofmuwiaptosttr,62.0,40.0,8.0
8359,Style Transfer,142.0,high-resolution network for photorealistic style transfer,3.0,60.0,4.0,201.0,1.0,2.8,126.9,69,https://arxiv.org/pdf/1904.11617,"SN 1996cr, located in the Circinus Galaxy (3.7 Mpc, z ~ 0.001) was non-detected in X-rays at ~ 1000 days yet brightened to ~ 4 x 10^{39} erg/s (0.5-8 keV) after 10 years (Bauer et al. 2008). A 1-D hydrodynamic model of the ejecta-CSM interaction produces good agreement with the measured X-ray light curves and spectra at multiple epochs. We conclude that the progenitor of SN 1996cr could have been a massive star, M > 30 M_solar, which went from an RSG to a brief W-R phase before exploding within its ~ 0.04 pc wind-blown shell (Dwarkadas et al. 2010). Further analysis of the deep Chandra HETG observations allows line-shape fitting of a handful of bright Si and Fe lines in the spectrum. The line shapes are well fit by axisymmetric emission models with an axis orientation ~ 55 degrees to our line-of-sight. In the deep 2009 epoch the higher ionization Fe XXVI emission is constrained to high lattitudes: the Occam-est way to get the Fe H-like emission coming from high latitude/polar regions is to have more CSM at/around the poles than at mid and lower lattitudes, along with a symmetric ejecta explosion/distribution. Similar CSM/ejecta characterization may be possible for other SNe and, with higher-throughput X-ray observations, for gamma-ray burst remnants as well.",Shinefophsttr,3.0,26.0,0.0
8360,Style Transfer,79.0,a deep learning framework for nucleus segmentation using image style transfer,4.0,201.0,1.0,80.0,4.0,2.8,128.10000000000002,70,https://www.biorxiv.org/content/biorxiv/early/2019/03/17/580605.full.pdf,"Single cell segmentation is typically one of the first and most crucial tasks of image-based cellular analysis. We present a deep learning approach aiming towards a truly general method for localizing nuclei across a diverse range of assays and light microscopy modalities. We outperform the 739 methods submitted to the 2018 Data Science Bowl on images representing a variety of realistic conditions, some of which were not represented in the training data. The key to our approach is to adapt our model to unseen and unlabeled data using image style transfer to generate augmented training samples. This allows the model to recognize nuclei in new and different experiments without requiring expert annotations.",Sadelefrfonuseusimsttr,33.0,21.0,0.0
8361,Style Transfer,5.0,characterizing and improving stability in neural style transfer,5.0,201.0,1.0,157.0,3.0,2.8,129.0,71,https://openaccess.thecvf.com/content_ICCV_2017/papers/Gupta_Characterizing_and_Improving_ICCV_2017_paper.pdf,"Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not require optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.",Schanimstinnesttr,79.0,46.0,10.0
8362,Style Transfer,16.0,what is wrong with style transfer for texts?,5.0,201.0,1.0,151.0,3.0,2.8,130.5,72,https://arxiv.org/pdf/1808.04365,"A number of recent machine learning papers work with an automated style transfer for texts and, counter to intuition, demonstrate that there is no consensus formulation of this NLP task. Different researchers propose different algorithms, datasets and target metrics to address it. This short opinion paper aims to discuss possible formalization of this NLP task in anticipation of a further growing interest to it.",Swhiswrwisttrfote,22.0,21.0,3.0
8363,Style Transfer,18.0,multimodal transfer: a hierarchical deep convolutional neural network for fast artistic style transfer,5.0,201.0,1.0,150.0,3.0,2.8,130.8,73,https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multimodal_Transfer_A_CVPR_2017_paper.pdf,"Transferring artistic styles onto everyday photographs has become an extremely popular task in both academia and industry. Recently, offline training has replaced online iterative optimization, enabling nearly real-time stylization. When those stylization networks are applied directly to high-resolution images, however, the style of localized regions often appears less similar to the desired artistic style. This is because the transfer process fails to capture small, intricate textures and maintain correct texture scales of the artworks. Here we propose a multimodal convolutional neural network that takes into consideration faithful representations of both color and luminance channels, and performs stylization hierarchically with multiple losses of increasing scales. Compared to state-of-the-art networks, our network can also perform style transfer in nearly real-time by performing much more sophisticated training offline. By properly handling style and texture cues at multiple scales using several modalities, we can transfer not just large-scale, obvious style cues but also subtle, exquisite ones. That is, our scheme can generate results that are visually pleasing and more similar to multiple desired artistic styles with color and texture cues at multiple scales.",Smutrahideconenefofaarsttr,99.0,32.0,6.0
8364,Style Transfer,14.0,style transfer via texture synthesis,5.0,201.0,1.0,164.0,3.0,2.8,133.8,74,https://ieeexplore.ieee.org/iel7/83/4358840/07874180.pdf,"Style transfer is a process of migrating a style from a given image to the content of another, synthesizing a new image, which is an artistic mixture of the two. Recent work on this problem adopting convolutional neural-networks (CNN) ignited a renewed interest in this field, due to the very impressive results obtained. There exists an alternative path toward handling the style transfer task, via the generalization of texture synthesis algorithms. This approach has been proposed over the years, but its results are typically less impressive compared with the CNN ones. In this paper, we propose a novel style transfer algorithm that extends the texture synthesis work of Kwatra et al. (2005), while aiming to get stylized images that are closer in quality to the CNN ones. We modify Kwatra’s algorithm in several key ways in order to achieve the desired transfer, with emphasis on a consistent way for keeping the content intact in selected regions, while producing hallucinated and rich style in others. The results obtained are visually pleasing and diverse, shown to be competitive with the recent CNN style transfer algorithms. The proposed algorithm is fast and flexible, being able to process any pair of content + style images.",Ssttrvitesy,93.0,69.0,3.0
8365,Style Transfer,99.0,formality style transfer with hybrid textual annotations,4.0,201.0,1.0,85.0,4.0,2.8,135.60000000000002,75,https://arxiv.org/pdf/1903.06353,"Formality style transformation is the task of modifying the formality of a given sentence without changing its content. Its challenge is the lack of large-scale sentence-aligned parallel data. In this paper, we propose an omnivorous model that takes parallel data and formality-classified data jointly to alleviate the data sparsity issue. We empirically demonstrate the effectiveness of our approach by achieving the state-of-art performance on a recently proposed benchmark dataset of formality transfer. Furthermore, our model can be readily adapted to other unsupervised text style transfer tasks like unsupervised sentiment transfer and achieve competitive results on three widely recognized benchmarks.",Sfosttrwihytean,19.0,27.0,5.0
8366,Style Transfer,26.0,fast neural style transfer for motion data,5.0,201.0,1.0,189.0,3.0,2.8,144.9,76,https://www.pure.ed.ac.uk/ws/files/34462628/ff.pdf,"Automating motion style transfer can help save animators time by allowing them to produce a single set of motions, which can then be automatically adapted for use with different characters. The proposed fast, efficient technique for performing neural style transfer of human motion data uses a feed-forward neural network trained on a large motion database. The proposed framework can transform the style of motion thousands of times faster than previous approaches that use optimization.",Sfanesttrfomoda,32.0,24.0,5.0
8367,Style Transfer,25.0,spectral style transfer for human motion between independent actions,5.0,201.0,1.0,191.0,3.0,2.8,145.2,77,https://dl.acm.org/doi/pdf/10.1145/2897824.2925955,"Human motion is complex and difficult to synthesize realistically. Automatic style transfer to transform the mood or identity of a character's motion is a key technology for increasing the value of already synthesized or captured motion data. Typically, state-of-the-art methods require all independent actions observed in the input to be present in a given style database to perform realistic style transfer. We introduce a spectral style transfer method for human motion between independent actions, thereby greatly reducing the required effort and cost of creating such databases. We leverage a spectral domain representation of the human motion to formulate a spatial correspondence free approach. We extract spectral intensity representations of reference and source styles for an arbitrary action, and transfer their difference to a novel motion which may contain previously unseen actions. Building on this core method, we introduce a temporally sliding window filter to perform the same analysis locally in time for heterogeneous motion processing. This immediately allows our approach to serve as a style database enhancement technique to fill-in non-existent actions in order to increase previous style transfer method's performance. We evaluate our method both via quantitative experiments, and through administering controlled user studies with respect to previous work, where significant improvement is observed with our approach.",Sspsttrfohumobeinac,47.0,29.0,4.0
8368,Style Transfer,34.0,neural style transfer: a paradigm shift for image-based artistic rendering?,5.0,201.0,1.0,185.0,3.0,2.8,146.10000000000002,78,https://hal.inria.fr/hal-01527495/document,"In this meta paper we discuss image-based artistic rendering (IB-AR) based on neural style transfer (NST) and argue, while NST may represent a paradigm shift for IB-AR, that it also has to evolve as an interactive tool that considers the design aspects and mechanisms of artwork production. IB-AR received significant attention in the past decades for visual communication, covering a plethora of techniques to mimic the appeal of artistic media. Example-based rendering represents one the most promising paradigms in IB-AR to (semi-)automatically simulate artistic media with high fidelity, but so far has been limited because it relies on pre-defined image pairs for training or informs only low-level image features for texture transfers. Advancements in deep learning showed to alleviate these limitations by matching content and style statistics via activations of neural network layers, thus making a generalized style transfer practicable. We categorize style transfers within the taxonomy of IB-AR, then propose a semiotic structure to derive a technical research agenda for NSTs with respect to the grand challenges of NPAR. We finally discuss the potentials of NSTs, thereby identifying applications such as casual creativity and art production.",Snesttrapashfoimarre,37.0,119.0,0.0
8371,Style Transfer,401.0,real-time monocular depth estimation using synthetic data with domain adaptation via image style transfer,1.0,80.0,4.0,115.0,3.0,2.8,186.8,79,http://arxiv.org/pdf/1907.06882v2,"Monocular depth estimation using learning-based approaches has become promising in recent years. However, most monocular depth estimators either need to rely on large quantities of ground truth depth data, which is extremely expensive and difficult to obtain, or predict disparity as an intermediary step using a secondary supervisory signal leading to blurring and other artefacts. Training a depth estimation model using pixel-perfect synthetic data can resolve most of these issues but introduces the problem of domain bias. This is the inability to apply a model trained on synthetic data to real-world scenarios. With advances in image style transfer and its connections with domain adaptation (Maximum Mean Discrepancy), we take advantage of style transfer and adversarial training to predict pixel perfect depth from a single real-world color image based on training over a large corpus of synthetic environment data. Experimental results indicate the efficacy of our approach compared to contemporary state-of-the-art techniques.",Sremodeesussydawidoadviimsttr,168.0,92.0,13.0
8372,Style Transfer,401.0,"style tokens: unsupervised style modeling, control and transfer in end-to-end speech synthesis",1.0,44.0,4.0,184.0,3.0,2.8,193.1,80,http://arxiv.org/pdf/1803.09017v1,"In this work, we propose ""global style tokens"" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable ""labels"" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",Ssttounstmocoantrinenspsy,323.0,30.0,50.0
8373,Style Transfer,401.0,semantic style transfer and turning two-bit doodles into fine artworks,1.0,100.0,4.0,163.0,3.0,2.8,209.2,81,http://arxiv.org/pdf/1603.01768v1,"Convolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms---whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings!",Ssesttrantutwdoinfiar,171.0,8.0,17.0
8374,Style Transfer,62.0,domain-aware universal style transfer,4.0,126.0,3.0,201.0,1.0,2.7,129.3,82,https://openaccess.thecvf.com/content/ICCV2021/papers/Hong_Domain-Aware_Universal_Style_Transfer_ICCV_2021_paper.pdf,Style transfer aims to transfer arbitrary visual styles to content images. We explore algorithms adapted from two papers that try to solve the problem of style transfer while generalizing on unseen styles or compromised visual quality. Majority of the improvements made focus on optimizing the algorithm for real-time style transfer while adapting to new styles with considerably less resources and constraints. We compare these strategies and compare how they measure up to produce visually appealing images. We explore two approaches to style transfer: neural style transfer with improvements and universal style transfer. We also make a comparison between the different images produced and how they can be qualitatively measured.,Sdounsttr,0.0,37.0,0.0
8375,Style Transfer,44.0,consistent video style transfer via relaxation and regularization,4.0,171.0,3.0,201.0,1.0,2.7,141.9,83,http://arxiv.org/pdf/2109.11418v1,"We present a method that decomposes, or ""unwraps"", an input video into a set of layered 2D atlases, each providing a unified representation of the appearance of an object (or background) over the video. For each pixel in the video, our method estimates its corresponding 2D coordinate in each of the atlases, giving us a consistent parameterization of the video, along with an associated alpha (opacity) value. Importantly, we design our atlases to be interpretable and semantic, which facilitates easy and intuitive editing in the atlas domain, with minimal manual work required. Edits applied to a single 2D atlas (or input video frame) are automatically and consistently mapped back to the original video frames, while preserving occlusions, deformation, and other complex scene effects such as shadows and reflections. Our method employs a coordinate-based Multilayer Perceptron (MLP) representation for mappings, atlases, and alphas, which are jointly optimized on a per-video basis, using a combination of video reconstruction and regularization losses. By operating purely in 2D, our method does not require any prior 3D knowledge about scene geometry or camera poses, and can handle complex dynamic real world videos. We demonstrate various video editing applications, including texture mapping, video style transfer, image-to-video texture transfer, and segmentation/labeling propagation, all automatically produced by editing a single 2D atlas image.",Scovisttrvireanre,4.0,50.0,1.0
8376,Style Transfer,401.0,"dear sir or madam, may i introduce the gyafc dataset: corpus, benchmarks and metrics for formality style transfer",1.0,108.0,3.0,76.0,4.0,2.7,186.3,84,http://arxiv.org/pdf/1803.06535v2,"Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics.",Sdesiormamaiinthgydacobeanmefofosttr,152.0,44.0,29.0
8377,Style Transfer,401.0,domain adaptive text style transfer,1.0,122.0,3.0,73.0,4.0,2.7,191.0,85,http://arxiv.org/pdf/1908.09395v1,"Text style transfer without parallel data has achieved some practical success. However, in the scenario where less data is available, these methods may yield poor performance. In this paper, we examine domain adaptation for text style transfer to leverage massively available data from other domains. These data may demonstrate domain shift, which impedes the benefits of utilizing such data for training. To address this challenge, we propose simple yet effective domain adaptive text style transfer models, enabling domain-adaptive information exchange. The proposed models presumably learn from the source domain to: (i) distinguish stylized information and generic content information; (ii) maximally preserve content information; and (iii) adaptively transfer the styles in a domain-aware manner. We evaluate the proposed models on two style transfer tasks (sentiment and formality) over multiple target domains where only limited non-parallel data is available. Extensive experiments demonstrate the effectiveness of the proposed model compared to the baselines.",Sdoadtesttr,23.0,43.0,4.0
8378,Style Transfer,401.0,a closed-form solution to universal style transfer,1.0,161.0,3.0,75.0,4.0,2.7,207.2,86,http://arxiv.org/pdf/1806.00868v1,"Universal style transfer tries to explicitly minimize the losses in feature space, thus it does not require training on any pre-defined styles. It usually uses different layers of VGG network as the encoders and trains several decoders to invert the features into images. Therefore, the effect of style transfer is achieved by feature transform. Although plenty of methods have been proposed, a theoretical analysis of feature transform is still missing. In this paper, we first propose a novel interpretation by treating it as the optimal transport problem. Then, we demonstrate the relations of our formulation with former works like Adaptive Instance Normalization (AdaIN) and Whitening and Coloring Transform (WCT). Finally, we derive a closed-form solution named Optimal Style Transfer (OST) under our formulation by additionally considering the content loss of Gatys. Comparatively, our solution can preserve better structure and achieve visually pleasing results. It is simple yet effective and we demonstrate its advantages both quantitatively and qualitatively. Besides, we hope our theoretical analysis can inspire future works in neural style transfer.",Saclsotounsttr,25.0,38.0,3.0
8379,Style Transfer,401.0,multimodal style transfer via graph cuts,1.0,152.0,3.0,90.0,4.0,2.7,208.1,87,http://arxiv.org/pdf/1904.04443v6,"An assumption widely used in recent neural style transfer methods is that image styles can be described by global statics of deep features like Gram or covariance matrices. Alternative approaches have represented styles by decomposing them into local pixel or neural patches. Despite the recent progress, most existing methods treat the semantic patterns of style image uniformly, resulting unpleasing results on complex styles. In this paper, we introduce a more flexible and general universal style transfer technique: multimodal style transfer (MST). MST explicitly considers the matching of semantic patterns in content and style images. Specifically, the style image features are clustered into sub-style components, which are matched with local content features under a graph cut formulation. A reconstruction network is trained to transfer each sub-style and render the final stylized result. We also generalize MST to improve some existing methods. Extensive experiments demonstrate the superior effectiveness, robustness, and flexibility of MST.",Smusttrvigrcu,19.0,45.0,1.0
8380,Style Transfer,401.0,"style transfer for texts: retrain, report errors, compare with rewrites",1.0,165.0,3.0,87.0,4.0,2.7,212.4,88,http://arxiv.org/abs/1908.06809v2,"This paper shows that standard assessment methodology for style transfer has several significant problems. First, the standard metrics for style accuracy and semantics preservation vary significantly on different re-runs. Therefore one has to report error margins for the obtained results. Second, starting with certain values of bilingual evaluation understudy (BLEU) between input and output and accuracy of the sentiment transfer the optimization of these two standard metrics diverge from the intuitive goal of the style transfer task. Finally, due to the nature of the task itself, there is a specific dependence between these two metrics that could be easily manipulated. Under these circumstances, we suggest taking BLEU between input and human-written reformulations into consideration for benchmarks. We also propose three new architectures that outperform state of the art in terms of this metric.",Ssttrfoterereercowire,17.0,44.0,4.0
8381,Style Transfer,401.0,a content transformation block for image style transfer,1.0,200.0,3.0,81.0,4.0,2.7,224.6,89,http://arxiv.org/pdf/2003.08407v1,"Style transfer has recently received a lot of attention, since it allows to study fundamental challenges in image understanding and synthesis. Recent work has significantly improved the representation of color and texture and com- putational speed and image resolution. The explicit transformation of image content has, however, been mostly neglected: while artistic style affects formal characteristics of an image, such as color, shape or texture, it also deforms, adds or removes content details. This paper explicitly focuses on a content-and style-aware stylization of a content image. Therefore, we introduce a content transformation module between the encoder and decoder. Moreover, we utilize similar content appearing in photographs and style samples to learn how style alters content details and we generalize this to other class details. Additionally, this work presents a novel normalization layer critical for high resolution image synthesis. The robustness and speed of our model enables a video stylization in real-time and high definition. We perform extensive qualitative and quantitative evaluations to demonstrate the validity of our approach.",Sacotrblfoimsttr,22.0,37.0,5.0
8382,Style Transfer,401.0,unpaired image-to-image translation using cycle-consistent adversarial networks,1.0,1.0,5.0,201.0,1.0,2.6,181.0,90,http://arxiv.org/pdf/1903.06399v1,"Generative Adversarial Networks (GANs) have been widely used for the image-to-image translation task. While these models rely heavily on the labeled image pairs, recently some GAN variants have been proposed to tackle the unpaired image translation task. These models exploited supervision at the domain level with a reconstruction process for unpaired image translation. On the other hand, parallel works have shown that leveraging perceptual loss functions based on high level deep features could enhance the generated image quality. Nevertheless, as these GAN-based models either depended on the pretrained deep network structure or relied on the labeled image pairs, they could not be directly applied to the unpaired image translation task. Moreover, despite the improvement of the introduced perceptual losses from deep neural networks, few researchers have explored the possibility of improving the generated image quality from classical image quality measures. To tackle the above two challenges, in this paper, we propose a unified quality-aware GAN-based framework for unpaired image-to-image translation, where a quality-aware loss is explicitly incorporated by comparing each source image and the reconstructed image at the domain level. Specifically, we design two detailed implementations of the quality loss. The first method is based on a classical image quality assessment measure by defining a classical quality-aware loss. The second method proposes an adaptive deep network based loss. Finally, extensive experimental results on many real-world datasets clearly show the quality improvement of our proposed framework, and the superiority of leveraging classical image quality measures for unpaired image translation compared to the deep network based model.",Sunimtruscyadne,3358.0,83.0,348.0
8383,Style Transfer,401.0,politeness transfer: a tag and generate approach,1.0,2.0,5.0,201.0,1.0,2.6,181.4,91,http://arxiv.org/pdf/1601.03313v2,"In this report we present a system that can generate political speeches for a desired political party. Furthermore, the system allows to specify whether a speech should hold a supportive or opposing opinion. The system relies on a combination of several state-of-the-art NLP methods which are discussed in this report. These include n-grams, Justeson & Katz POS tag filter, recurrent neural networks, and latent Dirichlet allocation. Sequences of words are generated based on probabilities obtained from two underlying models: A language model takes care of the grammatical correctness while a topic model aims for textual consistency. Both models were trained on the Convote dataset which contains transcripts from US congressional floor debates. Furthermore, we present a manual and an automated approach to evaluate the quality of generated speeches. In an experimental evaluation generated speeches have shown very high quality in terms of grammatical correctness and sentence transitions.",Spotrataangeap,37.0,46.0,4.0
8384,Style Transfer,401.0,a neural algorithm of artistic style,1.0,3.0,5.0,201.0,1.0,2.6,181.8,92,http://arxiv.org/pdf/1508.06576v2,"In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",Sanealofarst,1663.0,27.0,200.0
8385,Style Transfer,401.0,"exploring the structure of a real-time, arbitrary neural artistic stylization network",1.0,4.0,5.0,201.0,1.0,2.6,182.2,93,http://arxiv.org/pdf/1705.06830v2,"In this paper, we present a method which combines the flexibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image. The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved. We demonstrate that the learned embedding space is smooth and contains a rich structure and organizes semantic information associated with paintings in an entirely unsupervised manner.",Sexthstofarearnearstne,152.0,28.0,4.0
8386,Style Transfer,401.0,instance normalization: the missing ingredient for fast stylization,1.0,7.0,5.0,201.0,1.0,2.6,183.4,94,http://arxiv.org/pdf/1607.08022v3,"It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.",Sinnothmiinfofast,1825.0,5.0,106.0
8387,Style Transfer,401.0,deep feature consistent variational autoencoder,1.0,12.0,5.0,201.0,1.0,2.6,185.4,95,http://arxiv.org/pdf/1610.00291v1,"We present a novel method for constructing Variational Autoencoder (VAE). Instead of using pixel-by-pixel loss, we enforce deep feature consistency between the input and the output of a VAE, which ensures the VAE's output to preserve the spatial correlation characteristics of the input, thus leading the output to have a more natural visual appearance and better perceptual quality. Based on recent deep learning works such as style transfer, we employ a pre-trained deep convolutional neural network (CNN) and use its hidden features to define a feature perceptual loss for VAE training. Evaluated on the CelebA face dataset, we show that our model produces better results than other methods in the literature. We also show that our method can produce latent vectors that can capture the semantic information of face expressions and can be used to achieve state-of-the-art performance in facial attribute prediction.",Sdefecovaau,193.0,36.0,14.0
8388,Style Transfer,401.0,methods for detoxification of texts for the russian language,1.0,17.0,5.0,201.0,1.0,2.6,187.4,96,http://arxiv.org/pdf/2105.09052v1,"We introduce the first study of automatic detoxification of Russian texts to combat offensive language. Such a kind of textual style transfer can be used, for instance, for processing toxic content in social media. While much work has been done for the English language in this field, it has never been solved for the Russian language yet. We test two types of models - unsupervised approach based on BERT architecture that performs local corrections and supervised approach based on pretrained language GPT-2 model - and compare them with several baselines. In addition, we describe evaluation setup providing training datasets and metrics for automatic evaluation. The results show that the tested approaches can be successfully used for detoxification, although there is room for improvement.",Smefodeoftefothrula,1.0,52.0,0.0
8389,Style Transfer,401.0,text style transfer: a review and experimental evaluation,1.0,19.0,5.0,201.0,1.0,2.6,188.2,97,http://arxiv.org/pdf/1904.02295v1,"Research in the area of style transfer for text is currently bottlenecked by a lack of standard evaluation practices. This paper aims to alleviate this issue by experimentally identifying best practices with a Yelp sentiment dataset. We specify three aspects of interest (style transfer intensity, content preservation, and naturalness) and show how to obtain more reliable measures of them from human evaluation than in previous work. We propose a set of metrics for automated evaluation and demonstrate that they are more strongly correlated and in agreement with human judgment: direction-corrected Earth Mover's Distance, Word Mover's Distance on style-masked texts, and adversarial classification for the respective aspects. We also show that the three examined models exhibit tradeoffs between aspects of interest, demonstrating the importance of evaluating style transfer models at specific points of their tradeoff plots. We release software with our evaluation metrics to facilitate research.",Stesttrareanexev,7.0,93.0,0.0
8390,Style Transfer,401.0,texture networks: feed-forward synthesis of textures and stylized images,1.0,20.0,5.0,201.0,1.0,2.6,188.6,98,http://arxiv.org/pdf/1603.03417v1,"Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.",Stenefesyofteanstim,602.0,20.0,56.0
8391,Style Transfer,401.0,stylized neural painting,1.0,22.0,5.0,201.0,1.0,2.6,189.4,99,http://arxiv.org/pdf/2011.08114v1,"This paper proposes an image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles. Different from previous image-to-image translation methods that formulate the translation as pixel-wise prediction, we deal with such an artistic creation process in a vectorized environment and produce a sequence of physically meaningful stroke parameters that can be further used for rendering. Since a typical vector render is not differentiable, we design a novel neural renderer which imitates the behavior of the vector renderer and then frame the stroke prediction as a parameter searching process that maximizes the similarity between the input and the rendering output. We explored the zero-gradient problem on parameter searching and propose to solve this problem from an optimal transportation perspective. We also show that previous neural renderers have a parameter coupling problem and we re-design the rendering network with a rasterization network and a shading network that better handles the disentanglement of shape and color. Experiments show that the paintings generated by our method have a high degree of fidelity in both global appearance and local textures. Our method can be also jointly optimized with neural style transfer that further transfers visual style from other images. Our code and animated results are available at \url{https://jiupinjia.github.io/neuralpainter/}.",Sstnepa,1.0,43.0,1.0
8392,Style Transfer,401.0,stylegan2 distillation for feed-forward image manipulation,1.0,24.0,5.0,201.0,1.0,2.6,190.2,100,http://arxiv.org/pdf/2003.03581v2,"StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces' transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.",Sstdifofeimma,27.0,66.0,1.0
8675,Super-Resolution,34.0,residual dense network for image super-resolution,5.0,12.0,5.0,9.0,5.0,5.0,17.7,1,http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Residual_Dense_Network_CVPR_2018_paper.pdf,"A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.",Sredenefoimsu,1323.0,47.0,264.0
8676,Super-Resolution,40.0,esrgan: enhanced super-resolution generative adversarial networks,5.0,6.0,5.0,17.0,5.0,5.0,19.5,2,http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf,"The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.",Sesensugeadne,1067.0,53.0,240.0
8677,Super-Resolution,71.0,image super-resolution using deep convolutional networks,4.0,2.0,5.0,2.0,5.0,4.7,22.700000000000003,3,https://arxiv.org/pdf/1501.00092,"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.",Simsuusdecone,4197.0,72.0,653.0
8678,Super-Resolution,69.0,deep back-projection networks for super-resolution,4.0,19.0,5.0,30.0,5.0,4.7,37.3,4,http://openaccess.thecvf.com/content_cvpr_2018/papers/Haris_Deep_Back-Projection_Networks_CVPR_2018_paper.pdf,"The feed-forward architectures of recently proposed deep super-resolution networks learn representations of low-resolution inputs, and the non-linear mapping from those to high-resolution output. However, this approach does not fully address the mutual dependencies of low- and high-resolution images. We propose Deep Back-Projection Networks (DBPN), that exploit iterative up- and downsampling layers, providing an error feedback mechanism for projection errors at each stage. We construct mutually-connected up- and down-sampling stages each of which represents different types of image degradation and high-resolution components. We show that extending this idea to allow concatenation of features across up- and downsampling stages (Dense DBPN) allows us to reconstruct further improve super-resolution, yielding superior results and in particular establishing new state of the art results for large scaling factors such as 8Ã— across multiple data sets.",Sdebanefosu,692.0,54.0,121.0
8679,Super-Resolution,65.0,srflow: learning the super-resolution space with normalizing flow,4.0,37.0,5.0,21.0,5.0,4.7,40.6,5,https://arxiv.org/pdf/2006.14200,"Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions.",Ssrlethsuspwinofl,63.0,65.0,14.0
8680,Super-Resolution,23.0,feedback network for image super-resolution,5.0,50.0,4.0,16.0,5.0,4.6,31.7,6,https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Feedback_Network_for_Image_Super-Resolution_CVPR_2019_paper.pdf,"Recent advances in image super-resolution (SR) explored the power of deep learning to achieve a better reconstruction performance. However, the feedback mechanism, which commonly exists in human visual system, has not been fully exploited in existing deep learning based image SR methods. In this paper, we propose an image super-resolution feedback network (SRFBN) to refine low-level representations with high-level information. Specifically, we use hidden states in a recurrent neural network (RNN) with constraints to achieve such feedback manner. A feedback block is designed to handle the feedback connections and to generate powerful high-level representations. The proposed SRFBN comes with a strong early reconstruction ability and can create the final high-resolution image step by step. In addition, we introduce a curriculum learning strategy to make the network well suitable for more complicated tasks, where the low-resolution images are corrupted by multiple types of degradation. Extensive experimental results demonstrate the superiority of the proposed SRFBN in comparison with the state-of-the-art methods. Code is avaliable at https://github.com/Paper99/SRFBN_CVPR19.",Sfenefoimsu,282.0,56.0,56.0
8681,Super-Resolution,124.0,perceptual losses for real-time style transfer and super-resolution,3.0,5.0,5.0,3.0,5.0,4.4,40.1,7,http://arxiv.org/pdf/1603.08155v1,"We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",Speloforesttransu,5173.0,73.0,453.0
8682,Super-Resolution,144.0,photo-realistic single image super-resolution using a generative adversarial network,3.0,1.0,5.0,1.0,5.0,4.4,43.89999999999999,8,https://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf,"Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",Sphsiimsuusageadne,5444.0,83.0,663.0
8683,Super-Resolution,117.0,enhanced deep residual networks for single image super-resolution,3.0,20.0,5.0,4.0,5.0,4.4,44.3,9,http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf,"Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks. The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge[26].",Senderenefosiimsu,2288.0,40.0,555.0
8684,Super-Resolution,83.0,wide activation for efficient and accurate image super-resolution,4.0,23.0,5.0,93.0,4.0,4.4,62.0,10,https://arxiv.org/pdf/1808.08718,"In this report we demonstrate that with same parameters and computational budgets, models with wider features before ReLU activation have significantly better performance for single image super-resolution (SISR). The resulted SR residual network has a slim identity mapping pathway with wider (\(2\times\) to \(4\times\)) channels before activation in each residual block. To further widen activation (\(6\times\) to \(9\times\)) without computational overhead, we introduce linear low-rank convolution into SR networks and achieve even better accuracy-efficiency tradeoffs. In addition, compared with batch normalization or no normalization, we find training with weight normalization leads to better accuracy for deep super-resolution networks. Our proposed SR network \textit{WDSR} achieves better results on large-scale DIV2K image super-resolution benchmark in terms of PSNR with same or lower computational complexity. Based on WDSR, our method also won 1st places in NTIRE 2018 Challenge on Single Image Super-Resolution in all three realistic tracks. Experiments and ablation studies support the importance of wide activation for image super-resolution. Code is released at: this https URL",Swiacfoefanacimsu,189.0,41.0,28.0
8685,Super-Resolution,196.0,image super-resolution using very deep residual channel attention networks,3.0,25.0,5.0,8.0,5.0,4.4,71.2,11,https://openaccess.thecvf.com/content_ECCV_2018/papers/Yulun_Zhang_Image_Super-Resolution_Using_ECCV_2018_paper.pdf,"Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.",Simsuusvederechatne,1354.0,47.0,308.0
8686,Super-Resolution,82.0,deep unfolding network for image super-resolution,4.0,45.0,4.0,15.0,5.0,4.3,47.1,12,http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Deep_Unfolding_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf,"Learning-based single image super-resolution (SISR) methods are continuously showing superior effectiveness and efficiency over traditional model-based methods, largely due to the end-to-end training. However, different from model-based methods that can handle the SISR problem with different scale factors, blur kernels and noise levels under a unified MAP (maximum a posteriori) framework, learning-based methods generally lack such flexibility. To address this issue, this paper proposes an end-to-end trainable unfolding network which leverages both learningbased methods and model-based methods. Specifically, by unfolding the MAP inference via a half-quadratic splitting algorithm, a fixed number of iterations consisting of alternately solving a data subproblem and a prior subproblem can be obtained. The two subproblems then can be solved with neural modules, resulting in an end-to-end trainable, iterative network. As a result, the proposed network inherits the flexibility of model-based methods to super-resolve blurry, noisy images for different scale factors via a single model, while maintaining the advantages of learning-based methods. Extensive experiments demonstrate the superiority of the proposed deep unfolding network in terms of flexibility, effectiveness and also generalizability.",Sdeunnefoimsu,108.0,75.0,16.0
8687,Super-Resolution,64.0,learning a single convolutional super-resolution network for multiple degradations,4.0,65.0,4.0,39.0,5.0,4.3,56.900000000000006,13,https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_a_Single_CVPR_2018_paper.pdf,"Recent years have witnessed the unprecedented success of deep convolutional neural networks (CNNs) in single image super-resolution (SISR). However, existing CNN-based SISR methods mostly assume that a low-resolution (LR) image is bicubicly downsampled from a high-resolution (HR) image, thus inevitably giving rise to poor performance when the true degradation does not follow this assumption. Moreover, they lack scalability in learning a single model to nonblindly deal with multiple degradations. To address these issues, we propose a general framework with dimensionality stretching strategy that enables a single convolutional super-resolution network to take two key factors of the SISR degradation process, i.e., blur kernel and noise level, as input. Consequently, the super-resolver can handle multiple and even spatially variant degradations, which significantly improves the practicability. Extensive experimental results on synthetic and real LR images show that the proposed convolutional super-resolution network not only can produce favorable results on multiple degradations but also is computationally efficient, providing a highly effective and scalable solution to practical SISR applications.",Sleasicosunefomude,417.0,61.0,76.0
8688,Super-Resolution,29.0,deep learning for image super-resolution: a survey,5.0,81.0,4.0,95.0,4.0,4.3,69.6,14,https://arxiv.org/pdf/1902.06068,"Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.",Sdelefoimsuasu,368.0,239.0,19.0
8689,Super-Resolution,27.0,accelerating the super-resolution convolutional neural network,5.0,191.0,3.0,18.0,5.0,4.2,89.9,15,http://arxiv.org/pdf/2003.11996v1,"As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.",Sacthsuconene,1398.0,30.0,208.0
8690,Super-Resolution,174.0,recovering realistic texture in image super-resolution by deep spatial feature transform,3.0,15.0,5.0,68.0,4.0,4.1,78.6,16,https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Recovering_Realistic_Texture_CVPR_2018_paper.pdf,"Despite that convolutional neural networks (CNN) have recently demonstrated high-quality reconstruction for single-image super-resolution (SR), recovering natural and realistic texture remains a challenging problem. In this paper, we show that it is possible to recover textures faithful to semantic classes. In particular, we only need to modulate features of a few intermediate layers in a single network conditioned on semantic segmentation probability maps. This is made possible through a novel Spatial Feature Transform (SFT) layer that generates affine transformation parameters for spatial-wise feature modulation. SFT layers can be trained end-to-end together with the SR network using the same loss function. During testing, it accepts an input image of arbitrary size and generates a high-resolution image with just a single forward pass conditioned on the categorical priors. Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27] and EnhanceNet [38].",Srereteinimsubydespfetr,325.0,66.0,34.0
8691,Super-Resolution,111.0,learning texture transformer network for image super-resolution,3.0,47.0,4.0,13.0,5.0,4.0,55.99999999999999,17,http://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Learning_Texture_Transformer_Network_for_Image_Super-Resolution_CVPR_2020_paper.pdf,"We study on image super-resolution (SR), which aims to recover realistic textures from a low-resolution (LR) image. Recent progress has been made by taking high-resolution images as references (Ref), so that relevant textures can be transferred to LR images. However, existing SR approaches neglect to use attention mechanisms to transfer high-resolution (HR) textures from Ref images, which limits these approaches in challenging cases. In this paper, we propose a novel Texture Transformer Network for Image Super-Resolution (TTSR), in which the LR and Ref images are formulated as queries and keys in a transformer, respectively. TTSR consists of four closely-related modules optimized for image generation tasks, including a learnable texture extractor by DNN, a relevance embedding module, a hard-attention module for texture transfer, and a soft-attention module for texture synthesis. Such a design encourages joint feature learning across LR and Ref images, in which deep feature correspondences can be discovered by attention, and thus accurate texture features can be transferred. The proposed texture transformer can be further stacked in a cross-scale way, which enables texture recovery from different levels (e.g., from 1x to 4x magnification). Extensive experiments show that TTSR achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",Sletetrnefoimsu,110.0,49.0,9.0
8692,Super-Resolution,123.0,second-order attention network for single image super-resolution,3.0,54.0,4.0,14.0,5.0,4.0,62.7,18,http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf,"Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel train- able second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality.",Sseatnefosiimsu,378.0,47.0,67.0
8693,Super-Resolution,46.0,multi-scale residual network for image super-resolution,4.0,95.0,4.0,46.0,4.0,4.0,65.6,19,http://openaccess.thecvf.com/content_ECCV_2018/papers/Juncheng_Li_Multi-scale_Residual_Network_ECCV_2018_paper.pdf,"Recent studies have shown that deep neural networks can significantly improve the quality of single-image super-resolution. Current researches tend to use deeper convolutional neural networks to enhance performance. However, blindly increasing the depth of the network cannot ameliorate the network effectively. Worse still, with the depth of the network increases, more problems occurred in the training process and more training tricks are needed. In this paper, we propose a novel multi-scale residual network (MSRN) to fully exploit the image features, which outperform most of the state-of-the-art methods. Based on the residual block, we introduce convolution kernels of different sizes to adaptively detect the image features in different scales. Meanwhile, we let these features interact with each other to get the most efficacious image information, we call this structure Multi-scale Residual Block (MSRB). Furthermore, the outputs of each MSRB are used as the hierarchical features for global feature fusion. Finally, all these features are sent to the reconstruction module for recovering the high-quality image.",Smurenefoimsu,254.0,25.0,53.0
8694,Super-Resolution,148.0,structure-preserving super resolution with gradient guidance,3.0,68.0,4.0,24.0,5.0,4.0,78.8,20,http://openaccess.thecvf.com/content_CVPR_2020/papers/Ma_Structure-Preserving_Super_Resolution_With_Gradient_Guidance_CVPR_2020_paper.pdf,"Structures matter in single image super resolution (SISR). Recent studies benefiting from generative adversarial network (GAN) have promoted the development of SISR by recovering photo-realistic images. However, there are always undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super resolution method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Specifically, we exploit gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss which imposes a second-order restriction on the super-resolved images. Along with the previous image-space loss functions, the gradient-space objectives help generative networks concentrate more on geometric structures. Moreover, our method is model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results show that we achieve the best PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with state-of-the-art perceptual-driven SR methods. Visual results demonstrate our superiority in restoring structures while generating natural SR images.",Sstsurewigrgu,54.0,55.0,9.0
8695,Super-Resolution,184.0,real-world super-resolution via kernel estimation and noise injection,3.0,48.0,4.0,26.0,5.0,4.0,82.2,21,http://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Ji_Real-World_Super-Resolution_via_Kernel_Estimation_and_Noise_Injection_CVPRW_2020_paper.pdf,"Recent state-of-the-art super-resolution methods have achieved impressive performance on ideal datasets regardless of blur and noise. However, these methods always fail in real-world image super-resolution, since most of them adopt simple bicubic downsampling from high-quality images to construct Low-Resolution (LR) and HighResolution (HR) pairs for training which may lose track of frequency-related details. To address this issue, we focus on designing a novel degradation framework for real- world images by estimating various blur kernels as well as real noise distributions. Based on our novel degradation framework, we can acquire LR images sharing a common domain with real-world images. Then, we propose a real- world super-resolution model aiming at better perception. Extensive experiments on synthetic noise data and real- world images demonstrate that our method outperforms the state-of-the-art methods, resulting in lower noise and better visual quality. In addition, our method is the winner of NTIRE 2020 Challenge on both tracks of Real-World Super-Resolution, which significantly outperforms other competitors by large margins.",Sresuvikeesannoin,44.0,50.0,12.0
8696,Super-Resolution,96.0,detail-revealing deep video super-resolution,4.0,91.0,4.0,62.0,4.0,4.0,83.79999999999998,22,http://openaccess.thecvf.com/content_ICCV_2017/papers/Tao_Detail-Revealing_Deep_Video_ICCV_2017_paper.pdf,"Previous CNN-based video super-resolution approaches need to align multiple frames to the reference. In this paper, we show that proper frame alignment and motion compensation is crucial for achieving high quality results. We accordingly propose a “sub-pixel motion compensation” (SPMC) layer in a CNN framework. Analysis and experiments show the suitability of this layer in video SR. The final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal image details. Our implementation can generate visually and quantitatively high-quality results, superior to current state-of-the-arts, without the need of parameter tuning.",Sdedevisu,260.0,43.0,50.0
8697,Super-Resolution,99.0,ranksrgan: generative adversarial networks with ranker for image super-resolution,4.0,97.0,4.0,61.0,4.0,4.0,86.8,23,https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_RankSRGAN_Generative_Adversarial_Networks_With_Ranker_for_Image_Super-Resolution_ICCV_2019_paper.pdf,"Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: https://wenlongzhang0724.github.io/Projects/RankSRGAN",Srageadnewirafoimsu,113.0,50.0,19.0
8698,Super-Resolution,87.0,image super-resolution via deep recursive residual network,4.0,102.0,3.0,29.0,5.0,3.9,75.60000000000001,24,http://openaccess.thecvf.com/content_cvpr_2017/papers/Tai_Image_Super-Resolution_via_CVPR_2017_paper.pdf,"Recently, Convolutional Neural Network (CNN) based models have achieved great success in Single Image Super-Resolution (SISR). Owing to the strength of deep networks, these CNN models learn an effective nonlinear mapping from the low-resolution input image to the high-resolution target image, at the cost of requiring enormous parameters. This paper proposes a very deep CNN model (up to 52 convolutional layers) named Deep Recursive Residual Network (DRRN) that strives for deep yet concise networks. Specifically, residual learning is adopted, both in global and local manners, to mitigate the difficulty of training very deep networks, recursive learning is used to control the model parameters while increasing the depth. Extensive benchmark evaluation shows that DRRN significantly outperforms state of the art in SISR, while utilizing far fewer parameters. Code is available at https://github.com/tyshiwo/DRRN_CVPR17.",Simsuvidererene,1055.0,39.0,170.0
8699,Super-Resolution,88.0,deeply-recursive convolutional network for image super-resolution,4.0,168.0,3.0,19.0,5.0,3.9,99.3,25,http://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Deeply-Recursive_Convolutional_Network_CVPR_2016_paper.pdf,"We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/ vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.",Sdeconefoimsu,1456.0,34.0,189.0
8700,Super-Resolution,401.0,real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network,1.0,4.0,5.0,5.0,5.0,3.8,123.4,26,https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shi_Real-Time_Single_Image_CVPR_2016_paper.pdf,"Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",Sresiimanvisuusanefsuconene,2425.0,59.0,247.0
8701,Super-Resolution,141.0,meta-sr: a magnification-arbitrary network for super-resolution,3.0,56.0,4.0,59.0,4.0,3.7,82.4,27,https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Meta-SR_A_Magnification-Arbitrary_Network_for_Super-Resolution_CVPR_2019_paper.pdf,"Recent research on super-resolution has achieved greatsuccess due to the development of deep convolutional neu-ral networks (DCNNs). However, super-resolution of arbi-trary scale factor has been ignored for a long time. Mostprevious researchers regard super-resolution of differentscale factors as independent tasks. They train a specificmodel for each scale factor which is inefficient in comput-ing, and prior work only take the super-resolution of sev-eral integer scale factors into consideration. In this work,we propose a novel method called Meta-SR to firstly solvesuper-resolution of arbitrary scale factor (including non-integer scale factors) with a single model. In our Meta-SR,the Meta-Upscale Module is proposed to replace the tradi-tional upscale module. For arbitrary scale factor, the Meta-Upscale Module dynamically predicts the weights of the up-scale filters by taking the scale factor as input and use theseweights to generate the HR image of arbitrary size. For anylow-resolution image, our Meta-SR can continuously zoomin it with arbitrary scale factor by only using a single model.We evaluated the proposed method through extensive exper-iments on widely used benchmark datasets on single imagesuper-resolution. The experimental results show the superi-ority of our Meta-Upscale.",Smeamanefosu,124.0,38.0,24.0
8702,Super-Resolution,132.0,image super-resolution by neural texture transfer,3.0,61.0,4.0,63.0,4.0,3.7,82.9,28,https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Image_Super-Resolution_by_Neural_Texture_Transfer_CVPR_2019_paper.pdf,"Due to the significant information loss in low-resolution (LR) images, it has become extremely challenging to further advance the state-of-the-art of single image super-resolution (SISR). Reference-based super-resolution (RefSR), on the other hand, has proven to be promising in recovering high-resolution (HR) details when a reference (Ref) image with similar content as that of the LR input is given. However, the quality of RefSR can degrade severely when Ref is less similar. This paper aims to unleash the potential of RefSR by leveraging more texture details from Ref images with stronger robustness even when irrelevant Ref images are provided. Inspired by the recent work on image stylization, we formulate the RefSR problem as neural texture transfer. We design an end-to-end deep model which enriches HR details by adaptively transferring the texture from Ref images according to their textural similarity. Instead of matching content in the raw pixel space as done by previous methods, our key contribution is a multi-level matching conducted in the neural space. This matching scheme facilitates multi-scale neural transfer that allows the model to benefit more from those semantically related Ref patches, and gracefully degrade to SISR performance on the least relevant Ref inputs. We build a benchmark dataset for the general research of RefSR, which contains Ref images paired with LR inputs with varying levels of similarity. Both quantitative and qualitative evaluations demonstrate the superiority of our method over state-of-the-art.",Simsubynetetr,92.0,48.0,12.0
8703,Super-Resolution,180.0,recurrent back-projection network for video super-resolution,3.0,59.0,4.0,42.0,4.0,3.7,90.2,29,http://openaccess.thecvf.com/content_CVPR_2019/papers/Haris_Recurrent_Back-Projection_Network_for_Video_Super-Resolution_CVPR_2019_paper.pdf,"We proposed a novel architecture for the problem of video super-resolution. We integrate spatial and temporal contexts from continuous video frames using a recurrent encoder-decoder module, that fuses multi-frame information with the more traditional, single frame super-resolution path for the target frame. In contrast to most prior work where frames are pooled together by stacking or warping, our model, the Recurrent Back-Projection Network (RBPN) treats each context frame as a separate source of information. These sources are combined in an iterative refinement framework inspired by the idea of back-projection in multiple-image super-resolution. This is aided by explicitly representing estimated inter-frame motion with respect to the target, rather than explicitly aligning frames. We propose a new video super-resolution benchmark, allowing evaluation at a larger scale and considering videos in different motion regimes. Experimental results demonstrate that our RBPN is superior to existing methods on several datasets.",Srebanefovisu,134.0,41.0,33.0
8704,Super-Resolution,171.0,fsrnet: end-to-end learning face super-resolution with facial priors,3.0,94.0,4.0,74.0,4.0,3.7,111.1,30,http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_FSRNet_End-to-End_Learning_CVPR_2018_paper.pdf,"Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively.",Sfsenlefasuwifapr,216.0,53.0,38.0
8705,Super-Resolution,76.0,camera lens super-resolution,4.0,128.0,3.0,77.0,4.0,3.6000000000000005,97.1,31,http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Camera_Lens_Super-Resolution_CVPR_2019_paper.pdf,"Existing methods for single image super-resolution (SR) are typically evaluated with synthetic degradation models such as bicubic or Gaussian downsampling. In this paper, we investigate SR from the perspective of camera lenses, named as CameraSR, which aims to alleviate the intrinsic tradeoff between resolution (R) and field-of-view (V) in realistic imaging systems. Specifically, we view the R-V degradation as a latent model in the SR process and learn to reverse it with realistic low- and high-resolution image pairs. To obtain the paired images, we propose two novel data acquisition strategies for two representative imaging systems (i.e., DSLR and smartphone cameras), respectively. Based on the obtained City100 dataset, we quantitatively analyze the performance of commonly-used synthetic degradation models, and demonstrate the superiority of CameraSR as a practical solution to boost the performance of existing SR methods. Moreover, CameraSR can be readily generalized to different content and devices, which serves as an advanced digital zoom tool in realistic imaging systems.",Scalesu,55.0,38.0,9.0
8706,Super-Resolution,198.0,explorable super resolution,3.0,181.0,3.0,38.0,5.0,3.6,143.20000000000002,32,http://openaccess.thecvf.com/content_CVPR_2020/papers/Bahat_Explorable_Super_Resolution_CVPR_2020_paper.pdf,"Single image super resolution (SR) has seen major performance leaps in recent years. However, existing methods do not allow exploring the infinitely many plausible reconstructions that might have given rise to the observed low-resolution (LR) image. These different explanations to the LR image may dramatically vary in their textures and fine details, and may often encode completely different semantic information. In this paper, we introduce the task of explorable super resolution. We propose a framework comprising a graphical user interface with a neural network backend, allowing editing the SR output so as to explore the abundance of plausible HR explanations to the LR input. At the heart of our method is a novel module that can wrap any existing SR network, analytically guaranteeing that its SR outputs would precisely match the LR input, when down- sampled. Besides its importance in our setting, this module is guaranteed to decrease the reconstruction error of any SR network it wraps, and can be used to cope with blur kernels that are different from the one the network was trained for. We illustrate our approach in a variety of use cases, ranging from medical imaging and forensics, to graphics.",Sexsure,28.0,34.0,1.0
8707,Super-Resolution,401.0,deep plug-and-play super-resolution for arbitrary blur kernels,1.0,31.0,5.0,54.0,4.0,3.5,148.89999999999998,33,http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deep_Plug-And-Play_Super-Resolution_for_Arbitrary_Blur_Kernels_CVPR_2019_paper.pdf,"While deep neural networks (DNN) based single image super-resolution (SISR) methods are rapidly gaining popularity, they are mainly designed for the widely-used bicubic degradation, and there still remains the fundamental challenge for them to super-resolve low-resolution (LR) image with arbitrary blur kernels. In the meanwhile, plug-and-play image restoration has been recognized with high flexibility due to its modular structure for easy plug-in of denoiser priors. In this paper, we propose a principled formulation and framework by extending bicubic degradation based deep SISR with the help of plug-and-play framework to handle LR images with arbitrary blur kernels. Specifically, we design a new SISR degradation model so as to take advantage of existing blind deblurring methods for blur kernel estimation. To optimize the new degradation induced energy function, we then derive a plug-and-play algorithm via variable splitting technique, which allows us to plug any super-resolver prior rather than the denoiser prior as a modular part. Quantitative and qualitative evaluations on synthetic and real LR images demonstrate that the proposed deep plug-and-play super-resolution framework is flexible and effective to deal with blurry LR images.",Sdeplsufoarblke,148.0,79.0,20.0
8708,Super-Resolution,39.0,learning a deep convolutional network for image super-resolution,5.0,201.0,1.0,7.0,5.0,3.4,94.2,34,https://link.springer.com/content/pdf/10.1007/978-3-319-10593-2_13.pdf,"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.",Sleadeconefoimsu,2919.0,31.0,500.0
8709,Super-Resolution,114.0,blind super-resolution with iterative kernel correction,3.0,119.0,3.0,45.0,4.0,3.3,95.3,35,http://openaccess.thecvf.com/content_CVPR_2019/papers/Gu_Blind_Super-Resolution_With_Iterative_Kernel_Correction_CVPR_2019_paper.pdf,"Deep learning based methods have dominated super-resolution (SR) field due to their remarkable performance in terms of effectiveness and efficiency. Most of these methods assume that the blur kernel during downsampling is predefined/known (e.g., bicubic). However, the blur kernels involved in real applications are complicated and unknown, resulting in severe performance drop for the advanced SR methods. In this paper, we propose an Iterative Kernel Correction (IKC) method for blur kernel estimation in blind SR problem, where the blur kernels are unknown. We draw the observation that kernel mismatch could bring regular artifacts (either over-sharpening or over-smoothing), which can be applied to correct inaccurate blur kernels. Thus we introduce an iterative correction scheme -- IKC that achieves better results than direct kernel estimation. We further propose an effective SR network architecture using spatial feature transform (SFT) layers to handle multiple blur kernels, named SFTMD. Extensive experiments on synthetic and real-world images show that the proposed IKC method with SFTMD can provide visually favorable SR results and the state-of-the-art performance in blind SR problem.",Sblsuwiitkeco,123.0,42.0,30.0
8710,Super-Resolution,165.0,enhancenet: single image super-resolution through automated texture synthesis,3.0,135.0,3.0,52.0,4.0,3.3,119.1,36,http://openaccess.thecvf.com/content_ICCV_2017/papers/Sajjadi_EnhanceNet_Single_Image_ICCV_2017_paper.pdf,"Single image super-resolution is the task of inferring a high-resolution image from a single low-resolution input. Traditionally, the performance of algorithms for this task is measured using pixel-wise reconstruction measures such as peak signal-to-noise ratio (PSNR) which have been shown to correlate poorly with the human perception of image quality. As a result, algorithms minimizing these metrics tend to produce over-smoothed images that lack highfrequency textures and do not look natural despite yielding high PSNR values.,,We propose a novel application of automated texture synthesis in combination with a perceptual loss focusing on creating realistic textures rather than optimizing for a pixelaccurate reproduction of ground truth images during training. By using feed-forward fully convolutional neural networks in an adversarial training setting, we achieve a significant boost in image quality at high magnification ratios. Extensive experiments on a number of datasets show the effectiveness of our approach, yielding state-of-the-art results in both quantitative and qualitative benchmarks.",Sensiimsuthautesy,606.0,79.0,68.0
8711,Super-Resolution,102.0,kernel modeling super-resolution on real low-resolution images,3.0,185.0,3.0,83.0,4.0,3.3,129.5,37,https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Kernel_Modeling_Super-Resolution_on_Real_Low-Resolution_Images_ICCV_2019_paper.pdf,"Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super-resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their training data: low-resolution (LR) images are obtained by bicubic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images. To improve generalization and robustness of deep super-resolution CNNs on real photographs, we present a kernel modeling super-resolution network (KMSR) that incorporates blur-kernel modeling in the training. Our proposed KMSR consists of two stages: we first build a pool of realistic blur-kernels with a generative adversarial network (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental validations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels.",Skemosuonreloim,38.0,64.0,5.0
8712,Super-Resolution,401.0,fast and accurate image super resolution by deep cnn with skip connection and network in network,1.0,39.0,5.0,157.0,3.0,3.2,183.0,38,https://arxiv.org/pdf/1707.05425,"We propose a highly efficient and faster Single Image Super-Resolution (SISR) model with Deep Convolutional neural networks (Deep CNN). Deep CNN have recently shown that they have a significant reconstruction performance on single-image super-resolution. Current trend is using deeper CNN layers to improve performance. However, deep models demand larger computation resources and is not suitable for network edge devices like mobile, tablet and IoT devices. Our model achieves state of the art reconstruction performance with at least 10 times lower calculation cost by Deep CNN with Residual Net, Skip Connection and Network in Network (DCSCN). A combination of Deep CNNs and Skip connection layers is used as a feature extractor for image features on both local and global area. Parallelized 1x1 CNNs, like the one called Network in Network, is also used for image reconstruction. That structure reduces the dimensions of the previous layer's output for faster computation with less information loss, and make it possible to process original images directly. Also we optimize the number of layers and filters of each CNN to significantly reduce the calculation cost. Thus, the proposed algorithm not only achieves the state of the art performance but also achieves faster and efficient computation. Code is available at this https URL",Sfaanacimsurebydecnwiskcoanneinne,123.0,17.0,13.0
8713,Super-Resolution,401.0,"fast, accurate and lightweight super-resolution with neural architecture search",1.0,40.0,5.0,194.0,3.0,3.2,194.5,39,http://arxiv.org/pdf/1901.07261v3,"Deep convolutional neural networks demonstrate impressive results in the super-resolution domain. A series of studies concentrate on improving peak signal noise ratio (PSNR) by using much deeper layers, which are not friendly to constrained resources. Pursuing a trade-off between the restoration capacity and the simplicity of models is still non-trivial. Recent contributions are struggling to manually maximize this balance, while our work achieves the same goal automatically with neural architecture search. Specifically, we handle super-resolution with a multi-objective approach. We also propose an elastic search tactic at both micro and macro level, based on a hybrid controller that profits from evolutionary computation and reinforcement learning. Quantitative experiments help us to draw a conclusion that our generated models dominate most of the state-of-the-art methods with respect to the individual FLOPS.",Sfaacanlisuwinearse,81.0,25.0,12.0
8714,Super-Resolution,43.0,image super-resolution via sparse representation,4.0,201.0,1.0,11.0,5.0,3.1,96.6,40,https://www.ideals.illinois.edu/bitstream/handle/2142/16479/yang_jianchao.pdf?sequence=1,"This paper presents a new approach to single-image superresolution, based upon sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image patches, we can enforce the similarity of sparse representations between the low-resolution and high-resolution image patch pair with respect to their own dictionaries. Therefore, the sparse representation of a low-resolution image patch can be applied with the high-resolution image patch dictionary to generate a high-resolution image patch. The learned dictionary pair is a more compact representation of the patch pairs, compared to previous approaches, which simply sample a large amount of image patch pairs , reducing the computational cost substantially. The effectiveness of such a sparsity prior is demonstrated for both general image super-resolution (SR) and the special case of face hallucination. In both cases, our algorithm generates high-resolution images that are competitive or even superior in quality to images produced by other similar SR methods. In addition, the local sparse modeling of our approach is naturally robust to noise, and therefore the proposed algorithm can handle SR with noisy inputs in a more unified framework.",Simsuvispre,4117.0,80.0,537.0
8715,Super-Resolution,6.0,super-resolution microscopy demystified,5.0,201.0,1.0,53.0,4.0,3.1,98.1,41,http://arxiv.org/pdf/1904.07523v3,"Super-resolution microscopy (SRM) bypasses the diffraction limit, a physical barrier that restricts the optical resolution to roughly 250 nm and was previously thought to be impenetrable. SRM techniques allow the visualization of subcellular organization with unprecedented detail, but also confront biologists with the challenge of selecting the best-suited approach for their particular research question. Here, we provide guidance on how to use SRM techniques advantageously for investigating cellular structures and dynamics to promote new discoveries.In this Review, Schermelleh et al. give an overview of current super-resolution microscopy techniques and provide guidance on how best to use them to foster biological discovery.",Ssumide,344.0,197.0,2.0
8716,Super-Resolution,21.0,a deep journey into super-resolution: a survey,5.0,201.0,1.0,71.0,4.0,3.1,108.0,42,https://arxiv.org/pdf/1904.07523,"Deep convolutional networks based super-resolution is a fast-growing field with numerous practical applications. In this exposition, we extensively compare 30+ state-of-the-art super-resolution Convolutional Neural Networks (CNNs) over three classical and three recently introduced challenging datasets to benchmark single image super-resolution. We introduce a taxonomy for deep-learning based super-resolution networks that groups existing methods into nine categories including linear, residual, multi-branch, recursive, progressive, attention-based and adversarial designs. We also provide comparisons between the models in terms of network complexity, memory footprint, model input and output, learning details, the type of network losses and important architectural differences (e.g., depth, skip-connections, filters). The extensive evaluation performed, shows the consistent and rapid growth in the accuracy in the past few years along with a corresponding boost in model complexity and the availability of large-scale datasets. It is also observed that the pioneering methods identified as the benchmark have been significantly outperformed by the current contenders. Despite the progress in recent years, we identify several shortcomings of existing techniques and provide future research directions towards the solution of these open problems.",Sadejoinsuasu,67.0,103.0,10.0
8717,Super-Resolution,401.0,"""zero-shot"" super-resolution using deep internal learning",1.0,71.0,4.0,41.0,4.0,3.1,161.0,43,http://arxiv.org/pdf/2106.02619v1,"Deep Learning has led to a dramatic leap in Super-Resolution (SR) performance in the past few years. However, being supervised, these SR methods are restricted to specific training data, where the acquisition of the low-resolution (LR) images from their high-resolution (HR) counterparts is predetermined (e.g., bicubic downscaling), without any distracting artifacts (e.g., sensor noise, image compression, non-ideal PSF, etc). Real LR images, however, rarely obey these restrictions, resulting in poor SR results by SotA (State of the Art) methods. In this paper we introduce ""Zero-Shot"" SR, which exploits the power of Deep Learning, but does not rely on prior training. We exploit the internal recurrence of information inside a single image, and train a small image-specific CNN at test time, on examples extracted solely from the input image itself. As such, it can adapt itself to different settings per image. This allows to perform SR of real old photos, noisy images, biological data, and other images where the acquisition process is unknown or non-ideal. On such images, our method outperforms SotA CNN-based SR methods, as well as previous unsupervised SR methods. To the best of our knowledge, this is the first unsupervised CNN-based SR method.","S""zsuusdeinle",340.0,21.0,74.0
8718,Super-Resolution,401.0,lightweight image super-resolution with information multi-distillation network,1.0,85.0,4.0,60.0,4.0,3.1,172.3,44,https://arxiv.org/pdf/1909.11856,"In recent years, single image super-resolution (SISR) methods using deep convolution neural network (CNN) have achieved impressive results. Thanks to the powerful representation capabilities of the deep networks, numerous previous ways can learn the complex non-linear mapping between low-resolution (LR) image patches and their high-resolution (HR) versions. However, excessive convolutions will limit the application of super-resolution technology in low computing power devices. Besides, super-resolution of any arbitrary scale factor is a critical issue in practical applications, which has not been well solved in the previous approaches. To address these issues, we propose a lightweight information multi-distillation network (IMDN) by constructing the cascaded information multi-distillation blocks (IMDB), which contains distillation and selective fusion parts. Specifically, the distillation module extracts hierarchical features step-by-step, and fusion module aggregates them according to the importance of candidate features, which is evaluated by the proposed contrast-aware channel attention mechanism. To process real images with any sizes, we develop an adaptive cropping strategy (ACS) to super-resolve block-wise image patches using the same well-trained model. Extensive experiments suggest that the proposed method performs favorably against the state-of-the-art SR algorithms in term of visual quality, memory footprint, and inference time. Code is available at \urlhttps://github.com/Zheng222/IMDN.",Sliimsuwiinmune,113.0,60.0,18.0
8719,Super-Resolution,401.0,learning parallax attention for stereo image super-resolution,1.0,82.0,4.0,67.0,4.0,3.1,173.2,45,https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Learning_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPR_2019_paper.pdf,"Stereo image pairs can be used to improve the performance of super-resolution (SR) since additional information is provided from a second viewpoint. However, it is challenging to incorporate this information for SR since disparities between stereo images vary significantly. In this paper, we propose a parallax-attention stereo superresolution network (PASSRnet) to integrate the information from a stereo image pair for SR. Specifically, we introduce a parallax-attention mechanism with a global receptive field along the epipolar line to handle different stereo images with large disparity variations. We also propose a new and the largest dataset for stereo image SR (namely, Flickr1024). Extensive experiments demonstrate that the parallax-attention mechanism can capture correspondence between stereo images to improve SR performance with a small computational and memory cost. Comparative results show that our PASSRnet achieves the state-of-the-art performance on the Middlebury, KITTI 2012 and KITTI 2015 datasets.",Slepaatfostimsu,86.0,58.0,17.0
8720,Super-Resolution,401.0,blind super-resolution kernel estimation using an internal-gan,1.0,88.0,4.0,65.0,4.0,3.1,175.0,46,https://arxiv.org/pdf/1909.06581,"Super resolution (SR) methods typically assume that the low-resolution (LR) image was downscaled from the unknown high-resolution (HR) image by a fixed `ideal’ downscaling kernel (e.g. Bicubic downscaling). However, this is rarely the case in real LR images, in contrast to synthetically generated SR datasets. When the assumed downscaling kernel deviates from the true one, the performance of SR methods significantly deteriorates. This gave rise to Blind-SR - namely, SR when the downscaling kernel (``SR-kernel’’) is unknown. It was further shown that the true SR-kernel is the one that maximizes the recurrence of patches across scales of the LR image. In this paper we show how this powerful cross-scale recurrence property can be realized using Deep Internal Learning. We introduce ``KernelGAN’’, an image-specific Internal-GAN, which trains solely on the LR test image at test time, and learns its internal distribution of patches. Its Generator is trained to produce a downscaled version of the LR test image, such that its Discriminator cannot distinguish between the patch distribution of the downscaled image, and the patch distribution of the original LR image. The Generator, once trained, constitutes the downscaling operation with the correct image-specific SR-kernel. KernelGAN is fully unsupervised, requires no training data other than the input image itself, and leads to state-of-the-art results in Blind-SR when plugged into existing SR algorithms.",Sblsukeesusanin,98.0,41.0,24.0
8721,Super-Resolution,401.0,handheld multi-frame super-resolution,1.0,87.0,4.0,75.0,4.0,3.1,177.6,47,http://arxiv.org/pdf/1703.00154v2,"Compared to DSLR cameras, smartphone cameras have smaller sensors, which limits their spatial resolution; smaller apertures, which limits their light gathering ability; and smaller pixels, which reduces their signal-to-noise ratio. The use of color filter arrays (CFAs) requires demosaicing, which further degrades resolution. In this paper, we supplant the use of traditional demosaicing in single-frame and burst photography pipelines with a multiframe super-resolution algorithm that creates a complete RGB image directly from a burst of CFA raw images. We harness natural hand tremor, typical in handheld photography, to acquire a burst of raw frames with small offsets. These frames are then aligned and merged to form a single image with red, green, and blue values at every pixel site. This approach, which includes no explicit demosaicing step, serves to both increase image resolution and boost signal to noise ratio. Our algorithm is robust to challenging scene conditions: local motion, occlusion, or scene changes. It runs at 100 milliseconds per 12-megapixel RAW input burst frame on mass-produced mobile phones. Specifically, the algorithm is the basis of the Super-Res Zoom feature, as well as the default merge method in Night Sight mode (whether zooming or not) on Google's flagship phone.",Shamusu,60.0,101.0,3.0
8722,Super-Resolution,185.0,"to learn image super-resolution, use a gan to learn how to do image degradation first",3.0,114.0,3.0,110.0,3.0,3.0,134.1,48,https://openaccess.thecvf.com/content_ECCV_2018/papers/Adrian_Bulat_To_learn_image_ECCV_2018_paper.pdf,"This paper is on image and face super-resolution. The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling). We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images. To circumvent this problem, we propose a two-stage process which firstly trains a High-to-Low Generative Adversarial Network (GAN) to learn how to degrade and downsample high-resolution images requiring, during training, only unpaired high and low-resolution images. Once this is achieved, the output of this network is used to train a Low-to-High GAN for image super-resolution using this time paired low- and high-resolution images. Our main result is that this network can be now used to effectively increase the quality of real-world low-resolution images. We have applied the proposed pipeline for the problem of face super-resolution where we report large improvement over baselines and prior work although the proposed method is potentially applicable to other object categories.",Stoleimsuusagatolehotodoimdefi,161.0,43.0,16.0
8723,Super-Resolution,160.0,learning a no-reference quality metric for single-image super-resolution,3.0,186.0,3.0,141.0,3.0,3.0,164.7,49,https://arxiv.org/pdf/1612.05890,"Abstract Numerous single-image super-resolution algorithms have been proposed in the literature, but few studies address the problem of performance evaluation based on visual perception. While most super-resolution images are evaluated by full-reference metrics, the effectiveness is not clear and the required ground-truth images are not always available in practice. To address these problems, we conduct human subject studies using a large set of super-resolution images and propose a no-reference metric learned from visual perceptual scores. Specifically, we design three types of low-level statistical features in both spatial and frequency domains to quantify super-resolved artifacts, and learn a two-stage regression model to predict the quality scores of super-resolution images without referring to ground-truth images. Extensive experimental results show that the proposed metric is effective and efficient to assess the quality of super-resolution images based on human perception.",Sleanoqumefosisu,196.0,47.0,24.0
8724,Super-Resolution,401.0,meta-transfer learning for zero-shot super-resolution,1.0,101.0,3.0,22.0,5.0,3.0,167.29999999999998,50,https://openaccess.thecvf.com/content_CVPR_2020/papers/Soh_Meta-Transfer_Learning_for_Zero-Shot_Super-Resolution_CVPR_2020_paper.pdf,"Convolutional neural networks (CNNs) have shown dramatic improvements in single image super-resolution (SISR) by using large-scale external samples. Despite their remarkable performance based on the external dataset, they cannot exploit internal information within a specific image. Another problem is that they are applicable only to the specific condition of data that they are supervised. For instance, the low-resolution (LR) image should be a ""bicubic"" downsampled noise-free image from a high-resolution (HR) one. To address both issues, zero-shot super-resolution (ZSSR) has been proposed for flexible internal learning. However, they require thousands of gradient updates, i.e., long inference time. In this paper, we present Meta-Transfer Learning for Zero-Shot Super-Resolution (MZSR), which leverages ZSSR. Precisely, it is based on finding a generic initial parameter that is suitable for internal learning. Thus, we can exploit both external and internal information, where one single gradient update can yield quite considerable results. With our method, the network can quickly adapt to a given image condition. In this respect, our method can be applied to a large spectrum of image conditions within a fast adaptation process.",Smelefozesu,62.0,52.0,7.0
8725,Super-Resolution,62.0,frame-recurrent video super-resolution,4.0,201.0,1.0,50.0,4.0,2.8,114.0,51,http://openaccess.thecvf.com/content_cvpr_2018/papers/Sajjadi_Frame-Recurrent_Video_Super-Resolution_CVPR_2018_paper.pdf,"Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to generate high-quality images. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as a large number of separate multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, increasing the computational cost, and 2) each output frame is estimated independently conditioned on the input frames, limiting the system's ability to produce temporally consistent results. In this work, we propose an end-to-end trainable frame-recurrent video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and reduces the computational cost by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art.",Sfrvisu,207.0,45.0,41.0
8726,Super-Resolution,68.0,image super-resolution using dense skip connections,4.0,201.0,1.0,49.0,4.0,2.8,115.5,52,https://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf,"Recent studies have shown that the performance of single-image super-resolution methods can be significantly boosted by using deep convolutional neural networks. In this study, we present a novel single-image super-resolution method by introducing dense skip connections in a very deep network. In the proposed network, the feature maps of each layer are propagated into all subsequent layers, providing an effective way to combine the low-level features and high-level features to boost the reconstruction performance. In addition, the dense skip connections in the network enable short paths to be built directly from the output to each layer, alleviating the vanishing-gradient problem of very deep networks. Moreover, deconvolution layers are integrated into the network to learn the upsampling filters and to speedup the reconstruction process. Further, the proposed method substantially reduces the number of parameters, enhancing the computational efficiency. We evaluate the proposed method using images from four benchmark datasets and set a new state of the art.",Simsuusdeskco,630.0,31.0,67.0
8727,Super-Resolution,78.0,deep learning for single image super-resolution: a brief review,4.0,201.0,1.0,48.0,4.0,2.8,118.20000000000002,53,https://arxiv.org/pdf/1808.03344,"Single image super-resolution (SISR) is a notoriously challenging ill-posed problem that aims to obtain a high-resolution output from one of its low-resolution versions. Recently, powerful deep learning algorithms have been applied to SISR and have achieved state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods and group them into two categories according to their contributions to two essential aspects of SISR: The exploration of efficient neural network architectures for SISR and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is first established, and several critical limitations of the baseline are summarized. Then, representative works on overcoming these limitations are presented based on their original content, as well as our critical exposition and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally, we conclude this review with some current challenges and future trends in SISR that leverage deep learning algorithms.",Sdelefosiimsuabrre,327.0,146.0,14.0
8728,Super-Resolution,128.0,deep laplacian pyramid networks for fast and accurate super-resolution,3.0,201.0,1.0,12.0,5.0,2.8,122.4,54,https://openaccess.thecvf.com/content_cvpr_2017/papers/Lai_Deep_Laplacian_Pyramid_CVPR_2017_paper.pdf,"Convolutional neural networks have recently demonstrated high-quality reconstruction for single-image super-resolution. In this paper, we propose the Laplacian Pyramid Super-Resolution Network (LapSRN) to progressively reconstruct the sub-band residuals of high-resolution images. At each pyramid level, our model takes coarse-resolution feature maps as input, predicts the high-frequency residuals, and uses transposed convolutions for upsampling to the finer level. Our method does not require the bicubic interpolation as the pre-processing step and thus dramatically reduces the computational complexity. We train the proposed LapSRN with deep supervision using a robust Charbonnier loss function and achieve high-quality reconstruction. Furthermore, our network generates multi-scale predictions in one feed-forward pass through the progressive reconstruction, thereby facilitates resource-aware applications. Extensive quantitative and qualitative evaluations on benchmark datasets show that the proposed algorithm performs favorably against the state-of-the-art methods in terms of speed and accuracy.",Sdelapynefofaanacsu,1234.0,48.0,215.0
8729,Super-Resolution,103.0,single image super-resolution from transformed self-exemplars,3.0,201.0,1.0,40.0,5.0,2.8,123.3,55,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Huang_Single_Image_Super-Resolution_2015_CVPR_paper.pdf,"Self-similarity based super-resolution (SR) algorithms are able to produce visually pleasing results without extensive training on external databases. Such algorithms exploit the statistical prior that patches in a natural image tend to recur within and across scales of the same image. However, the internal dictionary obtained from the given image may not always be sufficiently expressive to cover the textural appearance variations in the scene. In this paper, we extend self-similarity based SR to overcome this drawback. We expand the internal patch search space by allowing geometric variations. We do so by explicitly localizing planes in the scene and using the detected perspective geometry to guide the patch search process. We also incorporate additional affine transformations to accommodate local shape variations. We propose a compositional model to simultaneously handle both types of transformations. We extensively evaluate the performance in both urban and natural scenes. Even without using any external training databases, we achieve significantly superior results on urban scenes, while maintaining comparable performance on natural scenes as other state-of-the-art SR algorithms.",Ssiimsufrtrse,1277.0,53.0,264.0
8730,Super-Resolution,159.0,accurate image super-resolution using very deep convolutional networks,3.0,201.0,1.0,6.0,5.0,2.8,129.9,56,https://openaccess.thecvf.com/content_cvpr_2016/papers/Kim_Accurate_Image_Super-Resolution_CVPR_2016_paper.pdf,"We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.",Sacimsuusvedecone,3231.0,35.0,713.0
8731,Super-Resolution,85.0,video super-resolution with convolutional neural networks,4.0,201.0,1.0,96.0,4.0,2.8,134.7,57,https://www.qiqindai.com/wp-content/uploads/2018/11/Video-Super-Resolution-With-Convolutional-Neural-Networks.pdf,"Convolutional neural networks (CNN) are a special type of deep neural networks (DNN). They have so far been successfully applied to image super-resolution (SR) as well as other image restoration tasks. In this paper, we consider the problem of video super-resolution. We propose a CNN that is trained on both the spatial and the temporal dimensions of videos to enhance their spatial resolution. Consecutive frames are motion compensated and used as input to a CNN that provides super-resolved video frames as output. We investigate different options of combining the video frames within one CNN architecture. While large image databases are available to train deep neural networks, it is more challenging to create a large video database of sufficient quality to train neural nets for video restoration. We show that by using images to pretrain our model, a relatively small video database is sufficient for the training of our model to achieve and even improve upon the current state-of-the-art. We compare our proposed approach to current video as well as image SR algorithms.",Svisuwiconene,336.0,48.0,42.0
8732,Super-Resolution,147.0,"fast, accurate, and lightweight super-resolution with cascading residual network",3.0,77.0,4.0,201.0,1.0,2.8,135.2,58,https://openaccess.thecvf.com/content_ECCV_2018/papers/Namhyuk_Ahn_Fast_Accurate_and_ECCV_2018_paper.pdf,"In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods.",Sfaacanlisuwicarene,373.0,43.0,99.0
8733,Super-Resolution,98.0,towards real scene super-resolution with raw images,4.0,201.0,1.0,86.0,4.0,2.8,135.60000000000002,59,https://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Towards_Real_Scene_Super-Resolution_With_Raw_Images_CVPR_2019_paper.pdf,"Most existing super-resolution methods do not perform well in real scenarios due to lack of realistic training data and information loss of the model input. To solve the first problem, we propose a new pipeline to generate realistic training data by simulating the imaging process of digital cameras. And to remedy the information loss of the input, we develop a dual convolutional neural network to exploit the originally captured radiance information in raw images. In addition, we propose to learn a spatially-variant color transformation which helps more effective color corrections. Extensive experiments demonstrate that super-resolution with raw data helps recover fine details and clear structures, and more importantly, the proposed network and data generation pipeline achieve superior results for single image super-resolution in real scenarios.",Storescsuwiraim,60.0,40.0,0.0
8734,Super-Resolution,401.0,audio super resolution using neural networks,1.0,43.0,4.0,182.0,3.0,2.8,192.1,60,https://arxiv.org/pdf/1708.00853,"We introduce a new audio processing technique that increases the sampling rate of signals such as speech or music using deep convolutional neural networks. Our model is trained on pairs of low and high-quality audio examples; at test-time, it predicts missing samples within a low-resolution signal in an interpolation process similar to image super-resolution. Our method is simple and does not involve specialized audio processing techniques; in our experiments, it outperforms baselines on standard speech and music benchmarks at upscaling ratios of 2x, 4x, and 6x. The method has practical applications in telephony, compression, and text-to-speech generation; it demonstrates the effectiveness of feed-forward convolutional architectures on an audio generation task.",Sausureusnene,59.0,32.0,15.0
8735,Super-Resolution,401.0,deep video super-resolution network using dynamic upsampling filters without explicit motion compensation,1.0,111.0,3.0,47.0,4.0,2.7,178.79999999999998,61,http://arxiv.org/pdf/2004.02432v2,"Video super-resolution (VSR) has become even more important recently to provide high resolution (HR) contents for ultra high definition displays. While many deep learning based VSR methods have been proposed, most of them rely heavily on the accuracy of motion estimation and compensation. We introduce a fundamentally different framework for VSR in this paper. We propose a novel end-to-end deep neural network that generates dynamic upsampling filters and a residual image, which are computed depending on the local spatio-temporal neighborhood of each pixel to avoid explicit motion compensation. With our approach, an HR image is reconstructed directly from the input image using the dynamic upsampling filters, and the fine details are added through the computed residual. Our network with the help of a new data augmentation technique can generate much sharper HR videos with temporal consistency, compared with the previous methods. We also provide analysis of our network through extensive experiments to show how the network deals with motions implicitly.",Sdevisuneusdyupfiwiexmoco,231.0,41.0,50.0
8736,Super-Resolution,401.0,frequency separation for real-world super-resolution,1.0,137.0,3.0,78.0,4.0,2.7,198.5,62,http://arxiv.org/pdf/1805.08725v3,"Most of the recent literature on image super-resolution (SR) assumes the availability of training data in the form of paired low resolution (LR) and high resolution (HR) images or the knowledge of the downgrading operator (usually bicubic downscaling). While the proposed methods perform well on standard benchmarks, they often fail to produce convincing results in real-world settings. This is because real-world images can be subject to corruptions such as sensor noise, which are severely altered by bicubic downscaling. Therefore, the models never see a real-world image during training, which limits their generalization capabilities. Moreover, it is cumbersome to collect paired LR and HR images in the same source domain. To address this problem, we propose DSGAN to introduce natural image characteristics in bicubically downscaled images. It can be trained in an unsupervised fashion on HR images, thereby generating LR images with the same characteristics as the original images. We then use the generated data to train a SR model, which greatly improves its performance on real-world images. Furthermore, we propose to separate the low and high image frequencies and treat them differently during training. Since the low frequencies are preserved by downsampling operations, we only require adversarial training to modify the high frequencies. This idea is applied to our DSGAN model as well as the SR model. We demonstrate the effectiveness of our method in several experiments through quantitative and qualitative analysis. Our solution is the winner of the AIM Challenge on Real World SR at ICCV 2019.",Sfrseforesu,48.0,46.0,5.0
8737,Super-Resolution,401.0,fast and accurate single image super-resolution via information distillation network,1.0,171.0,3.0,44.0,4.0,2.7,201.9,63,http://openaccess.thecvf.com/content_cvpr_2018/papers/Hui_Fast_and_Accurate_CVPR_2018_paper.pdf,"Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few numbers of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance. Code is available at https://github.com/Zheng222/IDN-Caffe.",Sfaanacsiimsuviindine,299.0,27.0,63.0
8738,Super-Resolution,401.0,deep image prior,1.0,7.0,5.0,201.0,1.0,2.6,183.4,64,http://arxiv.org/pdf/1712.05016v2,"The recent literature on deep learning offers new tools to learn a rich probability distribution over high dimensional data such as images or sounds. In this work we investigate the possibility of learning the prior distribution over neural network parameters using such tools. Our resulting variational Bayes algorithm generalizes well to new tasks, even when very few training examples are provided. Furthermore, this learned prior allows the model to extrapolate correctly far from a given task's training data on a meta-dataset of periodic signals.",Sdeimpr,1080.0,34.0,215.0
8739,Super-Resolution,401.0,learning temporal coherence via self-supervision for gan-based video generation,1.0,11.0,5.0,201.0,1.0,2.6,185.0,65,http://arxiv.org/abs/1811.09393v4,"Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as $L^2$ over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN. The project page https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental materials.",Sletecovisefogavige,40.0,104.0,5.0
8740,Super-Resolution,401.0,a unified neural architecture for instrumental audio tasks,1.0,18.0,5.0,201.0,1.0,2.6,187.8,66,http://arxiv.org/pdf/2108.03456v1,"We propose a unified model for three inter-related tasks: 1) to \textit{separate} individual sound sources from a mixed music audio, 2) to \textit{transcribe} each sound source to MIDI notes, and 3) to\textit{ synthesize} new pieces based on the timbre of separated sources. The model is inspired by the fact that when humans listen to music, our minds can not only separate the sounds of different instruments, but also at the same time perceive high-level representations such as score and timbre. To mirror such capability computationally, we designed a pitch-timbre disentanglement module based on a popular encoder-decoder neural architecture for source separation. The key inductive biases are vector-quantization for pitch representation and pitch-transformation invariant for timbre representation. In addition, we adopted a query-by-example method to achieve \textit{zero-shot} learning, i.e., the model is capable of doing source separation, transcription, and synthesis for \textit{unseen} instruments. The current design focuses on audio mixtures of two monophonic instruments. Experimental results show that our model outperforms existing multi-task baselines, and the transcribed score serves as a powerful auxiliary for separation tasks.",Saunnearfoinauta,4.0,25.0,0.0
8741,Super-Resolution,401.0,a tour of convolutional networks guided by linear interpreters,1.0,22.0,5.0,201.0,1.0,2.6,189.4,67,http://arxiv.org/pdf/1908.05168v1,"Convolutional networks are large linear systems divided into layers and connected by non-linear units. These units are the ""articulations"" that allow the network to adapt to the input. To understand how a network manages to solve a problem we must look at the articulated decisions in entirety. If we could capture the actions of non-linear units for a particular input, we would be able to replay the whole system back and forth as if it was always linear. It would also reveal the actions of non-linearities because the resulting linear system, a Linear Interpreter, depends on the input image. We introduce a hooking layer, called a LinearScope, which allows us to run the network and the linear interpreter in parallel. Its implementation is simple, flexible and efficient. From here we can make many curious inquiries: how do these linear systems look like? When the rows and columns of the transformation matrix are images, how do they look like? What type of basis do these linear transformations rely on? The answers depend on the problems presented, through which we take a tour to some popular architectures used for classification, super-resolution (SR) and image-to-image translation (I2I). For classification we observe that popular networks use a pixel-wise vote per class strategy and heavily rely on bias parameters. For SR and I2I we find that CNNs use wavelet-type basis similar to the human visual system. For I2I we reveal copy-move and template-creation strategies to generate outputs.",Satoofconegubyliin,6.0,64.0,0.0
8742,Super-Resolution,401.0,aim 2020 challenge on efficient super-resolution: methods and results,1.0,24.0,5.0,201.0,1.0,2.6,190.2,68,http://arxiv.org/pdf/2011.04988v1,"This paper reviews the second AIM realistic bokeh effect rendering challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world bokeh simulation problem, where the goal was to learn a realistic shallow focus technique using a large-scale EBB! bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR camera. The participants had to render bokeh effect based on only one single frame without any additional data from other cameras or sensors. The target metric used in this challenge combined the runtime and the perceptual quality of the solutions measured in the user study. To ensure the efficiency of the submitted models, we measured their runtime on standard desktop CPUs as well as were running the models on smartphone GPUs. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical bokeh effect rendering problem.",Sai20chonefsumeanre,25.0,73.0,1.0
8743,Super-Resolution,401.0,beyond a gaussian denoiser: residual learning of deep cnn for image denoising,1.0,26.0,5.0,201.0,1.0,2.6,191.0,69,http://arxiv.org/abs/1608.03981v1,"Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise (AWGN) at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks such as Gaussian denoising, single image super-resolution and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",Sbeagadereleofdecnfoimde,3164.0,49.0,572.0
8744,Super-Resolution,401.0,cgans with projection discriminator,1.0,27.0,5.0,201.0,1.0,2.6,191.4,70,http://arxiv.org/pdf/1802.05637v2,"We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.",Scgwiprdi,471.0,33.0,94.0
8745,Super-Resolution,401.0,a comprehensive guide to bayesian convolutional neural network with variational inference,1.0,29.0,5.0,201.0,1.0,2.6,192.2,71,http://arxiv.org/pdf/1901.02731v1,"Artificial Neural Networks are connectionist systems that perform a given task by learning on examples without having prior knowledge about the task. This is done by finding an optimal point estimate for the weights in every node. Generally, the network using point estimates as weights perform well with large datasets, but they fail to express uncertainty in regions with little or no data, leading to overconfident decisions.   In this paper, Bayesian Convolutional Neural Network (BayesCNN) using Variational Inference is proposed, that introduces probability distribution over the weights. Furthermore, the proposed BayesCNN architecture is applied to tasks like Image Classification, Image Super-Resolution and Generative Adversarial Networks. The results are compared to point-estimates based architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image CLassification task, on BSD300 dataset for Image Super Resolution task and on CIFAR10 dataset again for Generative Adversarial Network task.   BayesCNN is based on Bayes by Backprop which derives a variational approximation to the true posterior. We, therefore, introduce the idea of applying two convolutional operations, one for the mean and one for the variance. Our proposed method not only achieves performances equivalent to frequentist inference in identical architectures but also incorporate a measurement for uncertainties and regularisation. It further eliminates the use of dropout in the model. Moreover, we predict how certain the model prediction is based on the epistemic and aleatoric uncertainties and empirically show how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the training accuracy increases. Finally, we propose ways to prune the Bayesian architecture and to make it more computational and time effective.",Sacogutobaconenewivain,68.0,70.0,11.0
8746,Super-Resolution,401.0,lossless image compression through super-resolution,1.0,30.0,5.0,201.0,1.0,2.6,192.6,72,http://arxiv.org/pdf/2004.02872v1,"We introduce a simple and efficient lossless image compression algorithm. We store a low resolution version of an image as raw pixels, followed by several iterations of lossless super-resolution. For lossless super-resolution, we predict the probability of a high-resolution image, conditioned on the low-resolution input, and use entropy coding to compress this super-resolution operator. Super-Resolution based Compression (SReC) is able to achieve state-of-the-art compression rates with practical runtimes on large datasets. Code is available online at https://github.com/caoscott/SReC.",Sloimcothsu,12.0,54.0,0.0
8747,Super-Resolution,401.0,universally slimmable networks and improved training techniques,1.0,32.0,5.0,201.0,1.0,2.6,193.4,73,http://arxiv.org/pdf/1903.05134v2,"Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks",Sunslneanimtrte,132.0,32.0,27.0
8748,Super-Resolution,401.0,learning continuous image representation with local implicit image function,1.0,33.0,5.0,201.0,1.0,2.6,193.8,74,http://arxiv.org/pdf/2012.09161v2,"How to represent an image? While the visual world is presented in a continuous manner, machines store and see the images in a discrete way with 2D arrays of pixels. In this paper, we seek to learn a continuous representation for images. Inspired by the recent progress in 3D reconstruction with implicit neural representation, we propose Local Implicit Image Function (LIIF), which takes an image coordinate and the 2D deep features around the coordinate as inputs, predicts the RGB value at a given coordinate as an output. Since the coordinates are continuous, LIIF can be presented in arbitrary resolution. To generate the continuous representation for images, we train an encoder with LIIF representation via a self-supervised task with super-resolution. The learned continuous representation can be presented in arbitrary resolution even extrapolate to x30 higher resolution, where the training tasks are not provided. We further show that LIIF representation builds a bridge between discrete and continuous representation in 2D, it naturally supports the learning tasks with size-varied image ground-truths and significantly outperforms the method with resizing the ground-truths.",Slecoimrewiloimimfu,27.0,55.0,6.0
8749,Super-Resolution,401.0,image restoration using convolutional auto-encoders with symmetric skip connections,1.0,34.0,5.0,201.0,1.0,2.6,194.2,75,http://arxiv.org/pdf/1606.08921v3,"Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results.",Simreuscoauwisyskco,250.0,66.0,23.0
8750,Super-Resolution,401.0,zooming slowmo: an efficient one-stage framework for space-time video super-resolution,1.0,35.0,5.0,201.0,1.0,2.6,194.6,76,http://arxiv.org/pdf/1905.05169v1,"This paper shows that when applying machine learning to digital zoom for photography, it is beneficial to use real, RAW sensor data for training. Existing learning-based super-resolution methods do not use real sensor data, instead operating on RGB images. In practice, these approaches result in loss of detail and accuracy in their digitally zoomed output when zooming in on distant image regions. We also show that synthesizing sensor data by resampling high-resolution RGB images is an oversimplified approximation of real sensor data and noise, resulting in worse image quality. The key barrier to using real sensor data for training is that ground truth high-resolution imagery is missing. We show how to obtain the ground-truth data with optically zoomed images and contribute a dataset, SR-RAW, for real-world computational zoom. We use SR-RAW to train a deep network with a novel contextual bilateral loss (CoBi) that delivers critical robustness to mild misalignment in input-output image pairs. The trained network achieves state-of-the-art performance in 4X and 8X computational zoom.",Szoslanefonfrfospvisu,2.0,74.0,0.0
8752,Super-Resolution,401.0,fourier neural operator for parametric partial differential equations,1.0,38.0,5.0,201.0,1.0,2.6,195.8,77,http://arxiv.org/pdf/2010.08895v3,"The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.",Sfoneopfopapadieq,119.0,29.0,16.0
8753,Super-Resolution,104.0,super-resolution reconstruction of turbulent flows with machine learning,3.0,201.0,1.0,58.0,4.0,2.5,129.0,78,https://arxiv.org/pdf/1811.11328,"We use machine learning to perform super-resolution analysis of grossly under-resolved turbulent flow field data to reconstruct the high-resolution flow field. Two machine learning models are developed, namely, the convolutional neural network (CNN) and the hybrid downsampled skip-connection/multi-scale (DSC/MS) models. These machine learning models are applied to a two-dimensional cylinder wake as a preliminary test and show remarkable ability to reconstruct laminar flow from low-resolution flow field data. We further assess the performance of these models for two-dimensional homogeneous turbulence. The CNN and DSC/MS models are found to reconstruct turbulent flows from extremely coarse flow field images with remarkable accuracy. For the turbulent flow problem, the machine-leaning-based super-resolution analysis can greatly enhance the spatial resolution with as little as 50 training snapshot data, holding great potential to reveal subgrid-scale physics of complex turbulent flows. With the growing availability of flow field data from high-fidelity simulations and experiments, the present approach motivates the development of effective super-resolution models for a variety of fluid flows.",Ssureoftuflwimale,149.0,38.0,0.0
8754,Super-Resolution,126.0,toward real-world single image super-resolution: a new benchmark and a new model,3.0,201.0,1.0,43.0,4.0,2.5,131.1,79,https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Toward_Real-World_Single_Image_Super-Resolution_A_New_Benchmark_and_a_ICCV_2019_paper.pdf,"Most of the existing learning-based single image super-resolution (SISR) methods are trained and evaluated on simulated datasets, where the low-resolution (LR) images are generated by applying a simple and uniform degradation (i.e., bicubic downsampling) to their high-resolution (HR) counterparts. However, the degradations in real-world LR images are far more complicated. As a consequence, the SISR models trained on simulated data become less effective when applied to practical scenarios. In this paper, we build a real-world super-resolution (RealSR) dataset where paired LR-HR images on the same scene are captured by adjusting the focal length of a digital camera. An image registration algorithm is developed to progressively align the image pairs at different resolutions. Considering that the degradation kernels are naturally non-uniform in our dataset, we present a Laplacian pyramid based kernel prediction network (LP-KPN), which efficiently learns per-pixel kernels to recover the HR image. Our extensive experiments demonstrate that SISR models trained on our RealSR dataset deliver better visual quality with sharper edges and finer textures on real-world scenes than those trained on simulated datasets. Though our RealSR dataset is built by using only two cameras (Canon 5D3 and Nikon D810), the trained model generalizes well to other camera devices such as Sony a7II and mobile phones.",Storesiimsuanebeananemo,125.0,73.0,29.0
8755,Super-Resolution,112.0,unsupervised learning for real-world super-resolution,3.0,201.0,1.0,76.0,4.0,2.5,136.8,80,https://arxiv.org/pdf/1909.09629,"Most current super-resolution methods rely on low and high resolution image pairs to train a network in a fully supervised manner. However, such image pairs are not available in real-world applications. Instead of directly addressing this problem, most works employ the popular bicubic downsampling strategy to artificially generate a corresponding low resolution image. Unfortunately, this strategy introduces significant artifacts, removing natural sensor noise and other real-world characteristics. Super-resolution networks trained on such bicubic images therefore struggle to generalize to natural images. In this work, we propose an unsupervised approach for image super-resolution. Given only unpaired data, we learn to invert the effects of bicubic downsampling in order to restore the natural image characteristics present in the data. This allows us to generate realistic image pairs, faithfully reflecting the distribution of real-world images. Our super-resolution network can therefore be trained with direct pixel-wise supervision in the high resolution domain, while robustly generalizing to real input. We demonstrate the effectiveness of our approach in quantitative and qualitative experiments.",Sunleforesu,56.0,44.0,10.0
8756,Super-Resolution,86.0,super-resolution microscopy with dna-paint,4.0,201.0,1.0,105.0,3.0,2.5,137.7,81,http://www.microscopist.co.uk/wp-content/uploads/2021/02/super-res-DNA-Paint.pdf,"Super-resolution techniques have begun to transform biological and biomedical research by allowing researchers to observe structures well below the classic diffraction limit of light. DNA points accumulation for imaging in nanoscale topography (DNA-PAINT) offers an easy-to-implement approach to localization-based super-resolution microscopy, owing to the use of DNA probes. In DNA-PAINT, transient binding of short dye-labeled ('imager') oligonucleotides to their complementary target ('docking') strands creates the necessary 'blinking' to enable stochastic super-resolution microscopy. Using the programmability and specificity of DNA molecules as imaging and labeling probes allows researchers to decouple blinking from dye photophysics, alleviating limitations of current super-resolution techniques, making them compatible with virtually any single-molecule-compatible dye. Recent developments in DNA-PAINT have enabled spectrally unlimited multiplexing, precise molecule counting and ultra-high, molecular-scale (sub-5-nm) spatial resolution, reaching ∼1-nm localization precision. DNA-PAINT can be applied to a multitude of in vitro and cellular applications by linking docking strands to antibodies. Here, we present a protocol for the key aspects of the DNA-PAINT framework for both novice and expert users. This protocol describes the creation of DNA origami test samples, in situ sample preparation, multiplexed data acquisition, data simulation, super-resolution image reconstruction and post-processing such as drift correction, molecule counting (qPAINT) and particle averaging. Moreover, we provide an integrated software package, named Picasso, for the computational steps involved. The protocol is designed to be modular, so that individual components can be chosen and implemented per requirements of a specific application. The procedure can be completed in 1–2 d.",Ssumiwidn,353.0,90.0,7.0
8757,Super-Resolution,59.0,pixel recursive super resolution,4.0,201.0,1.0,146.0,3.0,2.5,141.9,82,https://openaccess.thecvf.com/content_ICCV_2017/papers/Dahl_Pixel_Recursive_Super_ICCV_2017_paper.pdf,"Super resolution is the problem of artificially enlarging a low resolution photograph to recover a plausible high resolution version. In the regime of high magnification factors, the problem is dramatically underspecified and many plausible, high resolution images may match a given low resolution image. In particular, traditional super resolution techniques fail in this regime due to the multimodality of the problem and strong prior information that must be imposed on image synthesis to produce plausible high resolution images. In this work we propose a new probabilistic deep network architecture, a pixel recursive super resolution model, that is an extension of PixelCNNs to address this problem. We demonstrate that this model produces a diversity of plausible high resolution images at large magnification factors. Furthermore, in human evaluation studies we demonstrate how previous methods fail to fool human observers. However, high resolution images sampled from this probabilistic deep network do fool a naive human observer a significant fraction of the time.",Spiresure,179.0,66.0,11.0
8758,Super-Resolution,49.0,"image super-resolution: the techniques, applications, and future",4.0,201.0,1.0,166.0,3.0,2.5,144.9,83,https://www.researchgate.net/profile/Linwei-Yue/publication/303182546_Image_super-resolution_The_techniques_applications_and_future/links/59e7584fa6fdcc6433a93d7f/Image-super-resolution-The-techniques-applications-and-future.pdf,"Super-resolution (SR) technique reconstructs a higher-resolution image or sequence from the observed LR images. As SR has been developed for more than three decades, both multi-frame and single-frame SR have significant applications in our daily life. This paper aims to provide a review of SR from the perspective of techniques and applications, and especially the main contributions in recent years. Regularized SR methods are most commonly employed in the last decade. Technical details are discussed in this article, including reconstruction models, parameter selection methods, optimization algorithms and acceleration strategies. Moreover, an exhaustive summary of the current applications using SR techniques has been presented. Lastly, the article discusses the current obstacles for future research.",Simsuthteapanfu,252.0,270.0,8.0
8759,Super-Resolution,75.0,raisr: rapid and accurate image super resolution,4.0,201.0,1.0,142.0,3.0,2.5,145.5,84,https://ieeexplore.ieee.org/iel7/6745852/6960042/07744595.pdf,"Given an image, we wish to produce an image of larger size with significantly more pixels and higher image quality. This is generally known as the single image super-resolution problem. The idea is that with sufficient training data (corresponding pairs of low and high resolution images) we can learn set of filters (i.e., a mapping) that when applied to given image that is not in the training set, will produce a higher resolution version of it, where the learning is preferably low complexity. In our proposed approach, the run-time is more than one to two orders of magnitude faster than the best competing methods currently available, while producing results comparable or better than state-of-the-art. A closely related topic is image sharpening and contrast enhancement, i.e., improving the visual quality of a blurry image by amplifying the underlying details (a wide range of frequencies). Our approach additionally includes an extremely efficient way to produce an image that is significantly sharper than the input blurry one, without introducing artifacts, such as halos and noise amplification. We illustrate how this effective sharpening algorithm, in addition to being of independent interest, can be used as a preprocessing step to induce the learning of more effective upscaling filters with built-in sharpening and contrast enhancement effect.",Sraraanacimsure,184.0,59.0,22.0
8760,Super-Resolution,54.0,learned spectral super-resolution,4.0,201.0,1.0,183.0,3.0,2.5,151.5,85,https://arxiv.org/pdf/1703.09470,"We describe a novel method for blind, single-image spectral super-resolution. While conventional super-resolution aims to increase the spatial resolution of an input image, our goal is to spectrally enhance the input, i.e., generate an image with the same spatial resolution, but a greatly increased number of narrow (hyper-spectral) wave-length bands. Just like the spatial statistics of natural images has rich structure, which one can exploit as prior to predict high-frequency content from a low resolution image, the same is also true in the spectral domain: the materials and lighting conditions of the observed world induce structure in the spectrum of wavelengths observed at a given pixel. Surprisingly, very little work exists that attempts to use this diagnosis and achieve blind spectral super-resolution from single images. We start from the conjecture that, just like in the spatial domain, we can learn the statistics of natural image spectra, and with its help generate finely resolved hyper-spectral images from RGB input. Technically, we follow the current best practice and implement a convolutional neural network (CNN), which is trained to carry out the end-to-end mapping from an entire RGB image to the corresponding hyperspectral image of equal size. We demonstrate spectral super-resolution both for conventional RGB images and for multi-spectral satellite data, outperforming the state-of-the-art.",Slespsu,60.0,54.0,15.0
8761,Super-Resolution,401.0,"tempogan: a temporally coherent, volumetric gan for super-resolution fluid flow",1.0,146.0,3.0,111.0,3.0,2.4000000000000004,212.0,86,https://arxiv.org/pdf/1801.09710,"We propose a temporally coherent generative model addressing the super-resolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate advected quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions.",Steatecovogafosuflfl,168.0,76.0,4.0
8762,Super-Resolution,401.0,gated fusion network for joint image deblurring and super-resolution,1.0,162.0,3.0,143.0,3.0,2.4000000000000004,228.0,87,http://arxiv.org/pdf/1807.10806v1,"Single-image super-resolution is a fundamental task for vision applications to enhance the image quality with respect to spatial resolution. If the input image contains degraded pixels, the artifacts caused by the degradation could be amplified by super-resolution methods. Image blur is a common degradation source. Images captured by moving or still cameras are inevitably affected by motion blur due to relative movements between sensors and objects. In this work, we focus on the super-resolution task with the presence of motion blur. We propose a deep gated fusion convolution neural network to generate a clear high-resolution frame from a single natural image with severe blur. By decomposing the feature extraction step into two task-independent streams, the dual-branch design can facilitate the training process by avoiding learning the mixed degradation all-in-one and thus enhance the final high-resolution prediction results. Extensive experiments demonstrate that our method generates sharper super-resolved images from low-resolution inputs with high computational efficiency.",Sgafunefojoimdeansu,49.0,47.0,8.0
8763,Super-Resolution,161.0,learning a single network for scale-arbitrary super-resolution,3.0,140.0,3.0,201.0,1.0,2.4,164.6,88,https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Learning_a_Single_Network_for_Scale-Arbitrary_Super-Resolution_ICCV_2021_paper.pdf,"Single image super-resolution (SISR) is a notoriously challenging ill-posed problem, which aims to obtain a high-resolution (HR) output from one of its low-resolution (LR) versions. To solve the SISR problem, recently powerful deep learning algorithms have been employed and achieved the state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods, and group them into two categories according to their major contributions to two essential aspects of SISR: the exploration of efficient neural network architectures for SISR, and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is firstly established and several critical limitations of the baseline are summarized. Then representative works on overcoming these limitations are presented based on their original contents as well as our critical understandings and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally we conclude this review with some vital current challenges and future trends in SISR leveraging deep learning algorithms.",Sleasinefoscsu,2.0,34.0,0.0
8764,Super-Resolution,187.0,video super-resolution with temporal group attention,3.0,176.0,3.0,201.0,1.0,2.4,186.8,89,https://openaccess.thecvf.com/content_CVPR_2020/papers/Isobe_Video_Super-Resolution_With_Temporal_Group_Attention_CVPR_2020_paper.pdf,"A new unified video analytics framework (ER3) is proposed for complex event retrieval, recognition and recounting, based on the proposed video imprint representation, which exploits temporal correlations among image features across video frames. With the video imprint representation, it is convenient to reverse map back to both temporal and spatial locations in video frames, allowing for both key frame identification and key areas localization within each frame. In the proposed framework, a dedicated feature alignment module is incorporated for redundancy removal across frames to produce the tensor representation, i.e., the video imprint. Subsequently, the video imprint is individually fed into both a reasoning network and a feature aggregation module, for event recognition/recounting and event retrieval tasks, respectively. Thanks to its attention mechanism inspired by the memory networks used in language modeling, the proposed reasoning network is capable of simultaneous event category recognition and localization of the key pieces of evidence for event recounting. In addition, the latent structure in our reasoning network highlights the areas of the video imprint, which can be directly used for event recounting. With the event retrieval task, the compact video representation aggregated from the video imprint contributes to better retrieval results than existing state-of-the-art methods.",Svisuwitegrat,34.0,38.0,6.0
8765,Super-Resolution,1.0,advances and challenges in super‐resolution,5.0,201.0,1.0,201.0,1.0,2.2,141.0,90,https://apps.dtic.mil/sti/pdfs/ADA462048.pdf,"Quantum teleportation is one of the most important protocols in quantum information. By exploiting the physical resource of entanglement, quantum teleportation serves as a key primitive in a variety of quantum information tasks and represents an important building block for quantum technologies, with a pivotal role in the continuing progress of quantum communication, quantum computing and quantum networks. Here we review the basic theoretical ideas behind quantum teleportation and its variant protocols. We focus on the main experiments, together with the technical advantages and disadvantages associated with the use of the various technologies, from photonic qubits and optical modes to atomic ensembles, trapped atoms, and solid-state systems. Analysing the current state-of-the-art, we finish by discussing open issues, challenges and potential future implementations.",Sadanchinsu,622.0,71.0,31.0
8766,Super-Resolution,2.0,robust super-resolution,5.0,201.0,1.0,201.0,1.0,2.2,141.3,91,http://arxiv.org/abs/chao-dyn/9803001v1,"It has been proposed to make practical use of chaos in communication, in enhancing mixing in chemical processes and in spreading the spectrum of switch-mode power suppies to avoid electromagnetic interference. It is however known that for most smooth chaotic systems, there is a dense set of periodic windows for any range of parameter values. Therefore in practical systems working in chaotic mode, slight inadvertent fluctuation of a parameter may take the system out of chaos. We say a chaotic attractor is robust if, for its parameter values there exists a neighborhood in the parameter space with no periodic attractor and the chaotic attractor is unique in that neighborhood. In this paper we show that robust chaos can occur in piecewise smooth systems and obtain the conditions of its occurrence. We illustrate this phenomenon with a practical example from electrical engineering.",Srosu,18.0,37.0,1.0
8768,Super-Resolution,4.0,limits on super-resolution and how to break them,5.0,201.0,1.0,201.0,1.0,2.2,141.9,92,https://scholar.archive.org/work/xlse2hj2mncmvbiazf5ku7mvrm/access/wayback/http://www.ri.cmu.edu/pub_files/pub3/baker_simon_2002_4/baker_simon_2002_4.pdf,"We consider the problem of dynamic pricing with limited supply. A seller has $k$ identical items for sale and is facing $n$ potential buyers (""agents"") that are arriving sequentially. Each agent is interested in buying one item. Each agent's value for an item is an IID sample from some fixed distribution with support $[0,1]$. The seller offers a take-it-or-leave-it price to each arriving agent (possibly different for different agents), and aims to maximize his expected revenue.   We focus on ""prior-independent"" mechanisms -- ones that do not use any information about the distribution. They are desirable because knowing the distribution is unrealistic in many practical scenarios. We study how the revenue of such mechanisms compares to the revenue of the optimal offline mechanism that knows the distribution (""offline benchmark"").   We present a prior-independent dynamic pricing mechanism whose revenue is at most $O((k \log n)^{2/3})$ less than the offline benchmark, for every distribution that is regular. In fact, this guarantee holds without *any* assumptions if the benchmark is relaxed to fixed-price mechanisms. Further, we prove a matching lower bound. The performance guarantee for the same mechanism can be improved to $O(\sqrt{k} \log n)$, with a distribution-dependent constant, if $k/n$ is sufficiently small. We show that, in the worst case over all demand distributions, this is essentially the best rate that can be obtained with a distribution-specific constant.   On a technical level, we exploit the connection to multi-armed bandits (MAB). While dynamic pricing with unlimited supply can easily be seen as an MAB problem, the intuition behind MAB approaches breaks when applied to the setting with limited supply. Our high-level conceptual contribution is that even the limited supply setting can be fruitfully treated as a bandit problem.",Slionsuanhotobrth,1227.0,93.0,83.0
8769,Super-Resolution,5.0,super-resolution imaging,5.0,201.0,1.0,201.0,1.0,2.2,142.2,93,http://arxiv.org/pdf/1902.06068v2,"Image Super-Resolution (SR) is an important class of image processing techniques to enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.",Ssuim,2234.0,55.0,90.0
8770,Super-Resolution,8.0,the 2015 super-resolution microscopy roadmap,5.0,201.0,1.0,201.0,1.0,2.2,143.10000000000002,94,https://iopscience.iop.org/article/10.1088/0022-3727/48/44/443001/pdf,"Far-field optical microscopy using focused light is an important tool in a number of scientific disciplines including chemical, (bio)physical and biomedical research, particularly with respect to the study of living cells and organisms. Unfortunately, the applicability of the optical microscope is limited, since the diffraction of light imposes limitations on the spatial resolution of the image. Consequently the details of, for example, cellular protein distributions, can be visualized only to a certain extent. Fortunately, recent years have witnessed the development of 'super-resolution' far-field optical microscopy (nanoscopy) techniques such as stimulated emission depletion (STED), ground state depletion (GSD), reversible saturated optical (fluorescence) transitions (RESOLFT), photoactivation localization microscopy (PALM), stochastic optical reconstruction microscopy (STORM), structured illumination microscopy (SIM) or saturated structured illumination microscopy (SSIM), all in one way or another addressing the problem of the limited spatial resolution of far-field optical microscopy. While SIM achieves a two-fold improvement in spatial resolution compared to conventional optical microscopy, STED, RESOLFT, PALM/STORM, or SSIM have all gone beyond, pushing the limits of optical image resolution to the nanometer scale. Consequently, all super-resolution techniques open new avenues of biomedical research. Because the field is so young, the potential capabilities of different super-resolution microscopy approaches have yet to be fully explored, and uncertainties remain when considering the best choice of methodology. Thus, even for experts, the road to the future is sometimes shrouded in mist. The super-resolution optical microscopy roadmap of Journal of Physics D: Applied Physics addresses this need for clarity. It provides guidance to the outstanding questions through a collection of short review articles from experts in the field, giving a thorough discussion on the concepts underlying super-resolution optical microscopy, the potential of different approaches, the importance of label optimization (such as reversible photoswitchable proteins) and applications in which these methods will have a significant impact.",Sth20sumiro,251.0,167.0,3.0
8771,Super-Resolution,9.0,example-based super-resolution,5.0,201.0,1.0,201.0,1.0,2.2,143.4,95,https://www.merl.com/publications/docs/TR2001-30.pdf,"This paper introduces a newly collected and novel dataset (StereoMSI) for example-based single and colour-guided spectral image super-resolution. The dataset was first released and promoted during the PIRM2018 spectral image super-resolution challenge. To the best of our knowledge, the dataset is the first of its kind, comprising 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the challenge and, for each of these, we have provided a split into training, validation and testing. This arrangement is a result of the challenge structure and phases, with the first track focusing on example-based spectral image super-resolution and the second one aiming at exploiting the registered stereo colour imagery to improve the resolution of the spectral images. Each of the tracks and splits has been selected to be consistent across a number of image quality metrics. The dataset is quite general in nature and can be used for a wide variety of applications in addition to the development of spectral image super-resolution methods.",Sexsu,2330.0,38.0,122.0
8772,Super-Resolution,10.0,towards a mathematical theory of super‐resolution,5.0,201.0,1.0,201.0,1.0,2.2,143.7,96,https://arxiv.org/pdf/1203.5871,"After motivating the need of a multiscale version of fractional calculus in quantum gravity, we review current proposals and the program to be carried out in order to reach a viable definition of scale-dependent fractional operators. We present different types of multifractional Laplacians and comment on their known or expected properties.",Stoamathofsu,673.0,47.0,68.0
8773,Super-Resolution,11.0,super-resolution image reconstruction: a technical overview,5.0,201.0,1.0,201.0,1.0,2.2,144.0,97,https://www.academia.edu/download/43777013/Super-Resolution_Image_Reconstruction_A_Technical_Overview.pdf,"Face super-resolution (FSR), also known as face hallucination, which is aimed at enhancing the resolution of low-resolution (LR) face images to generate high-resolution (HR) face images, is a domain-specific image super-resolution problem. Recently, FSR has received considerable attention and witnessed dazzling advances with the development of deep learning techniques. To date, few summaries of the studies on the deep learning-based FSR are available. In this survey, we present a comprehensive review of deep learning-based FSR methods in a systematic manner. First, we summarize the problem formulation of FSR and introduce popular assessment metrics and loss functions. Second, we elaborate on the facial characteristics and popular datasets used in FSR. Third, we roughly categorize existing methods according to the utilization of facial characteristics. In each category, we start with a general description of design principles, then present an overview of representative approaches, and then discuss the pros and cons among them. Fourth, we evaluate the performance of some state-of-the-art methods. Fifth, joint FSR and other tasks, and FSR-related applications are roughly introduced. Finally, we envision the prospects of further technological advancement in this field. A curated list of papers and resources to face super-resolution are available at \url{https://github.com/junjun-jiang/Face-Hallucination-Benchmark}",Ssuimreateov,3153.0,85.0,157.0
8775,Super-Resolution,13.0,space-time super-resolution,5.0,201.0,1.0,201.0,1.0,2.2,144.60000000000002,98,https://www.researchgate.net/profile/Eli-Shechtman/publication/7942078_Space-time_super-resolution/links/02e7e520897b2f3e60000000/Space-time-super-resolution.pdf,"The concept of the space-time as emerging in the world phase transition, vs. a priori exiting, is put forward. The theory of gravity with two basic symmetries, the global affine one and the general covariance, is developed. Implications for the Universe are indicated.",Sspsu,21.0,64.0,0.0
8776,Super-Resolution,15.0,super-resolution in medical imaging,5.0,201.0,1.0,201.0,1.0,2.2,145.2,99,http://arxiv.org/pdf/2010.13172v1,"Although high resolution isotropic 3D medical images are desired in clinical practice, their acquisition is not always feasible. Instead, lower resolution images are upsampled to higher resolution using conventional interpolation methods. Sophisticated learning-based super-resolution approaches are frequently unavailable in clinical setting, because such methods require training with high-resolution isotropic examples. To address this issue, we propose a learning-based super-resolution approach that can be trained using solely anisotropic images, i.e. without high-resolution ground truth data. The method exploits the latent space, generated by autoencoders trained on anisotropic images, to increase spatial resolution in low-resolution images. The method was trained and evaluated using 100 publicly available cardiac cine MR scans from the Automated Cardiac Diagnosis Challenge (ACDC). The quantitative results show that the proposed method performs better than conventional interpolation methods. Furthermore, the qualitative results indicate that especially finer cardiac structures are synthesized with high quality. The method has the potential to be applied to other anatomies and modalities and can be easily applied to any 3D anisotropic medical image dataset.",Ssuinmeim,4.0,18.0,0.0
8778,Super-Resolution,17.0,computer vision applied to super resolution,5.0,201.0,1.0,201.0,1.0,2.2,145.8,100,https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.7085&rep=rep1&type=pdf,"The first mobile camera phone was sold only 20 years ago, when taking pictures with one's phone was an oddity, and sharing pictures online was unheard of. Today, the smartphone is more camera than phone. How did this happen? This transformation was enabled by advances in computational photography -the science and engineering of making great images from small form factor, mobile cameras. Modern algorithmic and computing advances, including machine learning, have changed the rules of photography, bringing to it new modes of capture, post-processing, storage, and sharing. In this paper, we give a brief history of mobile computational photography and describe some of the key technological components, including burst photography, noise reduction, and super-resolution. At each step, we may draw naive parallels to the human visual system.",Scoviaptosure,282.0,44.0,15.0
